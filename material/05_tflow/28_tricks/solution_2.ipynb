{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scarichiamo e salviamo il dataset Stack Overflow.\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
    "data_dir = keras.utils.get_file(origin=dataset_url,\n",
    "                            extract=True,\n",
    "                            cache_subdir='datasets/stack_overflow')\n",
    "# Selezioniamo la cartella dove sono presenti i dati. \n",
    "# Di solito Ã¨ ~\\user\\username\\.keras\\dataset.\n",
    "# Da notare che questo dataset ha al suo interno due cartelle,\n",
    "# una di train ed una di test. Nel nostro esercizio utilizzeremo\n",
    "# soltanto quella di train.\n",
    "data_dir = pathlib.Path(data_dir).parent\n",
    "train_dir = pathlib.Path(data_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "# Creiamo i datset di training e validazione. Usiamo un validation\n",
    "# split di 0.2 (quindi con rapporto 80/20).\n",
    "train = keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED)\n",
    "\n",
    "val = keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classi nel dataset: ['csharp', 'java', 'javascript', 'python']\n"
     ]
    }
   ],
   "source": [
    "# Mostriamo a schermo le classi presenti nel dataset usando\n",
    "# l'attributo class_names.\n",
    "print(f'Classi nel dataset: {train.class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un layer di tipo TextVectorization.\n",
    "VOCAB_SIZE = 1000\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary')\n",
    "# Chiamiamo il metodo adapt sul testo da caratterizzare.\n",
    "train_text = train.map(lambda text, _: text)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 8s 30ms/step - loss: 0.9128 - sparse_categorical_accuracy: 0.6344 - val_loss: 0.6131 - val_sparse_categorical_accuracy: 0.7688\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.5395 - sparse_categorical_accuracy: 0.7948 - val_loss: 0.5351 - val_sparse_categorical_accuracy: 0.7912\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 4s 19ms/step - loss: 0.4386 - sparse_categorical_accuracy: 0.8339 - val_loss: 0.5254 - val_sparse_categorical_accuracy: 0.7894\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.3818 - sparse_categorical_accuracy: 0.8598 - val_loss: 0.5420 - val_sparse_categorical_accuracy: 0.7881\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.3389 - sparse_categorical_accuracy: 0.8742 - val_loss: 0.5594 - val_sparse_categorical_accuracy: 0.7831\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 5s 25ms/step - loss: 0.3012 - sparse_categorical_accuracy: 0.8923 - val_loss: 0.5816 - val_sparse_categorical_accuracy: 0.7856\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.2672 - sparse_categorical_accuracy: 0.9098 - val_loss: 0.5990 - val_sparse_categorical_accuracy: 0.7900\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.2352 - sparse_categorical_accuracy: 0.9244 - val_loss: 0.6259 - val_sparse_categorical_accuracy: 0.7881\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.2069 - sparse_categorical_accuracy: 0.9384 - val_loss: 0.6545 - val_sparse_categorical_accuracy: 0.7837\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.1777 - sparse_categorical_accuracy: 0.9489 - val_loss: 0.6880 - val_sparse_categorical_accuracy: 0.7806\n"
     ]
    }
   ],
   "source": [
    "# Integriamo il layer di vettorizzazione all'interno di un\n",
    "# modello di tipo sequenziale.\n",
    "model = keras.models.Sequential()\n",
    "model.add(\n",
    "    keras.Input(shape=(1,),\n",
    "    dtype=tf.string))\n",
    "model.add(\n",
    "    vectorize_layer)\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        64,\n",
    "        activation='relu',\n",
    "        name='dense_1'))\n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        4,\n",
    "        activation='softmax',\n",
    "        name='dense_2'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=keras.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_sparse_categorical_accuracy',\n",
    "        min_delta=0.01,\n",
    "        patience=3,\n",
    "        restore_best_weights=True),\n",
    "    keras.callbacks.TensorBoard()\n",
    "]\n",
    "\n",
    "history=model.fit(\n",
    "    train,\n",
    "    validation_data=val,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bff96ae05cbca38553397e8b82810313cdead769dbf63fb2d18b6e7c166b3c3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

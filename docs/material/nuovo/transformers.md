# Transformer

Si sente spesso parlare molto dei Transformer, e per buone ragioni. Hanno preso il mondo dell'NLP negli ultimi anni. Quella dei Trasformer è un'architettura che usa l'Attention per migliorare significativamente le performance dei modelli di traduzione NLP. Venne introdotta per la prima volta nel paper Attention is all you need e si è rapidamente stabilita come l'architettura leader per la maggior parte delle applicazioni sui dati testuali.

Da quel momento, numerosi progetti inclusi BERT di Google o GPT di OpenAI si sono basati su queste fondamenta, ed hanno avuto dei risultati che facilmente hanno battuto i benchmark allo stato dell'arte.


https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452

https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34
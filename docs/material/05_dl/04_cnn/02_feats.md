
## Motivi

La convoluzione sfrutta tre idee importanti che ci possono aiutare a migliorare un sistema di machine learning, ovvero *interazioni sparse*, *sharing dei parametri* e *rappresentazione equivariante*. Inoltre, la convoluzione fonrisce un mezzo per lavorare con degli input di dimeensioni variabili. Vediamo il tutto nel dettaglio.

##### Interazioni sparse

Le reti neurali classiche usano la moltiplicazione matriciale per una matrice di parametri con un aprametro separato che descrifvfe l'interazione tra ogni unità di input ed ogni unità di outpòut.- Questo significa che ogni unità di outpèut interagisce con ogni unità di input. Le reti convoluzionali, tuttavia,m hanno tipicamen te delle interazioni sparse (spesso conosicute come *connettività sparsa*). Questo avviene rendendo il kernel più piccolo dell'input. ad esempio,q uanod si elabora un'immagine, l'0immagine di input può avere migliaia, o milioni, di pixel, ma possiamo individuare piccole, singificative feature come contorni con dei ckernel che occupano al più qualche decina di pixel. Questo singifica che abbiamo bisongog di memorizzare pochi parametir, che riducono sia i requisiti di memoria del modello, sia che migliorano l'efficienza statistica. Signidfica anche che calcolare l'output richiede meno oeprazioni. Qeusti miglioramenti in efficienza sono noralmente abbastanza grandi.,. Se ci sono $m$ ingressi ed $n$ uscite, la moltiplicazione matriciale richiede $m \times n$ parametri, e l'algoritmo usato nella pratica ha una complessità di $O(m \times n)$ per campione. Se limitiamo il numeor di connessioni che ogni uscita può avere a $k$ l'approccio sparsamente conensso richiede solo $k \times n$ parametri ed un $O(k \times n)$. Per molte applicazioni pratiche è psosibile ottenere buone perfomrance sul task di machine learning mantenerndo $k$ diversi ordini di grandezza più piccolo di $m$.

##### Parameters sharing

Il parameters sharing si riferisce all'uso dello stesso parametro per più di una funzione in un modello. In una rete tradizionale, ogni elemento della matrice dei pesiè usato esatatmente una sola volta quando si calcola l'uscita di un layer. Viene moltiplicato per un elemento dell'input, e quindi mai rivisitato. Come sinonimo per il parameter sharing, si può dire che una rete abbia dei *pesi strettamente legati*, perché il valore del peso applicato ad un input è colelgato al valore del peso applicato altrove. In una CNN, ogni membro del kernel è usato ad ogni posizione dell'input, ad eccezione forse di alcuni pixel di contorno, a seconda delle decisioni di design che riguardano il contorno. Il parameter sharing usato dall'operazione di convoluzoien indica che piuttosto che apprendere un insieme di parameteri separato per oigni posizione, apprendiamo soltanto un insieme. Questo non influenza il runtime della forward propagation (è sempre $O(k \times n)$), ma riduce ulteriormente i requisiti di memoria dle modello a $k$ parametri. Ricordiamo che $k$ è normalemnte diversi ordini di grandezza più piccola di $M$. Dal momento che $m$ ed $n$ soino normalmente della stesso rordine, $k$ è praticamente trascurabile se comparato ad $m \times n$. La convoluzione è qunidion molto più efficiente della moltiplicazione matriciale densa in termini dei requisiti di memoria e dell'efficienza statistica.

##### Rappresentazione equivariante

Nel caso di convoluzione, la particolare forma di parameter sharing fa in modo che il layer abbia una proprietà chiamata *equivarianza* alla traslaziojne. Dire che una funzione è equivariante significa che se l'input cambia, l'output cambia allo stesso modo. Specificamente, una funzione $f(x)$ è equivariante ad una funzione $g$ se $f(g(x))=g(f(x))$. Nel caso di una convoluzione, se $g$ è una qualsiasi funzione che traduce l'input, ovvero, che lo trasla, la funzioen di convoluzione è equivariante a $g$. Ad esempio, se $I$ è una funzione che dà la luminositàù di un'immagine a coordinate intere. Sia $g$ una funzione che mappa una funzione dell'immagine ad un'altra funzione dell'immagine, in modo che $I^{'} = g(I)$ sia la funzione dell'immagine con $I^{'}(x, y) = I(x-1, y)$. Questo sposta ogni pixel di $I$ un'unità a destra. Se applichiamo questa trasforamzione ad $I$, e quindi applichiamo la convoluzione, il riusultato è lo stesso di applicare la convoluzione ad $I^{'}$, e quindi applicare la trasformazione $g$ all'output. Nel caso di dati monodimensionali, la convoluzione produce una sorta di "timeline" che mostra quanod appaiono diverse feature nell'input. Se spostiamo un evento successivamente nel tempo nell'ionput, la stessa rappresentazione apparirà nell'output,. soltanto dopo. In modo simile alle immagini, la convolzuioen crea una mappa bidimensionale di dove alcune feature appaiono nell'input. Se spostiamo l'oggetto nell'input, la sua rappresentazione si muoverà dello stesso quantitativo nell'poutpèut. Questo è utile per quando sappiamo che alcune funzioni di un piccolo numero di pixel circostanti è utile quando applicata a più locazioni dell'input. Per esempio, quando si elaborano delle immagine, è utile individuare dei contorni nel primo layer di una rete convoluzionale. Glis tessi contoprni appaiono più o meno ovunque nell'immagine, per cui è pratico condividere i parametri nell'intera immagine. In alcuni casi, potremmo voler evitare di condividere i parametri nell'intera immagine. Se ad esempio stiamo elaborando immagini che soino tagliate in modo da essere centrate su un volto di un'immagine, probabilmente vogliamo estrarre diverse feature a diverse posizioni - la parte della rete che leabora la pate superiore del volto deve guardare alle sopracciglia, mentre la parte della rete che elabora la parte inferiore del volto guarda alla mascella.

La convoluzione non è equivariante ad altre trasfoprmazioni, come cambi nella scala di rotazioni di un'immagine. Altri meccanismi sono necessarie per gestire questo tipo di trasformazioni.

Infine, alcuni tipi di dati non possono essere elaborati dalla rete neurale definita dalla moltiplicazione matriciale con una matrice a dimensione fissa.
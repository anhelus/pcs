# Gradient-Based Optimization

4.3 del libro "DEEP LEARNING BOOK"

La maggior parte degli algoritmi di deeep learning prevede un qualche tipo di ottimizzazione. Questa si riferisce al compito di minimizzare o massimizzare una funzione $f(x)$ modificando $x$. Normalmente, la maggior parte dei problemi di ottimizzazione sono posti in termini di minimizzazione di $f(x)$.

!!!note "Massimizzare f(x)$
    Se il problema è posto in termini di minimizzazione, ovviamente, la massimizzazione della funzione può essere fatta minimizzando $-f(x)$.


## Stochastic Gradient Descent

5.9 del libro


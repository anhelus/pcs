{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Python per il Calcolo Scientifico \u00b6 Benvenuti nel corso di Python per il Calcolo Scientifico ( PCS ). In questa pagina viene pubblicato il materiale e le informazioni relative al corso, nonch\u00e9 le slides proiettate a lezione. Disponibilit\u00e0 registrazioni Le registrazioni delle lezioni sono disponibili in una playlist YouTube a questo indirizzo . Modalit\u00e0 di esame \u00b6 Le modalit\u00e0 di esame previste sono due: un esame orale , costituito da un colloquio nel quale saranno poste al candidato tre domande, due di natura teorica, ed una di natura pratica, strettamente inerenti i contenuti del corso; un tema d'anno , costituito da un progetto a scelta degli studenti, organizzati in gruppi da massimo tre (3) componenti . Il tema d'anno dovr\u00e0 essere prettamente pratico, e corredato da una breve presentazione ( massimo dodici (12) slides ) ed una relazione sul lavoro svolto ( massimo quattro (4) pagine ). Date di appello \u00b6 Per informazioni sugli appelli, consultare la bacheca . Calendario delle lezioni \u00b6 Il calendario delle lezioni \u00e8 disponibile a questo link . Ricevimento \u00b6 E' possibile concordare un ricevimento inviando una e-mail all'indirizzo angelo.cardellicchio@stiima.cnr.it .","title":"Home"},{"location":"#python-per-il-calcolo-scientifico","text":"Benvenuti nel corso di Python per il Calcolo Scientifico ( PCS ). In questa pagina viene pubblicato il materiale e le informazioni relative al corso, nonch\u00e9 le slides proiettate a lezione. Disponibilit\u00e0 registrazioni Le registrazioni delle lezioni sono disponibili in una playlist YouTube a questo indirizzo .","title":"Python per il Calcolo Scientifico"},{"location":"#modalita-di-esame","text":"Le modalit\u00e0 di esame previste sono due: un esame orale , costituito da un colloquio nel quale saranno poste al candidato tre domande, due di natura teorica, ed una di natura pratica, strettamente inerenti i contenuti del corso; un tema d'anno , costituito da un progetto a scelta degli studenti, organizzati in gruppi da massimo tre (3) componenti . Il tema d'anno dovr\u00e0 essere prettamente pratico, e corredato da una breve presentazione ( massimo dodici (12) slides ) ed una relazione sul lavoro svolto ( massimo quattro (4) pagine ).","title":"Modalit\u00e0 di esame"},{"location":"#date-di-appello","text":"Per informazioni sugli appelli, consultare la bacheca .","title":"Date di appello"},{"location":"#calendario-delle-lezioni","text":"Il calendario delle lezioni \u00e8 disponibile a questo link .","title":"Calendario delle lezioni"},{"location":"#ricevimento","text":"E' possibile concordare un ricevimento inviando una e-mail all'indirizzo angelo.cardellicchio@stiima.cnr.it .","title":"Ricevimento"},{"location":"advices/","text":"Bacheca \u00b6 Date ed orari di appello \u00b6 Le date di appello sono le seguenti: Numero appello Data ed ora Aula Termine ultimo consegna tema d'anno Primo 14/07/2022, ore 16:00 XII 08/07/2022 Secondo 28/07/2022, ore 16:00 X 22/07/2022 Terzo 08/09/2022, ore 16:00 XII 02/09/2022 Quarto 22/09/2022, ore 16:00 XII 16/09/2022 Quinto 14/11/2022, ore 16:00 08/11/2022 Sesto 26/01/2023, ore 16:00 20/01/2023 Settimo 09/02/2023, ore 16:00 03/02/2023 Ottavo 16/02/2023, ore 16:00 10/02/2023 Modalit\u00e0 di svolgimento \u00b6 L'esame \u00e8 sostenuto in presenza , salvo comprovata impossibilit\u00e0 o situazioni contingenti.","title":"Bacheca avvisi"},{"location":"advices/#bacheca","text":"","title":"Bacheca"},{"location":"advices/#date-ed-orari-di-appello","text":"Le date di appello sono le seguenti: Numero appello Data ed ora Aula Termine ultimo consegna tema d'anno Primo 14/07/2022, ore 16:00 XII 08/07/2022 Secondo 28/07/2022, ore 16:00 X 22/07/2022 Terzo 08/09/2022, ore 16:00 XII 02/09/2022 Quarto 22/09/2022, ore 16:00 XII 16/09/2022 Quinto 14/11/2022, ore 16:00 08/11/2022 Sesto 26/01/2023, ore 16:00 20/01/2023 Settimo 09/02/2023, ore 16:00 03/02/2023 Ottavo 16/02/2023, ore 16:00 10/02/2023","title":"Date ed orari di appello"},{"location":"advices/#modalita-di-svolgimento","text":"L'esame \u00e8 sostenuto in presenza , salvo comprovata impossibilit\u00e0 o situazioni contingenti.","title":"Modalit\u00e0 di svolgimento"},{"location":"contributors/","text":"Crediti \u00b6 Un particolare ringraziamento a tutti coloro che hanno contribuito alla costruzione e manutenzione del materiale del corso. Nome Contributo Mirko Cal\u00f2 Registrazione delle lezioni. Simone Fidanza Revisione ed update della repository.","title":"Crediti"},{"location":"contributors/#crediti","text":"Un particolare ringraziamento a tutti coloro che hanno contribuito alla costruzione e manutenzione del materiale del corso. Nome Contributo Mirko Cal\u00f2 Registrazione delle lezioni. Simone Fidanza Revisione ed update della repository.","title":"Crediti"},{"location":"material/01_python/01_intro/exercises/","text":"E1 - Introduzione a Python \u00b6 E1.1 \u00b6 Creiamo una stringa che assuma valore PCS . S1.1 - Soluzione \u00b6 >>> s = 'PCS' E1.2 \u00b6 Valutiamo la lunghezza della stringa creata al punto 1. S1.2 - Soluzione \u00b6 >>> len ( s ) E1.3 \u00b6 Proviamo a creare una lista a partire dalla stringa creata al punto 1. S1.3 - Soluzione \u00b6 >>> l = [ 'pcs' ] >>> l = [ 'p' , 'c' , 's' ]","title":"Esercizi"},{"location":"material/01_python/01_intro/exercises/#e1-introduzione-a-python","text":"","title":"E1 - Introduzione a Python"},{"location":"material/01_python/01_intro/exercises/#e11","text":"Creiamo una stringa che assuma valore PCS .","title":"E1.1"},{"location":"material/01_python/01_intro/exercises/#s11-soluzione","text":">>> s = 'PCS'","title":"S1.1 - Soluzione"},{"location":"material/01_python/01_intro/exercises/#e12","text":"Valutiamo la lunghezza della stringa creata al punto 1.","title":"E1.2"},{"location":"material/01_python/01_intro/exercises/#s12-soluzione","text":">>> len ( s )","title":"S1.2 - Soluzione"},{"location":"material/01_python/01_intro/exercises/#e13","text":"Proviamo a creare una lista a partire dalla stringa creata al punto 1.","title":"E1.3"},{"location":"material/01_python/01_intro/exercises/#s13-soluzione","text":">>> l = [ 'pcs' ] >>> l = [ 'p' , 'c' , 's' ]","title":"S1.3 - Soluzione"},{"location":"material/01_python/01_intro/lecture/","text":"1 - Introduzione a Python \u00b6 Prima di iniziare a parlare del linguaggio Python, \u00e8 opportuno verificare che l'interprete sia installato nel nostro sistema. Per farlo, apriamo un terminale (Shell o Command Prompt, a seconda del nostro sistema), e scriviamo: $ python Se apparir\u00e0 una schermata simile a quella mostrata in figura, Python sar\u00e0 gi\u00e0 correttamente presente nel nostro sistema. In alternativa, dovremo provvedere ad installarlo seguendo la procedura indicata sul sito ufficiale , ed aggiungerlo al path di sistema. 1.1 - Python e tipizzazione \u00b6 1.1.1 Tipizzazione dinamica \u00b6 Python \u00e8 un linguaggio interpretato ed a tipizzazione dinamica . In breve, questo significa che l'interprete valuta il tipo di ciascuna variabile a runtime, e che questo pu\u00f2 cambiare durante l'esecuzione del programma. Ma, a conti fatti, in cosa si traduce per il programmatore? Beh, molto semplice. Immaginiamo di dover definire ed inizializzare una variabile di tipo intero in un linguaggio a tipizzazione statica , come ad esempio il C++. Per farlo, scriveremo qualcosa simile a: int var = 0 ; In Python, potremo omettere il tipo, che sar\u00e0 inferito direttamente dal valore assegnato alla variabile: var = 0 Immaginiamo ora che la nostra variabile debba diventare un decimale. In C++, dovremo effettuare il casting: float fVar = float ( var ); fVar + 1.1 ; In Python questo non sar\u00e0 necessario, e potremo effettuare direttamente le operazioni desiderate: var + 1.1 # Il risultato sar\u00e0 1.1 Questo pu\u00f2 apparentemente semplificare di molto la vita, in quanto non \u00e8 pi\u00f9 necessario preoccuparsi del tipo della variabile. Non \u00e8 per\u00f2 tutto oro ci\u00f2 che luccica: per comprenderlo, infatti, \u00e8 il momento di parlare del (pilatesco) principio del duck typing . 1.1.1.1 - Duck Typing \u00b6 Il duck typing \u00e8 riassumibile nella seguente massima: Duck Typing If it walks like a duck and it quacks like a duck, then it must be a duck. che in italiano suona pi\u00f9 o meno Se cammina come un papero, e starnazza come un papero, deve essere un papero . Traduciamola brevemente in \"informatichese\". Immaginiamo di istruire il nostro interprete Python ad assegnare alla nostra variabile var il valore di 1 . L'interprete nota che la variabile si \"comporta\" come un numero intero, e quindi \"stabilir\u00e0\" che si tratti proprio di questo. Proviamo ora a sommare a var un valore pari ad 1.1 . Il risultato, come ovvio, sar\u00e0 un numero decimale, e quindi l'interprete \"cambier\u00e0 idea\", in quanto i comportamenti assunti da var sono adesso assimilabili ad una variabile di tipo float . L'utilit\u00e0 del duck typing \u00e8 evidente: permette allo sviluppatore di \"risparmiare\" numerose operazioni di cast, rendendo il codice pi\u00f9 semplice da scrivere e manutenere. Tuttavia, occorre tenerne conto nel momento in cui si usano classi ed oggetti, in quanto l'interprete prover\u00e0 ad inferire ed usare automaticamente un tipo in base al contesto in cui viene usata la variabile, con le comodit\u00e0 (ed i potenziali disastri) che questo comporta. 1.2 - L'interprete Python \u00b6 Nella sezione introduttiva abbiamo visto come installare l'interprete Python, in modo da avere un ambiente di lavoro accessibile direttamente mediante riga di comando. Lanciamolo di nuovo usando il seguente comando da una shell: python Potremo quindi finalmente iniziare ad utilizzare Python. 1.3 - Calcoli e numeri \u00b6 Proviamo ad usare l'interprete come una semplice calcolatrice; per farlo, scriviamo direttamente dopo il simbolo >>> le operazioni che vogliamo eseguire, e premiamo il tasto Invio . Ad esempio: >>> 2 + 2 4 >>> 3 * 5 15 >>> 10 - 2 * 4 2 1.3.1 - Divisioni \u00b6 Le divisioni restituiscono sempre un numero in virgola mobile. Ad esempio: >>> 16 / 3 5.333333333333333 >>> 2 / 2 1.0 Proviamo ora ad usare altri due operatori, molto simili al classico operatore di divisione: >>> 16 // 3 5 >>> 16 % 3 1 Notiamo come in questi casi siano restituiti dei numeri interi. Il perch\u00e9 \u00e8 presto detto: gli operatori // e % calcolano, rispettivamente, il quoziente ed il resto della divisione e, come sappiamo, entrambi sono dei valori interi. 1.3.2 - Elevazione a potenza \u00b6 Per elevare un numero a potenza, \u00e8 necessario usare l'operatore ** , in cui l'operando sinistro \u00e8 la base, mentre quello destro l'esponente: >>> 3 ** 2 9 >>> 2 ** 8 256 Tipi numerici in Python Abbiamo finora parlato soltanto di numeri interi e decimali; tuttavia, Python supporta anche altri tipi, come ad esempio Decimal e Fraction . E' inoltre presente un supporto nativo ai numeri complessi, esprimibili usando il suffisso j per indicare la parte immaginaria. 1.4 - Stringhe \u00b6 In Python le stringhe possono indifferentemente essere racchiuse tra virgolette singole e doppie. >>> \"una stringa\" 'una stringa' >>> 'un \\' altra stringa' \"un'altra stringa\" Notiamo nella seconda istruzione l'uso del carattere di escape ( \\ ) che precede l'apostrofo; se lo omettessimo, l'interprete ci restituirebbe un errore sintattico ( SyntaxError ): >>> 'un' altra stringa ' File \"<stdin>\" , line 1 'un' altra stringa ^ SyntaxError : invalid syntax Nota Tutti i caratteri preceduti dal simbolo \\ saranno interpretati come escape character, a meno di aggiungere il simbolo r prima dell'inizio della stringa: >>> print ( 'C: \\n uova_cartella' ) C : uova_cartella >>> print ( r 'C:\\nuova_cartella' ) C : \\ nuova_cartella 1.4.1 - Stringhe su righe multiple \u00b6 Stringhe e liste La maggior parte dei concetti che vedremo nel seguito sono applicabili anche alle liste. Anzi, per essere precisi, derivano proprio dalle liste, in quanto Python considera una stringa un particolare tipo di lista. Le stringhe possono articolarsi su pi\u00f9 righe. Per farlo, possiamo usare le triple-quotes , ovvero tre virgolette di seguito, per indicare l'inizio e la fine della stringa: >>> print ( \"\"\"Questo \u00e8 un esempio \\ di riga multipla \\ \"\"\" ) Questo \u00e8 un esempio di riga multipla Nota Notiamo nel precedente snippet il carattere \\ , usato per evitare che venga automaticamente inserito dall'interprete il carattere newline ( \\n ) al termine di ogni riga. Infatti, si vede come il newline non sia stato aggiunto nelle righe evidenziate, mentre sia presente nella riga 2. 1.4.2 - Concatenazione di stringhe \u00b6 Concatenare due stringhe in Python \u00e8 estremamente semplice, e basta usare l'operatore + : >>> stringa_a = \"Prima stringa\" >>> stringa_b = \"Seconda stringa\" >>> print ( stringa_a + \" - \" + stringa_b ) Prima stringa - Seconda stringa Nota Se usiamo l'operatore * possiamo concatenare pi\u00f9 volte la stessa stringa: >>> 3 * 'co.' 'co.co.co.' Possiamo anche semplicemente porre le due stringhe l'una di seguito all'altra: >>> \"Py\" \"thon\" 'Python' Attenzione Bisogna fare particolare attenzione a non concatenare un literal (ovvero una stringa racchiusa tra virgolette) ad una variabile di tipo stringa . Se proviamo a farlo, l'interprete ci restituir\u00e0 questo errore: >>> py = \"Py\" >>> py \"thon\" File \"<stdin>\" , line 1 py \"thon\" ^ SyntaxError : invalid syntax Lo stesso errore si presenterebbe se al posto della variabile py usassimo il risultato di una operazione di concatenazione: >>> ( 'p' + 'y' ) 'thon' File \"<stdin>\" , line 1 ( 'p' + 'y' ) 'thon' ^ SyntaxError : invalid syntax Il consiglio, in questi casi \"ibridi\", \u00e8 quello di usare l'operatore standard di concatenazione, ovvero il + . Nota Esistono modi pi\u00f9 efficienti di concatenare delle stringhe, specialmente quando si ha a che fare con numerose operazioni di concatenazione in grossi cicli; l'approfondimento di tali metodi \u00e8 demandato al lettore. 1.4.3 - Indicizzazione di stringhe \u00b6 Python definisce le stringhe come degli array di caratteri ; \u00e8 quindi possibile indicizzarli. Ad esempio: >>> stringa = 'Python' >>> stringa [ 0 ] 'P' Anche i singoli caratteri sono considerati come delle stringhe, ovviamente di lunghezza unitaria: >>> lettera = 'P' >>> lettera [ 0 ] 'P' Python permette di accedere anche usando degli indici negativi , considerando quindi gli elementi che vanno da destra verso sinistra. In questo caso, l'indice del primo elemento da destra sar\u00e0 indicato con -1 : >>> stringa [ - 1 ] 'n' 1.4.4 - Slicing su stringhe \u00b6 L'operazione di slicing permette di estrarre una certa parte di una stringa. In generale, assume la seguente forma: >>> stringa [ i : j : s ] dove i \u00e8 l'indice iniziale, j quello finale ed s lo step utilizzato. E' importante sottolineare come l'elemento all'indice iniziale sar\u00e0 incluso, mentre quello all'indice finale sar\u00e0 escluso . Ad esempio: >>> stringa [ 0 : 2 ] 'Py' >>> stringa [ 2 : 5 ] 'tho' Se volessimo considerare tutti i caratteri fino a j (escluso), dovremmo usare la seguente notazione: >>> stringa [: j ] Se invece volessimo considerare tutti i caratteri a partire da i (incluso), dovremmo usare la seguente notazione: >>> stringa [ i :] Ad esempio: >>> stringa [ 1 :] 'ython' >>> stringa [: 5 ] 'Pytho' Anche in questo caso, \u00e8 possibile usare degli indici negativi. Ad esempio, se volessimo prendere tutti i caratteri dalla terzultima lettera fino alla fine, potremmo scrivere: >>> stringa [ - 3 :] 'hon' mentre se volessimo prendere tutti i caratteri fino alla terzultima lettera (esclusa): >>> stringa [: - 3 ] 'Pyt' Suggerimento E' possibile ottenere un'intera stringa mediante l'operazione di slicing in questo modo: >>> stringa [:] 'Python' 1.4.5 - Lunghezza di una stringa \u00b6 La funzione len() ci restituisce la lunghezza di una stringa: >>> len ( stringa ) 6 1.4.6 - Immutabilit\u00e0 di una stringa \u00b6 Le stringhe in Python sono immutabili . Come indica la parola stessa, questo significa che non possono essere modificate : se, ad esempio, provassimo a ridefinirne uno o pi\u00f9 elementi, acceduti magari mediante indexing o slicing, avremmo un errore. >>> stringa [ 0 ] = 'C' # Errore! Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > TypeError : 'str' object does not support item assignment Suggerimento Possiamo comunque assegnare il nome stringa ad una nuova variabile. 1.5 - Liste \u00b6 Abbiamo gi\u00e0 detto che una stringa altro non \u00e8 se non un caso particolare di lista . La domanda che sorge spontanea \u00e8 quindi: cosa \u00e8 una lista ? Le liste sono uno dei quattro tipi di strutture built-in che Python offre per memorizzare sequenze di dati. Da un punto di vista puramente \"concettuale\", potremmo considerarle alla stregua degli array presenti in altri linguaggi di programmazione, seppur con alcune, significative differenze. Possiamo creare una lista in questo modo: >>> lista = [ 1 , 2 , 3 , 4 , 5 ] [ 1 , 2 , 3 , 4 , 5 ] 1.5.1 - Concatenazione, indicizzazione e slicing su liste \u00b6 Come sulle stringhe, sulle liste \u00e8 possibile effettuare operazioni di indicizzazione, slicing e concatenazione: >>> lista [ 0 ] 1 >>> lista [ 2 :] [ 3 , 4 , 5 ] >>> lista_due = [ 6 , 7 ] >>> lista + lista_due [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ] >>> lista + [ 6 ] [ 1 , 2 , 3 , 4 , 5 , 6 ] 1.5.2 - Alcuni esempi \u00b6 Ecco alcuni esempi di slicing su lista, con annessi risultati ottenibili. Consideriamo la seguente stringa: >>> l = [ 1 , 2 , 3 , 4 , 5 , 6 ] Prendiamo gli elementi sugli indice pari (ovvero 0, 2 e 4): >>> l [ 0 :: 2 ] [ 1 , 3 , 5 ] Prendiamo tutti gli elementi a partire dal terzultimo e con indice pari: >>> l [( - 3 + 1 ):: 2 ] [ 5 ] Nota Nell'esempio precedente, usato un piccolo \"trucco\" per tenere in conto il fatto che l'indicizzazione parte da 0 e non da 1. Partiamo dal terzultimo elemento, e proseguiamo all'indietro verso l'origine: >>> l [ - 3 :: - 1 ] [ 4 , 3 , 2 , 1 ] Partiamo dall'ultimo elemento e proseguiamo sino al terz'ultimo dall'origine: >>> l [: 3 : - 1 ] [ 6 , 5 ] Prendiamo gli ultimi tre elementi in ordine inverso: >>> l [ len ( l ) - 1 : len ( l ) - 4 : - 1 ] [ 6 , 5 , 4 ] Prendiamo gli elementi agli indici pari in ordine inverso: >>> l [:: - 2 ] [ 6 , 4 , 2 ] 1.5.3 - Mutabilit\u00e0 di una lista \u00b6 A differenza delle stringhe, le liste sono oggetti mutabili . Di conseguenza, possiamo modificarne il contenuto: >>> lista [ 0 ] = 99 >>> lista [ 99 , 2 , 3 , 4 , 5 ] 1.5.4 - Operazioni sulle liste \u00b6 Possiamo anche eliminare elementi da una lista usando l'operatore [] combinato all'operazione di slicing: >>> lista [ 4 :] = [] >>> lista [ 99 , 2 , 3 , 4 ] Nota I pi\u00f9 attenti avranno notato che l'operatore [] non fa altro che indicare una lista vuota. Suggerimento Possiamo eliminare tutti gli elementi contenuti in una lista mediante lo slicing e l'operatore [] : >>> lista [:] = [] >>> lista [] Una lista pu\u00f2 contenere elementi tra loro eterogenei. E' addirittura consentito contenere degli iterabili , tra cui altre liste: >>> lista . append ([ 1 , 2 , 3 ]) >>> lista [ 99 , 2 , 3 , 4 , [ 1 , 2 , 3 ]] Nell'esempio precedente, abbiamo usato la funzione append() per inserire un elemento in coda alla lista. E' interessante notare l'elemento inserito in coda sia esso stesso una lista, e \"conviva\" tranquillamente con gli altri elementi di tipo numerico. Proviamo ad estendere ulteriormente la lista cambiando il primo elemento con una stringa: >>> lista [ 0 ] = stringa >>> lista [ 'Python' , 2 , 3 , 4 , [ 1 , 2 , 3 ]]","title":"Dispense"},{"location":"material/01_python/01_intro/lecture/#1-introduzione-a-python","text":"Prima di iniziare a parlare del linguaggio Python, \u00e8 opportuno verificare che l'interprete sia installato nel nostro sistema. Per farlo, apriamo un terminale (Shell o Command Prompt, a seconda del nostro sistema), e scriviamo: $ python Se apparir\u00e0 una schermata simile a quella mostrata in figura, Python sar\u00e0 gi\u00e0 correttamente presente nel nostro sistema. In alternativa, dovremo provvedere ad installarlo seguendo la procedura indicata sul sito ufficiale , ed aggiungerlo al path di sistema.","title":"1 - Introduzione a Python"},{"location":"material/01_python/01_intro/lecture/#11-python-e-tipizzazione","text":"","title":"1.1 - Python e tipizzazione"},{"location":"material/01_python/01_intro/lecture/#111-tipizzazione-dinamica","text":"Python \u00e8 un linguaggio interpretato ed a tipizzazione dinamica . In breve, questo significa che l'interprete valuta il tipo di ciascuna variabile a runtime, e che questo pu\u00f2 cambiare durante l'esecuzione del programma. Ma, a conti fatti, in cosa si traduce per il programmatore? Beh, molto semplice. Immaginiamo di dover definire ed inizializzare una variabile di tipo intero in un linguaggio a tipizzazione statica , come ad esempio il C++. Per farlo, scriveremo qualcosa simile a: int var = 0 ; In Python, potremo omettere il tipo, che sar\u00e0 inferito direttamente dal valore assegnato alla variabile: var = 0 Immaginiamo ora che la nostra variabile debba diventare un decimale. In C++, dovremo effettuare il casting: float fVar = float ( var ); fVar + 1.1 ; In Python questo non sar\u00e0 necessario, e potremo effettuare direttamente le operazioni desiderate: var + 1.1 # Il risultato sar\u00e0 1.1 Questo pu\u00f2 apparentemente semplificare di molto la vita, in quanto non \u00e8 pi\u00f9 necessario preoccuparsi del tipo della variabile. Non \u00e8 per\u00f2 tutto oro ci\u00f2 che luccica: per comprenderlo, infatti, \u00e8 il momento di parlare del (pilatesco) principio del duck typing .","title":"1.1.1 Tipizzazione dinamica"},{"location":"material/01_python/01_intro/lecture/#1111-duck-typing","text":"Il duck typing \u00e8 riassumibile nella seguente massima: Duck Typing If it walks like a duck and it quacks like a duck, then it must be a duck. che in italiano suona pi\u00f9 o meno Se cammina come un papero, e starnazza come un papero, deve essere un papero . Traduciamola brevemente in \"informatichese\". Immaginiamo di istruire il nostro interprete Python ad assegnare alla nostra variabile var il valore di 1 . L'interprete nota che la variabile si \"comporta\" come un numero intero, e quindi \"stabilir\u00e0\" che si tratti proprio di questo. Proviamo ora a sommare a var un valore pari ad 1.1 . Il risultato, come ovvio, sar\u00e0 un numero decimale, e quindi l'interprete \"cambier\u00e0 idea\", in quanto i comportamenti assunti da var sono adesso assimilabili ad una variabile di tipo float . L'utilit\u00e0 del duck typing \u00e8 evidente: permette allo sviluppatore di \"risparmiare\" numerose operazioni di cast, rendendo il codice pi\u00f9 semplice da scrivere e manutenere. Tuttavia, occorre tenerne conto nel momento in cui si usano classi ed oggetti, in quanto l'interprete prover\u00e0 ad inferire ed usare automaticamente un tipo in base al contesto in cui viene usata la variabile, con le comodit\u00e0 (ed i potenziali disastri) che questo comporta.","title":"1.1.1.1 - Duck Typing"},{"location":"material/01_python/01_intro/lecture/#12-linterprete-python","text":"Nella sezione introduttiva abbiamo visto come installare l'interprete Python, in modo da avere un ambiente di lavoro accessibile direttamente mediante riga di comando. Lanciamolo di nuovo usando il seguente comando da una shell: python Potremo quindi finalmente iniziare ad utilizzare Python.","title":"1.2 - L'interprete Python"},{"location":"material/01_python/01_intro/lecture/#13-calcoli-e-numeri","text":"Proviamo ad usare l'interprete come una semplice calcolatrice; per farlo, scriviamo direttamente dopo il simbolo >>> le operazioni che vogliamo eseguire, e premiamo il tasto Invio . Ad esempio: >>> 2 + 2 4 >>> 3 * 5 15 >>> 10 - 2 * 4 2","title":"1.3 - Calcoli e numeri"},{"location":"material/01_python/01_intro/lecture/#131-divisioni","text":"Le divisioni restituiscono sempre un numero in virgola mobile. Ad esempio: >>> 16 / 3 5.333333333333333 >>> 2 / 2 1.0 Proviamo ora ad usare altri due operatori, molto simili al classico operatore di divisione: >>> 16 // 3 5 >>> 16 % 3 1 Notiamo come in questi casi siano restituiti dei numeri interi. Il perch\u00e9 \u00e8 presto detto: gli operatori // e % calcolano, rispettivamente, il quoziente ed il resto della divisione e, come sappiamo, entrambi sono dei valori interi.","title":"1.3.1 - Divisioni"},{"location":"material/01_python/01_intro/lecture/#132-elevazione-a-potenza","text":"Per elevare un numero a potenza, \u00e8 necessario usare l'operatore ** , in cui l'operando sinistro \u00e8 la base, mentre quello destro l'esponente: >>> 3 ** 2 9 >>> 2 ** 8 256 Tipi numerici in Python Abbiamo finora parlato soltanto di numeri interi e decimali; tuttavia, Python supporta anche altri tipi, come ad esempio Decimal e Fraction . E' inoltre presente un supporto nativo ai numeri complessi, esprimibili usando il suffisso j per indicare la parte immaginaria.","title":"1.3.2 - Elevazione a potenza"},{"location":"material/01_python/01_intro/lecture/#14-stringhe","text":"In Python le stringhe possono indifferentemente essere racchiuse tra virgolette singole e doppie. >>> \"una stringa\" 'una stringa' >>> 'un \\' altra stringa' \"un'altra stringa\" Notiamo nella seconda istruzione l'uso del carattere di escape ( \\ ) che precede l'apostrofo; se lo omettessimo, l'interprete ci restituirebbe un errore sintattico ( SyntaxError ): >>> 'un' altra stringa ' File \"<stdin>\" , line 1 'un' altra stringa ^ SyntaxError : invalid syntax Nota Tutti i caratteri preceduti dal simbolo \\ saranno interpretati come escape character, a meno di aggiungere il simbolo r prima dell'inizio della stringa: >>> print ( 'C: \\n uova_cartella' ) C : uova_cartella >>> print ( r 'C:\\nuova_cartella' ) C : \\ nuova_cartella","title":"1.4 - Stringhe"},{"location":"material/01_python/01_intro/lecture/#141-stringhe-su-righe-multiple","text":"Stringhe e liste La maggior parte dei concetti che vedremo nel seguito sono applicabili anche alle liste. Anzi, per essere precisi, derivano proprio dalle liste, in quanto Python considera una stringa un particolare tipo di lista. Le stringhe possono articolarsi su pi\u00f9 righe. Per farlo, possiamo usare le triple-quotes , ovvero tre virgolette di seguito, per indicare l'inizio e la fine della stringa: >>> print ( \"\"\"Questo \u00e8 un esempio \\ di riga multipla \\ \"\"\" ) Questo \u00e8 un esempio di riga multipla Nota Notiamo nel precedente snippet il carattere \\ , usato per evitare che venga automaticamente inserito dall'interprete il carattere newline ( \\n ) al termine di ogni riga. Infatti, si vede come il newline non sia stato aggiunto nelle righe evidenziate, mentre sia presente nella riga 2.","title":"1.4.1 - Stringhe su righe multiple"},{"location":"material/01_python/01_intro/lecture/#142-concatenazione-di-stringhe","text":"Concatenare due stringhe in Python \u00e8 estremamente semplice, e basta usare l'operatore + : >>> stringa_a = \"Prima stringa\" >>> stringa_b = \"Seconda stringa\" >>> print ( stringa_a + \" - \" + stringa_b ) Prima stringa - Seconda stringa Nota Se usiamo l'operatore * possiamo concatenare pi\u00f9 volte la stessa stringa: >>> 3 * 'co.' 'co.co.co.' Possiamo anche semplicemente porre le due stringhe l'una di seguito all'altra: >>> \"Py\" \"thon\" 'Python' Attenzione Bisogna fare particolare attenzione a non concatenare un literal (ovvero una stringa racchiusa tra virgolette) ad una variabile di tipo stringa . Se proviamo a farlo, l'interprete ci restituir\u00e0 questo errore: >>> py = \"Py\" >>> py \"thon\" File \"<stdin>\" , line 1 py \"thon\" ^ SyntaxError : invalid syntax Lo stesso errore si presenterebbe se al posto della variabile py usassimo il risultato di una operazione di concatenazione: >>> ( 'p' + 'y' ) 'thon' File \"<stdin>\" , line 1 ( 'p' + 'y' ) 'thon' ^ SyntaxError : invalid syntax Il consiglio, in questi casi \"ibridi\", \u00e8 quello di usare l'operatore standard di concatenazione, ovvero il + . Nota Esistono modi pi\u00f9 efficienti di concatenare delle stringhe, specialmente quando si ha a che fare con numerose operazioni di concatenazione in grossi cicli; l'approfondimento di tali metodi \u00e8 demandato al lettore.","title":"1.4.2 - Concatenazione di stringhe"},{"location":"material/01_python/01_intro/lecture/#143-indicizzazione-di-stringhe","text":"Python definisce le stringhe come degli array di caratteri ; \u00e8 quindi possibile indicizzarli. Ad esempio: >>> stringa = 'Python' >>> stringa [ 0 ] 'P' Anche i singoli caratteri sono considerati come delle stringhe, ovviamente di lunghezza unitaria: >>> lettera = 'P' >>> lettera [ 0 ] 'P' Python permette di accedere anche usando degli indici negativi , considerando quindi gli elementi che vanno da destra verso sinistra. In questo caso, l'indice del primo elemento da destra sar\u00e0 indicato con -1 : >>> stringa [ - 1 ] 'n'","title":"1.4.3 - Indicizzazione di stringhe"},{"location":"material/01_python/01_intro/lecture/#144-slicing-su-stringhe","text":"L'operazione di slicing permette di estrarre una certa parte di una stringa. In generale, assume la seguente forma: >>> stringa [ i : j : s ] dove i \u00e8 l'indice iniziale, j quello finale ed s lo step utilizzato. E' importante sottolineare come l'elemento all'indice iniziale sar\u00e0 incluso, mentre quello all'indice finale sar\u00e0 escluso . Ad esempio: >>> stringa [ 0 : 2 ] 'Py' >>> stringa [ 2 : 5 ] 'tho' Se volessimo considerare tutti i caratteri fino a j (escluso), dovremmo usare la seguente notazione: >>> stringa [: j ] Se invece volessimo considerare tutti i caratteri a partire da i (incluso), dovremmo usare la seguente notazione: >>> stringa [ i :] Ad esempio: >>> stringa [ 1 :] 'ython' >>> stringa [: 5 ] 'Pytho' Anche in questo caso, \u00e8 possibile usare degli indici negativi. Ad esempio, se volessimo prendere tutti i caratteri dalla terzultima lettera fino alla fine, potremmo scrivere: >>> stringa [ - 3 :] 'hon' mentre se volessimo prendere tutti i caratteri fino alla terzultima lettera (esclusa): >>> stringa [: - 3 ] 'Pyt' Suggerimento E' possibile ottenere un'intera stringa mediante l'operazione di slicing in questo modo: >>> stringa [:] 'Python'","title":"1.4.4 - Slicing su stringhe"},{"location":"material/01_python/01_intro/lecture/#145-lunghezza-di-una-stringa","text":"La funzione len() ci restituisce la lunghezza di una stringa: >>> len ( stringa ) 6","title":"1.4.5 - Lunghezza di una stringa"},{"location":"material/01_python/01_intro/lecture/#146-immutabilita-di-una-stringa","text":"Le stringhe in Python sono immutabili . Come indica la parola stessa, questo significa che non possono essere modificate : se, ad esempio, provassimo a ridefinirne uno o pi\u00f9 elementi, acceduti magari mediante indexing o slicing, avremmo un errore. >>> stringa [ 0 ] = 'C' # Errore! Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > TypeError : 'str' object does not support item assignment Suggerimento Possiamo comunque assegnare il nome stringa ad una nuova variabile.","title":"1.4.6 - Immutabilit\u00e0 di una stringa"},{"location":"material/01_python/01_intro/lecture/#15-liste","text":"Abbiamo gi\u00e0 detto che una stringa altro non \u00e8 se non un caso particolare di lista . La domanda che sorge spontanea \u00e8 quindi: cosa \u00e8 una lista ? Le liste sono uno dei quattro tipi di strutture built-in che Python offre per memorizzare sequenze di dati. Da un punto di vista puramente \"concettuale\", potremmo considerarle alla stregua degli array presenti in altri linguaggi di programmazione, seppur con alcune, significative differenze. Possiamo creare una lista in questo modo: >>> lista = [ 1 , 2 , 3 , 4 , 5 ] [ 1 , 2 , 3 , 4 , 5 ]","title":"1.5 - Liste"},{"location":"material/01_python/01_intro/lecture/#151-concatenazione-indicizzazione-e-slicing-su-liste","text":"Come sulle stringhe, sulle liste \u00e8 possibile effettuare operazioni di indicizzazione, slicing e concatenazione: >>> lista [ 0 ] 1 >>> lista [ 2 :] [ 3 , 4 , 5 ] >>> lista_due = [ 6 , 7 ] >>> lista + lista_due [ 1 , 2 , 3 , 4 , 5 , 6 , 7 ] >>> lista + [ 6 ] [ 1 , 2 , 3 , 4 , 5 , 6 ]","title":"1.5.1 - Concatenazione, indicizzazione e slicing su liste"},{"location":"material/01_python/01_intro/lecture/#152-alcuni-esempi","text":"Ecco alcuni esempi di slicing su lista, con annessi risultati ottenibili. Consideriamo la seguente stringa: >>> l = [ 1 , 2 , 3 , 4 , 5 , 6 ] Prendiamo gli elementi sugli indice pari (ovvero 0, 2 e 4): >>> l [ 0 :: 2 ] [ 1 , 3 , 5 ] Prendiamo tutti gli elementi a partire dal terzultimo e con indice pari: >>> l [( - 3 + 1 ):: 2 ] [ 5 ] Nota Nell'esempio precedente, usato un piccolo \"trucco\" per tenere in conto il fatto che l'indicizzazione parte da 0 e non da 1. Partiamo dal terzultimo elemento, e proseguiamo all'indietro verso l'origine: >>> l [ - 3 :: - 1 ] [ 4 , 3 , 2 , 1 ] Partiamo dall'ultimo elemento e proseguiamo sino al terz'ultimo dall'origine: >>> l [: 3 : - 1 ] [ 6 , 5 ] Prendiamo gli ultimi tre elementi in ordine inverso: >>> l [ len ( l ) - 1 : len ( l ) - 4 : - 1 ] [ 6 , 5 , 4 ] Prendiamo gli elementi agli indici pari in ordine inverso: >>> l [:: - 2 ] [ 6 , 4 , 2 ]","title":"1.5.2 - Alcuni esempi"},{"location":"material/01_python/01_intro/lecture/#153-mutabilita-di-una-lista","text":"A differenza delle stringhe, le liste sono oggetti mutabili . Di conseguenza, possiamo modificarne il contenuto: >>> lista [ 0 ] = 99 >>> lista [ 99 , 2 , 3 , 4 , 5 ]","title":"1.5.3 - Mutabilit\u00e0 di una lista"},{"location":"material/01_python/01_intro/lecture/#154-operazioni-sulle-liste","text":"Possiamo anche eliminare elementi da una lista usando l'operatore [] combinato all'operazione di slicing: >>> lista [ 4 :] = [] >>> lista [ 99 , 2 , 3 , 4 ] Nota I pi\u00f9 attenti avranno notato che l'operatore [] non fa altro che indicare una lista vuota. Suggerimento Possiamo eliminare tutti gli elementi contenuti in una lista mediante lo slicing e l'operatore [] : >>> lista [:] = [] >>> lista [] Una lista pu\u00f2 contenere elementi tra loro eterogenei. E' addirittura consentito contenere degli iterabili , tra cui altre liste: >>> lista . append ([ 1 , 2 , 3 ]) >>> lista [ 99 , 2 , 3 , 4 , [ 1 , 2 , 3 ]] Nell'esempio precedente, abbiamo usato la funzione append() per inserire un elemento in coda alla lista. E' interessante notare l'elemento inserito in coda sia esso stesso una lista, e \"conviva\" tranquillamente con gli altri elementi di tipo numerico. Proviamo ad estendere ulteriormente la lista cambiando il primo elemento con una stringa: >>> lista [ 0 ] = stringa >>> lista [ 'Python' , 2 , 3 , 4 , [ 1 , 2 , 3 ]]","title":"1.5.4 - Operazioni sulle liste"},{"location":"material/01_python/02_syntax/exercises/","text":"E2 - Programmare in Python \u00b6 E2.1 \u00b6 Scriviamo una funzione che iteri fino a che il valore associato ad un contatore intero \u00e8 minore di 10. Usiamo un ciclo while . S2.1 - Soluzione \u00b6 def itera_while (): i = 0 while i < 10 : i = i + 1 print ( \" {} -ma iterazione\" . format ( i )) Il risultato ottenuto sar\u00e0: >>> itera_while () 1 - ma iterazione 2 - ma iterazione 3 - ma iterazione 4 - ma iterazione 5 - ma iterazione 6 - ma iterazione 7 - ma iterazione 8 - ma iterazione 9 - ma iterazione 10 - ma iterazione E2.2 \u00b6 Scriviamo una funzione che iteri fino a che una condizione booleana non \u00e8 False . Usiamo un ciclo for , ponendo come numero massimo di iterazioni 100 e se necessario, usando il metodo random.randint(a, b) . S2.2 - Soluzione \u00b6 def itera_for (): cond = True for i in range ( 100 ): eval = random . randint ( - 10 , 10 ) print ( 'Valuto numero {} ' . format ( eval )) if eval < 0 : print ( 'Esco' ) cond = False return cond else : print ( 'Continuo' ) return cond Il risultato ottenuto sar\u00e0: >>> itera_for () Valuto numero 6 Continuo Valuto numero 7 Continuo Valuto numero 8 Continuo Valuto numero - 4 Esco False E2.3 \u00b6 Estraiamo tutti gli indici pari di una lista arbitraria di dieci elementi in ordine inverso. Per farlo, usiamo sia la funzione range sia lo slicing. S2.3 - Soluzione \u00b6 def estrai_con_slice ( lista ): if len ( lista ) != 10 : print ( 'Errore!' ) return [] else : return lista [ - 2 :: - 2 ] def estrai_con_range ( lista ): if len ( lista ) != 10 : print ( 'Errore!' ) return [] else : l_out = [] for i in range ( 8 , - 1 , - 2 ): l_out . append ( lista [ i ]) return l_out Il risultato ottenuto sar\u00e0: >>> l = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] >>> estrai_con_slice ( l ) [ 10 , 8 , 6 , 4 , 2 ] >>> estrai_con_range ( l ) [ 10 , 8 , 6 , 4 , 2 ] E2.4 \u00b6 Utilizzare il pattern matching per stampare a schermo la parola \"Vero\" se il valore di una variabile \u00e8 True , e \"Falso\" altrimenti. S2.4 - Soluzione \u00b6 def match_case ( true_or_false ): match true_or_false : case True : return \"Vero\" case False : return \"Falso\" Il risultato ottenuto sar\u00e0: >>> a = True >>> match_case ( a ) 'Vero' >>> b = False >>> match_case ( b ) 'Falso' E2.5 \u00b6 Creare un metodo che raddoppi una lista passata come argomento in ingresso. Provare ad utilizzare un ciclo for e ricordare la differenza tra shallow e deep copy. S2.5 - Soluzione \u00b6 Potremmo essere tentati di scrivere una funzione come la seguente: def raddoppia_lista ( lista ): for elemento in lista : lista . append ( elemento ) print ( f \"Lista all'iterazione attuale: { lista } \" ) Proviamo a chiamare questa funzione; avremo subito un output ingestibile. Ci\u00f2 \u00e8 legato al fatto che Python \u00e8 fermo in un loop infinito: il metodo agisce sulla lista originaria, che ad ogni iterazione del ciclo \"ingloba\" un altro elemento, provocando di conseguenza un aumento delle dimensioni della lista e, quindi, un'ulteriore iterazione, e cos\u00ec via all'infinito. Possiamo per\u00f2 ottenere il risultato che ci serve usando il metodo deepcopy : from copy import deepcopy def raddoppia_lista_deep ( lista ): lista_appoggio = deepcopy ( lista ) for elemento in lista_appoggio : lista . append ( elemento ) print ( f \"Lista di appoggio: { lista_appoggio } \" ) print ( f \"Lista attuale: { lista } \" ) In questo caso, stiamo creando un'altra variabile, chiamata lista_appoggio , che sar\u00e0 utilizzata come \"buffer\" per aggiungere alla lista originaria gli elementi relativi a s\u00e9 stessa. Provando a chiamare questo codice otterremo il risultato desiderato: >>> raddoppia_lista_deep ([ 1 , 2 ]) Lista di appoggio : [ 1 , 2 ] Lista attuale : [ 1 , 2 , 1 ] Lista di appoggio : [ 1 , 2 ] Lista attuale : [ 1 , 2 , 1 , 2 ]","title":"Esercizi"},{"location":"material/01_python/02_syntax/exercises/#e2-programmare-in-python","text":"","title":"E2 - Programmare in Python"},{"location":"material/01_python/02_syntax/exercises/#e21","text":"Scriviamo una funzione che iteri fino a che il valore associato ad un contatore intero \u00e8 minore di 10. Usiamo un ciclo while .","title":"E2.1"},{"location":"material/01_python/02_syntax/exercises/#s21-soluzione","text":"def itera_while (): i = 0 while i < 10 : i = i + 1 print ( \" {} -ma iterazione\" . format ( i )) Il risultato ottenuto sar\u00e0: >>> itera_while () 1 - ma iterazione 2 - ma iterazione 3 - ma iterazione 4 - ma iterazione 5 - ma iterazione 6 - ma iterazione 7 - ma iterazione 8 - ma iterazione 9 - ma iterazione 10 - ma iterazione","title":"S2.1 - Soluzione"},{"location":"material/01_python/02_syntax/exercises/#e22","text":"Scriviamo una funzione che iteri fino a che una condizione booleana non \u00e8 False . Usiamo un ciclo for , ponendo come numero massimo di iterazioni 100 e se necessario, usando il metodo random.randint(a, b) .","title":"E2.2"},{"location":"material/01_python/02_syntax/exercises/#s22-soluzione","text":"def itera_for (): cond = True for i in range ( 100 ): eval = random . randint ( - 10 , 10 ) print ( 'Valuto numero {} ' . format ( eval )) if eval < 0 : print ( 'Esco' ) cond = False return cond else : print ( 'Continuo' ) return cond Il risultato ottenuto sar\u00e0: >>> itera_for () Valuto numero 6 Continuo Valuto numero 7 Continuo Valuto numero 8 Continuo Valuto numero - 4 Esco False","title":"S2.2 - Soluzione"},{"location":"material/01_python/02_syntax/exercises/#e23","text":"Estraiamo tutti gli indici pari di una lista arbitraria di dieci elementi in ordine inverso. Per farlo, usiamo sia la funzione range sia lo slicing.","title":"E2.3"},{"location":"material/01_python/02_syntax/exercises/#s23-soluzione","text":"def estrai_con_slice ( lista ): if len ( lista ) != 10 : print ( 'Errore!' ) return [] else : return lista [ - 2 :: - 2 ] def estrai_con_range ( lista ): if len ( lista ) != 10 : print ( 'Errore!' ) return [] else : l_out = [] for i in range ( 8 , - 1 , - 2 ): l_out . append ( lista [ i ]) return l_out Il risultato ottenuto sar\u00e0: >>> l = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] >>> estrai_con_slice ( l ) [ 10 , 8 , 6 , 4 , 2 ] >>> estrai_con_range ( l ) [ 10 , 8 , 6 , 4 , 2 ]","title":"S2.3 - Soluzione"},{"location":"material/01_python/02_syntax/exercises/#e24","text":"Utilizzare il pattern matching per stampare a schermo la parola \"Vero\" se il valore di una variabile \u00e8 True , e \"Falso\" altrimenti.","title":"E2.4"},{"location":"material/01_python/02_syntax/exercises/#s24-soluzione","text":"def match_case ( true_or_false ): match true_or_false : case True : return \"Vero\" case False : return \"Falso\" Il risultato ottenuto sar\u00e0: >>> a = True >>> match_case ( a ) 'Vero' >>> b = False >>> match_case ( b ) 'Falso'","title":"S2.4 - Soluzione"},{"location":"material/01_python/02_syntax/exercises/#e25","text":"Creare un metodo che raddoppi una lista passata come argomento in ingresso. Provare ad utilizzare un ciclo for e ricordare la differenza tra shallow e deep copy.","title":"E2.5"},{"location":"material/01_python/02_syntax/exercises/#s25-soluzione","text":"Potremmo essere tentati di scrivere una funzione come la seguente: def raddoppia_lista ( lista ): for elemento in lista : lista . append ( elemento ) print ( f \"Lista all'iterazione attuale: { lista } \" ) Proviamo a chiamare questa funzione; avremo subito un output ingestibile. Ci\u00f2 \u00e8 legato al fatto che Python \u00e8 fermo in un loop infinito: il metodo agisce sulla lista originaria, che ad ogni iterazione del ciclo \"ingloba\" un altro elemento, provocando di conseguenza un aumento delle dimensioni della lista e, quindi, un'ulteriore iterazione, e cos\u00ec via all'infinito. Possiamo per\u00f2 ottenere il risultato che ci serve usando il metodo deepcopy : from copy import deepcopy def raddoppia_lista_deep ( lista ): lista_appoggio = deepcopy ( lista ) for elemento in lista_appoggio : lista . append ( elemento ) print ( f \"Lista di appoggio: { lista_appoggio } \" ) print ( f \"Lista attuale: { lista } \" ) In questo caso, stiamo creando un'altra variabile, chiamata lista_appoggio , che sar\u00e0 utilizzata come \"buffer\" per aggiungere alla lista originaria gli elementi relativi a s\u00e9 stessa. Provando a chiamare questo codice otterremo il risultato desiderato: >>> raddoppia_lista_deep ([ 1 , 2 ]) Lista di appoggio : [ 1 , 2 ] Lista attuale : [ 1 , 2 , 1 ] Lista di appoggio : [ 1 , 2 ] Lista attuale : [ 1 , 2 , 1 , 2 ]","title":"S2.5 - Soluzione"},{"location":"material/01_python/02_syntax/lecture/","text":"2 - Programmare in Python \u00b6 2.1 - Alcuni concetti sintattici fondamentali \u00b6 Oltre al duck typing, esistono altri concetti che caratterizzano la sintassi di Python. Vediamoli brevemente. 2.1.1 - Uso delle parentesi \u00b6 Le parentesi tonde sono usate soltanto nel caso di chiamata a funzione, oltre che per esprimere la precedenza nelle operazioni. In tutti gli altri casi, sono opzionali e possono essere omesse. Ad esempio: a = 2 b = 3 c = 4 r_1 = a + b * c # Valore restituito: 14 r_2 = ( a + b ) * c # Valore restituito: 20 if a > 2 : # Questa notazione \u00e8 valida, ed \u00e8 equivalente ad (a > 2) 2. Le parentesi quadre sono usate per la creazione e l'accesso agli elementi di una struttura dati. # Creo una lista lista = [ 1 , 2 , 3 ] # Accedo al secondo elemento della lista lista [ 1 ] # Il valore acceduto \u00e8 2 3. Le parentesi graffe sono usate per la creazione di un dizionario. dizionario = { 'a' : 1 , 'b' : 2 } # Notiamo che per accedere ad una chiave di un dizionario useremo comunque la parentesi quadra. dizionario [ a ] # Il valore acceduto \u00e8 1, ovvero quello relativo alla chiave 'a' 2.1.2 - Ambito e termine di un'istruzione \u00b6 A differenza del C, che prevede che ogni istruzione sia terminata da un punto e virgola, Python prevede che un'istruzione termini quando si va a capo. Quindi: a = 1 # L'istruzione di assegnazione \u00e8 terminata! In altre parole, si pu\u00f2 omettere il punto e virgola. Per quello che riguarda invece la definizione di un ambito, ad esempio locale all'interno di una funzione, Python si affida ai due punti , che sostituiscono la parentesi graffa di apertura, ed al numero di indentazioni . Ambito e indentazioni In generale, possiamo dire che le istruzioni allo stesso livello di indentazione sono considerate dall'interprete Python come istruzioni appartenenti al medesimo ambito. Quindi: # L'inizio della funzione, e quindi dell'ambito # che questa delimita, \u00e8 contrassegnato dai due punti def funzione (): # Inizio ambito # Il codice deve essere allo stesso livello di indentazione a = 1 a + 1 # ... return 0 Le indentazioni Per ottenere l'indentazione, occorre usare il tasto tab sulla tastiera, oppure quattro spazi. E' comunque estremamente importante non mescolare le due tecniche. 2.2 - Programmazione strutturata \u00b6 Il linguaggio Python utilizza una sintassi per le strutture di controllo differente da quella usata nei tipici linguaggi C-like. 2.2.1 - Istruzioni condizionali ( if ) \u00b6 Partiamo dall'istruzione condizionale if . Questa, in Python, ha una sintassi di questo tipo. if condizione : istruzioni () elif altra_condizione : altre_istruzioni () else : ultime_istruzioni () Notiamo l'utilizzo della keyword elif come crasi della forma else if utilizzata in altri linguaggi di programmazione. Ad esempio, se volessimo verificare il valore di una variabile intera, potremmo scrivere: a = 5 if a < 5 : print ( 'a \u00e8 minore di 5' ) elif a == 5 : print ( 'a \u00e8 uguale a 5' ) else : print ( 'a \u00e8 maggiore di 5' ) L'output di questo controllo sar\u00e0: a \u00e8 uguale a 5 2.2.2 - Pattern matching \u00b6 Fino alla versione 3.10, Python non offriva il costrutto switch/case . A partire da quest'ultima, per\u00f2, il pattern matching \u00e8 stato implementato usando questa sintassi: match command : case \"caso 1\" : istruzioni () case \"altro caso\" : print ( \"Comando sconosciuto\" ) 2.2.3 - Cicli \u00b6 2.2.3.1 - Ciclo for \u00b6 Il ciclo for itera su una sequenza , come una lista o una stringa, ed ha una sintassi del tipo: for elemento in sequenza : istruzioni () Per fare un esempio, nel seguente blocco di codice vediamo come mostrare a schermo in maniera iterativa i numeri che vanno da 0 a 5 (escluso): vals = [ 0 , 1 , 2 , 3 , 4 ] for i in vals : print ( i ) Il risultato che sar\u00e0 stampato a schermo \u00e8: 0 1 2 3 4 Rispetto ai linguaggi \"classici\", quindi, il ciclo for opera esclusivamente sugli iterabili , per cui potrebbe in qualche caso occorrere una riprogettazione del codice. Tuttavia, questa caratteristica di Python comporta anche una maggiore semplicit\u00e0 del codice; ad esempio, vediamo come \u00e8 molto semplice iterare su una stringa: string = \"Python\" for char in string : print ( char ) A schermo vedremo in entrambi i casi il seguente risultato: P y t h o n No free lunches! Come ci ricorda il no free lunches theorem , non esistono pasti gratuiti ! Infatti, la maggiore semplicit\u00e0 sintattica offerta da Python non \u00e8 indolore, ma ha un costo. Uno script Python, infatti, per quanto ottimizzato, non potr\u00e0 quasi mai offrire performance paragonabili ad un codice ottimizzato in C o C++, a meno di non usare particolari (ed avanzati) accorgimenti. 2.2.3.2 - Ciclo while \u00b6 A differenza del ciclo for , il funzionamento del while \u00e8 analogo a quello delle controparti negli altri linguaggi di programmazione. La sintassi generica \u00e8: while ( condizione ): istruzioni () Ad esempio: import random i = True while ( i ): if random . randint ( - 5 , 5 ) > 0 : print ( \"Continuo!\" ) else : print ( \"Esco!\" ) i = False Il codice nel blocco precedente non fa altro che generare un valore numerico intero casuale nell'intervallo \\([-5, 5]\\) mediante la funzione randint . Se tale valore \u00e8 superiore a \\(0\\) , il ciclo continua, altrimenti si esce dallo stesso. A schermo vedremo, ad esempio: Continuo! Continuo! Esco! I valori booleani in Python I pi\u00f9 attenti avranno notato come i valori booleani in Python siano stati scritti come True e False . Questo non \u00e8 un refuso: la prima lettera \u00e8 proprio una maiuscola. 2.4 - La funzione range() \u00b6 Riprendiamo adesso il ciclo for visto in precedenza. vals = [ 0 , 1 , 2 , 3 , 4 ] for i in vals : print ( i ) Nonostante il codice sia gi\u00e0 compatto, scrivere manualmente la sequenza da iterare pu\u00f2 facilmente diventare un'operazione abbastanza complessa. Python ci viene quindi in aiuto tramite la funzione range(i, j, s) , che genera una sequenza avente tutti i numeri compresi tra i (incluso) e j (escluso) a passo s . Ad esempio, per generare i numeri compresi tra 0 e 4 scriveremo: >>> r = range ( 0 , 5 , 1 ) >>> print ( list ( r )) [0, 1, 2, 3, 4] Nota Notiamo che per mandare in output i valori di r dovremo convertirlo in lista ( list(r) ). Qualora omessi, i ed s assumono valori di default rispettivamente 0 ed 1: >>> r = range ( 5 ) >>> print ( list ( r )) [ 0 , 1 , 2 , 3 , 4 ] E' anche possibile specificare una sequenza decrementale ponendo i > j ed s < 0 : >>> r = range ( 5 , 1 , - 1 ) >>> print ( list ( r )) [ 5 , 4 , 3 , 2 ] Esercizio : Proviamo ad iterare su tutti i valori della lista ['Pippo', 'Pluto', 5, 'Paperino'] . Soluzione : Usiamo la funzione range() assieme alla funzione len() : l = [ 'Pippo' , 'Pluto' , 5 , 'Paperino' ] for i in range ( len ( l )): print ( l [ i ]) # Output: # Pippo # Pluto # 5 # Paperino In pratica, dato che la funzione len(l) ci restituisce il numero di elementi nella lista, ovvero \\(4\\) , stiamo andando a definire un range che va da 0 a 3. A questo punto, ci baster\u00e0 elemento per elemento ai valori contenuti all'interno della lista, ed avremo ottenuto il risultato sperato. 2.5 - Istruzioni break e continue \u00b6 Le istruzioni break e continue permettono rispettivamente di uscire dal ciclo o di saltare all'iterazione successiva . Ad esempio: while ( True ): if randint ( - 5 , 5 ) > 0 : print ( \"Continuo!\" ) continue else : print ( \"Esco!\" ) break print ( \"Sono uscito!\" ) Le istruzioni precedenti usciranno dal ciclo quando viene generato casualmente un numero negativo, mentre continueranno ad iterare quando viene generato casualmente un numero positivo. 2.6 - Definire una funzione \u00b6 In Python \u00e8 possibile definire una funzione in questo modo: def nome_funzione ( parametri ): # istruzioni return valore_ritorno E' importante notare che: non \u00e8 necessario definire un tipo, ma soltanto un valore di ritorno. Qualora la funzione non restituisca alcun valore, potr\u00e0 essere omessa l'istruzione return ; non \u00e8 (strettamente) necessario definire il tipo di ciascuno dei parametri passati; \u00e8 consentito inserire dei parametri opzionali , con valori di default. Esercizio : Creiamo una funzione che concateni ad una lista il doppio dei singoli valori presenti nella stessa. Soluzione : usiamo la funzione append per mettere in coda i nuovi elementi della lista. def raddoppia_lista ( lista ): for i in range ( len ( lista )): lista . append ( lista [ i ] * 2 ) return print ( lista ) l = [ 1 , 2 ] raddoppia_lista ( l ) # Risultato atteso: [1, 2, 2, 4] Esercizio : Creiamo una funzione che generi una lista di elementi casuali tra \\(0\\) e \\(10\\) , usando un parametro opzionale per specificarne la lunghezza. Soluzione : usiamo la funzione append() in accoppiata alla funzione randint() . import random def genera_lista_casuale ( lunghezza = 5 ): l = [] for i in range ( lunghezza ): l . append ( random . randint ( 0 , 10 )) return print ( l ) ... genera_lista_casuale () # Possibile risultato: [3, 1, 2, 0, 6] genera_lista_casuale ( 10 ) # Possibile risultato: [7, 9, 1, 10, 2, 4, 9, 1, 4, 8] Tipo dei parametri di ingresso Il duck typing fa s\u00ec che non venga effettuato alcun controllo sui parametri in ingresso. Ci\u00f2 per\u00f2 non significa che non si possa provare a chiamare (ad esempio) la funzione genera_lista_casuale() passando come parametro una stringa; ci\u00f2 tuttavia causer\u00e0 un (prevedibile) errore. 2.6.1 - Passaggio di parametri a funzione \u00b6 Python prevede che i parametri siano passati ad una funzione secondo una modalit\u00e0 ibrida chiamata call-by-assignment . In pratica, il passaggio avviene esclusivamente per valore , ma con effetti differenti su tipi mutabili ed immutabili. Ad esempio, provando a passare un valore primitivo (come un intero), Python si comporter\u00e0 come se si stesse effettuando un passaggio per valore: def raddoppia ( intero ): intero = intero * 2 print ( f 'Valore all \\' interno della funzione: { intero } ' ) Il risultato sar\u00e0: >>> intero = 1 >>> raddoppia ( intero ) \"Valore all'interno della funzione: 2\" >>> print ( f 'Valore all \\' esterno della funzione: { intero } ' ) \"Valore all'interno della funzione: 1\" Ci\u00f2 \u00e8 legato al fatto che il passaggio viene effettuato per valore, per cui la funzione raddoppia() agir\u00e0 su una copia della variabile passata come argomento, e non sulla variabile originaria. Se invece usassimo una funzione che modifica una lista: def aggiungi_a_lista ( lista , elemento ): lista . append ( elemento ) print ( f 'Valore all \\' interno della funzione: { lista } ' ) Il risultato sar\u00e0: >>> lista = [ 1 , 2 ] >>> aggiungi_a_lista ( lista , 3 ) \"Valore all'interno della funzione: [1, 2, 3]\" >>> print ( f 'Valore all \\' esterno della funzione: { lista } ' ) \"Valore all'interno della funzione: [1, 2, 3]\" In questo caso, essendo la lista mutabile, il passaggio viene effettuato nei fatti per reference : ci\u00f2 significa che le operazioni compiute all'interno della funzione aggiungi_a_lista() agiranno sulla lista originaria. Shallow e deep copy Di default, Python copia le variabili per mezzo di una shallow copy : ci\u00f2 significa che un'operazione di assignment del tipo a = b fa in modo che a punti allo stesso indirizzo di memoria di b e, di conseguenza, ogni modifica a b si rifletta su a . Per evitare un fenomeno di questo tipo occorre usare una deep copy grazie alla funzione deepcopy() della libreria copy . 2.6.2 - L'istruzione pass \u00b6 Chiudiamo accennando all'istruzione pass . Questa non fa assolutamente nulla; \u00e8 utile, ad esempio, quando vogliamo inserire una funzione (o una classe) vuota, che definiremo per qualche motivo in seguito: >>> def funzione_vuota (): ... pass ... >>> funzione_vuota () Nota Anche se di primo acchitto potrebbe non essere evidente, esistono diverse situazioni in cui l'istruzione pass risulta essere estremamente utile.","title":"Dispense"},{"location":"material/01_python/02_syntax/lecture/#2-programmare-in-python","text":"","title":"2 - Programmare in Python"},{"location":"material/01_python/02_syntax/lecture/#21-alcuni-concetti-sintattici-fondamentali","text":"Oltre al duck typing, esistono altri concetti che caratterizzano la sintassi di Python. Vediamoli brevemente.","title":"2.1 - Alcuni concetti sintattici fondamentali"},{"location":"material/01_python/02_syntax/lecture/#211-uso-delle-parentesi","text":"Le parentesi tonde sono usate soltanto nel caso di chiamata a funzione, oltre che per esprimere la precedenza nelle operazioni. In tutti gli altri casi, sono opzionali e possono essere omesse. Ad esempio: a = 2 b = 3 c = 4 r_1 = a + b * c # Valore restituito: 14 r_2 = ( a + b ) * c # Valore restituito: 20 if a > 2 : # Questa notazione \u00e8 valida, ed \u00e8 equivalente ad (a > 2) 2. Le parentesi quadre sono usate per la creazione e l'accesso agli elementi di una struttura dati. # Creo una lista lista = [ 1 , 2 , 3 ] # Accedo al secondo elemento della lista lista [ 1 ] # Il valore acceduto \u00e8 2 3. Le parentesi graffe sono usate per la creazione di un dizionario. dizionario = { 'a' : 1 , 'b' : 2 } # Notiamo che per accedere ad una chiave di un dizionario useremo comunque la parentesi quadra. dizionario [ a ] # Il valore acceduto \u00e8 1, ovvero quello relativo alla chiave 'a'","title":"2.1.1 - Uso delle parentesi"},{"location":"material/01_python/02_syntax/lecture/#212-ambito-e-termine-di-unistruzione","text":"A differenza del C, che prevede che ogni istruzione sia terminata da un punto e virgola, Python prevede che un'istruzione termini quando si va a capo. Quindi: a = 1 # L'istruzione di assegnazione \u00e8 terminata! In altre parole, si pu\u00f2 omettere il punto e virgola. Per quello che riguarda invece la definizione di un ambito, ad esempio locale all'interno di una funzione, Python si affida ai due punti , che sostituiscono la parentesi graffa di apertura, ed al numero di indentazioni . Ambito e indentazioni In generale, possiamo dire che le istruzioni allo stesso livello di indentazione sono considerate dall'interprete Python come istruzioni appartenenti al medesimo ambito. Quindi: # L'inizio della funzione, e quindi dell'ambito # che questa delimita, \u00e8 contrassegnato dai due punti def funzione (): # Inizio ambito # Il codice deve essere allo stesso livello di indentazione a = 1 a + 1 # ... return 0 Le indentazioni Per ottenere l'indentazione, occorre usare il tasto tab sulla tastiera, oppure quattro spazi. E' comunque estremamente importante non mescolare le due tecniche.","title":"2.1.2 - Ambito e termine di un'istruzione"},{"location":"material/01_python/02_syntax/lecture/#22-programmazione-strutturata","text":"Il linguaggio Python utilizza una sintassi per le strutture di controllo differente da quella usata nei tipici linguaggi C-like.","title":"2.2 - Programmazione strutturata"},{"location":"material/01_python/02_syntax/lecture/#221-istruzioni-condizionali-if","text":"Partiamo dall'istruzione condizionale if . Questa, in Python, ha una sintassi di questo tipo. if condizione : istruzioni () elif altra_condizione : altre_istruzioni () else : ultime_istruzioni () Notiamo l'utilizzo della keyword elif come crasi della forma else if utilizzata in altri linguaggi di programmazione. Ad esempio, se volessimo verificare il valore di una variabile intera, potremmo scrivere: a = 5 if a < 5 : print ( 'a \u00e8 minore di 5' ) elif a == 5 : print ( 'a \u00e8 uguale a 5' ) else : print ( 'a \u00e8 maggiore di 5' ) L'output di questo controllo sar\u00e0: a \u00e8 uguale a 5","title":"2.2.1 - Istruzioni condizionali (if)"},{"location":"material/01_python/02_syntax/lecture/#222-pattern-matching","text":"Fino alla versione 3.10, Python non offriva il costrutto switch/case . A partire da quest'ultima, per\u00f2, il pattern matching \u00e8 stato implementato usando questa sintassi: match command : case \"caso 1\" : istruzioni () case \"altro caso\" : print ( \"Comando sconosciuto\" )","title":"2.2.2 - Pattern matching"},{"location":"material/01_python/02_syntax/lecture/#223-cicli","text":"","title":"2.2.3 - Cicli"},{"location":"material/01_python/02_syntax/lecture/#2231-ciclo-for","text":"Il ciclo for itera su una sequenza , come una lista o una stringa, ed ha una sintassi del tipo: for elemento in sequenza : istruzioni () Per fare un esempio, nel seguente blocco di codice vediamo come mostrare a schermo in maniera iterativa i numeri che vanno da 0 a 5 (escluso): vals = [ 0 , 1 , 2 , 3 , 4 ] for i in vals : print ( i ) Il risultato che sar\u00e0 stampato a schermo \u00e8: 0 1 2 3 4 Rispetto ai linguaggi \"classici\", quindi, il ciclo for opera esclusivamente sugli iterabili , per cui potrebbe in qualche caso occorrere una riprogettazione del codice. Tuttavia, questa caratteristica di Python comporta anche una maggiore semplicit\u00e0 del codice; ad esempio, vediamo come \u00e8 molto semplice iterare su una stringa: string = \"Python\" for char in string : print ( char ) A schermo vedremo in entrambi i casi il seguente risultato: P y t h o n No free lunches! Come ci ricorda il no free lunches theorem , non esistono pasti gratuiti ! Infatti, la maggiore semplicit\u00e0 sintattica offerta da Python non \u00e8 indolore, ma ha un costo. Uno script Python, infatti, per quanto ottimizzato, non potr\u00e0 quasi mai offrire performance paragonabili ad un codice ottimizzato in C o C++, a meno di non usare particolari (ed avanzati) accorgimenti.","title":"2.2.3.1 - Ciclo for"},{"location":"material/01_python/02_syntax/lecture/#2232-ciclo-while","text":"A differenza del ciclo for , il funzionamento del while \u00e8 analogo a quello delle controparti negli altri linguaggi di programmazione. La sintassi generica \u00e8: while ( condizione ): istruzioni () Ad esempio: import random i = True while ( i ): if random . randint ( - 5 , 5 ) > 0 : print ( \"Continuo!\" ) else : print ( \"Esco!\" ) i = False Il codice nel blocco precedente non fa altro che generare un valore numerico intero casuale nell'intervallo \\([-5, 5]\\) mediante la funzione randint . Se tale valore \u00e8 superiore a \\(0\\) , il ciclo continua, altrimenti si esce dallo stesso. A schermo vedremo, ad esempio: Continuo! Continuo! Esco! I valori booleani in Python I pi\u00f9 attenti avranno notato come i valori booleani in Python siano stati scritti come True e False . Questo non \u00e8 un refuso: la prima lettera \u00e8 proprio una maiuscola.","title":"2.2.3.2 - Ciclo while"},{"location":"material/01_python/02_syntax/lecture/#24-la-funzione-range","text":"Riprendiamo adesso il ciclo for visto in precedenza. vals = [ 0 , 1 , 2 , 3 , 4 ] for i in vals : print ( i ) Nonostante il codice sia gi\u00e0 compatto, scrivere manualmente la sequenza da iterare pu\u00f2 facilmente diventare un'operazione abbastanza complessa. Python ci viene quindi in aiuto tramite la funzione range(i, j, s) , che genera una sequenza avente tutti i numeri compresi tra i (incluso) e j (escluso) a passo s . Ad esempio, per generare i numeri compresi tra 0 e 4 scriveremo: >>> r = range ( 0 , 5 , 1 ) >>> print ( list ( r )) [0, 1, 2, 3, 4] Nota Notiamo che per mandare in output i valori di r dovremo convertirlo in lista ( list(r) ). Qualora omessi, i ed s assumono valori di default rispettivamente 0 ed 1: >>> r = range ( 5 ) >>> print ( list ( r )) [ 0 , 1 , 2 , 3 , 4 ] E' anche possibile specificare una sequenza decrementale ponendo i > j ed s < 0 : >>> r = range ( 5 , 1 , - 1 ) >>> print ( list ( r )) [ 5 , 4 , 3 , 2 ] Esercizio : Proviamo ad iterare su tutti i valori della lista ['Pippo', 'Pluto', 5, 'Paperino'] . Soluzione : Usiamo la funzione range() assieme alla funzione len() : l = [ 'Pippo' , 'Pluto' , 5 , 'Paperino' ] for i in range ( len ( l )): print ( l [ i ]) # Output: # Pippo # Pluto # 5 # Paperino In pratica, dato che la funzione len(l) ci restituisce il numero di elementi nella lista, ovvero \\(4\\) , stiamo andando a definire un range che va da 0 a 3. A questo punto, ci baster\u00e0 elemento per elemento ai valori contenuti all'interno della lista, ed avremo ottenuto il risultato sperato.","title":"2.4 - La funzione range()"},{"location":"material/01_python/02_syntax/lecture/#25-istruzioni-break-e-continue","text":"Le istruzioni break e continue permettono rispettivamente di uscire dal ciclo o di saltare all'iterazione successiva . Ad esempio: while ( True ): if randint ( - 5 , 5 ) > 0 : print ( \"Continuo!\" ) continue else : print ( \"Esco!\" ) break print ( \"Sono uscito!\" ) Le istruzioni precedenti usciranno dal ciclo quando viene generato casualmente un numero negativo, mentre continueranno ad iterare quando viene generato casualmente un numero positivo.","title":"2.5 - Istruzioni break e continue"},{"location":"material/01_python/02_syntax/lecture/#26-definire-una-funzione","text":"In Python \u00e8 possibile definire una funzione in questo modo: def nome_funzione ( parametri ): # istruzioni return valore_ritorno E' importante notare che: non \u00e8 necessario definire un tipo, ma soltanto un valore di ritorno. Qualora la funzione non restituisca alcun valore, potr\u00e0 essere omessa l'istruzione return ; non \u00e8 (strettamente) necessario definire il tipo di ciascuno dei parametri passati; \u00e8 consentito inserire dei parametri opzionali , con valori di default. Esercizio : Creiamo una funzione che concateni ad una lista il doppio dei singoli valori presenti nella stessa. Soluzione : usiamo la funzione append per mettere in coda i nuovi elementi della lista. def raddoppia_lista ( lista ): for i in range ( len ( lista )): lista . append ( lista [ i ] * 2 ) return print ( lista ) l = [ 1 , 2 ] raddoppia_lista ( l ) # Risultato atteso: [1, 2, 2, 4] Esercizio : Creiamo una funzione che generi una lista di elementi casuali tra \\(0\\) e \\(10\\) , usando un parametro opzionale per specificarne la lunghezza. Soluzione : usiamo la funzione append() in accoppiata alla funzione randint() . import random def genera_lista_casuale ( lunghezza = 5 ): l = [] for i in range ( lunghezza ): l . append ( random . randint ( 0 , 10 )) return print ( l ) ... genera_lista_casuale () # Possibile risultato: [3, 1, 2, 0, 6] genera_lista_casuale ( 10 ) # Possibile risultato: [7, 9, 1, 10, 2, 4, 9, 1, 4, 8] Tipo dei parametri di ingresso Il duck typing fa s\u00ec che non venga effettuato alcun controllo sui parametri in ingresso. Ci\u00f2 per\u00f2 non significa che non si possa provare a chiamare (ad esempio) la funzione genera_lista_casuale() passando come parametro una stringa; ci\u00f2 tuttavia causer\u00e0 un (prevedibile) errore.","title":"2.6 - Definire una funzione"},{"location":"material/01_python/02_syntax/lecture/#261-passaggio-di-parametri-a-funzione","text":"Python prevede che i parametri siano passati ad una funzione secondo una modalit\u00e0 ibrida chiamata call-by-assignment . In pratica, il passaggio avviene esclusivamente per valore , ma con effetti differenti su tipi mutabili ed immutabili. Ad esempio, provando a passare un valore primitivo (come un intero), Python si comporter\u00e0 come se si stesse effettuando un passaggio per valore: def raddoppia ( intero ): intero = intero * 2 print ( f 'Valore all \\' interno della funzione: { intero } ' ) Il risultato sar\u00e0: >>> intero = 1 >>> raddoppia ( intero ) \"Valore all'interno della funzione: 2\" >>> print ( f 'Valore all \\' esterno della funzione: { intero } ' ) \"Valore all'interno della funzione: 1\" Ci\u00f2 \u00e8 legato al fatto che il passaggio viene effettuato per valore, per cui la funzione raddoppia() agir\u00e0 su una copia della variabile passata come argomento, e non sulla variabile originaria. Se invece usassimo una funzione che modifica una lista: def aggiungi_a_lista ( lista , elemento ): lista . append ( elemento ) print ( f 'Valore all \\' interno della funzione: { lista } ' ) Il risultato sar\u00e0: >>> lista = [ 1 , 2 ] >>> aggiungi_a_lista ( lista , 3 ) \"Valore all'interno della funzione: [1, 2, 3]\" >>> print ( f 'Valore all \\' esterno della funzione: { lista } ' ) \"Valore all'interno della funzione: [1, 2, 3]\" In questo caso, essendo la lista mutabile, il passaggio viene effettuato nei fatti per reference : ci\u00f2 significa che le operazioni compiute all'interno della funzione aggiungi_a_lista() agiranno sulla lista originaria. Shallow e deep copy Di default, Python copia le variabili per mezzo di una shallow copy : ci\u00f2 significa che un'operazione di assignment del tipo a = b fa in modo che a punti allo stesso indirizzo di memoria di b e, di conseguenza, ogni modifica a b si rifletta su a . Per evitare un fenomeno di questo tipo occorre usare una deep copy grazie alla funzione deepcopy() della libreria copy .","title":"2.6.1 - Passaggio di parametri a funzione"},{"location":"material/01_python/02_syntax/lecture/#262-listruzione-pass","text":"Chiudiamo accennando all'istruzione pass . Questa non fa assolutamente nulla; \u00e8 utile, ad esempio, quando vogliamo inserire una funzione (o una classe) vuota, che definiremo per qualche motivo in seguito: >>> def funzione_vuota (): ... pass ... >>> funzione_vuota () Nota Anche se di primo acchitto potrebbe non essere evidente, esistono diverse situazioni in cui l'istruzione pass risulta essere estremamente utile.","title":"2.6.2 - L'istruzione pass"},{"location":"material/01_python/03_data_structures/exercises/","text":"E3 - Esercizi sulle strutture dati \u00b6 E3.1 \u00b6 Proviamo a valutare il tempo necessario alle operazioni di insert e pop su una coda in Python usando la libreria time . Confrontiamo il risultato ottenuto con quello ottenibile implementando una coda come una struttura di tipo deque e usando gli opportuni metodi appendleft e popleft . S3.1 - Soluzione \u00b6 from time import time from collections import deque def queue_classica ( queue ): tic = time () queue . insert ( 0 , 4 ) queue . pop () toc = time () return toc - tic def queue_con_deque ( queue , pushed = 1 ): tic = time () queue . appendleft ( pushed ) queue . popleft () toc = time () return toc - tic Proviamo a chiamare le due funzioni: queue = list ( range ( 10000000 )) queue_d = deque ( queue ) queue_classica ( queue ) queue_con_deque ( queue_d ) Avremo un output simile al seguente: Tempo necessario con lista: 0 .08756685256958008 Tempo necessario con deque: 0 .0 E3.2 \u00b6 Selezioniamo tutti i nomi che iniziano con la lettera B dalla seguente lista: lista_nomi = [ \"Jax Teller\" , \"Walter White\" , \"Billy Butcher\" , \"Luke Skywalker\" , \"Bobby Singer\" , \"Johnny Lawrence\" ] Facciamolo usando un ciclo ed una list comprehension. S3.2 - Soluzione \u00b6 Usando un ciclo: output_for = [] for nome in lista_nomi : if nome [ 0 ] == \"B\" : output_for . append ( nome ) Usando una list comprehension: output = [ nome for nome in lista_nomi if nome [ 0 ] == \"B\" ] E3.3 \u00b6 Ottenere una lista che abbia al suo interno tutti i quadrati dei numeri che vanno da 1 a 10 S3.3 - Soluzione \u00b6 Usando un ciclo: def quadrato ( numero ): return numero ** 2 output = [] for i in range ( 1 , 11 ): output . append ( quadrato ( i )) Usando una list comprehension: output = [ quadrato ( i ) for i in range ( 1 , 11 )] E3.4 \u00b6 Ottenere una lista che abbia la stringa 'pari' in corrispondenza dei numeri pari, mentre quella 'dispari' in corrispondenza dei numeri dispari, per tutti i numeri che vanno da 1 a 10. S3.4 - Soluzione \u00b6 Usando un ciclo: output = [] for i in range ( 1 , 11 ): if i % 2 == 0 : output . append ( \"pari\" ) else : output . append ( \"dispari\" ) Usando una list comprehension: output = [ \"pari\" if i % 2 == 0 else \"dispari\" for i in range ( 1 , 11 )] Suggerimento Possiamo usare la forma base della list comprehension definendo una funzione accessoria. Ad esempio: def pari_o_dispari ( numero ): if numero % 2 == 0 : return 'pari' else : return 'dispari' E3.5 \u00b6 Scrivere una dict comprehension che permetta di ottenere il dizionario vecchio_o_giovane dato il seguente dizionario: dizionario = { 'Jax Teller' : 27 , 'Walter White' : 52 , 'Billy Butcher' : 41 , 'Luke Skywalker' : 79 , 'Bobby Singer' : 68 , 'Johnny Lawrence' : 49 } In particolare, il dizionario vecchio_o_giovane avr\u00e0 le stesse chiavi del dizionario di partenza, a cui sar\u00e0 associato il valore 'giovane' soltanto se il valore della chiave del dizionario di partenza \u00e8 inferiore a 65. S3.5 - Soluzione \u00b6 vecchio_o_giovane = { k : 'vecchio' if v > 65 else 'giovane' for ( k , v ) in dizionario . items ()} Nota Per iterare sul dizionario originale, usiamo il metodo items() che, come visto in precedenza, ci restituisce un oggetto di tipo dict_items il quale \u00e8, per l'appunto, iterabile.","title":"Esercizi"},{"location":"material/01_python/03_data_structures/exercises/#e3-esercizi-sulle-strutture-dati","text":"","title":"E3 - Esercizi sulle strutture dati"},{"location":"material/01_python/03_data_structures/exercises/#e31","text":"Proviamo a valutare il tempo necessario alle operazioni di insert e pop su una coda in Python usando la libreria time . Confrontiamo il risultato ottenuto con quello ottenibile implementando una coda come una struttura di tipo deque e usando gli opportuni metodi appendleft e popleft .","title":"E3.1"},{"location":"material/01_python/03_data_structures/exercises/#s31-soluzione","text":"from time import time from collections import deque def queue_classica ( queue ): tic = time () queue . insert ( 0 , 4 ) queue . pop () toc = time () return toc - tic def queue_con_deque ( queue , pushed = 1 ): tic = time () queue . appendleft ( pushed ) queue . popleft () toc = time () return toc - tic Proviamo a chiamare le due funzioni: queue = list ( range ( 10000000 )) queue_d = deque ( queue ) queue_classica ( queue ) queue_con_deque ( queue_d ) Avremo un output simile al seguente: Tempo necessario con lista: 0 .08756685256958008 Tempo necessario con deque: 0 .0","title":"S3.1 - Soluzione"},{"location":"material/01_python/03_data_structures/exercises/#e32","text":"Selezioniamo tutti i nomi che iniziano con la lettera B dalla seguente lista: lista_nomi = [ \"Jax Teller\" , \"Walter White\" , \"Billy Butcher\" , \"Luke Skywalker\" , \"Bobby Singer\" , \"Johnny Lawrence\" ] Facciamolo usando un ciclo ed una list comprehension.","title":"E3.2"},{"location":"material/01_python/03_data_structures/exercises/#s32-soluzione","text":"Usando un ciclo: output_for = [] for nome in lista_nomi : if nome [ 0 ] == \"B\" : output_for . append ( nome ) Usando una list comprehension: output = [ nome for nome in lista_nomi if nome [ 0 ] == \"B\" ]","title":"S3.2 - Soluzione"},{"location":"material/01_python/03_data_structures/exercises/#e33","text":"Ottenere una lista che abbia al suo interno tutti i quadrati dei numeri che vanno da 1 a 10","title":"E3.3"},{"location":"material/01_python/03_data_structures/exercises/#s33-soluzione","text":"Usando un ciclo: def quadrato ( numero ): return numero ** 2 output = [] for i in range ( 1 , 11 ): output . append ( quadrato ( i )) Usando una list comprehension: output = [ quadrato ( i ) for i in range ( 1 , 11 )]","title":"S3.3 - Soluzione"},{"location":"material/01_python/03_data_structures/exercises/#e34","text":"Ottenere una lista che abbia la stringa 'pari' in corrispondenza dei numeri pari, mentre quella 'dispari' in corrispondenza dei numeri dispari, per tutti i numeri che vanno da 1 a 10.","title":"E3.4"},{"location":"material/01_python/03_data_structures/exercises/#s34-soluzione","text":"Usando un ciclo: output = [] for i in range ( 1 , 11 ): if i % 2 == 0 : output . append ( \"pari\" ) else : output . append ( \"dispari\" ) Usando una list comprehension: output = [ \"pari\" if i % 2 == 0 else \"dispari\" for i in range ( 1 , 11 )] Suggerimento Possiamo usare la forma base della list comprehension definendo una funzione accessoria. Ad esempio: def pari_o_dispari ( numero ): if numero % 2 == 0 : return 'pari' else : return 'dispari'","title":"S3.4 - Soluzione"},{"location":"material/01_python/03_data_structures/exercises/#e35","text":"Scrivere una dict comprehension che permetta di ottenere il dizionario vecchio_o_giovane dato il seguente dizionario: dizionario = { 'Jax Teller' : 27 , 'Walter White' : 52 , 'Billy Butcher' : 41 , 'Luke Skywalker' : 79 , 'Bobby Singer' : 68 , 'Johnny Lawrence' : 49 } In particolare, il dizionario vecchio_o_giovane avr\u00e0 le stesse chiavi del dizionario di partenza, a cui sar\u00e0 associato il valore 'giovane' soltanto se il valore della chiave del dizionario di partenza \u00e8 inferiore a 65.","title":"E3.5"},{"location":"material/01_python/03_data_structures/exercises/#s35-soluzione","text":"vecchio_o_giovane = { k : 'vecchio' if v > 65 else 'giovane' for ( k , v ) in dizionario . items ()} Nota Per iterare sul dizionario originale, usiamo il metodo items() che, come visto in precedenza, ci restituisce un oggetto di tipo dict_items il quale \u00e8, per l'appunto, iterabile.","title":"S3.5 - Soluzione"},{"location":"material/01_python/03_data_structures/lecture/","text":"3 - Strutture dati in Python \u00b6 3.1 - Liste, pile e code \u00b6 Python ci offre una grande variet\u00e0 di metodi per gestire le liste; troviamo un elenco esaustivo a questo indirizzo . Grazie a questi metodi, \u00e8 possibile costruire una pila o una coda in modo molto pi\u00f9 semplice rispetto ad altri linguaggi. 3.1.1 - Pila \u00b6 Una pila (in inglese stack ) adotta una strategia di accesso ai dati di tipo Last-In, First-Out ( LIFO ). Questo significa che il primo elemento ad uscire (ovvero ad essere analizzato) \u00e8 quello in cima alla pila, ovvero l'ultimo ad esservi entrato. Esempio di pila Un tipico esempio di pila \u00e8 quella dei piatti da lavare. Quasi sicuramente, il piatto in cima alla pila sar\u00e0 l'ultimo che avremo preso dal tavolo; tuttavia, sar\u00e0 anche il primo ad essere lavato. Per implementare una pila a partire da una lista possiamo usare due metodi: il metodo append() ci permette di inserire un nuovo elemento in cima alla pila (ovvero alla posizione \\(n-1\\) -ma, con una lista ad \\(n\\) componenti); il metodo pop(pos) ci permette di estrarre l'elemento in posizione pos . Di default, non specificando alcun valore di pos , estrarremo l'elemento in posizione \\(n-1\\) -ma. Ad esempio: s = [ 1 , 2 , 3 ] s . append ( 4 ) # s sar\u00e0 pari a [1, 2, 3, 4] e = s . pop () # e sar\u00e0 pari a 4, mentre s sar\u00e0 pari a [1, 2, 3] 3.1.2 - Coda \u00b6 Una coda (in inglese queue ) adotta una strategia di accesso ai dati di tipo First-In, First-Out ( FIFO ). In questo caso, il primo elemento ad uscire \u00e8 presente da pi\u00f9 tempo in coda. Esempio di coda Un tipico esempio di coda \u00e8 quella che tutti quanti, prima o poi, abbiamo fatto alle Poste: il primo ad arrivare \u00e8 il primo ad essere servito, poi il secondo, il terzo, e via cos\u00ec. Per implementare una coda a partire da una lista, possiamo usare il metodo pop(pos) con pos = 0 , che ci permetter\u00e0 quindi di estrarre il primo elemento della coda, ed il metodo insert(pos, el) ci permette di inserire alla posizione pos l'elemento el . q = [ 1 , 2 , 3 ] q . insert ( 0 , 4 ) # q sar\u00e0 pari a [4, 1, 2, 3] e = q . pop ( 0 ) # e sar\u00e0 pari a 4, q sar\u00e0 pari a [1, 2, 3] Questo approccio, per quanto semplice, ha uno svantaggio: infatti, i metodi insert() e pop() sono computazionalmente onerosi, in quanto fanno in modo di riallocare lo spazio occupato dagli elementi della lista. In alternativa, possiamo usare una struttura contenuta nella libreria collections e chiamata deque . Il vantaggio sta nel fatto che la deque \u00e8 progettata specificamente per eseguire in maniera efficiente i metodi append() e pop() da entrambi i capi della struttura dati: from collections import deque q = deque ([ 1 , 2 , 3 ]) q . appendleft ( 4 ) # q sar\u00e0 pari a [4, 1, 2, 3] e = q . popleft () # e sar\u00e0 pari a 4, q sar\u00e0 pari a [1, 2, 3] Nota Sottolineamo che q non \u00e8 pi\u00f9 una lista, ma una deque . 3.2 - List comprehension \u00b6 Una delle tecniche pi\u00f9 usate per effettuare delle operazioni sugli elementi di una lista \u00e8 usare la tecnica della list comprehension , che permette di sostituire quasi completamente i classici cicli. Nella forma base, una list comprehension ha una sintassi di questo tipo: lista_output = [ f ( elemento ) for elemento in lista_input ] In altre parole, otterremo in output una lista ( lista_output ) applicando ad ogni elemento della lista originaria ( lista_input ) la funzione f() . Nota Per essere precisi, pi\u00f9 che di lista, sarebbe opportuno parlare di iterabile di input. 3.2.1 - Forma estesa con if-else \u00b6 La list comprehension pu\u00f2 anche includere delle istruzioni condizionali. Un primo esempio \u00e8 la seguente forma: lista_output_if = [ f ( elemento ) for elemento in lista_input if condizione ] In questo caso, la funzione f() sar\u00e0 chiamata esclusivamente sugli elementi che soddisfano la condizione indicata. Invece, se usassimo questa forma: lista_output_if_else = [ f ( elemento ) if condizione else g ( elemento ) for elemento in lista_input ] la funzione f() sarebbe invocata su tutti gli elementi che soddisfano la condizione , mentre la funzione g() su tutti quelli che non la soddisfano. Perch\u00e9 usare le list comprehension? Le list comprehension sono utili e versatili, e permettono, in molti casi, di sostituire i classici cicli con una sintassi pi\u00f9 snella. Tuttavia, bisogna fare attenzione a non abusare di questo strumento: infatti, facendolo si rischia di complicare inutilmente il nostro programma, rendendolo poco leggibile e manutenibile. Come regola generale, quindi, ricordiamo il principio del rasoio di Occam: anche se \u00e8 facile innamorarsi delle list comprehension, \u00e8 bene ricordarsi che anche i cicli sono leciti e funzionali , per cui non \u00e8 sempre necessario trovare a tutti i costi una soluzione usando una list comprehension. 3.3 - Le assignment expressions \u00b6 Come apparso dalla trattazione, le list comprehension sono state pensate per approcci puramente iterativi. Di conseguenza, risulta complesso implementare forme di ricorsione. Per ovviare a questo inconveniente, Python introduce, a partire dalla versione 3.8, le assignment expression . Da un punto di vista \"formale\", un'assignment expression permette di assegnare e restituire un valore all'interno di un'unica istruzione mediante il cosiddetto walrus operator : >>> print ( enjoy := True ) True Vediamo come utilizzare questo concetto per combinare ricorsione e list comprehension. Definiamo i valori di \\(F_0\\) ed \\(F_1\\) per la sequenza di Fibonacci : >>> fib = [ 0 , 1 ] Vediamo cosa succede se proviamo ad usare una assignment expression in modo da restituire una lista che abbia come primo elemento il secondo della precedente (ovvero 1 ), e come secondo la somma di tutti gli elementi della lista (ovvero 0 + 1 ): >>> ( fib := [ fib [ 1 ], fib [ 0 ] + fib [ 1 ]]) >>> fib [ 1 , 1 ] Notiamo che l'operazione ha modificato il valore della lista fib ! A noi, per\u00f2, interessa soltanto la somma degli elementi precedenti della lista (e quindi il secondo valore ottenuto). Per isolarlo, possiamo adoperare l'operatore booleano and : >>> fib = [ 0 , 1 ] >>> ( fib := [ fib [ 1 ], fib [ 0 ] + fib [ 1 ]]) and fib [ 1 ] 1 Proviamo a combinare i due passaggi precedenti, ed usare una list comprehension per concatenare i risultati ottenuti per i numeri che vanno fino ad \\(F_9\\) : >>> fib = [ 0 , 1 ] >>> fib += [( fib := [ fib [ 1 ], fib [ 0 ] + fib [ 1 ]]) and fib [ 1 ] for i in range ( 10 )] >>> fib [ 0 , 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , 89 ] 3.4 - Tuple \u00b6 Le tuple permettono di rappresentano un insieme di valori eterogenei separadoli da una virgola. Ad esempio: tupla = ( 'hello' , 'world' , 12 ) Un po' come avviene per le liste, uno dei valori della tupla pu\u00f2 a sua volta essere un'altra tupla. Ad esempio: tupla = ( 'hello' , 'world' , ( 1 , 2 )) A differenza di una lista, per\u00f2, le tuple sono immutabili . Ci\u00f2 non implica per\u00f2 che non possano contenere al loro interno oggetti mutabili. Guardiamo il seguente esempio: tupla = ( 'hello' , 'world' , [ 1 , 2 , 3 ]) La tupla avr\u00e0 al suo interno due stringhe (immutabili) ed una lista (mutabile). Proviamo a modificare la lista: tupla [ 2 ] = [ 2 , 2 , 3 ] Apparir\u00e0 un errore simile a questo: Traceback ( most recent call last ) : File \"<stdin>\" , line 1 , in <module> TypeError: 'tuple' object does not support item assignment Come prevedibile, abbiamo avuto un errore di assegnazione legato all'immutabilit\u00e0 della tupla. Proviamo adesso per\u00f2 a modificare direttamente la lista : tupla [ 2 ][ 0 ] = 2 # La tupla sar\u00e0 ('hello', 'world', [2, 2, 3]) L'operazione \u00e8 evidentemente ammissibile, ed il risultato \u00e8 stato proprio quello atteso. Tuple e liste Ad un attento osservatore non sfuggir\u00e0 come tuple e liste siano simili dal punto di vista sintattico, e differiscano in buona sostanza per la mutabilit\u00e0. Da qui discende che le tuple sono estremamente efficaci nel caso si debba esclusivamente accedere agli elementi contenuti, mentre le liste devono essere usate quando \u00e8 anche necessario modificare all'occorrenza detti elementi. 3.5 - Set \u00b6 Anche i set sono molto simili alle liste dal punto di vista sintattico, ma offrono una significativa differenza: infatti, in un set non possono esserci elementi ripetuti . Nota Notiamo un'evidente analogia con il concetto matematico di insieme. La sintassi da usare per creare un set \u00e8 la seguente. insieme = { 1 , \"stringa\" , 2 } Il set ammette al suo interno dati eterogenei, tuttavia non pu\u00f2 contenere al suo interno delle liste o dei dizionari. Questo \u00e8 legato al fatto che i set (cos\u00ec come gli stessi dizionari) sono delle hash table , e quindi sfruttano il concetto di hash per rappresentare i dati contenuti in maniera compatta ed efficiente. Il fatto che le liste ed i dizionari non possano essere rappresentati in questo modo li esclude in automatico dall'includibilit\u00e0 all'interno di un set. Un'altra considerazione da fare \u00e8 che il set non \u00e8 ordinato : ci\u00f2 rende impossibile accedere ad (e modificare ) un elemento del set mediante il suo indice, come succedeva per liste e tuple. Suggerimento I set possono essere usati per isolare gli elementi univoci presenti in una lista. Per farlo, basta convertire la lista in set: l = [ 1 , 2 , 2 , 3 ] # La lista sar\u00e0 [1, 2, 2, 3] s = set ( l ) # Il set sar\u00e0 [1, 2, 3] 3.6 - Dizionari \u00b6 Il quarto ed ultimo tipo di contenitore per sequenze di dati \u00e8 il dizionario , presente anche in altri linguaggi di programmazione con il nome di array associativo o hash map . L'elemento base di un dizionario \u00e8 la coppia chiave - valore , nella quale un certo valore (di qualsiasi tipo) \u00e8 associato ad una determinata chiave (di tipo immutabile). I dizionari hanno diverse caratteristiche comuni ai set, dall'inutilizzabilit\u00e0 delle liste come chiavi al fatto di non permettere chiavi ripetute. Inoltre, le coppie chiave - valore sono accedute, per l'appunto, per chiave, e non in base all'ordine delle coppie. Nota Una differenza tra set e dizionari sta nel fatto che questi ultimi sono ordinati a partire da Python 3.7. Per creare un dizionario, possiamo usare una sintassi simile a quella usata per i set. Ad esempio, per creare un dizionario vuoto: dizionario = {} Possiamo quindi inserire delle coppie chiave - valore in questo modo: dizionario [ 'k' ] = 'v' dizionario [ 1 ] = 'n' # Il dizionario sar\u00e0 {'k': 'v', 1: 'n'} Per accedere al valore associato ad una determinata chiave: dizionario [ 1 ] # Il valore restituito sar\u00e0 'n' 3.6.1 - Chiavi e valori \u00b6 E' possibile recuperare la lista di tutte le chiavi presenti in un dizionario usando il metodo keys() , che restituisce un oggetto di tipo dict_keys , a sua volta convertibile in lista: chiavi = dizionario . keys () # Restituisce dict_keys(['k', 1]), che non \u00e8 una lista! print ( list ( chiavi )) # Restituisce ['k', 1], che \u00e8 una lista! In modo analogo, si pu\u00f2 accedere a tutti i valori presenti nel dizionario mediante il metodo values() , che restituir\u00e0 un oggetto di tipo dict_values , da convertire anch'esso in lista: valori = dizionario . values () # Restituisce dict_values(['v', 'n']), che non \u00e8 una lista! print ( list ( valori )) # Restituisce ['v', 'n'], che \u00e8 una lista! Possiamo accedere anche a tutte le coppie chiave - valore mediante il metodo items() , che ci restituisce un oggetto di tipo dict_items , il quale pu\u00f2 essere convertito in una lista di tuple: coppie = dizionario . items () # Restituisce dict_items([('k', 'v'), (1, 'n')]) print ( list ( coppie )) # Restituisce una lista di tuple 3.6.2 - Creazione di un dizionario (non vuoto) \u00b6 Abbiamo diversi modi per creare un dizionario non vuoto. 3.6.2.1 - Uso dell'operatore {} \u00b6 Il pi\u00f9 semplice, che \u00e8 quello che useremo pi\u00f9 spesso, \u00e8 quello di dichiarare nell'operatore {} le coppie chiave - valore iniziali: >>> dizionario = { 'k1' : 1 , 'k2' : 2 } >>> dizionario { 'k1' : 1 , 'k2' : 2 } 3.6.2.2 - Uso del costruttore dict() \u00b6 Un altro modo \u00e8 usare il metodo costruttore dict() : dizionario = dict ( k1 = 1 , k2 = 2 ) # Il dizionario sar\u00e0 {'k1': 1, 'k2': 2} 3.6.2.3 - Uso della funzione zip \u00b6 Possiamo poi usare la funzione zip per creare un dizionario a partire da due liste: chiavi = [ 'k1' , 'k2' ] valori = [ 1 , 2 ] dizionario = dict ( zip ( chiavi , valori )) 3.6.2.4 - Dict comprehension \u00b6 Un modo per ottenere un dizionario a partire da un altro oggetto iterabile \u00e8 la dict comprehension , che ha una forma del tipo: output = { chiave : valore for valore in iterabile } Possiamo ad esempio creare un dizionario contenente come chiave i numeri da 1 a 9, e come valori corrispondenti i quadrati degli stessi: quadrati = { str ( i ): i ** 2 for i in range ( 1 , 10 )} print ( quadrati ) # Risultato: {'1': 1, '2': 4, '3': 9, '4': 16, '5': 25, '6': 36, '7': 49, '8': 64, '9': 81}","title":"Dispense"},{"location":"material/01_python/03_data_structures/lecture/#3-strutture-dati-in-python","text":"","title":"3 - Strutture dati in Python"},{"location":"material/01_python/03_data_structures/lecture/#31-liste-pile-e-code","text":"Python ci offre una grande variet\u00e0 di metodi per gestire le liste; troviamo un elenco esaustivo a questo indirizzo . Grazie a questi metodi, \u00e8 possibile costruire una pila o una coda in modo molto pi\u00f9 semplice rispetto ad altri linguaggi.","title":"3.1 - Liste, pile e code"},{"location":"material/01_python/03_data_structures/lecture/#311-pila","text":"Una pila (in inglese stack ) adotta una strategia di accesso ai dati di tipo Last-In, First-Out ( LIFO ). Questo significa che il primo elemento ad uscire (ovvero ad essere analizzato) \u00e8 quello in cima alla pila, ovvero l'ultimo ad esservi entrato. Esempio di pila Un tipico esempio di pila \u00e8 quella dei piatti da lavare. Quasi sicuramente, il piatto in cima alla pila sar\u00e0 l'ultimo che avremo preso dal tavolo; tuttavia, sar\u00e0 anche il primo ad essere lavato. Per implementare una pila a partire da una lista possiamo usare due metodi: il metodo append() ci permette di inserire un nuovo elemento in cima alla pila (ovvero alla posizione \\(n-1\\) -ma, con una lista ad \\(n\\) componenti); il metodo pop(pos) ci permette di estrarre l'elemento in posizione pos . Di default, non specificando alcun valore di pos , estrarremo l'elemento in posizione \\(n-1\\) -ma. Ad esempio: s = [ 1 , 2 , 3 ] s . append ( 4 ) # s sar\u00e0 pari a [1, 2, 3, 4] e = s . pop () # e sar\u00e0 pari a 4, mentre s sar\u00e0 pari a [1, 2, 3]","title":"3.1.1 - Pila"},{"location":"material/01_python/03_data_structures/lecture/#312-coda","text":"Una coda (in inglese queue ) adotta una strategia di accesso ai dati di tipo First-In, First-Out ( FIFO ). In questo caso, il primo elemento ad uscire \u00e8 presente da pi\u00f9 tempo in coda. Esempio di coda Un tipico esempio di coda \u00e8 quella che tutti quanti, prima o poi, abbiamo fatto alle Poste: il primo ad arrivare \u00e8 il primo ad essere servito, poi il secondo, il terzo, e via cos\u00ec. Per implementare una coda a partire da una lista, possiamo usare il metodo pop(pos) con pos = 0 , che ci permetter\u00e0 quindi di estrarre il primo elemento della coda, ed il metodo insert(pos, el) ci permette di inserire alla posizione pos l'elemento el . q = [ 1 , 2 , 3 ] q . insert ( 0 , 4 ) # q sar\u00e0 pari a [4, 1, 2, 3] e = q . pop ( 0 ) # e sar\u00e0 pari a 4, q sar\u00e0 pari a [1, 2, 3] Questo approccio, per quanto semplice, ha uno svantaggio: infatti, i metodi insert() e pop() sono computazionalmente onerosi, in quanto fanno in modo di riallocare lo spazio occupato dagli elementi della lista. In alternativa, possiamo usare una struttura contenuta nella libreria collections e chiamata deque . Il vantaggio sta nel fatto che la deque \u00e8 progettata specificamente per eseguire in maniera efficiente i metodi append() e pop() da entrambi i capi della struttura dati: from collections import deque q = deque ([ 1 , 2 , 3 ]) q . appendleft ( 4 ) # q sar\u00e0 pari a [4, 1, 2, 3] e = q . popleft () # e sar\u00e0 pari a 4, q sar\u00e0 pari a [1, 2, 3] Nota Sottolineamo che q non \u00e8 pi\u00f9 una lista, ma una deque .","title":"3.1.2 - Coda"},{"location":"material/01_python/03_data_structures/lecture/#32-list-comprehension","text":"Una delle tecniche pi\u00f9 usate per effettuare delle operazioni sugli elementi di una lista \u00e8 usare la tecnica della list comprehension , che permette di sostituire quasi completamente i classici cicli. Nella forma base, una list comprehension ha una sintassi di questo tipo: lista_output = [ f ( elemento ) for elemento in lista_input ] In altre parole, otterremo in output una lista ( lista_output ) applicando ad ogni elemento della lista originaria ( lista_input ) la funzione f() . Nota Per essere precisi, pi\u00f9 che di lista, sarebbe opportuno parlare di iterabile di input.","title":"3.2 - List comprehension"},{"location":"material/01_python/03_data_structures/lecture/#321-forma-estesa-con-if-else","text":"La list comprehension pu\u00f2 anche includere delle istruzioni condizionali. Un primo esempio \u00e8 la seguente forma: lista_output_if = [ f ( elemento ) for elemento in lista_input if condizione ] In questo caso, la funzione f() sar\u00e0 chiamata esclusivamente sugli elementi che soddisfano la condizione indicata. Invece, se usassimo questa forma: lista_output_if_else = [ f ( elemento ) if condizione else g ( elemento ) for elemento in lista_input ] la funzione f() sarebbe invocata su tutti gli elementi che soddisfano la condizione , mentre la funzione g() su tutti quelli che non la soddisfano. Perch\u00e9 usare le list comprehension? Le list comprehension sono utili e versatili, e permettono, in molti casi, di sostituire i classici cicli con una sintassi pi\u00f9 snella. Tuttavia, bisogna fare attenzione a non abusare di questo strumento: infatti, facendolo si rischia di complicare inutilmente il nostro programma, rendendolo poco leggibile e manutenibile. Come regola generale, quindi, ricordiamo il principio del rasoio di Occam: anche se \u00e8 facile innamorarsi delle list comprehension, \u00e8 bene ricordarsi che anche i cicli sono leciti e funzionali , per cui non \u00e8 sempre necessario trovare a tutti i costi una soluzione usando una list comprehension.","title":"3.2.1 - Forma estesa con if-else"},{"location":"material/01_python/03_data_structures/lecture/#33-le-assignment-expressions","text":"Come apparso dalla trattazione, le list comprehension sono state pensate per approcci puramente iterativi. Di conseguenza, risulta complesso implementare forme di ricorsione. Per ovviare a questo inconveniente, Python introduce, a partire dalla versione 3.8, le assignment expression . Da un punto di vista \"formale\", un'assignment expression permette di assegnare e restituire un valore all'interno di un'unica istruzione mediante il cosiddetto walrus operator : >>> print ( enjoy := True ) True Vediamo come utilizzare questo concetto per combinare ricorsione e list comprehension. Definiamo i valori di \\(F_0\\) ed \\(F_1\\) per la sequenza di Fibonacci : >>> fib = [ 0 , 1 ] Vediamo cosa succede se proviamo ad usare una assignment expression in modo da restituire una lista che abbia come primo elemento il secondo della precedente (ovvero 1 ), e come secondo la somma di tutti gli elementi della lista (ovvero 0 + 1 ): >>> ( fib := [ fib [ 1 ], fib [ 0 ] + fib [ 1 ]]) >>> fib [ 1 , 1 ] Notiamo che l'operazione ha modificato il valore della lista fib ! A noi, per\u00f2, interessa soltanto la somma degli elementi precedenti della lista (e quindi il secondo valore ottenuto). Per isolarlo, possiamo adoperare l'operatore booleano and : >>> fib = [ 0 , 1 ] >>> ( fib := [ fib [ 1 ], fib [ 0 ] + fib [ 1 ]]) and fib [ 1 ] 1 Proviamo a combinare i due passaggi precedenti, ed usare una list comprehension per concatenare i risultati ottenuti per i numeri che vanno fino ad \\(F_9\\) : >>> fib = [ 0 , 1 ] >>> fib += [( fib := [ fib [ 1 ], fib [ 0 ] + fib [ 1 ]]) and fib [ 1 ] for i in range ( 10 )] >>> fib [ 0 , 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , 89 ]","title":"3.3 - Le assignment expressions"},{"location":"material/01_python/03_data_structures/lecture/#34-tuple","text":"Le tuple permettono di rappresentano un insieme di valori eterogenei separadoli da una virgola. Ad esempio: tupla = ( 'hello' , 'world' , 12 ) Un po' come avviene per le liste, uno dei valori della tupla pu\u00f2 a sua volta essere un'altra tupla. Ad esempio: tupla = ( 'hello' , 'world' , ( 1 , 2 )) A differenza di una lista, per\u00f2, le tuple sono immutabili . Ci\u00f2 non implica per\u00f2 che non possano contenere al loro interno oggetti mutabili. Guardiamo il seguente esempio: tupla = ( 'hello' , 'world' , [ 1 , 2 , 3 ]) La tupla avr\u00e0 al suo interno due stringhe (immutabili) ed una lista (mutabile). Proviamo a modificare la lista: tupla [ 2 ] = [ 2 , 2 , 3 ] Apparir\u00e0 un errore simile a questo: Traceback ( most recent call last ) : File \"<stdin>\" , line 1 , in <module> TypeError: 'tuple' object does not support item assignment Come prevedibile, abbiamo avuto un errore di assegnazione legato all'immutabilit\u00e0 della tupla. Proviamo adesso per\u00f2 a modificare direttamente la lista : tupla [ 2 ][ 0 ] = 2 # La tupla sar\u00e0 ('hello', 'world', [2, 2, 3]) L'operazione \u00e8 evidentemente ammissibile, ed il risultato \u00e8 stato proprio quello atteso. Tuple e liste Ad un attento osservatore non sfuggir\u00e0 come tuple e liste siano simili dal punto di vista sintattico, e differiscano in buona sostanza per la mutabilit\u00e0. Da qui discende che le tuple sono estremamente efficaci nel caso si debba esclusivamente accedere agli elementi contenuti, mentre le liste devono essere usate quando \u00e8 anche necessario modificare all'occorrenza detti elementi.","title":"3.4 - Tuple"},{"location":"material/01_python/03_data_structures/lecture/#35-set","text":"Anche i set sono molto simili alle liste dal punto di vista sintattico, ma offrono una significativa differenza: infatti, in un set non possono esserci elementi ripetuti . Nota Notiamo un'evidente analogia con il concetto matematico di insieme. La sintassi da usare per creare un set \u00e8 la seguente. insieme = { 1 , \"stringa\" , 2 } Il set ammette al suo interno dati eterogenei, tuttavia non pu\u00f2 contenere al suo interno delle liste o dei dizionari. Questo \u00e8 legato al fatto che i set (cos\u00ec come gli stessi dizionari) sono delle hash table , e quindi sfruttano il concetto di hash per rappresentare i dati contenuti in maniera compatta ed efficiente. Il fatto che le liste ed i dizionari non possano essere rappresentati in questo modo li esclude in automatico dall'includibilit\u00e0 all'interno di un set. Un'altra considerazione da fare \u00e8 che il set non \u00e8 ordinato : ci\u00f2 rende impossibile accedere ad (e modificare ) un elemento del set mediante il suo indice, come succedeva per liste e tuple. Suggerimento I set possono essere usati per isolare gli elementi univoci presenti in una lista. Per farlo, basta convertire la lista in set: l = [ 1 , 2 , 2 , 3 ] # La lista sar\u00e0 [1, 2, 2, 3] s = set ( l ) # Il set sar\u00e0 [1, 2, 3]","title":"3.5 - Set"},{"location":"material/01_python/03_data_structures/lecture/#36-dizionari","text":"Il quarto ed ultimo tipo di contenitore per sequenze di dati \u00e8 il dizionario , presente anche in altri linguaggi di programmazione con il nome di array associativo o hash map . L'elemento base di un dizionario \u00e8 la coppia chiave - valore , nella quale un certo valore (di qualsiasi tipo) \u00e8 associato ad una determinata chiave (di tipo immutabile). I dizionari hanno diverse caratteristiche comuni ai set, dall'inutilizzabilit\u00e0 delle liste come chiavi al fatto di non permettere chiavi ripetute. Inoltre, le coppie chiave - valore sono accedute, per l'appunto, per chiave, e non in base all'ordine delle coppie. Nota Una differenza tra set e dizionari sta nel fatto che questi ultimi sono ordinati a partire da Python 3.7. Per creare un dizionario, possiamo usare una sintassi simile a quella usata per i set. Ad esempio, per creare un dizionario vuoto: dizionario = {} Possiamo quindi inserire delle coppie chiave - valore in questo modo: dizionario [ 'k' ] = 'v' dizionario [ 1 ] = 'n' # Il dizionario sar\u00e0 {'k': 'v', 1: 'n'} Per accedere al valore associato ad una determinata chiave: dizionario [ 1 ] # Il valore restituito sar\u00e0 'n'","title":"3.6 - Dizionari"},{"location":"material/01_python/03_data_structures/lecture/#361-chiavi-e-valori","text":"E' possibile recuperare la lista di tutte le chiavi presenti in un dizionario usando il metodo keys() , che restituisce un oggetto di tipo dict_keys , a sua volta convertibile in lista: chiavi = dizionario . keys () # Restituisce dict_keys(['k', 1]), che non \u00e8 una lista! print ( list ( chiavi )) # Restituisce ['k', 1], che \u00e8 una lista! In modo analogo, si pu\u00f2 accedere a tutti i valori presenti nel dizionario mediante il metodo values() , che restituir\u00e0 un oggetto di tipo dict_values , da convertire anch'esso in lista: valori = dizionario . values () # Restituisce dict_values(['v', 'n']), che non \u00e8 una lista! print ( list ( valori )) # Restituisce ['v', 'n'], che \u00e8 una lista! Possiamo accedere anche a tutte le coppie chiave - valore mediante il metodo items() , che ci restituisce un oggetto di tipo dict_items , il quale pu\u00f2 essere convertito in una lista di tuple: coppie = dizionario . items () # Restituisce dict_items([('k', 'v'), (1, 'n')]) print ( list ( coppie )) # Restituisce una lista di tuple","title":"3.6.1 - Chiavi e valori"},{"location":"material/01_python/03_data_structures/lecture/#362-creazione-di-un-dizionario-non-vuoto","text":"Abbiamo diversi modi per creare un dizionario non vuoto.","title":"3.6.2 - Creazione di un dizionario (non vuoto)"},{"location":"material/01_python/03_data_structures/lecture/#3621-uso-delloperatore","text":"Il pi\u00f9 semplice, che \u00e8 quello che useremo pi\u00f9 spesso, \u00e8 quello di dichiarare nell'operatore {} le coppie chiave - valore iniziali: >>> dizionario = { 'k1' : 1 , 'k2' : 2 } >>> dizionario { 'k1' : 1 , 'k2' : 2 }","title":"3.6.2.1 - Uso dell'operatore {}"},{"location":"material/01_python/03_data_structures/lecture/#3622-uso-del-costruttore-dict","text":"Un altro modo \u00e8 usare il metodo costruttore dict() : dizionario = dict ( k1 = 1 , k2 = 2 ) # Il dizionario sar\u00e0 {'k1': 1, 'k2': 2}","title":"3.6.2.2 - Uso del costruttore dict()"},{"location":"material/01_python/03_data_structures/lecture/#3623-uso-della-funzione-zip","text":"Possiamo poi usare la funzione zip per creare un dizionario a partire da due liste: chiavi = [ 'k1' , 'k2' ] valori = [ 1 , 2 ] dizionario = dict ( zip ( chiavi , valori ))","title":"3.6.2.3 - Uso della funzione zip"},{"location":"material/01_python/03_data_structures/lecture/#3624-dict-comprehension","text":"Un modo per ottenere un dizionario a partire da un altro oggetto iterabile \u00e8 la dict comprehension , che ha una forma del tipo: output = { chiave : valore for valore in iterabile } Possiamo ad esempio creare un dizionario contenente come chiave i numeri da 1 a 9, e come valori corrispondenti i quadrati degli stessi: quadrati = { str ( i ): i ** 2 for i in range ( 1 , 10 )} print ( quadrati ) # Risultato: {'1': 1, '2': 4, '3': 9, '4': 16, '5': 25, '6': 36, '7': 49, '8': 64, '9': 81}","title":"3.6.2.4 - Dict comprehension"},{"location":"material/01_python/04_classes/exercises/","text":"E4 - Programmazione orientata agli oggetti in Python \u00b6 E4.1 \u00b6 Scrivere una classe Persona applicando i concetti visti durante la lezione. S4.1 - Soluzione \u00b6 Scriviamo la classe Persona come segue: class Persona (): def __init__ ( self , nome , cognome , eta ): self . nome = nome self . cognome = cognome self . eta = eta @property def nome ( self ): return self . __nome @nome . setter def nome ( self , value ): if len ( value ) < 2 : raise ValueError ( 'La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri.' ) else : self . __nome = value @property def cognome ( self ): return self . __cognome @cognome . setter def cognome ( self , value ): if len ( value ) < 2 : raise ValueError ( 'La lunghezza del cognome non pu\u00f2 essere inferiore a due caratteri.' ) else : self . __cognome = value @property def eta ( self ): return self . __eta @eta . setter def eta ( self , value ): if value < 0 : raise ValueError ( \"L'et\u00e0 non pu\u00f2 essere negativa.\" ) else : self . __eta = value Alcune note: abbiamo riscritto la classe Persona in modo da trasformare tutti gli attributi in propriet\u00e0; per ogni propriet\u00e0, abbiamo specificato un getter, che restituisce il valore della stessa; oltre al getter, \u00e8 stato specificato un setter, nel quale vi \u00e8 anche una forma di validazione del valore passato in input. Vediamo come usare la nostra nuova classe: >>> draco = Persona ( 'Draco' , 'Malfoy' , 12 ) >>> print ( draco . nome ) 'Draco' >>> print ( draco . eta ) 12 >>> hermione = Persona ( '' , 'Granger' , 18 ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"<stdin>\" , line 3 , in __init__ File \"<stdin>\" , line 12 , in nome ValueError : La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri . Notiamo che, dal punto di vista dello script che richiama la classe, non ci sono differenze di sorta; tuttavia, la logica di validazione ci permette di evitare errori e situazioni incoerenti, ed \u00e8 inoltre possibile sfruttare le propriet\u00e0 per accedere agli attributi privati della classe. E4.2 \u00b6 Creiamo due classi: la prima \u00e8 la classe Quadrato , che modella tutti i quadrati; la seconda \u00e8 la classe Cerchio , che modella tutti i cerchi. Entrambe devono discendere da una classe base chiamata Figura . S4.2 - Soluzione \u00b6 from abc import ABC , abstractmethod from math import pi class Figura ( ABC ): @property def perimetro ( self ): return self . __perimetro @property def area ( self ): return self . __area @abstractmethod def perimetro ( self ): pass @abstractmethod def area ( self ): pass class Quadrato ( Figura ): def __init__ ( self , lato ): self . lato = lato @property def lato ( self ): return self . __lato @lato . setter def lato ( self , value ): self . __lato = value def perimetro ( self ): return self . lato * 4 def area ( self ): return self . lato ** 2 class Cerchio ( Figura ): def __init__ ( self , raggio ): self . raggio = raggio @property def raggio ( self ): return self . __raggio @raggio . setter def raggio ( self , value ): self . __raggio = value def perimetro ( self ): return 2 * pi * self . raggio def area ( self ): return pi * ( self . raggio ** 2 ) # Esempio di uso q = Quadrato ( 5 ) print ( 'Lato: {} - Perimetro: {} - Area: {} ' . format ( q . lato , q . perimetro (), q . area ())) c = Cerchio ( 5 ) print ( 'Raggio: {} - Perimetro: {} - Area: {} ' . format ( c . raggio , c . perimetro (), c . area ()))","title":"Esercizi"},{"location":"material/01_python/04_classes/exercises/#e4-programmazione-orientata-agli-oggetti-in-python","text":"","title":"E4 - Programmazione orientata agli oggetti in Python"},{"location":"material/01_python/04_classes/exercises/#e41","text":"Scrivere una classe Persona applicando i concetti visti durante la lezione.","title":"E4.1"},{"location":"material/01_python/04_classes/exercises/#s41-soluzione","text":"Scriviamo la classe Persona come segue: class Persona (): def __init__ ( self , nome , cognome , eta ): self . nome = nome self . cognome = cognome self . eta = eta @property def nome ( self ): return self . __nome @nome . setter def nome ( self , value ): if len ( value ) < 2 : raise ValueError ( 'La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri.' ) else : self . __nome = value @property def cognome ( self ): return self . __cognome @cognome . setter def cognome ( self , value ): if len ( value ) < 2 : raise ValueError ( 'La lunghezza del cognome non pu\u00f2 essere inferiore a due caratteri.' ) else : self . __cognome = value @property def eta ( self ): return self . __eta @eta . setter def eta ( self , value ): if value < 0 : raise ValueError ( \"L'et\u00e0 non pu\u00f2 essere negativa.\" ) else : self . __eta = value Alcune note: abbiamo riscritto la classe Persona in modo da trasformare tutti gli attributi in propriet\u00e0; per ogni propriet\u00e0, abbiamo specificato un getter, che restituisce il valore della stessa; oltre al getter, \u00e8 stato specificato un setter, nel quale vi \u00e8 anche una forma di validazione del valore passato in input. Vediamo come usare la nostra nuova classe: >>> draco = Persona ( 'Draco' , 'Malfoy' , 12 ) >>> print ( draco . nome ) 'Draco' >>> print ( draco . eta ) 12 >>> hermione = Persona ( '' , 'Granger' , 18 ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"<stdin>\" , line 3 , in __init__ File \"<stdin>\" , line 12 , in nome ValueError : La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri . Notiamo che, dal punto di vista dello script che richiama la classe, non ci sono differenze di sorta; tuttavia, la logica di validazione ci permette di evitare errori e situazioni incoerenti, ed \u00e8 inoltre possibile sfruttare le propriet\u00e0 per accedere agli attributi privati della classe.","title":"S4.1 - Soluzione"},{"location":"material/01_python/04_classes/exercises/#e42","text":"Creiamo due classi: la prima \u00e8 la classe Quadrato , che modella tutti i quadrati; la seconda \u00e8 la classe Cerchio , che modella tutti i cerchi. Entrambe devono discendere da una classe base chiamata Figura .","title":"E4.2"},{"location":"material/01_python/04_classes/exercises/#s42-soluzione","text":"from abc import ABC , abstractmethod from math import pi class Figura ( ABC ): @property def perimetro ( self ): return self . __perimetro @property def area ( self ): return self . __area @abstractmethod def perimetro ( self ): pass @abstractmethod def area ( self ): pass class Quadrato ( Figura ): def __init__ ( self , lato ): self . lato = lato @property def lato ( self ): return self . __lato @lato . setter def lato ( self , value ): self . __lato = value def perimetro ( self ): return self . lato * 4 def area ( self ): return self . lato ** 2 class Cerchio ( Figura ): def __init__ ( self , raggio ): self . raggio = raggio @property def raggio ( self ): return self . __raggio @raggio . setter def raggio ( self , value ): self . __raggio = value def perimetro ( self ): return 2 * pi * self . raggio def area ( self ): return pi * ( self . raggio ** 2 ) # Esempio di uso q = Quadrato ( 5 ) print ( 'Lato: {} - Perimetro: {} - Area: {} ' . format ( q . lato , q . perimetro (), q . area ())) c = Cerchio ( 5 ) print ( 'Raggio: {} - Perimetro: {} - Area: {} ' . format ( c . raggio , c . perimetro (), c . area ()))","title":"S4.2 - Soluzione"},{"location":"material/01_python/04_classes/lecture/","text":"4 - Programmazione orientata agli oggetti in Python \u00b6 Python offre un esteso supporto alla programmazione orientata agli oggetti. Prima di proseguire, per\u00f2, \u00e8 opportuno introdurre brevemente questo concetto. 4.1 - La programmazione orientata agli oggetti \u00b6 Quello della programmazione orientata agli oggetti (OOP) \u00e8 un paradigma di programmazione che permette di creare nuovi tipi definiti dall'utente, da intendersi come complementari ai tipi definiti dal linguaggio di programmazione. In tal senso, la OOP sposta il focus dalle funzioni , centrali nei linguaggi come il C e nel paradigma procedurale, ai dati . In tal senso, si arriva a dire che nella OOP tutto \u00e8 un oggetto . 4.1.1 - Classi \u00b6 Una classe \u00e8 un prototipo per un determinato tipo di dati definito dall'utente. Ad esempio: la classe Studente rappresenta tutte le propriet\u00e0 e le azioni associate ad uno studente; la classe Auto rappresenta tutte le propriet\u00e0 e le azioni associate ad un'auto; la classe Motore definisce i comportamenti dei motori; e via discorrendo. In generale, quindi, pu\u00f2 esistere una classe per ogni tipologia di oggetti presenti nel mondo, sia esso reale o informatico. Importante \u00e8 non confondere la classe con il singolo oggetto , chiamato istanza . Ad esempio: lo studente Angelo Cardellicchio \u00e8 un'istanza della classe Studente ; l'auto Opel Corsa targata AB 123 CD \u00e8 un'istanza della classe Auto ; l'auto Hyundai Tucson CD 321 AB \u00e8 un'istanza della classe Auto ; l'auto Opel Corsa targata AA 123 CC \u00e8 un'altra istanza della classe Auto . 4.1.1.1 - Metodi ed attributi \u00b6 Ogni classe ha dei metodi , che caratterizzano delle azioni che \u00e8 possibile effettuare su ogni istanza della classe, e degli attributi , ovvero delle caratteristiche dell'istanza. In particolare, ogni nuovo tipo, chiamato classe , avr\u00e0 opportuni attributi e metodi , ognuno dei quali accessibile dall'esterno mediante opportuni modificatori . Ad esempio, l'auto Opel Corsa targata AB 123 CD ha una casa costruttrice (Opel), un modello (Corsa), una targa (AB 123 CD), una cilindrata, e via dicendo. 4.2 - Classi in Python \u00b6 Per definire una classe, dovremo usare la parola chiave class : class NomeClasse ( ClasseBase ): # Attributi e metodi di classe... Con la sintassi precedente, abbiamo creato una classe chiamata NomeClasse discendente da una classe base ( ClasseBase ). 4.3 - Il metodo __init__ \u00b6 La maggior parte dei linguaggi di programmazione utilizza il concetto di costruttore per creare un'istanza di una classe. Il Python, tuttavia, non prevede l'utilizzo di un costruttore vero e proprio, quanto piuttosto di un metodo di inizializzazione dei singoli attributi dell'istanza. Da qui deriva il nome del metodo, ovvero __init__ : class NomeClasse ( ClasseBase ): def __init__ ( self , * args , ** kwargs ): # ... self . arg_1 = arg_1 # ... Unpacking Con la sintassi *args e **kwargs vogliamo rappresentare l'azione di unpacking di (rispettivamente) una lista ed un dizionario, mediante la quale stiamo passando tutti i valori contenuti all'interno della sequenza. Occorre prestare particolare attenzione all'uso della keyword self , che permette di riferirsi alla specifica istanza di una classe (per chi ha familiarit\u00e0 con i linguaggi come il C++, \u00e8 concettualmente simile alla parola chiave this ). Ad esempio: class Persona ( object ): def __init__ ( self , nome , cognome , eta = 18 ): self . nome = nome self . _cognome = cognome self . __eta = eta Questo snippet ci permette di evidenziare quattro punti: la classe generica object , da cui derivano tutte le classi Python (ma la cui dichiarazione pu\u00f2 comunque essere omessa); il funzionamento della parola chiave self , che permette di associare agli attributi della singola istanza un determinato valore; la possibilit\u00e0 di inserire tra i parametri dei valori opzionali e di default (in questo caso eta , che di default vale 18 ); la presenza di uno o due simboli _ ( underscore ) davanti ad alcuni attributi. Approfondiamo brevemente il punto 4. 4.4 - Modificatori di accesso \u00b6 Python prevede l'uso di modificatori di accesso ai dati; nello specifico, troviamo i classici public , protected e private . Tuttavia, a differenza di altri linguaggi, per distinguere tra i tre modificatori di accesso si utilizzano uno o due underscore come suffisso al nome dell'attributo; in particolare, usare un underscore singolo indica un attributo protected, mentre un underscore doppio indica un attributo private . Nel nostro caso: class Persona ( object ): def __init__ ( self , nome , cognome , eta = 18 ): self . nome = nome # Membro \"public\" self . _cognome = cognome # Membro \"protected\" self . __eta = eta # Membro \"private\" Attenzione Nonostante il modificatore di accesso, \u00e8 possibile accedere ai membri protetti dall'esterno della classe. Infatti: >>> p = Persona ( 'Jax' , 'Teller' ) >>> print ( p . nome ) 'Jax' >>> print ( p . _cognome ) 'Teller' Questo non vale per gli attributi privati: >>> try : >>> print ( p . __eta ) >>> except AttributeError : >>> print ( 'Et\u00e0 \u00e8 privato!' ) Et\u00e0 \u00e8 privato ! Questa sintassi pu\u00f2 ovviamente essere utilizzata per definire dei metodi protetti o privati. Suggerimento La sintassi che abbiamo mostrato nello snippet precedente \u00e8 relativa alla gestione delle eccezioni . 4.5 - Metodi \u00b6 La sintassi per definire il metodo di una classe \u00e8 analoga a quella usata per definire una funzione. def metodo ( self , * args , ** kwargs ): pass Esiste tuttavia una differenza fondamentale: infatti, il primo attributo di un metodo appartenente ad una classe \u00e8 sempre un riferimento all'istanza tramite la parola chiave self . Tale riferimento non va specificato quando il metodo viene chiamato dall'esterno: # ... p = Persona () # p \u00e8 un'istanza di Persona p . metodo ( parametro ) # richiamo il metodo dall'istanza # ... Nel codice precedente, abbiamo usato l'operatore . per accedere a metodo() definito all'interno della classe Persona . Approfondiamo adesso alcune particolari tipologie di metodi, ottenibili usando determinati decorator (cfr. appendice B). 4.5.1 - Metodi di classe \u00b6 Il decorator @classmethod ci permette di definire i cosiddetti metodi di classe : @classmethod def builder_stringa ( cls , stringa : str ): nome , cognome , eta = stringa . split ( ' ' ) return Persona ( nome , cognome , eta ) A differenza dei metodi standard, i metodi di classe hanno un riferimento alla classe ( cls ) e non all'istanza ( self ). Questo significa che sono dei metodi che si applicano all'intera classe , e non alla singola istanza. Un tipico esempio di utilizzo di un metodo di classe \u00e8 mostrato nello snippet precedente, nel quale stiamo creando un oggetto di classe Persona a partire da una stringa. Curiosit\u00e0 Il metodo precedente \u00e8, di fatto, un'implementazione del design pattern Builder. Per richiamare un metodo di classe occorre riferirsi al nome della classe stessa, e non ad una singola istanza: >>> persona = Persona . builder_stringa ( 'Bobby Munson 58' ) >>> print ( \" {} {} \" . format ( persona . nome , persona . _cognome )) Bobby Munson 4.5.2 - Metodi statici \u00b6 Mediante il decoratore @staticmethod possiamo definire un metodo statico . In Python il funzionamento di un metodo di questo tipo \u00e8 riassumibile in un comportamento assimilabile ad una funzione \"semplice\", definita per\u00f2 all'interno della classe, e richiamabile su istanze della stessa. Ad esempio: @staticmethod def nome_valido ( nome ): if len ( nome ) < 2 : return False else : return True Questo metodo \u00e8 quindi liberamente richiamabile mediante l'operatore . da una singola istanza: >>> print ( Persona . nome_valido ( 'Li' )) True Un'altra possibilit\u00e0 \u00e8 richiamarlo sulla classe stessa: >>> print ( Persona . nome_valido ( 'X' )) False 4.5.3 - Metodi astratti \u00b6 Possiamo definire dei metodi astratti (cfr. Appendice C) mediante il decorator @abstractmethod . Per farlo, la nostra classe deve discendere dalla classe ABC (acronimo che sta per Abstract Base Class ), contenuta nel package abc : from abc import ABC class ClasseBase ( ABC ): # ... @abstractmethod def metodo_da_sovrascrivere ( self ): pass I metodi contrassegnati con il decorator @abstractmethod dovranno essere implementati nelle classi derivate (in altre parole, dovremo farne l' override ): class ClasseDerivata ( ClasseBase ): # ... def metodo_da_sovrascrivere ( self ): # ... 4.6 - Le propriet\u00e0 \u00b6 In molti linguaggi di programmazione si usano tradizionalmente i metodi accessori ( getter ) e modificatori ( setter ) per accedere agli attributi delle istanze di una classe. Python non vieta di farlo: ad esempio, possiamo scrivere un metodo get_nome(self) per accedere al nome di una persona, ed un metodo set_nome(self, nome) per impostare detta propriet\u00e0. Tuttavia, \u00e8 possibile usare una sintassi pi\u00f9 compatta (e, in definitiva, maggiormente pythonic ) mediante il decorator @property , che rappresenta una funzione a quattro parametri: property ( fget = None , fset = None , fdel = None , doc = None ) In particolare: fget \u00e8 la funzione usata per recuperare il valore dell'attributo; fset \u00e8 la funzione usata per impostare il valore dell'attributo; fdel \u00e8 la funzione per rimuovere l'attributo; doc \u00e8 la funzione per documentare e descrivere l'attributo. Grazie a property , potremo seguire le \"best practice\" della OOP, rendendo privati gli attributi della classe ed accedendovi mediante opportuni metodi. class Persona (): def __init__ ( self , nome , cognome , eta ): self . nome = nome self . cognome = cognome self . eta = eta @property def nome ( self ): return self . __nome @nome . setter def nome ( self , value ): if len ( value ) < 2 : raise ValueError ( 'La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri.' ) else : self . __nome = value @property def cognome ( self ): return self . __cognome @cognome . setter def cognome ( self , value ): if len ( value ) < 2 : raise ValueError ( 'La lunghezza del cognome non pu\u00f2 essere inferiore a due caratteri.' ) else : self . __cognome = value @property def eta ( self ): return self . __eta @eta . setter def eta ( self , value ): if value < 0 : raise ValueError ( \"L'et\u00e0 non pu\u00f2 essere negativa.\" ) else : self . __eta = value Alcune note: abbiamo riscritto la classe Persona in modo da trasformare tutti gli attributi in propriet\u00e0; per ogni propriet\u00e0, abbiamo specificato un getter, che restituisce il valore della stessa; oltre al getter, \u00e8 stato specificato un setter, nel quale vi \u00e8 anche una forma di validazione del valore passato in input. Vediamo come usare la nostra nuova classe: >>> draco = Persona ( 'Draco' , 'Malfoy' , 12 ) >>> print ( draco . nome ) 'Draco' >>> print ( draco . eta ) 12 >>> hermione = Persona ( '' , 'Granger' , 18 ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"<stdin>\" , line 3 , in __init__ File \"<stdin>\" , line 12 , in nome ValueError : La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri . Notiamo che, dal punto di vista dello script che richiama la classe, non ci sono differenze di sorta; tuttavia, la logica di validazione ci permette di evitare errori e situazioni incoerenti, ed \u00e8 inoltre possibile sfruttare le propriet\u00e0 per accedere agli attributi privati della classe.","title":"Dispense"},{"location":"material/01_python/04_classes/lecture/#4-programmazione-orientata-agli-oggetti-in-python","text":"Python offre un esteso supporto alla programmazione orientata agli oggetti. Prima di proseguire, per\u00f2, \u00e8 opportuno introdurre brevemente questo concetto.","title":"4 - Programmazione orientata agli oggetti in Python"},{"location":"material/01_python/04_classes/lecture/#41-la-programmazione-orientata-agli-oggetti","text":"Quello della programmazione orientata agli oggetti (OOP) \u00e8 un paradigma di programmazione che permette di creare nuovi tipi definiti dall'utente, da intendersi come complementari ai tipi definiti dal linguaggio di programmazione. In tal senso, la OOP sposta il focus dalle funzioni , centrali nei linguaggi come il C e nel paradigma procedurale, ai dati . In tal senso, si arriva a dire che nella OOP tutto \u00e8 un oggetto .","title":"4.1 - La programmazione orientata agli oggetti"},{"location":"material/01_python/04_classes/lecture/#411-classi","text":"Una classe \u00e8 un prototipo per un determinato tipo di dati definito dall'utente. Ad esempio: la classe Studente rappresenta tutte le propriet\u00e0 e le azioni associate ad uno studente; la classe Auto rappresenta tutte le propriet\u00e0 e le azioni associate ad un'auto; la classe Motore definisce i comportamenti dei motori; e via discorrendo. In generale, quindi, pu\u00f2 esistere una classe per ogni tipologia di oggetti presenti nel mondo, sia esso reale o informatico. Importante \u00e8 non confondere la classe con il singolo oggetto , chiamato istanza . Ad esempio: lo studente Angelo Cardellicchio \u00e8 un'istanza della classe Studente ; l'auto Opel Corsa targata AB 123 CD \u00e8 un'istanza della classe Auto ; l'auto Hyundai Tucson CD 321 AB \u00e8 un'istanza della classe Auto ; l'auto Opel Corsa targata AA 123 CC \u00e8 un'altra istanza della classe Auto .","title":"4.1.1 - Classi"},{"location":"material/01_python/04_classes/lecture/#4111-metodi-ed-attributi","text":"Ogni classe ha dei metodi , che caratterizzano delle azioni che \u00e8 possibile effettuare su ogni istanza della classe, e degli attributi , ovvero delle caratteristiche dell'istanza. In particolare, ogni nuovo tipo, chiamato classe , avr\u00e0 opportuni attributi e metodi , ognuno dei quali accessibile dall'esterno mediante opportuni modificatori . Ad esempio, l'auto Opel Corsa targata AB 123 CD ha una casa costruttrice (Opel), un modello (Corsa), una targa (AB 123 CD), una cilindrata, e via dicendo.","title":"4.1.1.1 - Metodi ed attributi"},{"location":"material/01_python/04_classes/lecture/#42-classi-in-python","text":"Per definire una classe, dovremo usare la parola chiave class : class NomeClasse ( ClasseBase ): # Attributi e metodi di classe... Con la sintassi precedente, abbiamo creato una classe chiamata NomeClasse discendente da una classe base ( ClasseBase ).","title":"4.2 - Classi in Python"},{"location":"material/01_python/04_classes/lecture/#43-il-metodo-__init__","text":"La maggior parte dei linguaggi di programmazione utilizza il concetto di costruttore per creare un'istanza di una classe. Il Python, tuttavia, non prevede l'utilizzo di un costruttore vero e proprio, quanto piuttosto di un metodo di inizializzazione dei singoli attributi dell'istanza. Da qui deriva il nome del metodo, ovvero __init__ : class NomeClasse ( ClasseBase ): def __init__ ( self , * args , ** kwargs ): # ... self . arg_1 = arg_1 # ... Unpacking Con la sintassi *args e **kwargs vogliamo rappresentare l'azione di unpacking di (rispettivamente) una lista ed un dizionario, mediante la quale stiamo passando tutti i valori contenuti all'interno della sequenza. Occorre prestare particolare attenzione all'uso della keyword self , che permette di riferirsi alla specifica istanza di una classe (per chi ha familiarit\u00e0 con i linguaggi come il C++, \u00e8 concettualmente simile alla parola chiave this ). Ad esempio: class Persona ( object ): def __init__ ( self , nome , cognome , eta = 18 ): self . nome = nome self . _cognome = cognome self . __eta = eta Questo snippet ci permette di evidenziare quattro punti: la classe generica object , da cui derivano tutte le classi Python (ma la cui dichiarazione pu\u00f2 comunque essere omessa); il funzionamento della parola chiave self , che permette di associare agli attributi della singola istanza un determinato valore; la possibilit\u00e0 di inserire tra i parametri dei valori opzionali e di default (in questo caso eta , che di default vale 18 ); la presenza di uno o due simboli _ ( underscore ) davanti ad alcuni attributi. Approfondiamo brevemente il punto 4.","title":"4.3 - Il metodo __init__"},{"location":"material/01_python/04_classes/lecture/#44-modificatori-di-accesso","text":"Python prevede l'uso di modificatori di accesso ai dati; nello specifico, troviamo i classici public , protected e private . Tuttavia, a differenza di altri linguaggi, per distinguere tra i tre modificatori di accesso si utilizzano uno o due underscore come suffisso al nome dell'attributo; in particolare, usare un underscore singolo indica un attributo protected, mentre un underscore doppio indica un attributo private . Nel nostro caso: class Persona ( object ): def __init__ ( self , nome , cognome , eta = 18 ): self . nome = nome # Membro \"public\" self . _cognome = cognome # Membro \"protected\" self . __eta = eta # Membro \"private\" Attenzione Nonostante il modificatore di accesso, \u00e8 possibile accedere ai membri protetti dall'esterno della classe. Infatti: >>> p = Persona ( 'Jax' , 'Teller' ) >>> print ( p . nome ) 'Jax' >>> print ( p . _cognome ) 'Teller' Questo non vale per gli attributi privati: >>> try : >>> print ( p . __eta ) >>> except AttributeError : >>> print ( 'Et\u00e0 \u00e8 privato!' ) Et\u00e0 \u00e8 privato ! Questa sintassi pu\u00f2 ovviamente essere utilizzata per definire dei metodi protetti o privati. Suggerimento La sintassi che abbiamo mostrato nello snippet precedente \u00e8 relativa alla gestione delle eccezioni .","title":"4.4 - Modificatori di accesso"},{"location":"material/01_python/04_classes/lecture/#45-metodi","text":"La sintassi per definire il metodo di una classe \u00e8 analoga a quella usata per definire una funzione. def metodo ( self , * args , ** kwargs ): pass Esiste tuttavia una differenza fondamentale: infatti, il primo attributo di un metodo appartenente ad una classe \u00e8 sempre un riferimento all'istanza tramite la parola chiave self . Tale riferimento non va specificato quando il metodo viene chiamato dall'esterno: # ... p = Persona () # p \u00e8 un'istanza di Persona p . metodo ( parametro ) # richiamo il metodo dall'istanza # ... Nel codice precedente, abbiamo usato l'operatore . per accedere a metodo() definito all'interno della classe Persona . Approfondiamo adesso alcune particolari tipologie di metodi, ottenibili usando determinati decorator (cfr. appendice B).","title":"4.5 - Metodi"},{"location":"material/01_python/04_classes/lecture/#451-metodi-di-classe","text":"Il decorator @classmethod ci permette di definire i cosiddetti metodi di classe : @classmethod def builder_stringa ( cls , stringa : str ): nome , cognome , eta = stringa . split ( ' ' ) return Persona ( nome , cognome , eta ) A differenza dei metodi standard, i metodi di classe hanno un riferimento alla classe ( cls ) e non all'istanza ( self ). Questo significa che sono dei metodi che si applicano all'intera classe , e non alla singola istanza. Un tipico esempio di utilizzo di un metodo di classe \u00e8 mostrato nello snippet precedente, nel quale stiamo creando un oggetto di classe Persona a partire da una stringa. Curiosit\u00e0 Il metodo precedente \u00e8, di fatto, un'implementazione del design pattern Builder. Per richiamare un metodo di classe occorre riferirsi al nome della classe stessa, e non ad una singola istanza: >>> persona = Persona . builder_stringa ( 'Bobby Munson 58' ) >>> print ( \" {} {} \" . format ( persona . nome , persona . _cognome )) Bobby Munson","title":"4.5.1 - Metodi di classe"},{"location":"material/01_python/04_classes/lecture/#452-metodi-statici","text":"Mediante il decoratore @staticmethod possiamo definire un metodo statico . In Python il funzionamento di un metodo di questo tipo \u00e8 riassumibile in un comportamento assimilabile ad una funzione \"semplice\", definita per\u00f2 all'interno della classe, e richiamabile su istanze della stessa. Ad esempio: @staticmethod def nome_valido ( nome ): if len ( nome ) < 2 : return False else : return True Questo metodo \u00e8 quindi liberamente richiamabile mediante l'operatore . da una singola istanza: >>> print ( Persona . nome_valido ( 'Li' )) True Un'altra possibilit\u00e0 \u00e8 richiamarlo sulla classe stessa: >>> print ( Persona . nome_valido ( 'X' )) False","title":"4.5.2 - Metodi statici"},{"location":"material/01_python/04_classes/lecture/#453-metodi-astratti","text":"Possiamo definire dei metodi astratti (cfr. Appendice C) mediante il decorator @abstractmethod . Per farlo, la nostra classe deve discendere dalla classe ABC (acronimo che sta per Abstract Base Class ), contenuta nel package abc : from abc import ABC class ClasseBase ( ABC ): # ... @abstractmethod def metodo_da_sovrascrivere ( self ): pass I metodi contrassegnati con il decorator @abstractmethod dovranno essere implementati nelle classi derivate (in altre parole, dovremo farne l' override ): class ClasseDerivata ( ClasseBase ): # ... def metodo_da_sovrascrivere ( self ): # ...","title":"4.5.3 - Metodi astratti"},{"location":"material/01_python/04_classes/lecture/#46-le-proprieta","text":"In molti linguaggi di programmazione si usano tradizionalmente i metodi accessori ( getter ) e modificatori ( setter ) per accedere agli attributi delle istanze di una classe. Python non vieta di farlo: ad esempio, possiamo scrivere un metodo get_nome(self) per accedere al nome di una persona, ed un metodo set_nome(self, nome) per impostare detta propriet\u00e0. Tuttavia, \u00e8 possibile usare una sintassi pi\u00f9 compatta (e, in definitiva, maggiormente pythonic ) mediante il decorator @property , che rappresenta una funzione a quattro parametri: property ( fget = None , fset = None , fdel = None , doc = None ) In particolare: fget \u00e8 la funzione usata per recuperare il valore dell'attributo; fset \u00e8 la funzione usata per impostare il valore dell'attributo; fdel \u00e8 la funzione per rimuovere l'attributo; doc \u00e8 la funzione per documentare e descrivere l'attributo. Grazie a property , potremo seguire le \"best practice\" della OOP, rendendo privati gli attributi della classe ed accedendovi mediante opportuni metodi. class Persona (): def __init__ ( self , nome , cognome , eta ): self . nome = nome self . cognome = cognome self . eta = eta @property def nome ( self ): return self . __nome @nome . setter def nome ( self , value ): if len ( value ) < 2 : raise ValueError ( 'La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri.' ) else : self . __nome = value @property def cognome ( self ): return self . __cognome @cognome . setter def cognome ( self , value ): if len ( value ) < 2 : raise ValueError ( 'La lunghezza del cognome non pu\u00f2 essere inferiore a due caratteri.' ) else : self . __cognome = value @property def eta ( self ): return self . __eta @eta . setter def eta ( self , value ): if value < 0 : raise ValueError ( \"L'et\u00e0 non pu\u00f2 essere negativa.\" ) else : self . __eta = value Alcune note: abbiamo riscritto la classe Persona in modo da trasformare tutti gli attributi in propriet\u00e0; per ogni propriet\u00e0, abbiamo specificato un getter, che restituisce il valore della stessa; oltre al getter, \u00e8 stato specificato un setter, nel quale vi \u00e8 anche una forma di validazione del valore passato in input. Vediamo come usare la nostra nuova classe: >>> draco = Persona ( 'Draco' , 'Malfoy' , 12 ) >>> print ( draco . nome ) 'Draco' >>> print ( draco . eta ) 12 >>> hermione = Persona ( '' , 'Granger' , 18 ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"<stdin>\" , line 3 , in __init__ File \"<stdin>\" , line 12 , in nome ValueError : La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri . Notiamo che, dal punto di vista dello script che richiama la classe, non ci sono differenze di sorta; tuttavia, la logica di validazione ci permette di evitare errori e situazioni incoerenti, ed \u00e8 inoltre possibile sfruttare le propriet\u00e0 per accedere agli attributi privati della classe.","title":"4.6 - Le propriet\u00e0"},{"location":"material/01_python/05_modules/lecture/","text":"5 - Script e moduli \u00b6 Quando si usa Python la tentazione \u00e8 quella di interagire direttamente con l'interprete, lanciandolo da terminale ed eseguendo di volta in volta le istruzioni necessarie. Ovviamente questo approccio, seppur immediato, presenta diversi svantaggi. Ad esempio: non avremo a disposizione il syntax highlighting offerto da una normale IDE; non potremo recuperare il codice una volta chiuso l'interprete; non potremo n\u00e9 modificare, n\u00e9 verificare facilmente il funzionamento del codice. Appare quindi evidente come usare l'interprete non sia un modo ottimale di sviluppare codice Python. Di conseguenza, sar\u00e0 necessario definire, mediante la nostra IDE di riferimento, dei veri e propri script che saranno salvati sotto forma di file con estensione .py , ognuno dei quali contenenti una serie di istruzioni necessarie all'esecuzione del nostro programma. 5.1 - Il primo script \u00b6 Proviamo quindi a creare il nostro primo script Python. Per farlo, apriamo la nostra IDE di riferimento, come Visual Studio Code, e creiamo un file chiamato main.py , all'interno del quale inseriremo il seguente codice: # main.py def hello_world (): print ( 'Hello, world' ) hello_world () Adesso apriamo un terminale, spostiamoci nella cartella nel quale abbiamo salvato questo script, ed eseguiamolo: cd cartella_dove_risiede_lo_script python main.py Le due istruzioni precedenti: servono a cambiare cartella ( change directory , cd ), spostandoci nella cartella dove risiede lo script; dicono all'interprete Python di lanciare lo script main.py . A schermo, se tutto \u00e8 andato per il verso giusto, apparir\u00e0 la scritta Hello, world : Hello, world 5.2 - I moduli \u00b6 Quando le dimensioni della nostra code base (ovvero la quantit\u00e0 di codice che scriviamo nel nostro programma) iniziano ad essere particolarmente \"ingombranti\", \u00e8 opportuno adottare un approccio modulare , separando in file differenti parti di codice delegate a funzioni eterogenee. Facciamo un esempio. Immaginiamo di voler scrivere un programma che definisca delle funzioni per calcolare l'area delle principali figure geometriche. Modifichiamo quindi il nostro file main.py come segue: # main.py def calcola_area_quadrato ( lato ): return lato * lato def calcola_area_rettangolo ( base , altezza ): return base * altezza def calcola_area_triangolo ( base , altezza ): return ( base * altezza ) / 2 area_quadrato = calcola_area_quadrato ( 4 ) area_rettangolo = calcola_area_rettangolo ( 2 , 3 ) area_triangolo = calcola_area_triangolo ( 2 , 3 ) Immaginiamo di voler quindi aggiungere una funzione di calcolo trigonometrico: # main.py import math def calcola_tangente ( angolo ): return math . sin ( angolo ) / math . cos ( angolo ) tangente_pi = calcola_tangente ( math . pi ) Il codice del nostro file main.py comprender\u00e0 adesso funzioni di tipo geometrico e trigonometrico. Cosa succederebbe se volessimo integrare delle funzioni di calcolo integrale, o di altro tipo? Ovviamente, ci sarebbe da un lato un aumento delle dimensioni della code base, dall'altro un \"mix\" tra funzioni che afferiscono ad ambiti differenti (seppur simili tra loro). Una buona idea sarebbe quindi quella di separare le diverse parti del programma, magari raggruppando le funzioni geometriche nel file geometria.py , le funzioni trigonometriche nel file trigonometria.py , e via discorrendo. Questi file, che conterranno al loro interno prevalentemente funzioni (ma non solo), sono chiamati moduli . Nota La linea che distingue gli script dai moduli \u00e8 molto sottile, e nei fatti \u00e8 facile fare confusione ed utilizzarli in maniera \"intercambiabile\". Sottolineamo per\u00f2 che, idealmente, gli script devono contenere al loro interno soltanto del codice che sar\u00e0 eseguito , mentre i moduli solo del codice che sar\u00e0 invocato da uno o pi\u00f9 script. Interprete e nome di un modulo L'interprete \u00e8 in grado di risalire al nome di un modulo dal nome del file in cui \u00e8 contenuto. Se, ad esempio, definiamo un modulo nel file geometria.py , l'interprete associer\u00e0 a quel modulo il nome geometria . Detto nome \u00e8 inoltre accessibile globalmente e dall'interno del modulo richiamando la variabile globale __name__ . 5.2.1 - I moduli geometria e trigonometria \u00b6 Creiamo adesso il file geometria.py , all'interno del quale \"sposteremo\" le funzioni definite in precedenza per il calcolo geometrico. # geometria.py def calcola_area_quadrato ( lato ): return lato * lato def calcola_area_rettangolo ( base , altezza ): return base * altezza def calcola_area_triangolo ( base , altezza ): return ( base * altezza ) / 2 Analogamente, nel file trigonometria.py andremo a definire la funzione per il calcolo della tangente. # trigonometria.py import math def calcola_tangente ( angolo ): return math . sin ( angolo ) / math . cos ( angolo ) Riscriviamo ora il file main.py : # main.py import geometria import trigonometria if __name__ == \"__main__\" : print ( geometria . calcola_area_quadrato ( 4 )) print ( trigonometria . calcola_tangente ( math . pi )) Possiamo notare due cose. In primis, stiamo richiamando le funzioni calcola_area_quadrato() e calcola_tangente() definite nei moduli geometria e trigonometria , rispettivamente. Questi moduli sono importati all'interno del nostro script mediante la direttiva import . Al rigo 5, la \"strana\" sintassi mostrata serve a dichiarare quello che \u00e8 il main , ovvero il punto di \"accesso\" al codice del nostro programma. Il main \u00e8 normalmente presente in tutti i linguaggi di programmazione, alle volte sotto forme un po' differenti da quella qui mostrata; tuttavia, nel caso di script particolarmente semplici, il main pu\u00f2 essere tranquillamente omesso, in quanto l'interprete riuscir\u00e0 ad eseguirlo in maniera autonoma. Proviamo a lanciare lo script; per farlo, digitiamo l'istruzione python main.py da terminale. A schermo, se tutto \u00e8 andato per il verso giusto, vedremo i valori dell'area di un quadrato e della tangente di \\(\\pi\\) . 5.3 - Usare gli import \u00b6 Relativamente al modulo geometria , abbiamo usato esclusivamente la funzione calcola_area_quadrato() , \"trascurando\" le altre due funzioni comunque presenti nel modulo. In queste circostanze, possiamo usare una versione modificata della direttiva import , che assume la seguente forma: from modulo import funzione_o_classe il che, nel nostro caso specifico, diventa: from geometria import calcola_area_quadrato In questo modo, possiamo importare solamente quello che ci serve, il che risulta particolarmente utile a migliorare l'efficienza del nostro codice; il perch\u00e9 sar\u00e0 chiaro a breve. 5.3.1 - Alias \u00b6 La direttiva import ci permette di definire anche degli alias, particolarmente utili nel caso si usino dei nomi di package complessi. Ad esempio: import trigonometria as tr print ( tr . calcola_tangente ( math . pi )) 5.4 - La funzione dir() \u00b6 La funzione dir() restituisce una lista con tutti i nomi (sia di funzione, sia di classe) definiti da un modulo. Ad esempio: >>> dir ( geometria ) [ '__builtins__' , '__cached__' , '__doc__' , '__file__' , '__loader__' , '__name__' , '__package__' , '__spec__' , 'calcola_area_quadrato' , 'calcola_area_rettangolo' , 'calcola_area_triangolo' ] E' interessante notare come, oltre a funzioni, classi e variabili da noi definite, nel modulo geometria siano automaticamente definite altre variabili, che saranno importate usando import: import geometria if __name__ == \"__main__\" : print ( geometria . __file__ ) print ( geometria . calcola_area_quadrato ( 4 )) Notiamo che saremo in grado di accedere alla variabile __file__ del modulo geometria , che indica il percorso relativo dello stesso all'interno del file system. Ovviamente, questa variabile non \u00e8 quasi mai utile, ma comporta un ulteriore carico sul codice, da cui diventa evidente l'importanza dell'opportuno uso della direttiva from . 5.5 - Moduli della libreria standard \u00b6 Python ha diversi moduli appartenenti ad una libreria standard, i quali sono automaticamente disponibili a valle dell'installazione dell'interprete. Alcuni tra i pi\u00f9 utilizzati sono: sys : \u00e8 il modulo integrato nell'interprete, ed offre diverse utility necessarie al suo funzionamento; os : modulo delegato all'interazione con il sistema operativo su cui gira l'interprete; time : modulo usato per tutte le utility riguardanti il \"cronometraggio\" del tempo di esecuzione di una funzione; datetime : modulo usato per le funzionalit\u00e0 di data ed ora; copy : modulo usato per gestire, tra le altre cose, la deep copy di un oggetto. Per una lista esaustiva, si rimanda alla Python Library Reference . 5.6 - Package \u00b6 Chiudiamo la trattazione con un accenno ai package , ovvero a delle vere e proprie \"collezioni\" che raggruppano moduli tra loro coerenti, in modo da facilitarne il successivo accesso. In pratica, i package non sono altro se non delle cartelle contenenti pi\u00f9 moduli (quindi, file con estensione nome_modulo.py ), oltre ad un file, chiamato __init__.py , che permette all'interprete di riconoscere quella cartella come package e, occasionalmente, contiene delle istruzioni di inizializzazione del package. Per poter accedere ad un modulo contenuto all'interno di un package, possiamo usare la direttiva import , modificandola come segue: import nome_package.nome_modulo # oppure... from nome_package.nome_modulo import nome_funzione","title":"Dispense"},{"location":"material/01_python/05_modules/lecture/#5-script-e-moduli","text":"Quando si usa Python la tentazione \u00e8 quella di interagire direttamente con l'interprete, lanciandolo da terminale ed eseguendo di volta in volta le istruzioni necessarie. Ovviamente questo approccio, seppur immediato, presenta diversi svantaggi. Ad esempio: non avremo a disposizione il syntax highlighting offerto da una normale IDE; non potremo recuperare il codice una volta chiuso l'interprete; non potremo n\u00e9 modificare, n\u00e9 verificare facilmente il funzionamento del codice. Appare quindi evidente come usare l'interprete non sia un modo ottimale di sviluppare codice Python. Di conseguenza, sar\u00e0 necessario definire, mediante la nostra IDE di riferimento, dei veri e propri script che saranno salvati sotto forma di file con estensione .py , ognuno dei quali contenenti una serie di istruzioni necessarie all'esecuzione del nostro programma.","title":"5 - Script e moduli"},{"location":"material/01_python/05_modules/lecture/#51-il-primo-script","text":"Proviamo quindi a creare il nostro primo script Python. Per farlo, apriamo la nostra IDE di riferimento, come Visual Studio Code, e creiamo un file chiamato main.py , all'interno del quale inseriremo il seguente codice: # main.py def hello_world (): print ( 'Hello, world' ) hello_world () Adesso apriamo un terminale, spostiamoci nella cartella nel quale abbiamo salvato questo script, ed eseguiamolo: cd cartella_dove_risiede_lo_script python main.py Le due istruzioni precedenti: servono a cambiare cartella ( change directory , cd ), spostandoci nella cartella dove risiede lo script; dicono all'interprete Python di lanciare lo script main.py . A schermo, se tutto \u00e8 andato per il verso giusto, apparir\u00e0 la scritta Hello, world : Hello, world","title":"5.1 - Il primo script"},{"location":"material/01_python/05_modules/lecture/#52-i-moduli","text":"Quando le dimensioni della nostra code base (ovvero la quantit\u00e0 di codice che scriviamo nel nostro programma) iniziano ad essere particolarmente \"ingombranti\", \u00e8 opportuno adottare un approccio modulare , separando in file differenti parti di codice delegate a funzioni eterogenee. Facciamo un esempio. Immaginiamo di voler scrivere un programma che definisca delle funzioni per calcolare l'area delle principali figure geometriche. Modifichiamo quindi il nostro file main.py come segue: # main.py def calcola_area_quadrato ( lato ): return lato * lato def calcola_area_rettangolo ( base , altezza ): return base * altezza def calcola_area_triangolo ( base , altezza ): return ( base * altezza ) / 2 area_quadrato = calcola_area_quadrato ( 4 ) area_rettangolo = calcola_area_rettangolo ( 2 , 3 ) area_triangolo = calcola_area_triangolo ( 2 , 3 ) Immaginiamo di voler quindi aggiungere una funzione di calcolo trigonometrico: # main.py import math def calcola_tangente ( angolo ): return math . sin ( angolo ) / math . cos ( angolo ) tangente_pi = calcola_tangente ( math . pi ) Il codice del nostro file main.py comprender\u00e0 adesso funzioni di tipo geometrico e trigonometrico. Cosa succederebbe se volessimo integrare delle funzioni di calcolo integrale, o di altro tipo? Ovviamente, ci sarebbe da un lato un aumento delle dimensioni della code base, dall'altro un \"mix\" tra funzioni che afferiscono ad ambiti differenti (seppur simili tra loro). Una buona idea sarebbe quindi quella di separare le diverse parti del programma, magari raggruppando le funzioni geometriche nel file geometria.py , le funzioni trigonometriche nel file trigonometria.py , e via discorrendo. Questi file, che conterranno al loro interno prevalentemente funzioni (ma non solo), sono chiamati moduli . Nota La linea che distingue gli script dai moduli \u00e8 molto sottile, e nei fatti \u00e8 facile fare confusione ed utilizzarli in maniera \"intercambiabile\". Sottolineamo per\u00f2 che, idealmente, gli script devono contenere al loro interno soltanto del codice che sar\u00e0 eseguito , mentre i moduli solo del codice che sar\u00e0 invocato da uno o pi\u00f9 script. Interprete e nome di un modulo L'interprete \u00e8 in grado di risalire al nome di un modulo dal nome del file in cui \u00e8 contenuto. Se, ad esempio, definiamo un modulo nel file geometria.py , l'interprete associer\u00e0 a quel modulo il nome geometria . Detto nome \u00e8 inoltre accessibile globalmente e dall'interno del modulo richiamando la variabile globale __name__ .","title":"5.2 - I moduli"},{"location":"material/01_python/05_modules/lecture/#521-i-moduli-geometria-e-trigonometria","text":"Creiamo adesso il file geometria.py , all'interno del quale \"sposteremo\" le funzioni definite in precedenza per il calcolo geometrico. # geometria.py def calcola_area_quadrato ( lato ): return lato * lato def calcola_area_rettangolo ( base , altezza ): return base * altezza def calcola_area_triangolo ( base , altezza ): return ( base * altezza ) / 2 Analogamente, nel file trigonometria.py andremo a definire la funzione per il calcolo della tangente. # trigonometria.py import math def calcola_tangente ( angolo ): return math . sin ( angolo ) / math . cos ( angolo ) Riscriviamo ora il file main.py : # main.py import geometria import trigonometria if __name__ == \"__main__\" : print ( geometria . calcola_area_quadrato ( 4 )) print ( trigonometria . calcola_tangente ( math . pi )) Possiamo notare due cose. In primis, stiamo richiamando le funzioni calcola_area_quadrato() e calcola_tangente() definite nei moduli geometria e trigonometria , rispettivamente. Questi moduli sono importati all'interno del nostro script mediante la direttiva import . Al rigo 5, la \"strana\" sintassi mostrata serve a dichiarare quello che \u00e8 il main , ovvero il punto di \"accesso\" al codice del nostro programma. Il main \u00e8 normalmente presente in tutti i linguaggi di programmazione, alle volte sotto forme un po' differenti da quella qui mostrata; tuttavia, nel caso di script particolarmente semplici, il main pu\u00f2 essere tranquillamente omesso, in quanto l'interprete riuscir\u00e0 ad eseguirlo in maniera autonoma. Proviamo a lanciare lo script; per farlo, digitiamo l'istruzione python main.py da terminale. A schermo, se tutto \u00e8 andato per il verso giusto, vedremo i valori dell'area di un quadrato e della tangente di \\(\\pi\\) .","title":"5.2.1 - I moduli geometria e trigonometria"},{"location":"material/01_python/05_modules/lecture/#53-usare-gli-import","text":"Relativamente al modulo geometria , abbiamo usato esclusivamente la funzione calcola_area_quadrato() , \"trascurando\" le altre due funzioni comunque presenti nel modulo. In queste circostanze, possiamo usare una versione modificata della direttiva import , che assume la seguente forma: from modulo import funzione_o_classe il che, nel nostro caso specifico, diventa: from geometria import calcola_area_quadrato In questo modo, possiamo importare solamente quello che ci serve, il che risulta particolarmente utile a migliorare l'efficienza del nostro codice; il perch\u00e9 sar\u00e0 chiaro a breve.","title":"5.3 - Usare gli import"},{"location":"material/01_python/05_modules/lecture/#531-alias","text":"La direttiva import ci permette di definire anche degli alias, particolarmente utili nel caso si usino dei nomi di package complessi. Ad esempio: import trigonometria as tr print ( tr . calcola_tangente ( math . pi ))","title":"5.3.1 - Alias"},{"location":"material/01_python/05_modules/lecture/#54-la-funzione-dir","text":"La funzione dir() restituisce una lista con tutti i nomi (sia di funzione, sia di classe) definiti da un modulo. Ad esempio: >>> dir ( geometria ) [ '__builtins__' , '__cached__' , '__doc__' , '__file__' , '__loader__' , '__name__' , '__package__' , '__spec__' , 'calcola_area_quadrato' , 'calcola_area_rettangolo' , 'calcola_area_triangolo' ] E' interessante notare come, oltre a funzioni, classi e variabili da noi definite, nel modulo geometria siano automaticamente definite altre variabili, che saranno importate usando import: import geometria if __name__ == \"__main__\" : print ( geometria . __file__ ) print ( geometria . calcola_area_quadrato ( 4 )) Notiamo che saremo in grado di accedere alla variabile __file__ del modulo geometria , che indica il percorso relativo dello stesso all'interno del file system. Ovviamente, questa variabile non \u00e8 quasi mai utile, ma comporta un ulteriore carico sul codice, da cui diventa evidente l'importanza dell'opportuno uso della direttiva from .","title":"5.4 - La funzione dir()"},{"location":"material/01_python/05_modules/lecture/#55-moduli-della-libreria-standard","text":"Python ha diversi moduli appartenenti ad una libreria standard, i quali sono automaticamente disponibili a valle dell'installazione dell'interprete. Alcuni tra i pi\u00f9 utilizzati sono: sys : \u00e8 il modulo integrato nell'interprete, ed offre diverse utility necessarie al suo funzionamento; os : modulo delegato all'interazione con il sistema operativo su cui gira l'interprete; time : modulo usato per tutte le utility riguardanti il \"cronometraggio\" del tempo di esecuzione di una funzione; datetime : modulo usato per le funzionalit\u00e0 di data ed ora; copy : modulo usato per gestire, tra le altre cose, la deep copy di un oggetto. Per una lista esaustiva, si rimanda alla Python Library Reference .","title":"5.5 - Moduli della libreria standard"},{"location":"material/01_python/05_modules/lecture/#56-package","text":"Chiudiamo la trattazione con un accenno ai package , ovvero a delle vere e proprie \"collezioni\" che raggruppano moduli tra loro coerenti, in modo da facilitarne il successivo accesso. In pratica, i package non sono altro se non delle cartelle contenenti pi\u00f9 moduli (quindi, file con estensione nome_modulo.py ), oltre ad un file, chiamato __init__.py , che permette all'interprete di riconoscere quella cartella come package e, occasionalmente, contiene delle istruzioni di inizializzazione del package. Per poter accedere ad un modulo contenuto all'interno di un package, possiamo usare la direttiva import , modificandola come segue: import nome_package.nome_modulo # oppure... from nome_package.nome_modulo import nome_funzione","title":"5.6 - Package"},{"location":"material/02_libs/06_jupyter/lecture/","text":"6. iPython e Jupyter Lab \u00b6 Fino a questo momento ci siamo limitati a lanciare script Python direttamente da riga di comando. Tuttavia, \u00e8 evidente come questo approccio sia limitato, specialmente in applicazioni in ambito data science. Per ovviare a queste problematiche, all'interno del framework SciPy viene proposto Jupyter Lab , che introduce uno tra gli strumenti pi\u00f9 utilizzati dai data analyst al giorno d'oggi, ovvero i notebook . 6.1 - Anatomia di un notebook \u00b6 Un notebook \u00e8, in poche parole, un ambiente interattivo che permette di scrivere e testare il nostro codice. In particolare, ptoremo scrivere una o pi\u00f9 istruzioni, ed eseguirle in maniera separata dalle altre mediante il meccanismo delle celle , che altro non sono se non dei singoli \"blocchi\" di codice. Suggerimento I notebook Jupyter ci permettono di inserire anche commenti, descrizioni ed equazioni utilizzando due linguaggi di markup molto noti, ovvero Markdown e Latex . Vediamo adesso come creare ed utilizzare il nostro primo notebook. 6.2 - Installazione e lancio di Jupyter Lab \u00b6 Installazione di una libreria Ricordiamo che le diverse opzioni utilizzabili per installare una libreria sono descritte nel dettaglio nell' appendice B . Per installare Jupyter Lab, ricorriamo all'utilizzo di pip , preferibilmente all'interno di un ambiente virtuale: workon my-virtual-env ( my-virtual-env ) pip install jupyterlab A differenza delle altre librerie, Jupyter non andr\u00e0 (necessariamente) importato; infatti, \u00e8 possibile lanciare un ambiente interattivo utilizzando la seguente istruzione da riga di comando: jupyter lab Importare iPython In teoria \u00e8 possibile importare iPython ed utilizzare i metodi e le classi messe a disposizione come una qualsiasi libreria. Nei fatti, per\u00f2, molto spesso ci si limita ad utilizzare l'ambiente interattivo offerto dai notebook. 6.3 - Il primo notebook \u00b6 A questo punto ci troveremo davanti ad una schermata simile a quella mostrata nella figura successiva. Creiamo il nostro primo notebook premendo il pulsante Python 3 nel menu Notebook . Una volta terminata la procedura, potremo iniziare ad interagire con l'ambiente. Prima di procedere, per\u00f2, definiamo il nome del nostro notebook dal menu a sinistra. Proviamo a fare qualcosa di semplice: creiamo una funzione che sommi due variabili di tipo numerico, restituendo il risultato, e chiamiamola su due diversi valori. Per prima cosa, scriviamo il codice della funzione all'interno della prima cella: def somma ( a , b ): somma = a + b return somma Per eseguire il codice all'interno della cella, premiamo il tasto Play , oppure la combinazione di tasti Shift+Invio . Una volta eseguita la prima cella, Jupyter ne creer\u00e0 in automatico un'altra; al suo interno, potremo scrivere le istruzioni necessarie a chiamare la funzione somma() su due diversi valori. somma ( 5 , 7 ) Eseguiamo l'istruzione; noteremo che al di sotto della cella apparir\u00e0 il valore assunto dalla funzione. 6.4 - Altre operazioni utili \u00b6 Jupyter ci permette di effettuare una serie di operazioni utili, tra cui: cancellare un'intera cella; inserire una cella al di sopra o al di sotto di quella attualmente selezionata; stoppare il kernel; riavviare il kernel. Soffermiamoci per un attimo sulle ultime due operazioni. Pu\u00f2 capitare, infatti, che ci sia la necessit\u00e0 di interrompere il flusso attuale dell'esecuzione delle istruzioni, oppure ancora che sia necessario riavviare il notebook. Dato che Jupyter si basa sul concetto di kernel , il quale \u00e8 il responsabile per l'esecuzione del notebook, diremo in gergo che possiamo interrompere , o stoppare , il kernel, oppure ancora che possiamo riavviarlo . L'interruzione del kernel si limita a fermare l'esecuzione della cella attuale: ci\u00f2 non comporta alcuna perdita di dati, e potremo riprendere ad eseguire il codice nel notebook in ogni momento, sia dall'inizio di quella cella, sia dall'interno di un'altra. Il riavvio del kernel, invece, \"blocca\" completamente l'esecuzione, andando a cancellare anche le variabili presenti in memoria: si tratta, quindi, di un vero e proprio \"reset\", da utilizzare quando, ad esempio, abbiamo la necessit\u00e0 di riorganizzare il codice, oppure quando abbiamo effettuato un numero eccessivo di modifiche per le quali i risultati iniziano a non essere coerenti con le nostre attese.","title":"Dispense"},{"location":"material/02_libs/06_jupyter/lecture/#6-ipython-e-jupyter-lab","text":"Fino a questo momento ci siamo limitati a lanciare script Python direttamente da riga di comando. Tuttavia, \u00e8 evidente come questo approccio sia limitato, specialmente in applicazioni in ambito data science. Per ovviare a queste problematiche, all'interno del framework SciPy viene proposto Jupyter Lab , che introduce uno tra gli strumenti pi\u00f9 utilizzati dai data analyst al giorno d'oggi, ovvero i notebook .","title":"6. iPython e Jupyter Lab"},{"location":"material/02_libs/06_jupyter/lecture/#61-anatomia-di-un-notebook","text":"Un notebook \u00e8, in poche parole, un ambiente interattivo che permette di scrivere e testare il nostro codice. In particolare, ptoremo scrivere una o pi\u00f9 istruzioni, ed eseguirle in maniera separata dalle altre mediante il meccanismo delle celle , che altro non sono se non dei singoli \"blocchi\" di codice. Suggerimento I notebook Jupyter ci permettono di inserire anche commenti, descrizioni ed equazioni utilizzando due linguaggi di markup molto noti, ovvero Markdown e Latex . Vediamo adesso come creare ed utilizzare il nostro primo notebook.","title":"6.1 - Anatomia di un notebook"},{"location":"material/02_libs/06_jupyter/lecture/#62-installazione-e-lancio-di-jupyter-lab","text":"Installazione di una libreria Ricordiamo che le diverse opzioni utilizzabili per installare una libreria sono descritte nel dettaglio nell' appendice B . Per installare Jupyter Lab, ricorriamo all'utilizzo di pip , preferibilmente all'interno di un ambiente virtuale: workon my-virtual-env ( my-virtual-env ) pip install jupyterlab A differenza delle altre librerie, Jupyter non andr\u00e0 (necessariamente) importato; infatti, \u00e8 possibile lanciare un ambiente interattivo utilizzando la seguente istruzione da riga di comando: jupyter lab Importare iPython In teoria \u00e8 possibile importare iPython ed utilizzare i metodi e le classi messe a disposizione come una qualsiasi libreria. Nei fatti, per\u00f2, molto spesso ci si limita ad utilizzare l'ambiente interattivo offerto dai notebook.","title":"6.2 - Installazione e lancio di Jupyter Lab"},{"location":"material/02_libs/06_jupyter/lecture/#63-il-primo-notebook","text":"A questo punto ci troveremo davanti ad una schermata simile a quella mostrata nella figura successiva. Creiamo il nostro primo notebook premendo il pulsante Python 3 nel menu Notebook . Una volta terminata la procedura, potremo iniziare ad interagire con l'ambiente. Prima di procedere, per\u00f2, definiamo il nome del nostro notebook dal menu a sinistra. Proviamo a fare qualcosa di semplice: creiamo una funzione che sommi due variabili di tipo numerico, restituendo il risultato, e chiamiamola su due diversi valori. Per prima cosa, scriviamo il codice della funzione all'interno della prima cella: def somma ( a , b ): somma = a + b return somma Per eseguire il codice all'interno della cella, premiamo il tasto Play , oppure la combinazione di tasti Shift+Invio . Una volta eseguita la prima cella, Jupyter ne creer\u00e0 in automatico un'altra; al suo interno, potremo scrivere le istruzioni necessarie a chiamare la funzione somma() su due diversi valori. somma ( 5 , 7 ) Eseguiamo l'istruzione; noteremo che al di sotto della cella apparir\u00e0 il valore assunto dalla funzione.","title":"6.3 - Il primo notebook"},{"location":"material/02_libs/06_jupyter/lecture/#64-altre-operazioni-utili","text":"Jupyter ci permette di effettuare una serie di operazioni utili, tra cui: cancellare un'intera cella; inserire una cella al di sopra o al di sotto di quella attualmente selezionata; stoppare il kernel; riavviare il kernel. Soffermiamoci per un attimo sulle ultime due operazioni. Pu\u00f2 capitare, infatti, che ci sia la necessit\u00e0 di interrompere il flusso attuale dell'esecuzione delle istruzioni, oppure ancora che sia necessario riavviare il notebook. Dato che Jupyter si basa sul concetto di kernel , il quale \u00e8 il responsabile per l'esecuzione del notebook, diremo in gergo che possiamo interrompere , o stoppare , il kernel, oppure ancora che possiamo riavviarlo . L'interruzione del kernel si limita a fermare l'esecuzione della cella attuale: ci\u00f2 non comporta alcuna perdita di dati, e potremo riprendere ad eseguire il codice nel notebook in ogni momento, sia dall'inizio di quella cella, sia dall'interno di un'altra. Il riavvio del kernel, invece, \"blocca\" completamente l'esecuzione, andando a cancellare anche le variabili presenti in memoria: si tratta, quindi, di un vero e proprio \"reset\", da utilizzare quando, ad esempio, abbiamo la necessit\u00e0 di riorganizzare il codice, oppure quando abbiamo effettuato un numero eccessivo di modifiche per le quali i risultati iniziano a non essere coerenti con le nostre attese.","title":"6.4 - Altre operazioni utili"},{"location":"material/02_libs/07_numpy/01_intro/lecture/","text":"7.1 Introduzione a NumPy \u00b6 La libreria NumPy , nome derivante dalla crasi tra Num erical Py thon, \u00e8 una tra le pi\u00f9 utilizzate nelle applicazioni di calcolo scientifico in Python. Nella pratica, possiamo pensare a NumPy come ad uno standard de facto : infatti, le classi ed i metodi messi a disposizione dalla libreria sono estensivamente utilizzate nella quasi totalit\u00e0 degli altri tool Python per le scienze matematiche, chimiche e fisiche, oltre che per l'ingegneria. Partiamo nella nostra disamina dalla procedura di installazione della libreria. 7.1.1 Installare NumPy \u00b6 Installazione di una libreria Al solito, ricordiamo che le diverse opzioni utilizzabili per installare una libreria sono descritte nel dettaglio nell' appendice B . Per installare NumPy, ricorriamo all'utilizzo di pip , preferibilmente all'interno di un ambiente virtuale: workon my-virtual-env ( my-virtual-env ) pip install numpy 7.1.2 - Introduzione a NumPy \u00b6 7.1.2.1 - Gli ndarray \u00b6 Abbiamo visto in precedenza che per usare un package o un modulo Python all'interno dei nostri programmi dovremo per prima cosa importarlo: import numpy as np Una volta importato NumPy, potremo passare ad utilizzare la struttura dati \"principe\" della libreria, ovvero l' array , analogo a quelli descritti dalla classica formulazione matematica. Nello specifico, NumPy ci mette a disposizione gli ndarray , ovvero delle strutture dati in grado di rappresentare array ad \\(n\\) dimensioni, contenenti dati di tipo omogeneo . Nota Anche ndarray \u00e8 un'abbreviazione che sta per n-d imensional array . Il metodo pi\u00f9 semplice per creare un array \u00e8 usare il costruttore array a cui viene passata una lista: >>> a = np . array ([ 1 , 2 , 3 ]) 7.1.2.2 - Array vs liste \u00b6 Sono diverse le differenze che intercorrono tra un array ed una classica lista; le principali sono riassunte nella seguente tabella. Caratteristica ndarray Lista Dimensione Fissa Non fissa Elementi Omogenei (stesso tipo) Eterogenei (qualsiasi tipo) Ambito Operazioni algebriche General-purpose In pratica: un array ha dimensione fissa, a differenza della lista. Cambiarne la dimensione comporter\u00e0 quindi la creazione di un nuovo array, e la cancellazione di quello originario; gli elementi di un array devono essere dello stesso tipo (tale limitazione non vale ovviamente per le liste); gli array sono pensati specificamente per le operazioni algebriche, laddove le liste sono pensate per degli scopi generici. 7.1.3 - NumPy e le operazioni algebriche \u00b6 Abbiamo detto che gli array NumPy sono progettati specificamente per le operazioni algebriche. Ovviamente, ci\u00f2 assume una notevole rilevanza ai nostri fini. Per capirlo, facciamo un esemplice esempio, nel quale moltiplichiamo tra loro due vettori riga elemento-per-elemento . 7.1.3.1 - Approccio con liste \u00b6 Per effettuare l'operazione appena descritta potremmo usare un ciclo for o una list comprehension: # ciclo for c = [] for i in range ( len ( a )): c . append ( a [ i ] * b [ i ]) # list comprehension c = [ a [ i ] * b [ i ] for i in range ( len ( a ))] Il risultato dell'operazione sar\u00e0 in entrambi i casi corretto . Tuttavia, i cicli sono computazionalmente costosi : ci\u00f2 significa che, specialmente all'aumentare del numero di elementi contenuti nei vettori, sar\u00e0 necessario pagare un costo crescente. Questo potrebbe essere in qualche modo arginato dal ricorso ad un linguaggio pi\u00f9 efficiente, come ad esempio il C; tuttavia, provando ad estendere il calcolo a due dimensioni, il codice diverr\u00e0: for i in range ( len ( a )): for j in range ( len ( b )): c . append ( a [ i ][ j ] * b [ i ][ j ] Il numero di cicli annidati aumenter\u00e0 ovviamente in maniera direttamente proporzionale alla dimensionalit\u00e0 degli array coinvolti. Ci\u00f2 implica che per un array ad \\(m\\) dimensioni avremo altrettanti cicli annidati, con tutto ci\u00f2 che ne consegue in termini di complessit\u00e0 di codice. Ed \u00e8 proprio in questa situazione che NumPy ci viene in aiuto. Infatti, per moltiplicare due array di qualsiasi dimensionalit\u00e0 ci basta usare la seguente istruzione: c = a * b Evidentemente, una sintassi di questo tipo risulta essere molto pi\u00f9 concisa e semplice rispetto all'uso dei cicli annidati, ed \u00e8 inoltre molto simile a quella che possiamo trovare sulle formule \"reali\" usate nei libri di testo. L'uso di questa sintassi si esplicita in due concetti fondamentali sui quali risulta essere basato NumPy: la vettorizzazione del codice, ovvero la possibilit\u00e0 di scrivere istruzioni matriciali senza usare esplicitamente dei cicli; il broadcasting , che riguarda la possibilit\u00e0 di usare una sintassi comune ed indipendente dalla dimensionalit\u00e0 degli array coinvolti nelle operazioni.","title":"Dispense"},{"location":"material/02_libs/07_numpy/01_intro/lecture/#71-introduzione-a-numpy","text":"La libreria NumPy , nome derivante dalla crasi tra Num erical Py thon, \u00e8 una tra le pi\u00f9 utilizzate nelle applicazioni di calcolo scientifico in Python. Nella pratica, possiamo pensare a NumPy come ad uno standard de facto : infatti, le classi ed i metodi messi a disposizione dalla libreria sono estensivamente utilizzate nella quasi totalit\u00e0 degli altri tool Python per le scienze matematiche, chimiche e fisiche, oltre che per l'ingegneria. Partiamo nella nostra disamina dalla procedura di installazione della libreria.","title":"7.1 Introduzione a NumPy"},{"location":"material/02_libs/07_numpy/01_intro/lecture/#711-installare-numpy","text":"Installazione di una libreria Al solito, ricordiamo che le diverse opzioni utilizzabili per installare una libreria sono descritte nel dettaglio nell' appendice B . Per installare NumPy, ricorriamo all'utilizzo di pip , preferibilmente all'interno di un ambiente virtuale: workon my-virtual-env ( my-virtual-env ) pip install numpy","title":"7.1.1 Installare NumPy"},{"location":"material/02_libs/07_numpy/01_intro/lecture/#712-introduzione-a-numpy","text":"","title":"7.1.2 - Introduzione a NumPy"},{"location":"material/02_libs/07_numpy/01_intro/lecture/#7121-gli-ndarray","text":"Abbiamo visto in precedenza che per usare un package o un modulo Python all'interno dei nostri programmi dovremo per prima cosa importarlo: import numpy as np Una volta importato NumPy, potremo passare ad utilizzare la struttura dati \"principe\" della libreria, ovvero l' array , analogo a quelli descritti dalla classica formulazione matematica. Nello specifico, NumPy ci mette a disposizione gli ndarray , ovvero delle strutture dati in grado di rappresentare array ad \\(n\\) dimensioni, contenenti dati di tipo omogeneo . Nota Anche ndarray \u00e8 un'abbreviazione che sta per n-d imensional array . Il metodo pi\u00f9 semplice per creare un array \u00e8 usare il costruttore array a cui viene passata una lista: >>> a = np . array ([ 1 , 2 , 3 ])","title":"7.1.2.1 - Gli ndarray"},{"location":"material/02_libs/07_numpy/01_intro/lecture/#7122-array-vs-liste","text":"Sono diverse le differenze che intercorrono tra un array ed una classica lista; le principali sono riassunte nella seguente tabella. Caratteristica ndarray Lista Dimensione Fissa Non fissa Elementi Omogenei (stesso tipo) Eterogenei (qualsiasi tipo) Ambito Operazioni algebriche General-purpose In pratica: un array ha dimensione fissa, a differenza della lista. Cambiarne la dimensione comporter\u00e0 quindi la creazione di un nuovo array, e la cancellazione di quello originario; gli elementi di un array devono essere dello stesso tipo (tale limitazione non vale ovviamente per le liste); gli array sono pensati specificamente per le operazioni algebriche, laddove le liste sono pensate per degli scopi generici.","title":"7.1.2.2 - Array vs liste"},{"location":"material/02_libs/07_numpy/01_intro/lecture/#713-numpy-e-le-operazioni-algebriche","text":"Abbiamo detto che gli array NumPy sono progettati specificamente per le operazioni algebriche. Ovviamente, ci\u00f2 assume una notevole rilevanza ai nostri fini. Per capirlo, facciamo un esemplice esempio, nel quale moltiplichiamo tra loro due vettori riga elemento-per-elemento .","title":"7.1.3 - NumPy e le operazioni algebriche"},{"location":"material/02_libs/07_numpy/01_intro/lecture/#7131-approccio-con-liste","text":"Per effettuare l'operazione appena descritta potremmo usare un ciclo for o una list comprehension: # ciclo for c = [] for i in range ( len ( a )): c . append ( a [ i ] * b [ i ]) # list comprehension c = [ a [ i ] * b [ i ] for i in range ( len ( a ))] Il risultato dell'operazione sar\u00e0 in entrambi i casi corretto . Tuttavia, i cicli sono computazionalmente costosi : ci\u00f2 significa che, specialmente all'aumentare del numero di elementi contenuti nei vettori, sar\u00e0 necessario pagare un costo crescente. Questo potrebbe essere in qualche modo arginato dal ricorso ad un linguaggio pi\u00f9 efficiente, come ad esempio il C; tuttavia, provando ad estendere il calcolo a due dimensioni, il codice diverr\u00e0: for i in range ( len ( a )): for j in range ( len ( b )): c . append ( a [ i ][ j ] * b [ i ][ j ] Il numero di cicli annidati aumenter\u00e0 ovviamente in maniera direttamente proporzionale alla dimensionalit\u00e0 degli array coinvolti. Ci\u00f2 implica che per un array ad \\(m\\) dimensioni avremo altrettanti cicli annidati, con tutto ci\u00f2 che ne consegue in termini di complessit\u00e0 di codice. Ed \u00e8 proprio in questa situazione che NumPy ci viene in aiuto. Infatti, per moltiplicare due array di qualsiasi dimensionalit\u00e0 ci basta usare la seguente istruzione: c = a * b Evidentemente, una sintassi di questo tipo risulta essere molto pi\u00f9 concisa e semplice rispetto all'uso dei cicli annidati, ed \u00e8 inoltre molto simile a quella che possiamo trovare sulle formule \"reali\" usate nei libri di testo. L'uso di questa sintassi si esplicita in due concetti fondamentali sui quali risulta essere basato NumPy: la vettorizzazione del codice, ovvero la possibilit\u00e0 di scrivere istruzioni matriciali senza usare esplicitamente dei cicli; il broadcasting , che riguarda la possibilit\u00e0 di usare una sintassi comune ed indipendente dalla dimensionalit\u00e0 degli array coinvolti nelle operazioni.","title":"7.1.3.1 - Approccio con liste"},{"location":"material/02_libs/07_numpy/02_array/exercises/","text":"E7.2 - Gli array \u00b6 Esercizio E7.2.1 \u00b6 Scriviamo una funzione che restituisca il prodotto riga per colonna di due vettori v1 e v2 . Usiamo una list comprehension, e verifichiamo che la lunghezza dei due vettori sia coerente. Valutiamo inoltre il tempo necessario all'esecuzione. Il metodo dovr\u00e0 funzionare indipendentemente dall\u2019ordine in cui sono passati i parametri. Provare ad effettuare la stessa operazione in NumPy. Soluzione S7.2.1 \u00b6 Ecco una possibile soluzione: from time import time import numpy as np def riga_per_colonna ( v1 , v2 ): tic = time () if v1 . shape [ 0 ] == 1 : if v2 . shape [ 1 ] == 1 and v1 . shape [ 1 ] == v2 . shape [ 0 ]: prod = sum ([ v1 [ 0 ][ i ] * v2 [ i ] for i in range ( v2 . shape [ 0 ])]) elif v2 . shape [ 0 ] == 1 : if v1 . shape [ 1 ] == 1 and v2 . shape [ 1 ] == v1 . shape [ 0 ]: prod = sum ([ v1 [ i ] * v2 [ 0 ][ i ] for i in range ( v1 . shape [ 0 ])]) else : return 'Le dimensioni non sono coerenti!' toc = time () return prod , toc - tic v1 = np . array ([[ 1 , 2 , 3 , 4 ]]) v2 = np . array ([[ 1 ],[ 2 ],[ 3 ],[ 4 ]]) res = riga_per_colonna ( v1 , v2 ) res = riga_per_colonna ( v2 , v1 ) res = riga_per_colonna ( np . array ([[ 1 ]]), v2 ) L'equivalente operazione in NumPy \u00e8 data da: res = np . dot ( v1 , v2 ) Esercizio E7.2.2 \u00b6 Scriviamo una funzione crea_array(dim_1, dim_2, val_min, val_max) che crei array di dimensioni arbitrarie dim_1 \\(\\times\\) dim_2 fatti di numeri interi casuali compresi tra val_min e val_max . Di default, la funzione dovr\u00e0 creare dei vettori riga. Provare ad effettuare la stessa operazione in NumPy. Soluzione S7.2.2 \u00b6 Ecco una possibile soluzione: import numpy as np from random import randint def crea_array ( dim_1 , dim_2 = 1 , val_min = 0 , val_max = 100 ): rows = [[ randint ( val_min , val_max ) for i in range ( dim_2 )] for j in range ( dim_1 )] return np . array ( rows ) a_1 = crea_array ( 4 , 1 ) a_2 = crea_array ( 2 , 2 ) Ovviamente, per NumPy ci baster\u00e0 usare il metodo randint : from numpy import random a_1 = random . randint ( 0 , 100 , ( 4 , 1 )) a_2 = random . randint ( 0 , 100 , ( 2 , 2 ))","title":"Esercizi"},{"location":"material/02_libs/07_numpy/02_array/exercises/#e72-gli-array","text":"","title":"E7.2 - Gli array"},{"location":"material/02_libs/07_numpy/02_array/exercises/#esercizio-e721","text":"Scriviamo una funzione che restituisca il prodotto riga per colonna di due vettori v1 e v2 . Usiamo una list comprehension, e verifichiamo che la lunghezza dei due vettori sia coerente. Valutiamo inoltre il tempo necessario all'esecuzione. Il metodo dovr\u00e0 funzionare indipendentemente dall\u2019ordine in cui sono passati i parametri. Provare ad effettuare la stessa operazione in NumPy.","title":"Esercizio E7.2.1"},{"location":"material/02_libs/07_numpy/02_array/exercises/#soluzione-s721","text":"Ecco una possibile soluzione: from time import time import numpy as np def riga_per_colonna ( v1 , v2 ): tic = time () if v1 . shape [ 0 ] == 1 : if v2 . shape [ 1 ] == 1 and v1 . shape [ 1 ] == v2 . shape [ 0 ]: prod = sum ([ v1 [ 0 ][ i ] * v2 [ i ] for i in range ( v2 . shape [ 0 ])]) elif v2 . shape [ 0 ] == 1 : if v1 . shape [ 1 ] == 1 and v2 . shape [ 1 ] == v1 . shape [ 0 ]: prod = sum ([ v1 [ i ] * v2 [ 0 ][ i ] for i in range ( v1 . shape [ 0 ])]) else : return 'Le dimensioni non sono coerenti!' toc = time () return prod , toc - tic v1 = np . array ([[ 1 , 2 , 3 , 4 ]]) v2 = np . array ([[ 1 ],[ 2 ],[ 3 ],[ 4 ]]) res = riga_per_colonna ( v1 , v2 ) res = riga_per_colonna ( v2 , v1 ) res = riga_per_colonna ( np . array ([[ 1 ]]), v2 ) L'equivalente operazione in NumPy \u00e8 data da: res = np . dot ( v1 , v2 )","title":"Soluzione S7.2.1"},{"location":"material/02_libs/07_numpy/02_array/exercises/#esercizio-e722","text":"Scriviamo una funzione crea_array(dim_1, dim_2, val_min, val_max) che crei array di dimensioni arbitrarie dim_1 \\(\\times\\) dim_2 fatti di numeri interi casuali compresi tra val_min e val_max . Di default, la funzione dovr\u00e0 creare dei vettori riga. Provare ad effettuare la stessa operazione in NumPy.","title":"Esercizio E7.2.2"},{"location":"material/02_libs/07_numpy/02_array/exercises/#soluzione-s722","text":"Ecco una possibile soluzione: import numpy as np from random import randint def crea_array ( dim_1 , dim_2 = 1 , val_min = 0 , val_max = 100 ): rows = [[ randint ( val_min , val_max ) for i in range ( dim_2 )] for j in range ( dim_1 )] return np . array ( rows ) a_1 = crea_array ( 4 , 1 ) a_2 = crea_array ( 2 , 2 ) Ovviamente, per NumPy ci baster\u00e0 usare il metodo randint : from numpy import random a_1 = random . randint ( 0 , 100 , ( 4 , 1 )) a_2 = random . randint ( 0 , 100 , ( 2 , 2 ))","title":"Soluzione S7.2.2"},{"location":"material/02_libs/07_numpy/02_array/lecture/","text":"7.2 - Gli array \u00b6 Nella lezione precedente abbiamo introdotto il concetto gli array, ovvero la struttura dati \"centrale\" nell'ecosistema di NumPy. In questa lezione (e nelle successive) ne approfondiremo aspetti e caratteristiche principali. 7.2.1 - Array e liste \u00b6 Di primo acchito, l'impressione che si pu\u00f2 avere osservando gli array \u00e8 che questi siano molto simili alle classiche liste. Tuttavia, come abbiamo gi\u00e0 visto, esistono diverse differenze notevoli, riassumibili in linea di massima affermando che \u00e8 preferibile usare un array quando si devono svolgere operazioni di tipo matematico su dati omogenei. Gli array NumPy sono istanze della classe ndarray , crasi che sta per \\(n\\) -dimensional array. Mediante questa classe possiamo rappresentare strutture dati con un numero arbitrario di dimensioni, ovvero vettori, matrici e tensori. Il primo passo per utilizzare un array \u00e8, come accennato in precedenza, crearlo. In tal senso, ci sono diversi metodi, ma ricordiamo di seguito quello pi\u00f9 \"semplice\", che prevede l'uso del costruttore array() a cui passare una lista di elementi dello stesso tipo: >>> a = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) >>> a array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) Passando invece una lista i cui elementi sono a loro volta delle liste, potremo ottenere in uscita un array multidimensionale: >>> b = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> b array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) Notiamo infine che gli array non sono necessariamente numerici. Possiamo, ad esempio, creare un array di stringhe: >>> c = np . array ([ \"s1\" , \"s2\" ]) >>> c array ([ 's1' , 's2' ], dtype = '<U2' ) 7.2.1.1 - Array eterogenei \u00b6 In precedenza si \u00e8 accennato al fatto che gli array, a differenza delle liste, debbano contenere dati omogenei. Cosa succederebbe quindi se provassimo a passare al metodo array() una lista composta da dati di tipo eterogeneo? Partiamo verificando cosa accade ad esempio usando un intero ed un float. >>> d = np . array ([ 1 , 1. ]) >>> d array ([ 1. , 1. ]) Notiamo subito che \u00e8 stata effettuata in maniera implicita ed automatica un'operazione di conversione di tipo, e tutti i valori passati sono stati convertiti in formato float. Interessante \u00e8 anche valutare cosa accade se proviamo a passare una lista contenente un numero ed una stringa: >>> e = np . array ([ 1 , \"s3\" ]) >>> e array ([ '1' , 's3' ], dtype = '<U11' ) Notiamo come anche in questo caso sia stata effettuata una conversione di tipo, passando stavolta da intero a stringa. Upcasting La regola da tenere a mente \u00e8 che NumPy (e, in generale, Python) seguono il principio dell' upcasting : in altre parole, quando deve essere fatta una conversione tra diversi tipi di dati, si fa in modo di scegliere il tipo a pi\u00f9 alta precisione, minimizzando i rischi di perdita di informazioni. 7.2.2 - Il numero di elementi di un array \u00b6 Gli array NumPy hanno dimensione prefissata, e sono quindi in grado di contenere un numero fisso di oggetti di un certo tipo. Per definire (o conoscere) questo valore si utilizza una propriet\u00e0 chiamata shape che, a grandi linee, rappresenta la forma dell'array. La shape di un array \u00e8 in pratica una tupla di numeri interi, ovviamente non negativi, ciascuno dei quali determina il numero di elementi per ciascuna delle dimensioni dell'array. Creiamo ad esempio un array che rappresenti una matrice \\(2 \\times 3\\) , ovvero a due righe e tre colonne: >>> a = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> a array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) Vediamo che valore assume la propriet\u00e0 shape di questo array: >>> a . shape ( 2 , 3 ) Come ci aspettavamo, il nostro array ha cardinalit\u00e0 due sulla prima dimensione (ovvero il numero di righe) e tre sulla seconda (ovvero il numero di colonne). 7.2.3 - Altri metodi per creare un array \u00b6 Oltre al metodo visto in precedenza, possiamo creare un array utilizzando direttamente il costruttore della classe ndarray : >>> a = np . ndarray ([ 3 , 3 ]) # oppure a = np.ndarray(shape=(3, 3)) >>> a array ([[ 0.00000000e+000 , 0.00000000e+000 , 0.00000000e+000 ], [ 0.00000000e+000 , 0.00000000e+000 , 3.02368175e-321 ], [ 6.69431255e+151 , 1.68534231e+246 , 6.69431467e+151 ]]) Notiamo che il costruttore accetta una lista contenente la shape dell'array, che in questo caso diverr\u00e0 un \\(3 \\times 3\\) . Nota Notiamo come i numeri con cui viene \"riempito\" l'array sono al momento casuali. Oltre a questa tecnica base, esistono diversi modi per creare array di un certo tipo. Vediamoli in breve. 7.2.3.1 - Array con valori zero ed unitari \u00b6 Possiamo creare un array di dimensioni arbitrarie in cui tutti gli elementi sono pari ad 1. Per farlo, usiamo la funzione ones() : >>> u = np . ones ( shape = ( 3 , 3 )) >>> u array ([[ 1. , 1. , 1. ], [ 1. , 1. , 1. ], [ 1. , 1. , 1. ]]) In modo simile, possiamo creare array di dimensioni arbitrarie in cui tutti gli elementi sono pari a zero mediante la funzione zeros() : >>> z = np . zeros ( shape = ( 3 , 3 )) >>> z array ([[ 0. , 0. , 0. ], [ 0. , 0. , 0. ], [ 0. , 0. , 0. ]]) 7.2.3.2 - Array vuoti \u00b6 Possiamo creare un array vuoto mediante la funzione empty() : >>> e = np . empty ( shape = ( 3 , 3 )) >>> e array ([[ 0.00000000e+000 , 0.00000000e+000 , 0.00000000e+000 ], [ 0.00000000e+000 , 0.00000000e+000 , 1.67982320e-321 ], [ 5.96555652e-302 , 1.14188703e-104 , 9.91401238e-278 ]]) Questa funzione pu\u00f2 risultare utile quando vogliamo preallocare spazio per un array. Nota I pi\u00f9 attenti avranno notato che, in realt\u00e0, l'array generato da empty() non \u00e8 vuoto, ma contiene valori casuali. In tal senso, d\u00e0 risultati equivalenti all'uso diretto del costruttore ndarray() . 7.2.3.4 - Matrice identit\u00e0 \u00b6 Possiamo creare una matrice identit\u00e0 usando la funzione eye() : >>> i = np . eye ( 3 ) >>> i array ([[ 1. , 0. , 0. ], [ 0. , 1. , 0. ], [ 0. , 0. , 1. ]]) Attenzione In questo caso, notiamo come non si possa passare una tupla o una lista per indicare le dimensioni dell'array. Tuttavia, possiamo specificare sia il numero delle righe (con il primo parametro) che il numero delle colonne (con il secondo parametro). 7.2.3.5 - Matrici diagonali \u00b6 La funzione diag() viene usata sia per creare una matrice diagonale a partire da un vettore (che, ovviamente, sar\u00e0 poi la diagonale della matrice), sia per estrarre la diagonale di una matrice. Per capire questa dualit\u00e0, immaginiamo per prima cosa di avere a disposizione un vettore riga a tre elementi, che vogliamo trasformare in modo tale che si comporti come la diagonale di una matrice. >>> x = np . array ([ 5 , 2 , 3 ]) >>> x array ([ 5 , 2 , 3 ]) Potremo creare una matrice diagonale a partire da questo vettore passandolo come parametro alla funzione diag() : >>> d = np . diag ( x ) >>> d array ([[ 5 , 0 , 0 ], [ 0 , 2 , 0 ], [ 0 , 0 , 3 ]]) Vediamo invece come affrontare il problema duale. Immaginiamo di avere quindi un array, e volerne estrarre la diagonale: >>> x = np . array ([[ 5 , 5 , 5 ], [ 2 , 1 , 3 ], [ 4 , 3 , 6 ]]) >>> x array ([[ 5 , 2 , 2 ], [ 2 , 1 , 3 ], [ 4 , 3 , 6 ]]) Per farlo, dovremo anche questa volta usare la funzione diag() : >>> d = np . diag ( x ) >>> d array ([ 5 , 1 , 6 ]) Suggerimento Il fatto che la funzione diag() sia usata per operazioni duali pu\u00f2, a ragione, causare confusione. Basta per\u00f2 ricordare che passando un vettore si ottiene una matrice, mentre passando una matrice si ottiene un vettore, ed il gioco \u00e8 fatto. Attenzione Ovviamente, la funzione diag() accetta solo input monodimensionali (vettori) e bidimensionali (matrici)! 7.2.3.6 - Matrici triangolari \u00b6 Concludiamo questa breve carrellata mostrando due metodi in grado di estrarre la matrice triangolare, rispettivamente superiore ed inferiore. Supponiamo di avere la matrice x definita in precedenza. Per estrarre la matrice triangolare superiore, dovremo usare la funzione triu() : >>> tu = np . triu ( x ) >>> tu array ([[ 5 , 2 , 2 ], [ 0 , 1 , 3 ], [ 0 , 0 , 6 ]]) Per estrarre invece la matrice triangolare inferiore, dovremo usare la funzione tril() : >>> tl = np . tril ( x ) >>> tl array ([[ 5 , 0 , 0 ], [ 2 , 1 , 0 ], [ 4 , 3 , 6 ]]) Suggerimento In questo caso, le funzioni tril() e triu() possono tranquillamente essere applicate agli array n-dimensionali. Inoltre, non \u00e8 richiesto le diverse dimensioni dell'array abbiano la stessa cardinalit\u00e0. 7.2.4 - Accesso agli elementi di un array \u00b6 Cos\u00ec come per le liste, il modo pi\u00f9 immediato per accedere al valore di un elemento in un array \u00e8 usare l'operatore [] , specificando contestualmente l'indice dell'elemento cui si vuole accedere. Ad esempio, per selezionare il primo elemento di un vettore: >>> a = np . array ([ 1 , 2 , 3 , 4 ]) >>> a [ 0 ] 1 Nel caso di array ad \\(n\\) dimensioni, \u00e8 necessario indicare l'indice per ciascuna delle dimensioni dell'array. Ad esempio, per un array bidimensionale potremmo selezionare l'elemento alla prima riga e prima colonna con una sintassi di questo tipo: >>> b = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b [ 0 ][ 0 ] 1 7.2.5 - Maschere booleane \u00b6 Possiamo accedere ad un sottoinsieme di elementi dell'array mediante una \"maschera\", ovvero un altro array, di dimensioni uguali a quelle di partenza, al cui interno sono presenti esclusivamente dei valori booleani. Cos\u00ec facendo, estrarremo soltanto gli elementi la cui corrispondente posizione all'interno della maschera ha valore True . Ad esempio, possiamo selezionare tutti gli elementi appartenenti alla prima colonna dell'array b : >>> mask = np . array ([[ True , False ], [ True , False ]]) >>> b [ mask ] array ([ 1 , 3 ]) Ancora, possiamo scegliere tutti gli elementi che soddisfano una certa condizione logico/matematica: >>> mask = ( b > 2 ) >>> mask array ([[ False , False ], [ True , True ]]) >>> b [ mask ] array ([ 3 , 4 ]) Interessante notare come la precedente notazione possa essere ulteriormente sintetizzata usando delle relazioni logiche: >>> b [ b > 2 ] array ([ 3 , 4 ]) Volendo, possiamo adattare la forma precedente all'uso di espressioni arbitrariamente complesse: >>> b [ b % 2 == 0 ] array ([ 2 , 4 ]) >>> b [( b > 1 ) & ( b < 4 )] array ([ 2 , 3 ]) 7.2.6 - Slicing degli array \u00b6 Cos\u00ec come le liste, anche gli array consentono le operazioni di slicing: >>> a = np . array ([ 1 , 2 , 3 , 4 ]) >>> a [ 0 : 2 ] array ([ 1 , 2 ]) Per gli array multidimensionali, lo slicing si intende sulla \\(n\\) -ma dimensione dell'array. Questo concetto \u00e8 facile da comprendere se si visualizza l'array ad \\(n\\) -dimensioni come un array di array: >>> b array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b [ 0 : 1 ] # Lo slicing avviene sulla seconda dimensione array ([[ 1 , 2 ]]) 7.2.7 - La funzione nonzero() \u00b6 Possiamo usare la funzione nonzero() per selezionare gli elementi e gli indici di un array il cui valore non sia pari a zero. Ad esempio: >>> x = np . array ([[ 3 , 0 , 0 ], [ 0 , 4 , 0 ], [ 5 , 6 , 0 ]]) >>> x array ([[ 3 , 0 , 0 ], [ 0 , 4 , 0 ], [ 5 , 6 , 0 ]]) >>> np . nonzero ( x ) ( array ([ 0 , 1 , 2 , 2 ], dtype = int64 ), array ([ 0 , 1 , 0 , 1 ], dtype = int64 )) La funzione nonzero() restituisce una tupla con gli indici per riga e colonna degli elementi diversi da zero. In particolare, la tupla risultante avr\u00e0 un numero di elementi pari a ciascuna delle dimensioni dell'array x di ingresso, e l' \\(i\\) -mo vettore individuer\u00e0 gli indici relativi alla \\(i\\) -ma dimensione. Ad esempio, in questo caso, il primo array rappresenta gli indici relativi alla prima dimensione dei valori non nulli (in questo caso, gli indici di riga), mentre il secondo gli indici relativi alla seconda dimensione (indici di colonna). Notiamo quindi che avremo i seguenti elementi diversi da zero: Indice di riga Indice di colonna Valore 0 0 3 1 1 4 2 0 5 2 1 6 Ottenere una lista di tuple Possiamo ottenere una lista di tuple rappresentative delle coppie di indici per gli elementi non nulli sfruttando la funzione zip : ```py s = np.nonzero(tarry) s (array([0, 1, 2, 2], dtype=int64), array([0, 1, 0, 1], dtype=int64)) coords = list(zip(s[0], s[1])) coords [(0, 0), (1, 1), (2, 0), (2, 1)] ``` 7.2.7 - Fancy indexing \u00b6 Chiudiamo questa lezione parlando di una tecnica molto interessante chiamata fancy indexing , consistente nell'usare un array di indici per accedere a pi\u00f9 elementi contemporaneamente. Ad esempio: >>> rand = np . random . RandomState ( 42 ) >>> x = rand . randint ( 100 , size = 10 ) >>> indexes = np . array ([[ 1 , 4 ],[ 5 , 2 ]]) >>> x array ([ 51 , 92 , 14 , 71 , 60 , 20 , 82 , 86 , 74 , 74 ]) >>> x [ indexes ] array ([[ 92 , 60 ], [ 20 , 14 ]]) Nel codice precedente, stiamo: usando la funzione randint() per generare un array di numeri interi casuali compresi tra 0 e 100; generando un array bidimensionale indexes ; restituendo, mediante il fancy indexing, un array con le dimensioni di indexes e gli elementi di x presi nelle posizioni indicate da indexes . La potenza del fancy indexing sta proprio in questo: non solo siamo in grado di accedere facilmente a pi\u00f9 elementi di un array mediante un'unica operazione, ma possiamo anche ridisporre questi elementi come pi\u00f9 ci aggrada!","title":"Dispense"},{"location":"material/02_libs/07_numpy/02_array/lecture/#72-gli-array","text":"Nella lezione precedente abbiamo introdotto il concetto gli array, ovvero la struttura dati \"centrale\" nell'ecosistema di NumPy. In questa lezione (e nelle successive) ne approfondiremo aspetti e caratteristiche principali.","title":"7.2 - Gli array"},{"location":"material/02_libs/07_numpy/02_array/lecture/#721-array-e-liste","text":"Di primo acchito, l'impressione che si pu\u00f2 avere osservando gli array \u00e8 che questi siano molto simili alle classiche liste. Tuttavia, come abbiamo gi\u00e0 visto, esistono diverse differenze notevoli, riassumibili in linea di massima affermando che \u00e8 preferibile usare un array quando si devono svolgere operazioni di tipo matematico su dati omogenei. Gli array NumPy sono istanze della classe ndarray , crasi che sta per \\(n\\) -dimensional array. Mediante questa classe possiamo rappresentare strutture dati con un numero arbitrario di dimensioni, ovvero vettori, matrici e tensori. Il primo passo per utilizzare un array \u00e8, come accennato in precedenza, crearlo. In tal senso, ci sono diversi metodi, ma ricordiamo di seguito quello pi\u00f9 \"semplice\", che prevede l'uso del costruttore array() a cui passare una lista di elementi dello stesso tipo: >>> a = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) >>> a array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) Passando invece una lista i cui elementi sono a loro volta delle liste, potremo ottenere in uscita un array multidimensionale: >>> b = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> b array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) Notiamo infine che gli array non sono necessariamente numerici. Possiamo, ad esempio, creare un array di stringhe: >>> c = np . array ([ \"s1\" , \"s2\" ]) >>> c array ([ 's1' , 's2' ], dtype = '<U2' )","title":"7.2.1 - Array e liste"},{"location":"material/02_libs/07_numpy/02_array/lecture/#7211-array-eterogenei","text":"In precedenza si \u00e8 accennato al fatto che gli array, a differenza delle liste, debbano contenere dati omogenei. Cosa succederebbe quindi se provassimo a passare al metodo array() una lista composta da dati di tipo eterogeneo? Partiamo verificando cosa accade ad esempio usando un intero ed un float. >>> d = np . array ([ 1 , 1. ]) >>> d array ([ 1. , 1. ]) Notiamo subito che \u00e8 stata effettuata in maniera implicita ed automatica un'operazione di conversione di tipo, e tutti i valori passati sono stati convertiti in formato float. Interessante \u00e8 anche valutare cosa accade se proviamo a passare una lista contenente un numero ed una stringa: >>> e = np . array ([ 1 , \"s3\" ]) >>> e array ([ '1' , 's3' ], dtype = '<U11' ) Notiamo come anche in questo caso sia stata effettuata una conversione di tipo, passando stavolta da intero a stringa. Upcasting La regola da tenere a mente \u00e8 che NumPy (e, in generale, Python) seguono il principio dell' upcasting : in altre parole, quando deve essere fatta una conversione tra diversi tipi di dati, si fa in modo di scegliere il tipo a pi\u00f9 alta precisione, minimizzando i rischi di perdita di informazioni.","title":"7.2.1.1 - Array eterogenei"},{"location":"material/02_libs/07_numpy/02_array/lecture/#722-il-numero-di-elementi-di-un-array","text":"Gli array NumPy hanno dimensione prefissata, e sono quindi in grado di contenere un numero fisso di oggetti di un certo tipo. Per definire (o conoscere) questo valore si utilizza una propriet\u00e0 chiamata shape che, a grandi linee, rappresenta la forma dell'array. La shape di un array \u00e8 in pratica una tupla di numeri interi, ovviamente non negativi, ciascuno dei quali determina il numero di elementi per ciascuna delle dimensioni dell'array. Creiamo ad esempio un array che rappresenti una matrice \\(2 \\times 3\\) , ovvero a due righe e tre colonne: >>> a = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> a array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) Vediamo che valore assume la propriet\u00e0 shape di questo array: >>> a . shape ( 2 , 3 ) Come ci aspettavamo, il nostro array ha cardinalit\u00e0 due sulla prima dimensione (ovvero il numero di righe) e tre sulla seconda (ovvero il numero di colonne).","title":"7.2.2 - Il numero di elementi di un array"},{"location":"material/02_libs/07_numpy/02_array/lecture/#723-altri-metodi-per-creare-un-array","text":"Oltre al metodo visto in precedenza, possiamo creare un array utilizzando direttamente il costruttore della classe ndarray : >>> a = np . ndarray ([ 3 , 3 ]) # oppure a = np.ndarray(shape=(3, 3)) >>> a array ([[ 0.00000000e+000 , 0.00000000e+000 , 0.00000000e+000 ], [ 0.00000000e+000 , 0.00000000e+000 , 3.02368175e-321 ], [ 6.69431255e+151 , 1.68534231e+246 , 6.69431467e+151 ]]) Notiamo che il costruttore accetta una lista contenente la shape dell'array, che in questo caso diverr\u00e0 un \\(3 \\times 3\\) . Nota Notiamo come i numeri con cui viene \"riempito\" l'array sono al momento casuali. Oltre a questa tecnica base, esistono diversi modi per creare array di un certo tipo. Vediamoli in breve.","title":"7.2.3 - Altri metodi per creare un array"},{"location":"material/02_libs/07_numpy/02_array/lecture/#7231-array-con-valori-zero-ed-unitari","text":"Possiamo creare un array di dimensioni arbitrarie in cui tutti gli elementi sono pari ad 1. Per farlo, usiamo la funzione ones() : >>> u = np . ones ( shape = ( 3 , 3 )) >>> u array ([[ 1. , 1. , 1. ], [ 1. , 1. , 1. ], [ 1. , 1. , 1. ]]) In modo simile, possiamo creare array di dimensioni arbitrarie in cui tutti gli elementi sono pari a zero mediante la funzione zeros() : >>> z = np . zeros ( shape = ( 3 , 3 )) >>> z array ([[ 0. , 0. , 0. ], [ 0. , 0. , 0. ], [ 0. , 0. , 0. ]])","title":"7.2.3.1 - Array con valori zero ed unitari"},{"location":"material/02_libs/07_numpy/02_array/lecture/#7232-array-vuoti","text":"Possiamo creare un array vuoto mediante la funzione empty() : >>> e = np . empty ( shape = ( 3 , 3 )) >>> e array ([[ 0.00000000e+000 , 0.00000000e+000 , 0.00000000e+000 ], [ 0.00000000e+000 , 0.00000000e+000 , 1.67982320e-321 ], [ 5.96555652e-302 , 1.14188703e-104 , 9.91401238e-278 ]]) Questa funzione pu\u00f2 risultare utile quando vogliamo preallocare spazio per un array. Nota I pi\u00f9 attenti avranno notato che, in realt\u00e0, l'array generato da empty() non \u00e8 vuoto, ma contiene valori casuali. In tal senso, d\u00e0 risultati equivalenti all'uso diretto del costruttore ndarray() .","title":"7.2.3.2 - Array vuoti"},{"location":"material/02_libs/07_numpy/02_array/lecture/#7234-matrice-identita","text":"Possiamo creare una matrice identit\u00e0 usando la funzione eye() : >>> i = np . eye ( 3 ) >>> i array ([[ 1. , 0. , 0. ], [ 0. , 1. , 0. ], [ 0. , 0. , 1. ]]) Attenzione In questo caso, notiamo come non si possa passare una tupla o una lista per indicare le dimensioni dell'array. Tuttavia, possiamo specificare sia il numero delle righe (con il primo parametro) che il numero delle colonne (con il secondo parametro).","title":"7.2.3.4 - Matrice identit\u00e0"},{"location":"material/02_libs/07_numpy/02_array/lecture/#7235-matrici-diagonali","text":"La funzione diag() viene usata sia per creare una matrice diagonale a partire da un vettore (che, ovviamente, sar\u00e0 poi la diagonale della matrice), sia per estrarre la diagonale di una matrice. Per capire questa dualit\u00e0, immaginiamo per prima cosa di avere a disposizione un vettore riga a tre elementi, che vogliamo trasformare in modo tale che si comporti come la diagonale di una matrice. >>> x = np . array ([ 5 , 2 , 3 ]) >>> x array ([ 5 , 2 , 3 ]) Potremo creare una matrice diagonale a partire da questo vettore passandolo come parametro alla funzione diag() : >>> d = np . diag ( x ) >>> d array ([[ 5 , 0 , 0 ], [ 0 , 2 , 0 ], [ 0 , 0 , 3 ]]) Vediamo invece come affrontare il problema duale. Immaginiamo di avere quindi un array, e volerne estrarre la diagonale: >>> x = np . array ([[ 5 , 5 , 5 ], [ 2 , 1 , 3 ], [ 4 , 3 , 6 ]]) >>> x array ([[ 5 , 2 , 2 ], [ 2 , 1 , 3 ], [ 4 , 3 , 6 ]]) Per farlo, dovremo anche questa volta usare la funzione diag() : >>> d = np . diag ( x ) >>> d array ([ 5 , 1 , 6 ]) Suggerimento Il fatto che la funzione diag() sia usata per operazioni duali pu\u00f2, a ragione, causare confusione. Basta per\u00f2 ricordare che passando un vettore si ottiene una matrice, mentre passando una matrice si ottiene un vettore, ed il gioco \u00e8 fatto. Attenzione Ovviamente, la funzione diag() accetta solo input monodimensionali (vettori) e bidimensionali (matrici)!","title":"7.2.3.5 - Matrici diagonali"},{"location":"material/02_libs/07_numpy/02_array/lecture/#7236-matrici-triangolari","text":"Concludiamo questa breve carrellata mostrando due metodi in grado di estrarre la matrice triangolare, rispettivamente superiore ed inferiore. Supponiamo di avere la matrice x definita in precedenza. Per estrarre la matrice triangolare superiore, dovremo usare la funzione triu() : >>> tu = np . triu ( x ) >>> tu array ([[ 5 , 2 , 2 ], [ 0 , 1 , 3 ], [ 0 , 0 , 6 ]]) Per estrarre invece la matrice triangolare inferiore, dovremo usare la funzione tril() : >>> tl = np . tril ( x ) >>> tl array ([[ 5 , 0 , 0 ], [ 2 , 1 , 0 ], [ 4 , 3 , 6 ]]) Suggerimento In questo caso, le funzioni tril() e triu() possono tranquillamente essere applicate agli array n-dimensionali. Inoltre, non \u00e8 richiesto le diverse dimensioni dell'array abbiano la stessa cardinalit\u00e0.","title":"7.2.3.6 - Matrici triangolari"},{"location":"material/02_libs/07_numpy/02_array/lecture/#724-accesso-agli-elementi-di-un-array","text":"Cos\u00ec come per le liste, il modo pi\u00f9 immediato per accedere al valore di un elemento in un array \u00e8 usare l'operatore [] , specificando contestualmente l'indice dell'elemento cui si vuole accedere. Ad esempio, per selezionare il primo elemento di un vettore: >>> a = np . array ([ 1 , 2 , 3 , 4 ]) >>> a [ 0 ] 1 Nel caso di array ad \\(n\\) dimensioni, \u00e8 necessario indicare l'indice per ciascuna delle dimensioni dell'array. Ad esempio, per un array bidimensionale potremmo selezionare l'elemento alla prima riga e prima colonna con una sintassi di questo tipo: >>> b = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b [ 0 ][ 0 ] 1","title":"7.2.4 - Accesso agli elementi di un array"},{"location":"material/02_libs/07_numpy/02_array/lecture/#725-maschere-booleane","text":"Possiamo accedere ad un sottoinsieme di elementi dell'array mediante una \"maschera\", ovvero un altro array, di dimensioni uguali a quelle di partenza, al cui interno sono presenti esclusivamente dei valori booleani. Cos\u00ec facendo, estrarremo soltanto gli elementi la cui corrispondente posizione all'interno della maschera ha valore True . Ad esempio, possiamo selezionare tutti gli elementi appartenenti alla prima colonna dell'array b : >>> mask = np . array ([[ True , False ], [ True , False ]]) >>> b [ mask ] array ([ 1 , 3 ]) Ancora, possiamo scegliere tutti gli elementi che soddisfano una certa condizione logico/matematica: >>> mask = ( b > 2 ) >>> mask array ([[ False , False ], [ True , True ]]) >>> b [ mask ] array ([ 3 , 4 ]) Interessante notare come la precedente notazione possa essere ulteriormente sintetizzata usando delle relazioni logiche: >>> b [ b > 2 ] array ([ 3 , 4 ]) Volendo, possiamo adattare la forma precedente all'uso di espressioni arbitrariamente complesse: >>> b [ b % 2 == 0 ] array ([ 2 , 4 ]) >>> b [( b > 1 ) & ( b < 4 )] array ([ 2 , 3 ])","title":"7.2.5 - Maschere booleane"},{"location":"material/02_libs/07_numpy/02_array/lecture/#726-slicing-degli-array","text":"Cos\u00ec come le liste, anche gli array consentono le operazioni di slicing: >>> a = np . array ([ 1 , 2 , 3 , 4 ]) >>> a [ 0 : 2 ] array ([ 1 , 2 ]) Per gli array multidimensionali, lo slicing si intende sulla \\(n\\) -ma dimensione dell'array. Questo concetto \u00e8 facile da comprendere se si visualizza l'array ad \\(n\\) -dimensioni come un array di array: >>> b array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b [ 0 : 1 ] # Lo slicing avviene sulla seconda dimensione array ([[ 1 , 2 ]])","title":"7.2.6 - Slicing degli array"},{"location":"material/02_libs/07_numpy/02_array/lecture/#727-la-funzione-nonzero","text":"Possiamo usare la funzione nonzero() per selezionare gli elementi e gli indici di un array il cui valore non sia pari a zero. Ad esempio: >>> x = np . array ([[ 3 , 0 , 0 ], [ 0 , 4 , 0 ], [ 5 , 6 , 0 ]]) >>> x array ([[ 3 , 0 , 0 ], [ 0 , 4 , 0 ], [ 5 , 6 , 0 ]]) >>> np . nonzero ( x ) ( array ([ 0 , 1 , 2 , 2 ], dtype = int64 ), array ([ 0 , 1 , 0 , 1 ], dtype = int64 )) La funzione nonzero() restituisce una tupla con gli indici per riga e colonna degli elementi diversi da zero. In particolare, la tupla risultante avr\u00e0 un numero di elementi pari a ciascuna delle dimensioni dell'array x di ingresso, e l' \\(i\\) -mo vettore individuer\u00e0 gli indici relativi alla \\(i\\) -ma dimensione. Ad esempio, in questo caso, il primo array rappresenta gli indici relativi alla prima dimensione dei valori non nulli (in questo caso, gli indici di riga), mentre il secondo gli indici relativi alla seconda dimensione (indici di colonna). Notiamo quindi che avremo i seguenti elementi diversi da zero: Indice di riga Indice di colonna Valore 0 0 3 1 1 4 2 0 5 2 1 6 Ottenere una lista di tuple Possiamo ottenere una lista di tuple rappresentative delle coppie di indici per gli elementi non nulli sfruttando la funzione zip : ```py s = np.nonzero(tarry) s (array([0, 1, 2, 2], dtype=int64), array([0, 1, 0, 1], dtype=int64)) coords = list(zip(s[0], s[1])) coords [(0, 0), (1, 1), (2, 0), (2, 1)] ```","title":"7.2.7 - La funzione nonzero()"},{"location":"material/02_libs/07_numpy/02_array/lecture/#727-fancy-indexing","text":"Chiudiamo questa lezione parlando di una tecnica molto interessante chiamata fancy indexing , consistente nell'usare un array di indici per accedere a pi\u00f9 elementi contemporaneamente. Ad esempio: >>> rand = np . random . RandomState ( 42 ) >>> x = rand . randint ( 100 , size = 10 ) >>> indexes = np . array ([[ 1 , 4 ],[ 5 , 2 ]]) >>> x array ([ 51 , 92 , 14 , 71 , 60 , 20 , 82 , 86 , 74 , 74 ]) >>> x [ indexes ] array ([[ 92 , 60 ], [ 20 , 14 ]]) Nel codice precedente, stiamo: usando la funzione randint() per generare un array di numeri interi casuali compresi tra 0 e 100; generando un array bidimensionale indexes ; restituendo, mediante il fancy indexing, un array con le dimensioni di indexes e gli elementi di x presi nelle posizioni indicate da indexes . La potenza del fancy indexing sta proprio in questo: non solo siamo in grado di accedere facilmente a pi\u00f9 elementi di un array mediante un'unica operazione, ma possiamo anche ridisporre questi elementi come pi\u00f9 ci aggrada!","title":"7.2.7 - Fancy indexing"},{"location":"material/02_libs/07_numpy/03_fundamentals/exercises/","text":"Risoluzione degli esercizi \u00b6 Scriviamo la funzione rettifica(array) , che restituisce un array delle stesse dimensioni di quello in ingresso ma che porta tutti i valori negativi a 0. def rettifica ( array ): array [ array < 0 ] = 0 return array rettifica ( np . array ([ - 1 , 2 , - 3 , 4 ])) Scriviamo la funzione asort(vect) che restituisce un vettore riga ordinato in modo discendente. def asort ( array ): s = np . sort ( array ) return s [ - 1 :: - 1 ] asort ( np . array ([ 3 , 2 , 5 , - 1 ]))","title":"Esercizi"},{"location":"material/02_libs/07_numpy/03_fundamentals/exercises/#risoluzione-degli-esercizi","text":"Scriviamo la funzione rettifica(array) , che restituisce un array delle stesse dimensioni di quello in ingresso ma che porta tutti i valori negativi a 0. def rettifica ( array ): array [ array < 0 ] = 0 return array rettifica ( np . array ([ - 1 , 2 , - 3 , 4 ])) Scriviamo la funzione asort(vect) che restituisce un vettore riga ordinato in modo discendente. def asort ( array ): s = np . sort ( array ) return s [ - 1 :: - 1 ] asort ( np . array ([ 3 , 2 , 5 , - 1 ]))","title":"Risoluzione degli esercizi"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/","text":"7.3 - Operazioni fondamentali sugli array \u00b6 7.3.1 - Operazioni algebriche di base \u00b6 NumPy ci offre la possibilit\u00e0 di effettuare diversi tipi di operazioni algebriche di base sugli array. Ad esempio, \u00e8 possibile sommare due array: >>> a = np . array ([ 1 , 2 ]) >>> b = np . array ([ 3 , 4 ]) >>> a + b array ([ 4 , 6 ]) Possiamo ovviamente anche fare le altre operazioni fondamentali: >>> a - b array ([ - 2 , - 2 ]) >>> a * b array ([ 3 , 8 ]) >>> a / b array ([ 0.33333333 , 0.5 ]) >>> b / a array ([ 3. , 2. ]) Moltiplicazione e divisione Per comprendere appieno il comportamento degli operatori * e /, dovremo parlare del broadcasting. Lo faremo in una delle prossime lezioni. 7.3.2 - La funzione sum() \u00b6 La funzione sum(axis=None) ci permette di sommare tutti gli elementi di un array lungo l'asse specificato. Ad esempio, per sommare tutti gli elementi di un vettore: >>> a = np . array ([ 1 , 2 , 3 , 4 ]) >>> a . sum () 10 In caso di array multidimensionale, dovremo specificare, come gi\u00e0 detto, l'asse. Ad esempio, per sommare gli elementi per colonna, dovremo passare il parametro 0 : >>> b = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b . sum ( axis = 0 ) array ([ 4 , 6 ]) Per sommare gli elementi per riga, invece, dovremo passare il parametro 1 : >>> b . sum ( axis = 1 ) array ([ 3 , 7 ]) 7.3.3 - La funzione dot() \u00b6 La funzione dot() ci permette di effettuare l'operazione di moltiplicazione matriciale standard: >>> a = np . array ([[ 1 , 2 ]]) >>> b = np . array ([[ 3 ], [ 4 ]]) >>> a . dot ( b ) array ([ 11 ]) >>> b . dot ( a ) array ([[ 3 , 6 ], [ 4 , 8 ]]) 7.3.4 - La funzione sort() \u00b6 Mediante la funzione sort() \u00e8 possibile ordinare gli elementi di un array. Ad esempio: >>> arr = np . array ([ 2 , 1 , 5 , 3 , 7 , 4 , 6 , 8 ]) >>> np . sort ( arr ) array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]) L'ordine avviene maniera ascendente (ovvero dall'elemento pi\u00f9 piccolo al pi\u00f9 grande). In caso di array \\(n\\) -dimensionale, possiamo anche specificare l'asse lungo il quale avviene l'ordinamento, specificando il parametro axis. Ad esempio: >>> mat = np . array ([[ 2 , 3 , 1 ], [ 4 , 2 , 6 ], [ 7 , 5 , 1 ]]) >>> mat array ([[ 2 , 3 , 1 ], [ 4 , 2 , 6 ], [ 7 , 5 , 1 ]]) Per ordinare lungo le colonne: >>> np . sort ( mat , axis = 0 ) array ([[ 2 , 2 , 1 ], [ 4 , 3 , 1 ], [ 7 , 5 , 6 ]]) Mentre per ordinare lungo le righe: >>> np . sort ( mat , axis = 1 ) array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ], [ 1 , 5 , 7 ]]) Possiamo anche specificare diversi algoritmi di ordinamento mediante l'argomento kind , che ci permette di scegliere tra il quick sort, il merge sort e l'heap sort. Nota Esistono anche altre funzioni per l'ordinamento di un array, come argsort() , lexsort() , searchsorted() e partition() . 7.3.5 - Concatenare due array \u00b6 Possiamo concatenare due array usando la funzione concatenate() : >>> a = np . array ([ 1 , 2 , 3 , 4 ]) >>> b = np . array ([ 5 , 6 , 7 , 8 ]) >>> np . concatenate (( a , b )) array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]) Si pu\u00f2 anche in questo caso usare il parametro axis per specificare l'asse lungo quale concatenare due diversi array: >>> x = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> y = np . array ([[ 5 , 6 ], [ 7 , 8 ]]) >>> np . concatenate (( x , y ), axis = 0 ) array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ], [ 7 , 8 ]]) >>> np . concatenate (( x , y ), axis = 1 ) array ([[ 1 , 2 , 5 , 6 ], [ 3 , 4 , 7 , 8 ]]) Ovviamente, le dimensioni degli array devono essere coerenti affinch\u00e9 vengano concatenati. Ad esempio, con questo array: >>> z = np . array ([[ 9 , 10 ]]) la concatenazione per righe \u00e8 ammissibile: >>> np . concatenate (( x , z ), axis = 0 ) array ([[ 1 , 2 ], [ 3 , 4 ], [ 9 , 10 ]]) mentre la concatenazione per colonne non \u00e8 possibile: >>> np . concatenate (( x , z ), axis = 1 ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"<__array_function__ internals>\" , line 5 , in concatenate ValueError : all the input array dimensions for the concatenation axis must match exactly , but along dimension 0 , the array at index 0 has size 2 and the array at index 1 has size 1 7.3.6 - Rimozione ed inserimento di elementi in un array \u00b6 7.3.6.1 - La funzione delete() \u00b6 La funzione delete(arr, obj, axis=None) ci permette di rimuovere uno o pi\u00f9 elementi di un array specificandone gli indici. La funzione accetta i seguenti parametri: arr : l'array sul quale vogliamo effettuare l'operazione di rimozione; obj : gli indici degli elementi da rimuovere; axis : l'asse su cui vogliamo operare. Ad esempio, immaginiamo di voler rimuovere il primo elemento di un vettore: >>> arr = np . array ([ 1 , 2 , 3 , 4 ]) >>> np . delete ( arr , 0 ) array ([ 2 , 3 , 4 ]) La funzione pu\u00f2 essere anche applicata su pi\u00f9 indici usando una sequenza: >>> np . delete ( arr , range ( 2 )) array ([ 3 , 4 ]) Possiamo anche usare lo slicing: >>> idx = range ( 4 ) >>> np . delete ( arr , idx [ 0 : 2 ]) array ([ 3 , 4 ]) Suggerimento La precedente notazione pu\u00f2 essere rimpiazzata dalla funzione slice(start, stop, step) , che crea un oggetto di classe slice sugli indici che vanno da start a stop con passo step . Questo pu\u00f2 essere usato per scopi analoghi ai precedenti; ad esempio: >>> np . delete ( a , slice ( 0 , 2 , 1 )) array ([ 3 , 4 ]) 7.3.6.1.1 - Array multidimensionali e delete() \u00b6 La funzione delete() pu\u00f2 essere usata anche su array multidimensionali. In questo caso, \u00e8 opportuno specificare l'asse su cui operare. Ad esempio, se vogliamo rimuovere la prima riga dal seguente array, dobbiamo dare il valore 0 al parametro axis : >>> mat = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) >>> np . delete ( mat , 0 , 0 ) array ([[ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Invece, se vogliamo rimuovere la prima colonna, dobbiamo passare il valore 1 : >>> np . delete ( mat , 0 , 1 ) array ([[ 2 , 3 ], [ 5 , 6 ], [ 8 , 9 ]]) Se non specificassimo alcun valore per il parametro axis , otterremmo questo risultato: >>> np . delete ( mat , 0 ) array ([ 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]) In altre parole, non specificando un valore per axis , rimuoveremmo il primo elemento dell'array \"vettorizzato\". Rimozione mediante maschere booleane Spesso \u00e8 preferibile usare, al posto della notazione precedente, una maschera booleana: >>> mask = np . array ([[ True , False , True ], [ False , False , True ], [ False , True , True ]]) >>> mtrx [ mask ] array ([ 1 , 3 , 6 , 8 , 9 ]) 7.3.6.2 - La funzione insert() \u00b6 La funzione insert(arr, obj, values, axis=None) permette di inserire un elemento all'interno di un array. I parametri accettati dalla funzione sono: arr : l'array sul quale vogliamo effettuare l'operazione di inserzione; obj : gli indici su cui inserire i nuovi valori; values : i valori da inserire agli indici specificati da obj ; axis : l'asse su cui vogliamo operare. Ad esempio, per inserire una nuova riga nella matrice precedente, dovremo specificare l'indice di riga ( 3 ), gli elementi della riga da inserire ( [10, 11, 12] ) e l'asse ( 0 ): >>> np . insert ( mat , 3 , [ 10 , 11 , 12 ], 0 ) array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]) Cambiando l'asse in 1 , si effettua l'inserzione sulle colonne: >>> np . insert ( mat , 3 , [ 10 , 11 , 12 ], 1 ) array ([[ 1 , 2 , 3 , 10 ], [ 4 , 5 , 6 , 11 ], [ 7 , 8 , 9 , 12 ]]) Non specificando alcun asse, infine, si inserisce l'elemento specificato nella matrice vettorizzata: >>> np . insert ( mat , 9 , 10 ) array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]) 7.3.6.3 - La funzione append() \u00b6 La funzione append(arr, values, axis=None) permette di inserire in coda ad un array i valori specificati. I parametri accettati dalla funzione sono: arr : l'array sul quale vogliamo effettuare l'operazione di inserzione; values : i valori da inserire in coda all'array; axis : l'asse su cui vogliamo operare. Al solito, non specificando l'asse effettuiamo la concatenazione sulla matrice vettorizzata: >>> np . append ( mat , [[ 10 , 11 , 12 ]]) array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ]) Se specifichiamo il valore 0 sul parametro axis , effettuiamo la concatenazione per righe: >>> np . append ( mat , [[ 10 , 11 , 12 ]], axis = 0 ) array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]) Se specifichiamo il valore 1 sul parametro axis , invece, effettuiamo la concatenazione per colonne: >>> np . append ( mat , [[ 10 ], [ 11 ], [ 12 ]], axis = 1 ) array ([[ 1 , 2 , 3 , 10 ], [ 4 , 5 , 6 , 11 ], [ 7 , 8 , 9 , 12 ]]) Attenzione Nell'ultima istruzione, abbiamo usato un vettore colonna , mentre nella penultima un vettore riga . 7.3.7 - Dimensioni e forma di un array \u00b6 Esistono diverse propriet\u00e0 di un array che ne descrivono dimensioni e forma. Tornando alla nostra matrice mat , possiamo conoscere il numero di assi mediante l'attributo ndarray.ndim : >>> mat . ndim 2 Il numero di elementi \u00e8 invece definito dall'attributo ndarray.size : >>> mat . size 9 L'attributo ndarray.shape restituisce invece una tupla di interi che indica il numero di elementi per ciascuno degli assi dell'array: >>> mat . shape ( 3 , 3 ) 7.3.8 - Modificare le dimensioni di un array \u00b6 Possiamo modificare le dimensioni di un array mediante la funzione reshape(arr, new_shape) . I parametri passati alla funzione sono: arr : l'array di cui modificare le dimensioni; new_shape : le nuove dimensioni dell'array. Se volessimo modificare le dimensioni di una matrice da \\(4 \\times 4\\) a \\(2 \\times 8\\) , potremmo usare la funzione reshape() come segue: >>> mat = np . array ([[ 1 , 2 , 3 , 4 ], [ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ], [ 13 , 14 , 15 , 16 ]]) >>> np . reshape ( mat , ( 2 , 8 )) array ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 ]]) Suggerimento Una forma alternativa \u00e8 la seguente: >>> mat . reshape (( 2 , 8 )) array ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 ]]) Ci\u00f2 significa che la funzione reshape() \u00e8 sia disponibile nella libreria NumPy, sia come metodo sugli oggetti di classe ndarray . Attenzione Le nuove dimensioni dell'array devono essere coerenti con quelle dell'array di partenza! 7.3.9 - Flattening di un array \u00b6 Abbiamo gi\u00e0 visto in precedenza la vettorizzazione di un array, effettuata in automatico in alcune situazioni (come ad esempio la chiamata di delete() o insert() senza specificare il parametro axis ). Tuttavia, possiamo usare la funzione flatten() per effettuare manualmente questa operazione: >>> mat . flatten () array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 ]) La funzione ravel() Un altro modo per vettorizzare un array \u00e8 utilizzare la funzione ravel() , che restituisce un risultato analogo alla flatten() , a meno di una importante differenza: infatti, laddove flatten() restituisce una copia dell'array vettorizzato, ravel() mantiene un riferimento all'array originario.","title":"Dispense"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#73-operazioni-fondamentali-sugli-array","text":"","title":"7.3 - Operazioni fondamentali sugli array"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#731-operazioni-algebriche-di-base","text":"NumPy ci offre la possibilit\u00e0 di effettuare diversi tipi di operazioni algebriche di base sugli array. Ad esempio, \u00e8 possibile sommare due array: >>> a = np . array ([ 1 , 2 ]) >>> b = np . array ([ 3 , 4 ]) >>> a + b array ([ 4 , 6 ]) Possiamo ovviamente anche fare le altre operazioni fondamentali: >>> a - b array ([ - 2 , - 2 ]) >>> a * b array ([ 3 , 8 ]) >>> a / b array ([ 0.33333333 , 0.5 ]) >>> b / a array ([ 3. , 2. ]) Moltiplicazione e divisione Per comprendere appieno il comportamento degli operatori * e /, dovremo parlare del broadcasting. Lo faremo in una delle prossime lezioni.","title":"7.3.1 - Operazioni algebriche di base"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#732-la-funzione-sum","text":"La funzione sum(axis=None) ci permette di sommare tutti gli elementi di un array lungo l'asse specificato. Ad esempio, per sommare tutti gli elementi di un vettore: >>> a = np . array ([ 1 , 2 , 3 , 4 ]) >>> a . sum () 10 In caso di array multidimensionale, dovremo specificare, come gi\u00e0 detto, l'asse. Ad esempio, per sommare gli elementi per colonna, dovremo passare il parametro 0 : >>> b = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b . sum ( axis = 0 ) array ([ 4 , 6 ]) Per sommare gli elementi per riga, invece, dovremo passare il parametro 1 : >>> b . sum ( axis = 1 ) array ([ 3 , 7 ])","title":"7.3.2 - La funzione sum()"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#733-la-funzione-dot","text":"La funzione dot() ci permette di effettuare l'operazione di moltiplicazione matriciale standard: >>> a = np . array ([[ 1 , 2 ]]) >>> b = np . array ([[ 3 ], [ 4 ]]) >>> a . dot ( b ) array ([ 11 ]) >>> b . dot ( a ) array ([[ 3 , 6 ], [ 4 , 8 ]])","title":"7.3.3 - La funzione dot()"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#734-la-funzione-sort","text":"Mediante la funzione sort() \u00e8 possibile ordinare gli elementi di un array. Ad esempio: >>> arr = np . array ([ 2 , 1 , 5 , 3 , 7 , 4 , 6 , 8 ]) >>> np . sort ( arr ) array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]) L'ordine avviene maniera ascendente (ovvero dall'elemento pi\u00f9 piccolo al pi\u00f9 grande). In caso di array \\(n\\) -dimensionale, possiamo anche specificare l'asse lungo il quale avviene l'ordinamento, specificando il parametro axis. Ad esempio: >>> mat = np . array ([[ 2 , 3 , 1 ], [ 4 , 2 , 6 ], [ 7 , 5 , 1 ]]) >>> mat array ([[ 2 , 3 , 1 ], [ 4 , 2 , 6 ], [ 7 , 5 , 1 ]]) Per ordinare lungo le colonne: >>> np . sort ( mat , axis = 0 ) array ([[ 2 , 2 , 1 ], [ 4 , 3 , 1 ], [ 7 , 5 , 6 ]]) Mentre per ordinare lungo le righe: >>> np . sort ( mat , axis = 1 ) array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ], [ 1 , 5 , 7 ]]) Possiamo anche specificare diversi algoritmi di ordinamento mediante l'argomento kind , che ci permette di scegliere tra il quick sort, il merge sort e l'heap sort. Nota Esistono anche altre funzioni per l'ordinamento di un array, come argsort() , lexsort() , searchsorted() e partition() .","title":"7.3.4 - La funzione sort()"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#735-concatenare-due-array","text":"Possiamo concatenare due array usando la funzione concatenate() : >>> a = np . array ([ 1 , 2 , 3 , 4 ]) >>> b = np . array ([ 5 , 6 , 7 , 8 ]) >>> np . concatenate (( a , b )) array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]) Si pu\u00f2 anche in questo caso usare il parametro axis per specificare l'asse lungo quale concatenare due diversi array: >>> x = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> y = np . array ([[ 5 , 6 ], [ 7 , 8 ]]) >>> np . concatenate (( x , y ), axis = 0 ) array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ], [ 7 , 8 ]]) >>> np . concatenate (( x , y ), axis = 1 ) array ([[ 1 , 2 , 5 , 6 ], [ 3 , 4 , 7 , 8 ]]) Ovviamente, le dimensioni degli array devono essere coerenti affinch\u00e9 vengano concatenati. Ad esempio, con questo array: >>> z = np . array ([[ 9 , 10 ]]) la concatenazione per righe \u00e8 ammissibile: >>> np . concatenate (( x , z ), axis = 0 ) array ([[ 1 , 2 ], [ 3 , 4 ], [ 9 , 10 ]]) mentre la concatenazione per colonne non \u00e8 possibile: >>> np . concatenate (( x , z ), axis = 1 ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"<__array_function__ internals>\" , line 5 , in concatenate ValueError : all the input array dimensions for the concatenation axis must match exactly , but along dimension 0 , the array at index 0 has size 2 and the array at index 1 has size 1","title":"7.3.5 - Concatenare due array"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#736-rimozione-ed-inserimento-di-elementi-in-un-array","text":"","title":"7.3.6 - Rimozione ed inserimento di elementi in un array"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#7361-la-funzione-delete","text":"La funzione delete(arr, obj, axis=None) ci permette di rimuovere uno o pi\u00f9 elementi di un array specificandone gli indici. La funzione accetta i seguenti parametri: arr : l'array sul quale vogliamo effettuare l'operazione di rimozione; obj : gli indici degli elementi da rimuovere; axis : l'asse su cui vogliamo operare. Ad esempio, immaginiamo di voler rimuovere il primo elemento di un vettore: >>> arr = np . array ([ 1 , 2 , 3 , 4 ]) >>> np . delete ( arr , 0 ) array ([ 2 , 3 , 4 ]) La funzione pu\u00f2 essere anche applicata su pi\u00f9 indici usando una sequenza: >>> np . delete ( arr , range ( 2 )) array ([ 3 , 4 ]) Possiamo anche usare lo slicing: >>> idx = range ( 4 ) >>> np . delete ( arr , idx [ 0 : 2 ]) array ([ 3 , 4 ]) Suggerimento La precedente notazione pu\u00f2 essere rimpiazzata dalla funzione slice(start, stop, step) , che crea un oggetto di classe slice sugli indici che vanno da start a stop con passo step . Questo pu\u00f2 essere usato per scopi analoghi ai precedenti; ad esempio: >>> np . delete ( a , slice ( 0 , 2 , 1 )) array ([ 3 , 4 ])","title":"7.3.6.1 - La funzione delete()"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#73611-array-multidimensionali-e-delete","text":"La funzione delete() pu\u00f2 essere usata anche su array multidimensionali. In questo caso, \u00e8 opportuno specificare l'asse su cui operare. Ad esempio, se vogliamo rimuovere la prima riga dal seguente array, dobbiamo dare il valore 0 al parametro axis : >>> mat = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) >>> np . delete ( mat , 0 , 0 ) array ([[ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Invece, se vogliamo rimuovere la prima colonna, dobbiamo passare il valore 1 : >>> np . delete ( mat , 0 , 1 ) array ([[ 2 , 3 ], [ 5 , 6 ], [ 8 , 9 ]]) Se non specificassimo alcun valore per il parametro axis , otterremmo questo risultato: >>> np . delete ( mat , 0 ) array ([ 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]) In altre parole, non specificando un valore per axis , rimuoveremmo il primo elemento dell'array \"vettorizzato\". Rimozione mediante maschere booleane Spesso \u00e8 preferibile usare, al posto della notazione precedente, una maschera booleana: >>> mask = np . array ([[ True , False , True ], [ False , False , True ], [ False , True , True ]]) >>> mtrx [ mask ] array ([ 1 , 3 , 6 , 8 , 9 ])","title":"7.3.6.1.1 - Array multidimensionali e delete()"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#7362-la-funzione-insert","text":"La funzione insert(arr, obj, values, axis=None) permette di inserire un elemento all'interno di un array. I parametri accettati dalla funzione sono: arr : l'array sul quale vogliamo effettuare l'operazione di inserzione; obj : gli indici su cui inserire i nuovi valori; values : i valori da inserire agli indici specificati da obj ; axis : l'asse su cui vogliamo operare. Ad esempio, per inserire una nuova riga nella matrice precedente, dovremo specificare l'indice di riga ( 3 ), gli elementi della riga da inserire ( [10, 11, 12] ) e l'asse ( 0 ): >>> np . insert ( mat , 3 , [ 10 , 11 , 12 ], 0 ) array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]) Cambiando l'asse in 1 , si effettua l'inserzione sulle colonne: >>> np . insert ( mat , 3 , [ 10 , 11 , 12 ], 1 ) array ([[ 1 , 2 , 3 , 10 ], [ 4 , 5 , 6 , 11 ], [ 7 , 8 , 9 , 12 ]]) Non specificando alcun asse, infine, si inserisce l'elemento specificato nella matrice vettorizzata: >>> np . insert ( mat , 9 , 10 ) array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ])","title":"7.3.6.2 - La funzione insert()"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#7363-la-funzione-append","text":"La funzione append(arr, values, axis=None) permette di inserire in coda ad un array i valori specificati. I parametri accettati dalla funzione sono: arr : l'array sul quale vogliamo effettuare l'operazione di inserzione; values : i valori da inserire in coda all'array; axis : l'asse su cui vogliamo operare. Al solito, non specificando l'asse effettuiamo la concatenazione sulla matrice vettorizzata: >>> np . append ( mat , [[ 10 , 11 , 12 ]]) array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ]) Se specifichiamo il valore 0 sul parametro axis , effettuiamo la concatenazione per righe: >>> np . append ( mat , [[ 10 , 11 , 12 ]], axis = 0 ) array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]) Se specifichiamo il valore 1 sul parametro axis , invece, effettuiamo la concatenazione per colonne: >>> np . append ( mat , [[ 10 ], [ 11 ], [ 12 ]], axis = 1 ) array ([[ 1 , 2 , 3 , 10 ], [ 4 , 5 , 6 , 11 ], [ 7 , 8 , 9 , 12 ]]) Attenzione Nell'ultima istruzione, abbiamo usato un vettore colonna , mentre nella penultima un vettore riga .","title":"7.3.6.3 - La funzione append()"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#737-dimensioni-e-forma-di-un-array","text":"Esistono diverse propriet\u00e0 di un array che ne descrivono dimensioni e forma. Tornando alla nostra matrice mat , possiamo conoscere il numero di assi mediante l'attributo ndarray.ndim : >>> mat . ndim 2 Il numero di elementi \u00e8 invece definito dall'attributo ndarray.size : >>> mat . size 9 L'attributo ndarray.shape restituisce invece una tupla di interi che indica il numero di elementi per ciascuno degli assi dell'array: >>> mat . shape ( 3 , 3 )","title":"7.3.7 - Dimensioni e forma di un array"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#738-modificare-le-dimensioni-di-un-array","text":"Possiamo modificare le dimensioni di un array mediante la funzione reshape(arr, new_shape) . I parametri passati alla funzione sono: arr : l'array di cui modificare le dimensioni; new_shape : le nuove dimensioni dell'array. Se volessimo modificare le dimensioni di una matrice da \\(4 \\times 4\\) a \\(2 \\times 8\\) , potremmo usare la funzione reshape() come segue: >>> mat = np . array ([[ 1 , 2 , 3 , 4 ], [ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ], [ 13 , 14 , 15 , 16 ]]) >>> np . reshape ( mat , ( 2 , 8 )) array ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 ]]) Suggerimento Una forma alternativa \u00e8 la seguente: >>> mat . reshape (( 2 , 8 )) array ([[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 ]]) Ci\u00f2 significa che la funzione reshape() \u00e8 sia disponibile nella libreria NumPy, sia come metodo sugli oggetti di classe ndarray . Attenzione Le nuove dimensioni dell'array devono essere coerenti con quelle dell'array di partenza!","title":"7.3.8 - Modificare le dimensioni di un array"},{"location":"material/02_libs/07_numpy/03_fundamentals/lecture/#739-flattening-di-un-array","text":"Abbiamo gi\u00e0 visto in precedenza la vettorizzazione di un array, effettuata in automatico in alcune situazioni (come ad esempio la chiamata di delete() o insert() senza specificare il parametro axis ). Tuttavia, possiamo usare la funzione flatten() per effettuare manualmente questa operazione: >>> mat . flatten () array ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 ]) La funzione ravel() Un altro modo per vettorizzare un array \u00e8 utilizzare la funzione ravel() , che restituisce un risultato analogo alla flatten() , a meno di una importante differenza: infatti, laddove flatten() restituisce una copia dell'array vettorizzato, ravel() mantiene un riferimento all'array originario.","title":"7.3.9 - Flattening di un array"},{"location":"material/02_libs/07_numpy/04_algebra/exercises/","text":"E7.4 - Operazioni algebriche in NumPy \u00b6 Esercizio E7.4.1 \u00b6 Verificare che il prodotto tra una matrice invertibile e la sua inversa sia la matrice identit\u00e0. Soluzione S7.4.1 \u00b6 Ecco una possibile soluzione. import numpy as np mat = np . array ([[ 5 , 0 , 1 ], [ 0 , 2 , 2 ], [ 0 , 0 , 3 ]]) mat_inv = np . linalg . inv ( mat ) np . eye ( 3 ) == mat . dot ( mat_inv ) Esercizio E7.4.2 \u00b6 Scriviamo la funzione calcola_determinante(mat) che permetta di calcolare il determinante di una matrice \\(2 \\times 2\\) senza usare l'apposita funzione NumPy . Soluzione S7.4.2 \u00b6 Ecco una possibile soluzione: def calcola_determinante ( mat ): if len ( mat . shape ) == 2 and mat . shape [ 0 ] == mat . shape [ 1 ] and mat . shape [ 0 ] == 2 : return mat [ 0 ][ 0 ] * mat [ 1 ][ 1 ] - mat [ 0 ][ 1 ] * mat [ 1 ][ 0 ] raise ValueError ( 'La matrice non ha le dimensioni attese.' ) Esercizio E7.4.3 \u00b6 Scriviamo la funzione inverti_se_invertibile(mat) che, data una matrice bidimensionale, restituisca l'inversa soltanto se mat \u00e8 bidimensionale, quadrata, e il determinante sia diverso da zero. Usare esclusivamente le istruzioni if . Soluzione S7.4.3 \u00b6 Ecco una possibile soluzione: def inverti_se_invertibile ( mat ): if len ( mat . shape ) == 2 \\ and mat . shape [ 0 ] == mat . shape [ 1 ] \\ and linalg . det ( mat ) != 0 : return linalg . inv ( mat ) raise ValueError ( 'La matrice passata non \u00e8 invertibile.' )","title":"Esercizi"},{"location":"material/02_libs/07_numpy/04_algebra/exercises/#e74-operazioni-algebriche-in-numpy","text":"","title":"E7.4 - Operazioni algebriche in NumPy"},{"location":"material/02_libs/07_numpy/04_algebra/exercises/#esercizio-e741","text":"Verificare che il prodotto tra una matrice invertibile e la sua inversa sia la matrice identit\u00e0.","title":"Esercizio E7.4.1"},{"location":"material/02_libs/07_numpy/04_algebra/exercises/#soluzione-s741","text":"Ecco una possibile soluzione. import numpy as np mat = np . array ([[ 5 , 0 , 1 ], [ 0 , 2 , 2 ], [ 0 , 0 , 3 ]]) mat_inv = np . linalg . inv ( mat ) np . eye ( 3 ) == mat . dot ( mat_inv )","title":"Soluzione S7.4.1"},{"location":"material/02_libs/07_numpy/04_algebra/exercises/#esercizio-e742","text":"Scriviamo la funzione calcola_determinante(mat) che permetta di calcolare il determinante di una matrice \\(2 \\times 2\\) senza usare l'apposita funzione NumPy .","title":"Esercizio E7.4.2"},{"location":"material/02_libs/07_numpy/04_algebra/exercises/#soluzione-s742","text":"Ecco una possibile soluzione: def calcola_determinante ( mat ): if len ( mat . shape ) == 2 and mat . shape [ 0 ] == mat . shape [ 1 ] and mat . shape [ 0 ] == 2 : return mat [ 0 ][ 0 ] * mat [ 1 ][ 1 ] - mat [ 0 ][ 1 ] * mat [ 1 ][ 0 ] raise ValueError ( 'La matrice non ha le dimensioni attese.' )","title":"Soluzione S7.4.2"},{"location":"material/02_libs/07_numpy/04_algebra/exercises/#esercizio-e743","text":"Scriviamo la funzione inverti_se_invertibile(mat) che, data una matrice bidimensionale, restituisca l'inversa soltanto se mat \u00e8 bidimensionale, quadrata, e il determinante sia diverso da zero. Usare esclusivamente le istruzioni if .","title":"Esercizio E7.4.3"},{"location":"material/02_libs/07_numpy/04_algebra/exercises/#soluzione-s743","text":"Ecco una possibile soluzione: def inverti_se_invertibile ( mat ): if len ( mat . shape ) == 2 \\ and mat . shape [ 0 ] == mat . shape [ 1 ] \\ and linalg . det ( mat ) != 0 : return linalg . inv ( mat ) raise ValueError ( 'La matrice passata non \u00e8 invertibile.' )","title":"Soluzione S7.4.3"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/","text":"7.4 - Operazioni matriciali \u00b6 NumPy mette a disposizione il package linalg per permettere di effettuare numerose operazioni matriciali. La maggior parte degli esempi che vedremo nel seguito prevedono l'utilizzo di questo package, per cui possiamo partire importandolo. from numpy import linalg 7.4.1 - Matrice trasposta \u00b6 La prima operazione che vediamo non richiede l'uso del modulo linalg , ed \u00e8 quella che ci permette di effettuare la trasposta di una matrice. Per farlo, usiamo la funzione transpose() . >>> x = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> np . transpose ( x ) array ([[ 1 , 4 ], [ 2 , 5 ], [ 3 , 6 ]]) 7.4.2 - Matrice inversa \u00b6 Possiamo calcolare l'inversa di una matrice usando la funzione inv(mat) del package linalg , dove mat \u00e8 la matrice da invertire. Ad esempio: >>> mat = np . array ([[ 5 , 0 , 0 ], [ 0 , 2 , 0 ], [ 0 , 0 , 4 ]]) >>> linalg . inv ( mat ) array ([[ 0.2 , 0. , 0. ], [ 0. , 0.5 , 0. ], [ 0. , 0. , 0.25 ]]) Ovviamente, la matrice mat deve essere invertibile. Nel caso passassimo una matrice rettangolare, infatti, verrebbe lanciato un LinAlgError : >>> mat = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> linalg . inv ( mat ) Traceback ( most recent call last ): numpy . linalg . LinAlgError : Last 2 dimensions of the array must be square Lo stesso accade per una matrice singolare: >>> mat = np . array ([[ 1 , 1 , 1 ], [ 2 , 2 , 2 ], [ 0 , 0 , 1 ]]) >>> linalg . inv ( mat ) Traceback ( most recent call last ): numpy . linalg . LinAlgError : Singular matrix 7.4.3 - Prodotti vettoriali e matriciali \u00b6 7.4.3.1 - La funzione dot() \u00b6 Nella scorsa lezione abbiamo visto un esempio di uso della funzione dot(a, b) , necessaria a calcolare il prodotto matriciale tra gli array a ed b . Ovviamente, si applicano tutte le regole valevoli per il calcolo del prodotto matriciale (ovvero quello relativo alla moltiplicazione righe per colonne); riassumiamole nella seguente tabella sulla base delle dimensionalit\u00e0 di a ed b . Dimensionalit\u00e0 a Dimensionalit\u00e0 b Risultato Note Monodimensionale (vettore) Monodimensionale (vettore) Prodotto scalare / Bidimensionale (matrice) Bidimensionale (matrice) Prodotto matriciale Preferire la funzione matmul() Scalare \\(n\\) -dimensionale Prodotto scalare per array \\(n\\) -dimensionale Preferire la funzione multiply(a, b) o l'operatore * \\(n\\) -dimensionale Scalare Prodotto scalare per array \\(n\\) -dimensionale Preferire la funzione multiply(a, b) o l'operatore * Nel caso entrambi gli array siano \\(n\\) -dimensionali, si applicano altre regole, che \u00e8 possibile recuperare a questo indirizzo . 7.4.3.2 - Prodotto interno \u00b6 Possiamo usare la funzione inner(a, b) per calcolare il prodotto interno (o scalare ) tra i vettori a e b : >>> a = np . array ([ 1 , 2 , 3 ]) >>> b = np . array ([ 4 , 5 , 6 ]) >>> np . inner ( a , b ) 32 Definizione di prodotto interno Ricordiamo che per due generici vettori monodimensionali \\(v_1 = [v_{11}, \\ldots, v_{1j}], v_2 = [v_{21}, \\ldots, v_{2j}]\\) il prodotto scalare \u00e8 dato da: $$ p = \\sum {i=1}^j v \\cdot v_{2i} $$ Un lettore attento avr\u00e0 notato che, nella pratica, per vettori monodimensionali, le funzioni inner() e dot() restituiscono lo stesso risultato: np . inner ( a , b ) # Output: 32 a . dot ( b ) # Output: 32, stesso di b.dot() La differenza tra le due funzioni \u00e8 visibile quando si utilizzano array a dimensionalit\u00e0 maggiore di 1 (anche comuni matrici vanno bene). Infatti: >>> a = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b = np . array ([[ 5 , 6 ], [ 7 , 8 >>> np . inner ( a , b ) # Risultato con inner() array ([[ 17 , 23 ], [ 39 , 53 ]]) >>> a . dot ( b ) # Risultato con dot() array ([[ 19 , 22 ], [ 43 , 50 ]]) In pratica, riprendendo la documentazione: per quello che riguarda la funzione dot() , questa \u00e8 equivalente a matmul() , e quindi rappresenta una moltiplicazione matriciale che, nel caso di vettori monodimensionali, equivale al prodotto vettoriale, mentre per \\(n\\) dimensioni \u00e8 la somma dei prodotti tra l'ultima dimensione del primo vettore e delle dimensioni che vanno da 2 ad \\(n\\) del secondo; per quello che riguarda la funzione inner() , rappresenta il prodotto vettoriale nel caso ad una dimensione, mentre nel caso di \\(n\\) dimensioni rappresenta la somma dei prodotti lungo l'ultima dimensione. In altri termini: a . dot ( b ) == sum ( a [ i , :] * b [:, j ]) np . inner ( a , b ) == sum ( a [ i , :] * b [ j , :]) ovvero: \\[ dot = \\left(\\begin{array}{cc} 1 & 2\\\\ 3 & 4 \\end{array}\\right) \\left(\\begin{array}{cc} 5 & 6\\\\ 7 & 8 \\end{array}\\right) = \\\\ = \\left(\\begin{array}{cc} 1 \\cdot 5 + 2 \\cdot 7 & 1 \\cdot 6 + 2 \\cdot 8 \\\\ 3 \\cdot 5 + 4 \\cdot 7 & 1 \\cdot 6 + 4 \\cdot 8 \\end{array}\\right) = \\left(\\begin{array}{cc} 19 & 22\\\\ 43 & 50 \\end{array}\\right) \\] \\[ inner = \\left(\\begin{array}{cc} 1 & 2\\\\ 3 & 4 \\end{array}\\right) \\left(\\begin{array}{cc} 5 & 6\\\\ 7 & 8 \\end{array}\\right) = \\\\ = \\left(\\begin{array}{cc} 17 & 23\\\\ 39 & 53 \\end{array}\\right) \\] 7.4.3.2 - Prodotto esterno \u00b6 Possiamo usare la funzione outer(a, b) per calcolare il prodotto esterno tra due vettori. In particolare, dati due vettori \\(a = [a_1, a_2, \\ldots, a_n]\\) e \\(b = [b_1, b_2, \\ldots, b_n]\\) , il prodotto esterno \u00e8 definito come la matrice \\(P\\) tale che: \\[ P = \\left[ \\begin{array}{ccc} a_1 \\cdot b_1 & \\ldots & a_1 \\cdot b_n \\\\ \\vdots & \\ddots & \\vdots \\\\ a_n \\cdot b_1 & \\ldots & a_n \\cdot b_n \\end{array} \\right] \\] Ad esempio: >>> np . outer ( a , b ) array ([[ 5 , 6 , 7 , 8 ], [ 10 , 12 , 14 , 16 ], [ 15 , 18 , 21 , 24 ], [ 20 , 24 , 28 , 32 ]]) 7.4.3.3 - La funzione matmul \u00b6 Quando abbiamo parlato della funzione dot(a, b) abbiamo visto come sia possibile usarla per effettuare il prodotto matriciale tra le matrici mat_1 e mat_2 . Tuttavia, esiste un'altra possibilit\u00e0, che \u00e8 anche quella consigliata , ovvero usare la funzione matmul(a, b) : >>> a = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b = np . array ([[ 5 , 6 ], [ 7 , 8 ]]) >>> np . matmul ( a , b ) array ([[ 19 , 22 ], [ 43 , 50 ]]) La funzione matmul() ha una differenza fondamentale rispetto alla funzione dot() , in quanto non accetta scalari come parametro (anche se \u00e8 possibile passare vettori ed array \\(n\\) -dimensionali). Esiste in realt\u00e0 un'altra differenza importante, che riguarda le operazioni \\(n\\) -dimensionali, ma che non tratteremo in questa sede. 7.4.4 - Potenza di matrice \u00b6 La funzione matrix_power(a, n) del package linalg permette di elevare a potenza n della matrice a . Ad esempio: >>> linalg . matrix_power ( a , 5 ) array ([[ 1069 , 1558 ], [ 2337 , 3406 ]]) 7.4.5 - Decomposizione ai valori singolari \u00b6 La decomposizione ai valori singolari , detta anche SVD dall'acronimo inglese Singular Value Decomposition , \u00e8 una tecnica di decomposizione di una matrice che permette di scomporla in modo da semplificarci la vita in alcune situazioni. Approfondimento Per un'approfondimento sui principi alla base della SVD, consultare l' appendice E.1 . L'implementazione da zero della SVD \u00e8 estremamente complessa; tuttavia, NumPy ci viene quindi in aiuto con la funzione svd(mat) del package linalg : >>> ( u , s , v ) = linalg . svd ( a ) >>> u array ([[ - 0.40455358 , - 0.9145143 ], [ - 0.9145143 , 0.40455358 ]]) >>> s array ([ 5.4649857 , 0.36596619 ]) >>> v array ([[ - 0.57604844 , - 0.81741556 ], [ 0.81741556 , - 0.57604844 ]]) 7.4.6 - Autovalori ed autovettori \u00b6 Per calcolare gli autovalori e gli autovettori di una matrice, NumPy ci mette a disposizione la funzione eig(a) , sempre appartenente al package linalg , che restituisce gli autovalori e gli autovettori destri di una matrice quadrata: >>> ( v , w ) = linalg . eig ( a ) >>> v array ([ - 0.37228132 , 5.37228132 ]) >>> w array ([[ - 0.82456484 , - 0.41597356 ], [ 0.56576746 , - 0.90937671 ]]) 7.4.7 - Norma \u00b6 La funzione linalg.norm(a) ci permette di calcolare la norma di una matrice. Opzionalmente, possiamo specificare tre parametri, ovvero: ord , che rappresenta l'ordine della norma da calcolare (di default, viene calcolata la norma di Frobenius); axis , che indica l'asse (o gli assi, in caso di array multidimensionale) su cui operare; keepdims , usata per restituire, opzionalmente, l'asse su cui viene calcolata la norma. Per calcolare la norma di Frobenius della matrice mat possiamo usare questa sintassi: >>> linalg . norm ( a ) 5.477225575051661 7.4.8 - Determinante, rango e traccia \u00b6 Possiamo calcolare rapidamente determinante, rango e traccia di una matrice mediante le funzioni det(a) , matrix_rank(a) e trace(a) , quest'ultima non appartenente al package linalg . Ad esempio: >>> linalg . det ( a ) - 2.0000000000000004 >>> linalg . matrix_rank ( a ) 2 >>> np . trace ( a ) 5 La funzione trace() pu\u00f2 anche essere usata per calcolare la sommatoria delle sovra/sotto diagonali specificando il parametro offset . Ad esempio: >>> mtrx = np . array ([[ 5 , 2 , 9 ], [ 2 , 3 , 1 ], [ 4 , - 2 , 12 ]]) >>> np . trace ( mtrx , offset = 1 ) 3 >>> np . trace ( mtrx , offset =- 1 ) 0 7.4.9 - Risoluzione di sistemi di equazioni lineari \u00b6 Chiudiamo questa (necessariamente breve!) carrellata sulle operazioni di algebra lineare con la funzione solve(a, b) , che permette di risolvere un sistema di equazioni lineari nel quale la matrice a \u00e8 la matrice dei coefficienti, mentre il vettore b \u00e8 il vettore dei termini noti. Ad esempio: >>> b = np . array ([ 3 , 2 , 3 ]) >>> linalg . solve ( mat , b ) array ([ - 7.5 , 4.5 , 3.5 ]) Ovviamente, la matrice a deve essere quadrata, mentre il vettore b deve avere esattamente n elementi, con n ordine di a !","title":"Dispense"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#74-operazioni-matriciali","text":"NumPy mette a disposizione il package linalg per permettere di effettuare numerose operazioni matriciali. La maggior parte degli esempi che vedremo nel seguito prevedono l'utilizzo di questo package, per cui possiamo partire importandolo. from numpy import linalg","title":"7.4 - Operazioni matriciali"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#741-matrice-trasposta","text":"La prima operazione che vediamo non richiede l'uso del modulo linalg , ed \u00e8 quella che ci permette di effettuare la trasposta di una matrice. Per farlo, usiamo la funzione transpose() . >>> x = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> np . transpose ( x ) array ([[ 1 , 4 ], [ 2 , 5 ], [ 3 , 6 ]])","title":"7.4.1 - Matrice trasposta"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#742-matrice-inversa","text":"Possiamo calcolare l'inversa di una matrice usando la funzione inv(mat) del package linalg , dove mat \u00e8 la matrice da invertire. Ad esempio: >>> mat = np . array ([[ 5 , 0 , 0 ], [ 0 , 2 , 0 ], [ 0 , 0 , 4 ]]) >>> linalg . inv ( mat ) array ([[ 0.2 , 0. , 0. ], [ 0. , 0.5 , 0. ], [ 0. , 0. , 0.25 ]]) Ovviamente, la matrice mat deve essere invertibile. Nel caso passassimo una matrice rettangolare, infatti, verrebbe lanciato un LinAlgError : >>> mat = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> linalg . inv ( mat ) Traceback ( most recent call last ): numpy . linalg . LinAlgError : Last 2 dimensions of the array must be square Lo stesso accade per una matrice singolare: >>> mat = np . array ([[ 1 , 1 , 1 ], [ 2 , 2 , 2 ], [ 0 , 0 , 1 ]]) >>> linalg . inv ( mat ) Traceback ( most recent call last ): numpy . linalg . LinAlgError : Singular matrix","title":"7.4.2 - Matrice inversa"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#743-prodotti-vettoriali-e-matriciali","text":"","title":"7.4.3 - Prodotti vettoriali e matriciali"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#7431-la-funzione-dot","text":"Nella scorsa lezione abbiamo visto un esempio di uso della funzione dot(a, b) , necessaria a calcolare il prodotto matriciale tra gli array a ed b . Ovviamente, si applicano tutte le regole valevoli per il calcolo del prodotto matriciale (ovvero quello relativo alla moltiplicazione righe per colonne); riassumiamole nella seguente tabella sulla base delle dimensionalit\u00e0 di a ed b . Dimensionalit\u00e0 a Dimensionalit\u00e0 b Risultato Note Monodimensionale (vettore) Monodimensionale (vettore) Prodotto scalare / Bidimensionale (matrice) Bidimensionale (matrice) Prodotto matriciale Preferire la funzione matmul() Scalare \\(n\\) -dimensionale Prodotto scalare per array \\(n\\) -dimensionale Preferire la funzione multiply(a, b) o l'operatore * \\(n\\) -dimensionale Scalare Prodotto scalare per array \\(n\\) -dimensionale Preferire la funzione multiply(a, b) o l'operatore * Nel caso entrambi gli array siano \\(n\\) -dimensionali, si applicano altre regole, che \u00e8 possibile recuperare a questo indirizzo .","title":"7.4.3.1 - La funzione dot()"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#7432-prodotto-interno","text":"Possiamo usare la funzione inner(a, b) per calcolare il prodotto interno (o scalare ) tra i vettori a e b : >>> a = np . array ([ 1 , 2 , 3 ]) >>> b = np . array ([ 4 , 5 , 6 ]) >>> np . inner ( a , b ) 32 Definizione di prodotto interno Ricordiamo che per due generici vettori monodimensionali \\(v_1 = [v_{11}, \\ldots, v_{1j}], v_2 = [v_{21}, \\ldots, v_{2j}]\\) il prodotto scalare \u00e8 dato da: $$ p = \\sum {i=1}^j v \\cdot v_{2i} $$ Un lettore attento avr\u00e0 notato che, nella pratica, per vettori monodimensionali, le funzioni inner() e dot() restituiscono lo stesso risultato: np . inner ( a , b ) # Output: 32 a . dot ( b ) # Output: 32, stesso di b.dot() La differenza tra le due funzioni \u00e8 visibile quando si utilizzano array a dimensionalit\u00e0 maggiore di 1 (anche comuni matrici vanno bene). Infatti: >>> a = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b = np . array ([[ 5 , 6 ], [ 7 , 8 >>> np . inner ( a , b ) # Risultato con inner() array ([[ 17 , 23 ], [ 39 , 53 ]]) >>> a . dot ( b ) # Risultato con dot() array ([[ 19 , 22 ], [ 43 , 50 ]]) In pratica, riprendendo la documentazione: per quello che riguarda la funzione dot() , questa \u00e8 equivalente a matmul() , e quindi rappresenta una moltiplicazione matriciale che, nel caso di vettori monodimensionali, equivale al prodotto vettoriale, mentre per \\(n\\) dimensioni \u00e8 la somma dei prodotti tra l'ultima dimensione del primo vettore e delle dimensioni che vanno da 2 ad \\(n\\) del secondo; per quello che riguarda la funzione inner() , rappresenta il prodotto vettoriale nel caso ad una dimensione, mentre nel caso di \\(n\\) dimensioni rappresenta la somma dei prodotti lungo l'ultima dimensione. In altri termini: a . dot ( b ) == sum ( a [ i , :] * b [:, j ]) np . inner ( a , b ) == sum ( a [ i , :] * b [ j , :]) ovvero: \\[ dot = \\left(\\begin{array}{cc} 1 & 2\\\\ 3 & 4 \\end{array}\\right) \\left(\\begin{array}{cc} 5 & 6\\\\ 7 & 8 \\end{array}\\right) = \\\\ = \\left(\\begin{array}{cc} 1 \\cdot 5 + 2 \\cdot 7 & 1 \\cdot 6 + 2 \\cdot 8 \\\\ 3 \\cdot 5 + 4 \\cdot 7 & 1 \\cdot 6 + 4 \\cdot 8 \\end{array}\\right) = \\left(\\begin{array}{cc} 19 & 22\\\\ 43 & 50 \\end{array}\\right) \\] \\[ inner = \\left(\\begin{array}{cc} 1 & 2\\\\ 3 & 4 \\end{array}\\right) \\left(\\begin{array}{cc} 5 & 6\\\\ 7 & 8 \\end{array}\\right) = \\\\ = \\left(\\begin{array}{cc} 17 & 23\\\\ 39 & 53 \\end{array}\\right) \\]","title":"7.4.3.2 - Prodotto interno"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#7432-prodotto-esterno","text":"Possiamo usare la funzione outer(a, b) per calcolare il prodotto esterno tra due vettori. In particolare, dati due vettori \\(a = [a_1, a_2, \\ldots, a_n]\\) e \\(b = [b_1, b_2, \\ldots, b_n]\\) , il prodotto esterno \u00e8 definito come la matrice \\(P\\) tale che: \\[ P = \\left[ \\begin{array}{ccc} a_1 \\cdot b_1 & \\ldots & a_1 \\cdot b_n \\\\ \\vdots & \\ddots & \\vdots \\\\ a_n \\cdot b_1 & \\ldots & a_n \\cdot b_n \\end{array} \\right] \\] Ad esempio: >>> np . outer ( a , b ) array ([[ 5 , 6 , 7 , 8 ], [ 10 , 12 , 14 , 16 ], [ 15 , 18 , 21 , 24 ], [ 20 , 24 , 28 , 32 ]])","title":"7.4.3.2 - Prodotto esterno"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#7433-la-funzione-matmul","text":"Quando abbiamo parlato della funzione dot(a, b) abbiamo visto come sia possibile usarla per effettuare il prodotto matriciale tra le matrici mat_1 e mat_2 . Tuttavia, esiste un'altra possibilit\u00e0, che \u00e8 anche quella consigliata , ovvero usare la funzione matmul(a, b) : >>> a = np . array ([[ 1 , 2 ], [ 3 , 4 ]]) >>> b = np . array ([[ 5 , 6 ], [ 7 , 8 ]]) >>> np . matmul ( a , b ) array ([[ 19 , 22 ], [ 43 , 50 ]]) La funzione matmul() ha una differenza fondamentale rispetto alla funzione dot() , in quanto non accetta scalari come parametro (anche se \u00e8 possibile passare vettori ed array \\(n\\) -dimensionali). Esiste in realt\u00e0 un'altra differenza importante, che riguarda le operazioni \\(n\\) -dimensionali, ma che non tratteremo in questa sede.","title":"7.4.3.3 - La funzione matmul"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#744-potenza-di-matrice","text":"La funzione matrix_power(a, n) del package linalg permette di elevare a potenza n della matrice a . Ad esempio: >>> linalg . matrix_power ( a , 5 ) array ([[ 1069 , 1558 ], [ 2337 , 3406 ]])","title":"7.4.4 - Potenza di matrice"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#745-decomposizione-ai-valori-singolari","text":"La decomposizione ai valori singolari , detta anche SVD dall'acronimo inglese Singular Value Decomposition , \u00e8 una tecnica di decomposizione di una matrice che permette di scomporla in modo da semplificarci la vita in alcune situazioni. Approfondimento Per un'approfondimento sui principi alla base della SVD, consultare l' appendice E.1 . L'implementazione da zero della SVD \u00e8 estremamente complessa; tuttavia, NumPy ci viene quindi in aiuto con la funzione svd(mat) del package linalg : >>> ( u , s , v ) = linalg . svd ( a ) >>> u array ([[ - 0.40455358 , - 0.9145143 ], [ - 0.9145143 , 0.40455358 ]]) >>> s array ([ 5.4649857 , 0.36596619 ]) >>> v array ([[ - 0.57604844 , - 0.81741556 ], [ 0.81741556 , - 0.57604844 ]])","title":"7.4.5 - Decomposizione ai valori singolari"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#746-autovalori-ed-autovettori","text":"Per calcolare gli autovalori e gli autovettori di una matrice, NumPy ci mette a disposizione la funzione eig(a) , sempre appartenente al package linalg , che restituisce gli autovalori e gli autovettori destri di una matrice quadrata: >>> ( v , w ) = linalg . eig ( a ) >>> v array ([ - 0.37228132 , 5.37228132 ]) >>> w array ([[ - 0.82456484 , - 0.41597356 ], [ 0.56576746 , - 0.90937671 ]])","title":"7.4.6 - Autovalori ed autovettori"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#747-norma","text":"La funzione linalg.norm(a) ci permette di calcolare la norma di una matrice. Opzionalmente, possiamo specificare tre parametri, ovvero: ord , che rappresenta l'ordine della norma da calcolare (di default, viene calcolata la norma di Frobenius); axis , che indica l'asse (o gli assi, in caso di array multidimensionale) su cui operare; keepdims , usata per restituire, opzionalmente, l'asse su cui viene calcolata la norma. Per calcolare la norma di Frobenius della matrice mat possiamo usare questa sintassi: >>> linalg . norm ( a ) 5.477225575051661","title":"7.4.7 - Norma"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#748-determinante-rango-e-traccia","text":"Possiamo calcolare rapidamente determinante, rango e traccia di una matrice mediante le funzioni det(a) , matrix_rank(a) e trace(a) , quest'ultima non appartenente al package linalg . Ad esempio: >>> linalg . det ( a ) - 2.0000000000000004 >>> linalg . matrix_rank ( a ) 2 >>> np . trace ( a ) 5 La funzione trace() pu\u00f2 anche essere usata per calcolare la sommatoria delle sovra/sotto diagonali specificando il parametro offset . Ad esempio: >>> mtrx = np . array ([[ 5 , 2 , 9 ], [ 2 , 3 , 1 ], [ 4 , - 2 , 12 ]]) >>> np . trace ( mtrx , offset = 1 ) 3 >>> np . trace ( mtrx , offset =- 1 ) 0","title":"7.4.8 - Determinante, rango e traccia"},{"location":"material/02_libs/07_numpy/04_algebra/lecture/#749-risoluzione-di-sistemi-di-equazioni-lineari","text":"Chiudiamo questa (necessariamente breve!) carrellata sulle operazioni di algebra lineare con la funzione solve(a, b) , che permette di risolvere un sistema di equazioni lineari nel quale la matrice a \u00e8 la matrice dei coefficienti, mentre il vettore b \u00e8 il vettore dei termini noti. Ad esempio: >>> b = np . array ([ 3 , 2 , 3 ]) >>> linalg . solve ( mat , b ) array ([ - 7.5 , 4.5 , 3.5 ]) Ovviamente, la matrice a deve essere quadrata, mentre il vettore b deve avere esattamente n elementi, con n ordine di a !","title":"7.4.9 - Risoluzione di sistemi di equazioni lineari"},{"location":"material/02_libs/07_numpy/05_polynomials/exercises/","text":"E7.5 - Operazioni polinomiali in NumPy \u00b6 Esercizio E7.5.1 \u00b6 Scriviamo la funzione somma_polinomi(pol_1, pol_2) che permetta di sommare due polinomi di grandezza arbitraria. Soluzione S7.5.1 \u00b6 def somma_polinomi ( pol_1 , pol_2 ): if len ( pol_1 ) < len ( pol_2 ): while len ( pol_1 ) < len ( pol_2 ): pol_1 . insert ( 0 , 0 ) elif len ( pol_2 ) < len ( pol_1 ): while len ( pol_2 ) < len ( pol_1 ): pol_2 . insert ( 0 , 0 ) return [( pol_1 [ i ] + pol_2 [ i ]) for i in range ( len ( pol_1 ))] somma_polinomi ([ 0 , 1 , 2 ], [ 2 , 2 , 1 ]) somma_polinomi ([ 1 , 2 ], [ 2 , 2 , 1 ]) somma_polinomi ([ 1 , 2 ], [ 2 , 2 , 2 , 1 ]) Esercizio E7.5.2 \u00b6 Scriviamo la funzione calcola_media(array, pesi) che restituisce il valor medio di un array; usiamo una lista. Il parametro pesi \u00e8 opzionale; nel caso sia lasciato il valore opzionale (lista vuota), la media sar\u00e0 aritmetica; in caso contrario, verifichiamo la coerenza delle dimensioni dei vettori e restituiamo la media pesata. Soluzione S7.5.2 \u00b6 def calcola_media ( array , pesi = []): if pesi == []: return sum ( array ) / len ( array ) else : if len ( pesi ) == len ( array ): return sum ([( pesi [ i ] * array [ i ]) for i in range ( len ( array ))]) / sum ( pesi ) raise ValueError ( 'La lunghezza dei pesi non corrisponde a quella degli array.' ) calcola_media ([ 5 , 4 , 5 ]) calcola_media ([ 5 , 4 , 5 ], [ 0 , 1 , 0 ]) calcola_media ([ 5 , 4 , 5 ], [ 0 , 1 ]) Esercizio E7.5.3 \u00b6 Scriviamo la funzione descrivi(array) che permette di descrivere un array in termini non parametrici, individuando mediana, deviazione standard e range interquartile (ovvero tra il 25-percentile ed il 75-percentile). Soluzione S7.5.2 \u00b6 def descrivi ( array ): return ( np . median ( array ), np . std ( array ), np . percentile ( array , 25 ) - np . percentile ( array , 75 ) ) descrivi ( np . array ([ 3 , 5 , 3 , 2 , 1 , 8 ]))","title":"Esercizi"},{"location":"material/02_libs/07_numpy/05_polynomials/exercises/#e75-operazioni-polinomiali-in-numpy","text":"","title":"E7.5 - Operazioni polinomiali in NumPy"},{"location":"material/02_libs/07_numpy/05_polynomials/exercises/#esercizio-e751","text":"Scriviamo la funzione somma_polinomi(pol_1, pol_2) che permetta di sommare due polinomi di grandezza arbitraria.","title":"Esercizio E7.5.1"},{"location":"material/02_libs/07_numpy/05_polynomials/exercises/#soluzione-s751","text":"def somma_polinomi ( pol_1 , pol_2 ): if len ( pol_1 ) < len ( pol_2 ): while len ( pol_1 ) < len ( pol_2 ): pol_1 . insert ( 0 , 0 ) elif len ( pol_2 ) < len ( pol_1 ): while len ( pol_2 ) < len ( pol_1 ): pol_2 . insert ( 0 , 0 ) return [( pol_1 [ i ] + pol_2 [ i ]) for i in range ( len ( pol_1 ))] somma_polinomi ([ 0 , 1 , 2 ], [ 2 , 2 , 1 ]) somma_polinomi ([ 1 , 2 ], [ 2 , 2 , 1 ]) somma_polinomi ([ 1 , 2 ], [ 2 , 2 , 2 , 1 ])","title":"Soluzione S7.5.1"},{"location":"material/02_libs/07_numpy/05_polynomials/exercises/#esercizio-e752","text":"Scriviamo la funzione calcola_media(array, pesi) che restituisce il valor medio di un array; usiamo una lista. Il parametro pesi \u00e8 opzionale; nel caso sia lasciato il valore opzionale (lista vuota), la media sar\u00e0 aritmetica; in caso contrario, verifichiamo la coerenza delle dimensioni dei vettori e restituiamo la media pesata.","title":"Esercizio E7.5.2"},{"location":"material/02_libs/07_numpy/05_polynomials/exercises/#soluzione-s752","text":"def calcola_media ( array , pesi = []): if pesi == []: return sum ( array ) / len ( array ) else : if len ( pesi ) == len ( array ): return sum ([( pesi [ i ] * array [ i ]) for i in range ( len ( array ))]) / sum ( pesi ) raise ValueError ( 'La lunghezza dei pesi non corrisponde a quella degli array.' ) calcola_media ([ 5 , 4 , 5 ]) calcola_media ([ 5 , 4 , 5 ], [ 0 , 1 , 0 ]) calcola_media ([ 5 , 4 , 5 ], [ 0 , 1 ])","title":"Soluzione S7.5.2"},{"location":"material/02_libs/07_numpy/05_polynomials/exercises/#esercizio-e753","text":"Scriviamo la funzione descrivi(array) che permette di descrivere un array in termini non parametrici, individuando mediana, deviazione standard e range interquartile (ovvero tra il 25-percentile ed il 75-percentile).","title":"Esercizio E7.5.3"},{"location":"material/02_libs/07_numpy/05_polynomials/exercises/#soluzione-s752_1","text":"def descrivi ( array ): return ( np . median ( array ), np . std ( array ), np . percentile ( array , 25 ) - np . percentile ( array , 75 ) ) descrivi ( np . array ([ 3 , 5 , 3 , 2 , 1 , 8 ]))","title":"Soluzione S7.5.2"},{"location":"material/02_libs/07_numpy/05_polynomials/lecture/","text":"7.5 - Operazioni polinomiali in NumPy \u00b6 Il modulo numpy.polynomial.polynomial ci offre numerose classi e funzionalit\u00e0 per il trattamento dei polinomi. Vediamo quali sono le principali. Immaginiamo di avere due diversi polinomi (cui non assoceremo alcun significato fisico), ovvero: \\[ \\begin{cases} c1: 2x + 1 \\\\ c2: x^2 + 3x + 2 \\end{cases} \\] Vediamo come usare dei metodi forniti dal modulo polynomial per effettuare delle operazioni su di loro. Prima di partire, per\u00f2, introduciamo gli oggetti di classe poly1d , che ci permettono di rappresentare in maniera compiuta un polinomio. In particolare, partendo dai coefficienti di un generico polinomio p , potremo ottenere un oggetto poly1d invocando l'omonimo costruttore: p_pol = np . poly1d ( p ) Il vantaggio principale degli oggetti poly1d sta sia nella loro rappresentazione, sia nel fatto che possono essere direttamente utilizzati all'interno delle funzioni per il calcolo polinomiale che vedremo nel seguito. 7.5.1 - Addizione di polinomi \u00b6 Per effettuare l'addizione di due polinomi, possiamo usare il metodo polyadd(c1, c2) , che accetta come parametri due vettori c1 e c2 che rappresentano, rispettivamente, i coefficienti del polinomio 1 e 2. Volendo sommare il primo ed il secondo polinomio, potremo scrivere: >>> from numpy.polynomial import polynomial as poly >>> c1 = ( 0 , 2 , 1 ) >>> c2 = ( 1 , 3 , 2 ) >>> poly . polyadd ( c1 , c2 ) Questa operazione ci dar\u00e0 il risultato atteso, ovvero \\(x^2 + 5x + 3\\) . Notiamo come le dimensioni di c1 e di c2 debbano essere tra loro coerenti . Se infatti omettessimo il coefficiente \\(0\\) al termine di secondo grado in c1 , il risultato sarebbe il seguente: >>> c3 = ( 2 , 1 ) >>> poly . polyadd ( c3 , c2 ) array ([ 3. , 4. , 2. ]) Ovviamente, il risultato precedente pu\u00f2 essere errato in base al valore assunto dal polinomio c3 . 7.5.2 - Sottrazione di polinomi \u00b6 Possiamo poi sottrarre due polinomi usando la funzione polysub(c1, c2) , i cui parametri sono identici a quelli passati a polyadd() : >>> poly . polysub ( c2 , c1 ) array ([ 1. , 1. , 1. ]) 7.5.3 - Moltiplicazione di polinomi \u00b6 Le considerazioni precedenti possono essere banalmente traslate al caso della moltiplicazione tra polinomi, ottenibile mediante la funzione polymul(c1, c2) . >>> poly . polymul ( c1 , c2 ) array ([ 0. , 2. , 7. , 7. , 2. ]) Attenzione! Nelle ultime versioni di NumPy, i coefficienti sono ordinati da quello a grado pi\u00f9 basso a quello di grado pi\u00f9 alto! 7.5.4 - Divisione tra polinomi \u00b6 La divisione tra polinomi \u00e8 un'operazione leggermente pi\u00f9 complessa delle altre, e prevede l'uso della funzione polydiv(c1, c2) , che restituir\u00e0 stavolta due array: il primo, q , rappresenta i coefficienti del polinomio quoziente, mentre il secondo, r , indica i coefficienti del polinomio resto. Nel nostro caso: >>> ( q , r ) = poly . polydiv ( c1 , c2 ) >>> q ; r array ([ 0.5 ]) array ([ - 0.5 , 0.5 ]) Anche in questo caso, i coefficienti sono ordinati da quello a grado pi\u00f9 basso a quello a grado pi\u00f9 alto. 7.5.5 - Elevazione a potenza \u00b6 Chiudiamo questa breve panoramica parlando dell'elevazione a potenza di un polinomio, effettuabile mediante la funzione polypow(c, pow) , con c vettore dei coefficienti, e pow potenza a cui elevare: >>> poly . polypow ( c1 , 2 ) array ([ 0. , 0. , 4. , 4. , 1. ]) Anche in questo caso, vengono riportati i termini pari a zero nei risultati. 7.5.6 - Valore assunto da un polinomio \u00b6 Per valutare il valore \\(y\\) assunto dal polinomio per un determinato valore di \\(x\\) , usiamo la funzione polyval(x, p) , che accetta come argomento un intero (o una lista di interi) x ed un polinomio p . Se volessimo valutare il valore assunto da \\(y\\) per \\(x \\in [1, 2]\\) sulla retta rappresentata dal polinomio c1 , ad esempio, potremmo usare polyval() come segue: >>> poly . polyval ([ 1 , 2 ], c1 ) array ([ 3. , 8. ]) 7.5.7 - Derivata ed integrale di funzioni polinomiali \u00b6 Concludiamo questa breve carrellata con due metodi in grado di calcolare, rispettivamente, la derivata e l'integrale di una funzione polinomiale. Il metodo polyder(p, m) , infatti, permette di calcolare la derivata di ordine m del polinomio p (di default, m=2 ): >>> poly . polyder ( c1 ) array ([ 2. , 2. ]) Il metodo duale \u00e8 polyint(p, m) , che prevedibilmente calcola l'integrale di ordine m del polinomio p : >>> poly . polyint ( c1 ) array ([ 0. , 0. , 1. , 0.33333333 ]) Attenzione Entrambe le funzioni si aspettano i coefficienti del polinomio passato ordinati dal grado pi\u00f9 basso a quello pi\u00f9 alto.","title":"Dispense"},{"location":"material/02_libs/07_numpy/05_polynomials/lecture/#75-operazioni-polinomiali-in-numpy","text":"Il modulo numpy.polynomial.polynomial ci offre numerose classi e funzionalit\u00e0 per il trattamento dei polinomi. Vediamo quali sono le principali. Immaginiamo di avere due diversi polinomi (cui non assoceremo alcun significato fisico), ovvero: \\[ \\begin{cases} c1: 2x + 1 \\\\ c2: x^2 + 3x + 2 \\end{cases} \\] Vediamo come usare dei metodi forniti dal modulo polynomial per effettuare delle operazioni su di loro. Prima di partire, per\u00f2, introduciamo gli oggetti di classe poly1d , che ci permettono di rappresentare in maniera compiuta un polinomio. In particolare, partendo dai coefficienti di un generico polinomio p , potremo ottenere un oggetto poly1d invocando l'omonimo costruttore: p_pol = np . poly1d ( p ) Il vantaggio principale degli oggetti poly1d sta sia nella loro rappresentazione, sia nel fatto che possono essere direttamente utilizzati all'interno delle funzioni per il calcolo polinomiale che vedremo nel seguito.","title":"7.5 - Operazioni polinomiali in NumPy"},{"location":"material/02_libs/07_numpy/05_polynomials/lecture/#751-addizione-di-polinomi","text":"Per effettuare l'addizione di due polinomi, possiamo usare il metodo polyadd(c1, c2) , che accetta come parametri due vettori c1 e c2 che rappresentano, rispettivamente, i coefficienti del polinomio 1 e 2. Volendo sommare il primo ed il secondo polinomio, potremo scrivere: >>> from numpy.polynomial import polynomial as poly >>> c1 = ( 0 , 2 , 1 ) >>> c2 = ( 1 , 3 , 2 ) >>> poly . polyadd ( c1 , c2 ) Questa operazione ci dar\u00e0 il risultato atteso, ovvero \\(x^2 + 5x + 3\\) . Notiamo come le dimensioni di c1 e di c2 debbano essere tra loro coerenti . Se infatti omettessimo il coefficiente \\(0\\) al termine di secondo grado in c1 , il risultato sarebbe il seguente: >>> c3 = ( 2 , 1 ) >>> poly . polyadd ( c3 , c2 ) array ([ 3. , 4. , 2. ]) Ovviamente, il risultato precedente pu\u00f2 essere errato in base al valore assunto dal polinomio c3 .","title":"7.5.1 - Addizione di polinomi"},{"location":"material/02_libs/07_numpy/05_polynomials/lecture/#752-sottrazione-di-polinomi","text":"Possiamo poi sottrarre due polinomi usando la funzione polysub(c1, c2) , i cui parametri sono identici a quelli passati a polyadd() : >>> poly . polysub ( c2 , c1 ) array ([ 1. , 1. , 1. ])","title":"7.5.2 - Sottrazione di polinomi"},{"location":"material/02_libs/07_numpy/05_polynomials/lecture/#753-moltiplicazione-di-polinomi","text":"Le considerazioni precedenti possono essere banalmente traslate al caso della moltiplicazione tra polinomi, ottenibile mediante la funzione polymul(c1, c2) . >>> poly . polymul ( c1 , c2 ) array ([ 0. , 2. , 7. , 7. , 2. ]) Attenzione! Nelle ultime versioni di NumPy, i coefficienti sono ordinati da quello a grado pi\u00f9 basso a quello di grado pi\u00f9 alto!","title":"7.5.3 - Moltiplicazione di polinomi"},{"location":"material/02_libs/07_numpy/05_polynomials/lecture/#754-divisione-tra-polinomi","text":"La divisione tra polinomi \u00e8 un'operazione leggermente pi\u00f9 complessa delle altre, e prevede l'uso della funzione polydiv(c1, c2) , che restituir\u00e0 stavolta due array: il primo, q , rappresenta i coefficienti del polinomio quoziente, mentre il secondo, r , indica i coefficienti del polinomio resto. Nel nostro caso: >>> ( q , r ) = poly . polydiv ( c1 , c2 ) >>> q ; r array ([ 0.5 ]) array ([ - 0.5 , 0.5 ]) Anche in questo caso, i coefficienti sono ordinati da quello a grado pi\u00f9 basso a quello a grado pi\u00f9 alto.","title":"7.5.4 - Divisione tra polinomi"},{"location":"material/02_libs/07_numpy/05_polynomials/lecture/#755-elevazione-a-potenza","text":"Chiudiamo questa breve panoramica parlando dell'elevazione a potenza di un polinomio, effettuabile mediante la funzione polypow(c, pow) , con c vettore dei coefficienti, e pow potenza a cui elevare: >>> poly . polypow ( c1 , 2 ) array ([ 0. , 0. , 4. , 4. , 1. ]) Anche in questo caso, vengono riportati i termini pari a zero nei risultati.","title":"7.5.5 - Elevazione a potenza"},{"location":"material/02_libs/07_numpy/05_polynomials/lecture/#756-valore-assunto-da-un-polinomio","text":"Per valutare il valore \\(y\\) assunto dal polinomio per un determinato valore di \\(x\\) , usiamo la funzione polyval(x, p) , che accetta come argomento un intero (o una lista di interi) x ed un polinomio p . Se volessimo valutare il valore assunto da \\(y\\) per \\(x \\in [1, 2]\\) sulla retta rappresentata dal polinomio c1 , ad esempio, potremmo usare polyval() come segue: >>> poly . polyval ([ 1 , 2 ], c1 ) array ([ 3. , 8. ])","title":"7.5.6 - Valore assunto da un polinomio"},{"location":"material/02_libs/07_numpy/05_polynomials/lecture/#757-derivata-ed-integrale-di-funzioni-polinomiali","text":"Concludiamo questa breve carrellata con due metodi in grado di calcolare, rispettivamente, la derivata e l'integrale di una funzione polinomiale. Il metodo polyder(p, m) , infatti, permette di calcolare la derivata di ordine m del polinomio p (di default, m=2 ): >>> poly . polyder ( c1 ) array ([ 2. , 2. ]) Il metodo duale \u00e8 polyint(p, m) , che prevedibilmente calcola l'integrale di ordine m del polinomio p : >>> poly . polyint ( c1 ) array ([ 0. , 0. , 1. , 0.33333333 ]) Attenzione Entrambe le funzioni si aspettano i coefficienti del polinomio passato ordinati dal grado pi\u00f9 basso a quello pi\u00f9 alto.","title":"7.5.7 - Derivata ed integrale di funzioni polinomiali"},{"location":"material/02_libs/07_numpy/06_statistics/lecture/","text":"7.6 - Statistica in NumPy \u00b6 NumPy ci mette a disposizione diverse funzioni per il calcolo statistico. Vediamone assieme una breve carrellata. 7.6.1 - Minimo e massimo di un array \u00b6 Partiamo con due funzioni che possono essere utili per determinare il valore massimo ed il valore minimo di un array a , ovvero amin(a) ed amax(a) . Entrambe queste funzioni accettano (opzionalmente) un valore per il parametro axis , indicante al solito la direzione lungo la quale viene effettuata l'operazione. Ad esempio, se volessimo trovare il minimo ed il massimo di un vettore generato casualmente: >>> rng = np . random . default_rng ( 42 ) >>> a = rng . integers ( low = 0 , high = 10 , size = 5 ) >>> a array ([ 0 , 7 , 6 , 4 , 4 ], dtype = int64 ) >>> np . amin ( a ) 0 >>> np . amax ( a ) 7 Attenzione Nel codice precedente abbiamo usato la funzione default_rng() del package random di NumPy per generare un vettore di numeri casuali. Per una matrice, ed in generale per ogni array N-dimensionale, il procedimento da seguire \u00e8 analogo: >>> b = rng . integers ( low = 0 , high = 10 , size = ( 3 , 3 )) >>> b array ([[ 8 , 0 , 6 ], [ 2 , 0 , 5 ], [ 9 , 7 , 7 ]], dtype = int64 ) >>> np . amin ( b ) 0 >>> np . amax ( b ) 9 Immaginiamo adesso di voler trovare il minimo ed il massimo per colonna per b . Al solito, specifichiamo il parametro axis , che assumer\u00e0 valore pari a 0 : >>> np . amin ( b , axis = 0 ) array ([ 2 , 0 , 5 ], dtype = int64 ) >>> np . amax ( b , axis = 0 ) array ([ 9 , 7 , 7 ], dtype = int64 ) Ovviamente, per trovare il minimo ed il massimo per riga , dovremo cambiare il valore di axis in 1 : >>> np . amin ( b , axis = 1 ) array ([ 0 , 0 , 7 ], dtype = int64 ) >>> np . amax ( b , axis = 1 ) array ([ 8 , 5 , 9 ], dtype = int64 ) Possiamo anche specificare una tupla per il valore del parametro axis ; in tal caso, la ricerca del massimo o del minimo avverr\u00e0 lungo tutti gli assi specificati dalla tupla. Ad esempio, specificando (0, 1) , effettueremo la ricerca del minimo (o del massimo) elemento nella matrice: >>> np . amin ( b , axis = ( 0 , 1 )) 0 >>> np . amax ( b , axis = ( 0 , 1 )) 9 7.6.2 - Percentile e quantile \u00b6 Ricordiamo che il q-percentile di un vettore \\(V\\) di lunghezza \\(N\\) \u00e8 definito come il valore pari a \\(\\frac{q}{100}\\) calcolato a partire da una copia ordinata di \\(V\\) . Per fare un esempio, supponiamo di avere un vettore ordinato di elementi che vanno da \\(1\\) a \\(10\\) , e di calcolare il \\(50\\) -percentile mediante la funzione percentile() di NumPy. >>> a = np . arange ( 1 , 11 ) >>> np . percentile ( a , 50 ) 5.5 Esistono diversi modi di calcolare il q-percentile; in tal senso, \u00e8 opportuno consultare la reference e l'articolo Sample quantiles in statistical packages di Hyndman, R. J., & Fan, Y . In realt\u00e0, la funzione percentile(a, q) usa, per il \\(50\\) -percentile, il calcolo della mediana, per cui \u00e8 equivalente alla funzione median(a) . In questo caso specifico, avremo un discostamento dal risultato atteso, dovuto ad errori di interpolazione introdotti da NumPy: np . percentile ( a , 50 ) Il concetto di quantile \u00e8 analogo a quello di percentile; tuttavia, in questo caso, non abbiamo a che fare con valori percentuali, bens\u00ec con valori normalizzati tra \\(0\\) e \\(1\\) . Per cui, se usassimo la funzione quantile(a, q) come in precedenza: >>> np . quantile ( a , .5 ) 5.5 Anche le funzioni percentile() e quantile() prevedono come argomento opzionale il parametro axis . Ad esempio: >>> np . percentile ( b , 50 , axis = 0 ) array ([ 8. , 0. , 6. ]) >>> np . percentile ( b , 50 , axis = 1 ) array ([ 6. , 2. , 7. ]) Come previsto, dando il valore 0 al parametro axis avremo il calcolo del percentile su ciascuna colonna, mentre passando il valore 1 avremo il calcolo del percentile su ciascuna riga. 7.6.3 - Media aritmetica e media pesata \u00b6 Per il calcolo del valore medio di un array NumPy ci mette a disposizione due metodi. Il primo \u00e8 la funzione average(a, weights) , che viene usata per calcolare una media pesata degli elementi di a ponderati per gli elementi di weights (a patto che, ovviamente, le dimensioni dei due array siano coerenti). Il calcolo che viene effettuato da NumPy con la funzione average() \u00e8 quindi il seguente: avg = sum ( a * weights ) / sum ( weights ) Per cui, se volessimo assegnare un peso maggiore al primo ed al quarto elemento di un array a generato casualmente, potremmo fare come segue: >>> w = np . array ([ 3 , 1 , 1 , 3 , 1 ]) >>> np . average ( a , weights = w ) 3.2222222222222223 Il risultato si discosta leggermente dalla semplice media, calcolata come: >>> np . average ( a ) 4.2 Suggerimento Teniamo sempre a mente che la media \u00e8 ponderata per la sommatoria dei valori assunti dai pesi! La funzione mean(a) \u00e8 invece rappresentativa della media aritmetica degli elementi di un array, ed equivale alla funzione average(a) senza la specifica del vettore dei pesi. Ad esempio: >>> np . mean ( a ) 4.2 Concludiamo ricordando che anche in questo caso possiamo specificare il valore del parametro axis : >>> np . mean ( b , axis = 0 ) array ([ 6.33333333 , 2.33333333 , 6. ]) >>> np . mean ( b , axis = 1 ) array ([ 4.66666667 , 2.33333333 , 7.66666667 ]) 7.6.4 - Varianza e deviazione standard \u00b6 Non possono mancare le funzioni std(a) e var(a) , dedicate al calcolo della deviazione standard e della varianza di un vettore: >>> np . std ( a ) 2.4 >>> np . var ( a ) 5.76 Anche in questo caso, possiamo specificare gli assi lungo i quali effettuare l'operazione desiderata: >>> np . var ( b , axis = 0 ) array ([ 9.55555556 , 10.88888889 , 0.66666667 ]) >>> np . var ( b , axis = 1 ) array ([ 11.55555556 , 4.22222222 , 0.88888889 ]) 7.6.5 - Matrice di covarianza \u00b6 La matrice di covarianza \u00e8 la matrice che racchiude tutti i coefficienti di correlazione , che ci permettono di valutare come una certa variabile \\(x_i\\) varia al variare di un'altra variabile \\(x_j\\) . In generale, esistono diversi tipi di coefficienti di correlazione; il pi\u00f9 semplice \u00e8 quello di Pearson, che stima una correlazione di tipo lineare (ovvero, \u00e8 tanto pi\u00f9 alto quanto le due variabili crescono secondo un rapporto lineare), ed \u00e8 quello usato dalle funzioni cov(a) e corrcoef(a) , la seconda delle quali riporta i valori normalizzati dei risultati ottenibili anche con la prima. In questo caso, a pu\u00f2 essere monodimensionale o bidimensionale, ma ogni riga di a rappresenta una variabile , mentre ogni colonna rappresenta una osservazione . Facciamo qualche esempio. Immaginiamo di avere due variabili che assumono rispettivamente valori [1, 2, 3] e [4, 5, 6] . In questo caso, \u00e8 evidente come la correlazione sia massima, in quanto le osservazioni della seconda variabile hanno un semplice offset (o bias ) rispetto a quelle della prima. Proviamo a calcolare la matrice di correlazione: x = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) np . cov ( x ) np . corrcoef ( x ) Notiamo che, dato che i coefficienti di correlazione assumono valore pari ad 1, le due variabili sono fortemente correlate tra loro. Se invece avessimo una situazione di questo tipo: >>> x = np . array ([[ 1 , 2 , 3 ], [ - 1 , - 2 , - 3 ]]) >>> np . cov ( x ) array ([[ 1. , - 1. ], [ - 1. , 1. ]]) >>> np . corrcoef ( x ) array ([[ 1. , - 1. ], [ - 1. , 1. ]]) In questo caso, \u00e8 evidente come le variabili siano anticorrelate , il che significa che quando la prima sale, la seconda scende, e viceversa. Per apprezzare le differenze tra le funzioni cov() e corrcoef() , dobbiamo usare valori differenti (e non banali) per x . Ad esempio: >>> x = np . array ([[ 2 , 3 , - 1 ], [ 1 , 5 , 2 ], [ 4 , 2 , 2 ]]) >>> np . cov ( x ) array ([[ 4.33333333 , 2.16666667 , 0.66666667 ], [ 2.16666667 , 4.33333333 , - 1.66666667 ], [ 0.66666667 , - 1.66666667 , 1.33333333 ]]) >>> np . corrcoef ( x ) array ([[ 1. , 0.5 , 0.2773501 ], [ 0.5 , 1. , - 0.69337525 ], [ 0.2773501 , - 0.69337525 , 1. ]]) In sostanza, corrcoef() restituisce la matrice dei coefficienti \\(R\\) , la cui relazione con la matrice di covarianza \\(C\\) restituita da cov() \u00e8: \\[ R_{ij} = \\frac{C_{ij}}{\\sqrt{C_{ii} \\cdot C_{jj}}} \\] 7.6.6 - Istogramma \u00b6 Un istogramma offre una visualizzazione grafica dei valori contenuti in un vettore, raggruppandoli all'interno di un certo numero di partizioni ( bin ). Ad esempio, una possibile rappresentazione a due partizioni del vettore \\(A = [1, 2, 3, 4]\\) \u00e8 data dal vettore \\([2, 2]\\) . Questo si spiega col fatto che le due partizioni suddividono il range di valori assunti da \\(A\\) in due parti, con la prima inerente gli elementi \\(1\\) e \\(2\\) , e la seconda gli elementi \\(3\\) e \\(4\\) . Una volta calcolate le partizioni, queste andranno \"riempite\" contando il numero di elementi presenti in ciascuna partizione, il che ci riporta al vettore \\([2, 2]\\) . Nota Ovviamente, \u00e8 possibile specificare, oltre al numero di partizioni, anche gli estremi delle stesse, che potrebbero non coincidere con quelli del vettore. NumPy ci permette di ottenere l'istogramma di un vettore mediante l'insieme di funzioni histogram(a, bins, range) , che ci permette di calcolare l'istogramma (monodimensionale) dell'array a in funzione del numero di partizioni (opzionale) e del range (opzionale). Ad esempio: h , b = np . histogram ( a ) In questo caso, abbiamo lasciato il valore di default di bins , ovvero 10. Nota Notiamo che la funzione histogram() restituisce due valori: il primo \u00e8 dato dai valori assunti dall'istogramma (ovvero dal numero di elementi che ricade in ciascun bin), mentre il secondo \u00e8 dato dai limiti di ogni bin.","title":"Dispense"},{"location":"material/02_libs/07_numpy/06_statistics/lecture/#76-statistica-in-numpy","text":"NumPy ci mette a disposizione diverse funzioni per il calcolo statistico. Vediamone assieme una breve carrellata.","title":"7.6 - Statistica in NumPy"},{"location":"material/02_libs/07_numpy/06_statistics/lecture/#761-minimo-e-massimo-di-un-array","text":"Partiamo con due funzioni che possono essere utili per determinare il valore massimo ed il valore minimo di un array a , ovvero amin(a) ed amax(a) . Entrambe queste funzioni accettano (opzionalmente) un valore per il parametro axis , indicante al solito la direzione lungo la quale viene effettuata l'operazione. Ad esempio, se volessimo trovare il minimo ed il massimo di un vettore generato casualmente: >>> rng = np . random . default_rng ( 42 ) >>> a = rng . integers ( low = 0 , high = 10 , size = 5 ) >>> a array ([ 0 , 7 , 6 , 4 , 4 ], dtype = int64 ) >>> np . amin ( a ) 0 >>> np . amax ( a ) 7 Attenzione Nel codice precedente abbiamo usato la funzione default_rng() del package random di NumPy per generare un vettore di numeri casuali. Per una matrice, ed in generale per ogni array N-dimensionale, il procedimento da seguire \u00e8 analogo: >>> b = rng . integers ( low = 0 , high = 10 , size = ( 3 , 3 )) >>> b array ([[ 8 , 0 , 6 ], [ 2 , 0 , 5 ], [ 9 , 7 , 7 ]], dtype = int64 ) >>> np . amin ( b ) 0 >>> np . amax ( b ) 9 Immaginiamo adesso di voler trovare il minimo ed il massimo per colonna per b . Al solito, specifichiamo il parametro axis , che assumer\u00e0 valore pari a 0 : >>> np . amin ( b , axis = 0 ) array ([ 2 , 0 , 5 ], dtype = int64 ) >>> np . amax ( b , axis = 0 ) array ([ 9 , 7 , 7 ], dtype = int64 ) Ovviamente, per trovare il minimo ed il massimo per riga , dovremo cambiare il valore di axis in 1 : >>> np . amin ( b , axis = 1 ) array ([ 0 , 0 , 7 ], dtype = int64 ) >>> np . amax ( b , axis = 1 ) array ([ 8 , 5 , 9 ], dtype = int64 ) Possiamo anche specificare una tupla per il valore del parametro axis ; in tal caso, la ricerca del massimo o del minimo avverr\u00e0 lungo tutti gli assi specificati dalla tupla. Ad esempio, specificando (0, 1) , effettueremo la ricerca del minimo (o del massimo) elemento nella matrice: >>> np . amin ( b , axis = ( 0 , 1 )) 0 >>> np . amax ( b , axis = ( 0 , 1 )) 9","title":"7.6.1 - Minimo e massimo di un array"},{"location":"material/02_libs/07_numpy/06_statistics/lecture/#762-percentile-e-quantile","text":"Ricordiamo che il q-percentile di un vettore \\(V\\) di lunghezza \\(N\\) \u00e8 definito come il valore pari a \\(\\frac{q}{100}\\) calcolato a partire da una copia ordinata di \\(V\\) . Per fare un esempio, supponiamo di avere un vettore ordinato di elementi che vanno da \\(1\\) a \\(10\\) , e di calcolare il \\(50\\) -percentile mediante la funzione percentile() di NumPy. >>> a = np . arange ( 1 , 11 ) >>> np . percentile ( a , 50 ) 5.5 Esistono diversi modi di calcolare il q-percentile; in tal senso, \u00e8 opportuno consultare la reference e l'articolo Sample quantiles in statistical packages di Hyndman, R. J., & Fan, Y . In realt\u00e0, la funzione percentile(a, q) usa, per il \\(50\\) -percentile, il calcolo della mediana, per cui \u00e8 equivalente alla funzione median(a) . In questo caso specifico, avremo un discostamento dal risultato atteso, dovuto ad errori di interpolazione introdotti da NumPy: np . percentile ( a , 50 ) Il concetto di quantile \u00e8 analogo a quello di percentile; tuttavia, in questo caso, non abbiamo a che fare con valori percentuali, bens\u00ec con valori normalizzati tra \\(0\\) e \\(1\\) . Per cui, se usassimo la funzione quantile(a, q) come in precedenza: >>> np . quantile ( a , .5 ) 5.5 Anche le funzioni percentile() e quantile() prevedono come argomento opzionale il parametro axis . Ad esempio: >>> np . percentile ( b , 50 , axis = 0 ) array ([ 8. , 0. , 6. ]) >>> np . percentile ( b , 50 , axis = 1 ) array ([ 6. , 2. , 7. ]) Come previsto, dando il valore 0 al parametro axis avremo il calcolo del percentile su ciascuna colonna, mentre passando il valore 1 avremo il calcolo del percentile su ciascuna riga.","title":"7.6.2 - Percentile e quantile"},{"location":"material/02_libs/07_numpy/06_statistics/lecture/#763-media-aritmetica-e-media-pesata","text":"Per il calcolo del valore medio di un array NumPy ci mette a disposizione due metodi. Il primo \u00e8 la funzione average(a, weights) , che viene usata per calcolare una media pesata degli elementi di a ponderati per gli elementi di weights (a patto che, ovviamente, le dimensioni dei due array siano coerenti). Il calcolo che viene effettuato da NumPy con la funzione average() \u00e8 quindi il seguente: avg = sum ( a * weights ) / sum ( weights ) Per cui, se volessimo assegnare un peso maggiore al primo ed al quarto elemento di un array a generato casualmente, potremmo fare come segue: >>> w = np . array ([ 3 , 1 , 1 , 3 , 1 ]) >>> np . average ( a , weights = w ) 3.2222222222222223 Il risultato si discosta leggermente dalla semplice media, calcolata come: >>> np . average ( a ) 4.2 Suggerimento Teniamo sempre a mente che la media \u00e8 ponderata per la sommatoria dei valori assunti dai pesi! La funzione mean(a) \u00e8 invece rappresentativa della media aritmetica degli elementi di un array, ed equivale alla funzione average(a) senza la specifica del vettore dei pesi. Ad esempio: >>> np . mean ( a ) 4.2 Concludiamo ricordando che anche in questo caso possiamo specificare il valore del parametro axis : >>> np . mean ( b , axis = 0 ) array ([ 6.33333333 , 2.33333333 , 6. ]) >>> np . mean ( b , axis = 1 ) array ([ 4.66666667 , 2.33333333 , 7.66666667 ])","title":"7.6.3 - Media aritmetica e media pesata"},{"location":"material/02_libs/07_numpy/06_statistics/lecture/#764-varianza-e-deviazione-standard","text":"Non possono mancare le funzioni std(a) e var(a) , dedicate al calcolo della deviazione standard e della varianza di un vettore: >>> np . std ( a ) 2.4 >>> np . var ( a ) 5.76 Anche in questo caso, possiamo specificare gli assi lungo i quali effettuare l'operazione desiderata: >>> np . var ( b , axis = 0 ) array ([ 9.55555556 , 10.88888889 , 0.66666667 ]) >>> np . var ( b , axis = 1 ) array ([ 11.55555556 , 4.22222222 , 0.88888889 ])","title":"7.6.4 - Varianza e deviazione standard"},{"location":"material/02_libs/07_numpy/06_statistics/lecture/#765-matrice-di-covarianza","text":"La matrice di covarianza \u00e8 la matrice che racchiude tutti i coefficienti di correlazione , che ci permettono di valutare come una certa variabile \\(x_i\\) varia al variare di un'altra variabile \\(x_j\\) . In generale, esistono diversi tipi di coefficienti di correlazione; il pi\u00f9 semplice \u00e8 quello di Pearson, che stima una correlazione di tipo lineare (ovvero, \u00e8 tanto pi\u00f9 alto quanto le due variabili crescono secondo un rapporto lineare), ed \u00e8 quello usato dalle funzioni cov(a) e corrcoef(a) , la seconda delle quali riporta i valori normalizzati dei risultati ottenibili anche con la prima. In questo caso, a pu\u00f2 essere monodimensionale o bidimensionale, ma ogni riga di a rappresenta una variabile , mentre ogni colonna rappresenta una osservazione . Facciamo qualche esempio. Immaginiamo di avere due variabili che assumono rispettivamente valori [1, 2, 3] e [4, 5, 6] . In questo caso, \u00e8 evidente come la correlazione sia massima, in quanto le osservazioni della seconda variabile hanno un semplice offset (o bias ) rispetto a quelle della prima. Proviamo a calcolare la matrice di correlazione: x = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) np . cov ( x ) np . corrcoef ( x ) Notiamo che, dato che i coefficienti di correlazione assumono valore pari ad 1, le due variabili sono fortemente correlate tra loro. Se invece avessimo una situazione di questo tipo: >>> x = np . array ([[ 1 , 2 , 3 ], [ - 1 , - 2 , - 3 ]]) >>> np . cov ( x ) array ([[ 1. , - 1. ], [ - 1. , 1. ]]) >>> np . corrcoef ( x ) array ([[ 1. , - 1. ], [ - 1. , 1. ]]) In questo caso, \u00e8 evidente come le variabili siano anticorrelate , il che significa che quando la prima sale, la seconda scende, e viceversa. Per apprezzare le differenze tra le funzioni cov() e corrcoef() , dobbiamo usare valori differenti (e non banali) per x . Ad esempio: >>> x = np . array ([[ 2 , 3 , - 1 ], [ 1 , 5 , 2 ], [ 4 , 2 , 2 ]]) >>> np . cov ( x ) array ([[ 4.33333333 , 2.16666667 , 0.66666667 ], [ 2.16666667 , 4.33333333 , - 1.66666667 ], [ 0.66666667 , - 1.66666667 , 1.33333333 ]]) >>> np . corrcoef ( x ) array ([[ 1. , 0.5 , 0.2773501 ], [ 0.5 , 1. , - 0.69337525 ], [ 0.2773501 , - 0.69337525 , 1. ]]) In sostanza, corrcoef() restituisce la matrice dei coefficienti \\(R\\) , la cui relazione con la matrice di covarianza \\(C\\) restituita da cov() \u00e8: \\[ R_{ij} = \\frac{C_{ij}}{\\sqrt{C_{ii} \\cdot C_{jj}}} \\]","title":"7.6.5 - Matrice di covarianza"},{"location":"material/02_libs/07_numpy/06_statistics/lecture/#766-istogramma","text":"Un istogramma offre una visualizzazione grafica dei valori contenuti in un vettore, raggruppandoli all'interno di un certo numero di partizioni ( bin ). Ad esempio, una possibile rappresentazione a due partizioni del vettore \\(A = [1, 2, 3, 4]\\) \u00e8 data dal vettore \\([2, 2]\\) . Questo si spiega col fatto che le due partizioni suddividono il range di valori assunti da \\(A\\) in due parti, con la prima inerente gli elementi \\(1\\) e \\(2\\) , e la seconda gli elementi \\(3\\) e \\(4\\) . Una volta calcolate le partizioni, queste andranno \"riempite\" contando il numero di elementi presenti in ciascuna partizione, il che ci riporta al vettore \\([2, 2]\\) . Nota Ovviamente, \u00e8 possibile specificare, oltre al numero di partizioni, anche gli estremi delle stesse, che potrebbero non coincidere con quelli del vettore. NumPy ci permette di ottenere l'istogramma di un vettore mediante l'insieme di funzioni histogram(a, bins, range) , che ci permette di calcolare l'istogramma (monodimensionale) dell'array a in funzione del numero di partizioni (opzionale) e del range (opzionale). Ad esempio: h , b = np . histogram ( a ) In questo caso, abbiamo lasciato il valore di default di bins , ovvero 10. Nota Notiamo che la funzione histogram() restituisce due valori: il primo \u00e8 dato dai valori assunti dall'istogramma (ovvero dal numero di elementi che ricade in ciascun bin), mentre il secondo \u00e8 dato dai limiti di ogni bin.","title":"7.6.6 - Istogramma"},{"location":"material/02_libs/08_matplotlib/lecture/","text":"8 - Visualizzare i dati in Python \u00b6 Finora ci siamo limitati a visualizzare dati e risultati ottenuti usando prima la riga di comando, e poi i metodi forniti dai notebook Jupyter. Tuttavia, \u00e8 chiaro come questo modo di procedere sia limitante: cosa ne \u00e8 di tutti i coloratissimi grafici che possiamo ammirare in siti ed articoli scientifici? Nella realt\u00e0, per ottenerli dovremo necessariamente integrare il nostro ambiente di lavoro con altre librerie: ne esistono diverse, ma la pi\u00f9 importante ed utilizzata \u00e8 senza ombra di dubbio Matplotlib , cui si pu\u00f2 affiancare Seaborn , che tratteremo in una delle prossime lezioni. 8.1 - Installazione della libreria \u00b6 Per prima cosa, installiamo le libreria. Al solito, potrete consultare le diverse opzioni in appendice ; qui riportiamo l'opzione di installazione mediante pip : pip install matplotlib Per importare la libreria all'interno del nostro codice, usiamo un alias: import matplotlib.pylot as plt # import di matplotlib L'API pyplot Sottolineamo l'uso dell'API pyplot per Matplotlib al posto dell'API \"standard\". In tal modo, avremo a disposizione una serie di funzioni per il plot che ricorda molto quella usata dal MATLAB. 8.2 - Il primo plot \u00b6 Dopo aver installato la libreria, proviamo a creare il nostro primo plot. Per farlo, possiamo usare uno script, un terminale o un notebook, ed inserire il seguente codice: rng = np . random . default_rng ( 42 ) x = np . arange ( 1 , 6 ) y = rng . integers ( low = 0 , high = 10 , size = 5 ) fig , ax = plt . subplots () ax . plot ( x , y ) plt . show () Se tutto \u00e8 andato per il verso giusto, dovremmo vedere a schermo questa immagine: Suggerimento Dovreste vedere a schermo esattamente questa immagine perch\u00e9 nella generazione dei numeri casuali, che avviene alla riga 1, viene usato il seed 42 . Se provate ad usarne un altro, vedrete un'altra immagine. Cerchiamo adesso di approfondire i concetti di funzionamento di Matplotlib. 8.3 - Figure ed assi \u00b6 Alla base del funzionamento di Matplotlib abbiamo quattro classi fondamentali. Per prima cosa, ci sono le Figure , che rappresentano l'intera figura mostrata da Matplotlib. Questa, ovviamente, terr\u00e0 traccia di tutto ci\u00f2 che vi \u00e8 al suo interno, e potr\u00e0 contenere un numero arbitrario degli elementi che vedremo a breve. Abbiamo poi gli Axes , oggetti che rappresentano il plot vero e proprio, ovvero la regione dell'immagine all'interno del quale vengono \"disegnati\" i dati. La relazione tra Figure ed Axes \u00e8 strettamente gerarchica: una Figure pu\u00f2 avere diversi Axes , ma ogni Axes appartiene esclusivamente ad una Figure . All'interno di un oggetto Axes troviamo poi due o tre oggetti di tipo Axis , ognuno dei quali rappresenta l'asse vero e proprio (in altri termini, \\(x\\) , \\(y\\) e, per le figure tridimensionali, \\(z\\) ). Gli oggetti Axis ci permettono quindi di definire gli intervalli dati, l'eventuale griglia, e via discorrendo. Attenzione Fate attenzione a non confondere gli Axes con gli Axis , nonostante l'infelice scelta dei nomi! In ultimo, abbiamo gli artist , che rappresentano tutto quello che \u00e8 possibile visualizzare su una figura, incluso testo, label, plot, numeri, e via discorrendo. Torniamo brevemente al precedente snippet. Dopo aver importato i package necessari, ed aver creato un vettore di numeri interi casuali, abbiamo creato una Figure ed un Axes usando la funzione subplots() : fig , ax = plt . subplots () A quel punto, abbiamo effettuato il plot dei valori di x ed y su nostro oggetto Axes : ax . plot ( x , y ) In ultimo, abbiamo mostrato a schermo la figura usando la funzione plt.show() . Vediamo adesso qualche esempio maggiormente \"corposo\". 8.4 - Esempi con Matplotlib \u00b6 8.4.1: Plot di pi\u00f9 funzioni \u00b6 In questo esempio, vogliamo mostrare sullo stesso Axes il plot di due diverse funzioni, in particolare una retta ed un seno. Ricordiamo che questo \u00e8 possibile grazie al fatto che i plot vengono considerati degli artist, e quindi \u00e8 possibile inserirne un numero arbitrario. Vediamo come fare. Per prima cosa, definiamo i nostri dati: x = np . arange ( 0. , 10. , 0.01 ) y_1 = 1 + 2 * x y_2 = np . sin ( x ) Notiamo che stiamo usando un unico vettore per le ascisse, di modo da fornire una base comune al nostro plot. Adesso, creiamo la nostra Figure con relativo Axes , ed effettuiamo il plot di entrambe le funzioni. fig , ax = plt . subplots () ax . plot ( x , y_1 , label = 'Retta' ) ax . plot ( x , y_2 , label = 'Funzione sinusoidale' ) Notiamo che abbiamo impostato un parametro label che indica l'etichetta assegnata ai due plot; questa sar\u00e0 utilizzata pi\u00f9 tardi per generare la legenda. Passiamo adesso ad impostare il titolo e le label sugli assi \\(x\\) e \\(y\\) usando rispettivamente le funzioni set_title() , set_xlabel() e set_ylabel() : ax . set_title ( 'Plot di due funzioni matematiche' ) ax . set_xlabel ( 'Asse x' ) ax . set_ylabel ( 'Asse y' ) Usiamo adesso la funzione grid() per mostrare una griglia sulla figura, e la funzione legend() per far apparire la legenda che descrive le funzioni visualizzate. ax . legend () ax . grid () In ultimo, mostriamo a schermo la figura con la funzione show() : plt . show () Il risultato ottenuto \u00e8 mostrato in figura. 8.4.2: Subplot \u00b6 Abbiamo detto che possiamo definire pi\u00f9 Axes per un'unica Figure ; per farlo, possiamo parametrizzare la funzione subplots(i, j) , in maniera tale che vengano creati \\(i \\times j\\) plot all'interno della stessa figura. Per creare 2 subplot in \"riga\", ad esempio, usiamo questa istruzione: fig , ( ax_1 , ax_2 ) = plt . subplots ( 2 , 1 ) Possiamo poi usare la funzione suptitle() per dare un titolo all'intera figura: fig . suptitle ( 'Due subplot di pi\u00f9 funzioni matematiche' ) A questo punto, procediamo ad effettuare i plot sui relativi assi nella solita maniera: # Primo subplot ax_1 . plot ( x , y_1 , label = 'Retta' ) ax_1 . set_ylabel ( 'Asse y' ) ax_1 . legend () ax_1 . grid () # Secondo subplot ax_2 . plot ( x , y_2 , label = 'Funzione sinusoidale' ) ax_2 . set_xlabel ( 'Asse x' ) ax_2 . set_ylabel ( 'Asse y' ) ax_2 . legend () ax_2 . grid () # Mostro la figura plt . show () Il risultato sar\u00e0 simile a quello mostrato in figura: 8.4.3: Rappresentazione di un istogramma \u00b6 Abbiamo gi\u00e0 parlato degli istogrammi in precedenza. Tuttavia, la loro vera potenza sta nella rappresentazione visiva che offrono, ed in tal senso Matplotlib ci viene in soccorso offrendoci la funzione hist() . Per prima cosa, creiamo un vettore di interi. x = rng . integers ( low = 0 , high = 100 , size = 1000 ) Al solito, creiamo la nostra figura, ed usiamo la funzione hist() passandogli il vettore x creato in precedenza e il parametro density , che ci permetter\u00e0 di normalizzare l'istogramma (ovvero, fare in modo tale che la sommatoria dei singoli bin sia esattamente pari ad 1). fig , ax = plt . subplots () ax . hist ( x , edgecolor = 'black' , linewidth = 1.2 , density = True ) Notiamo anche l'uso dei parametri edgecolor , che permette di impostare il colore del bordo di ciascuna barra dell'istogramma, e linewidth , che consente di specificarne lo spessore. Al solito, usiamo i metodi opportuni per impostare titolo e label degli assi, e mostriamo la figura. ax . set_xlabel ( 'Bin' ) ax . set_ylabel ( 'Conteggio dei singoli elementi' ) ax . set_title ( 'Esempio di istogramma' ) plt . show () Il risultato sar\u00e0 simile a quello mostrato nella figura successiva.","title":"Dispense"},{"location":"material/02_libs/08_matplotlib/lecture/#8-visualizzare-i-dati-in-python","text":"Finora ci siamo limitati a visualizzare dati e risultati ottenuti usando prima la riga di comando, e poi i metodi forniti dai notebook Jupyter. Tuttavia, \u00e8 chiaro come questo modo di procedere sia limitante: cosa ne \u00e8 di tutti i coloratissimi grafici che possiamo ammirare in siti ed articoli scientifici? Nella realt\u00e0, per ottenerli dovremo necessariamente integrare il nostro ambiente di lavoro con altre librerie: ne esistono diverse, ma la pi\u00f9 importante ed utilizzata \u00e8 senza ombra di dubbio Matplotlib , cui si pu\u00f2 affiancare Seaborn , che tratteremo in una delle prossime lezioni.","title":"8 - Visualizzare i dati in Python"},{"location":"material/02_libs/08_matplotlib/lecture/#81-installazione-della-libreria","text":"Per prima cosa, installiamo le libreria. Al solito, potrete consultare le diverse opzioni in appendice ; qui riportiamo l'opzione di installazione mediante pip : pip install matplotlib Per importare la libreria all'interno del nostro codice, usiamo un alias: import matplotlib.pylot as plt # import di matplotlib L'API pyplot Sottolineamo l'uso dell'API pyplot per Matplotlib al posto dell'API \"standard\". In tal modo, avremo a disposizione una serie di funzioni per il plot che ricorda molto quella usata dal MATLAB.","title":"8.1 - Installazione della libreria"},{"location":"material/02_libs/08_matplotlib/lecture/#82-il-primo-plot","text":"Dopo aver installato la libreria, proviamo a creare il nostro primo plot. Per farlo, possiamo usare uno script, un terminale o un notebook, ed inserire il seguente codice: rng = np . random . default_rng ( 42 ) x = np . arange ( 1 , 6 ) y = rng . integers ( low = 0 , high = 10 , size = 5 ) fig , ax = plt . subplots () ax . plot ( x , y ) plt . show () Se tutto \u00e8 andato per il verso giusto, dovremmo vedere a schermo questa immagine: Suggerimento Dovreste vedere a schermo esattamente questa immagine perch\u00e9 nella generazione dei numeri casuali, che avviene alla riga 1, viene usato il seed 42 . Se provate ad usarne un altro, vedrete un'altra immagine. Cerchiamo adesso di approfondire i concetti di funzionamento di Matplotlib.","title":"8.2 - Il primo plot"},{"location":"material/02_libs/08_matplotlib/lecture/#83-figure-ed-assi","text":"Alla base del funzionamento di Matplotlib abbiamo quattro classi fondamentali. Per prima cosa, ci sono le Figure , che rappresentano l'intera figura mostrata da Matplotlib. Questa, ovviamente, terr\u00e0 traccia di tutto ci\u00f2 che vi \u00e8 al suo interno, e potr\u00e0 contenere un numero arbitrario degli elementi che vedremo a breve. Abbiamo poi gli Axes , oggetti che rappresentano il plot vero e proprio, ovvero la regione dell'immagine all'interno del quale vengono \"disegnati\" i dati. La relazione tra Figure ed Axes \u00e8 strettamente gerarchica: una Figure pu\u00f2 avere diversi Axes , ma ogni Axes appartiene esclusivamente ad una Figure . All'interno di un oggetto Axes troviamo poi due o tre oggetti di tipo Axis , ognuno dei quali rappresenta l'asse vero e proprio (in altri termini, \\(x\\) , \\(y\\) e, per le figure tridimensionali, \\(z\\) ). Gli oggetti Axis ci permettono quindi di definire gli intervalli dati, l'eventuale griglia, e via discorrendo. Attenzione Fate attenzione a non confondere gli Axes con gli Axis , nonostante l'infelice scelta dei nomi! In ultimo, abbiamo gli artist , che rappresentano tutto quello che \u00e8 possibile visualizzare su una figura, incluso testo, label, plot, numeri, e via discorrendo. Torniamo brevemente al precedente snippet. Dopo aver importato i package necessari, ed aver creato un vettore di numeri interi casuali, abbiamo creato una Figure ed un Axes usando la funzione subplots() : fig , ax = plt . subplots () A quel punto, abbiamo effettuato il plot dei valori di x ed y su nostro oggetto Axes : ax . plot ( x , y ) In ultimo, abbiamo mostrato a schermo la figura usando la funzione plt.show() . Vediamo adesso qualche esempio maggiormente \"corposo\".","title":"8.3 - Figure ed assi"},{"location":"material/02_libs/08_matplotlib/lecture/#84-esempi-con-matplotlib","text":"","title":"8.4 - Esempi con Matplotlib"},{"location":"material/02_libs/08_matplotlib/lecture/#841-plot-di-piu-funzioni","text":"In questo esempio, vogliamo mostrare sullo stesso Axes il plot di due diverse funzioni, in particolare una retta ed un seno. Ricordiamo che questo \u00e8 possibile grazie al fatto che i plot vengono considerati degli artist, e quindi \u00e8 possibile inserirne un numero arbitrario. Vediamo come fare. Per prima cosa, definiamo i nostri dati: x = np . arange ( 0. , 10. , 0.01 ) y_1 = 1 + 2 * x y_2 = np . sin ( x ) Notiamo che stiamo usando un unico vettore per le ascisse, di modo da fornire una base comune al nostro plot. Adesso, creiamo la nostra Figure con relativo Axes , ed effettuiamo il plot di entrambe le funzioni. fig , ax = plt . subplots () ax . plot ( x , y_1 , label = 'Retta' ) ax . plot ( x , y_2 , label = 'Funzione sinusoidale' ) Notiamo che abbiamo impostato un parametro label che indica l'etichetta assegnata ai due plot; questa sar\u00e0 utilizzata pi\u00f9 tardi per generare la legenda. Passiamo adesso ad impostare il titolo e le label sugli assi \\(x\\) e \\(y\\) usando rispettivamente le funzioni set_title() , set_xlabel() e set_ylabel() : ax . set_title ( 'Plot di due funzioni matematiche' ) ax . set_xlabel ( 'Asse x' ) ax . set_ylabel ( 'Asse y' ) Usiamo adesso la funzione grid() per mostrare una griglia sulla figura, e la funzione legend() per far apparire la legenda che descrive le funzioni visualizzate. ax . legend () ax . grid () In ultimo, mostriamo a schermo la figura con la funzione show() : plt . show () Il risultato ottenuto \u00e8 mostrato in figura.","title":"8.4.1: Plot di pi\u00f9 funzioni"},{"location":"material/02_libs/08_matplotlib/lecture/#842-subplot","text":"Abbiamo detto che possiamo definire pi\u00f9 Axes per un'unica Figure ; per farlo, possiamo parametrizzare la funzione subplots(i, j) , in maniera tale che vengano creati \\(i \\times j\\) plot all'interno della stessa figura. Per creare 2 subplot in \"riga\", ad esempio, usiamo questa istruzione: fig , ( ax_1 , ax_2 ) = plt . subplots ( 2 , 1 ) Possiamo poi usare la funzione suptitle() per dare un titolo all'intera figura: fig . suptitle ( 'Due subplot di pi\u00f9 funzioni matematiche' ) A questo punto, procediamo ad effettuare i plot sui relativi assi nella solita maniera: # Primo subplot ax_1 . plot ( x , y_1 , label = 'Retta' ) ax_1 . set_ylabel ( 'Asse y' ) ax_1 . legend () ax_1 . grid () # Secondo subplot ax_2 . plot ( x , y_2 , label = 'Funzione sinusoidale' ) ax_2 . set_xlabel ( 'Asse x' ) ax_2 . set_ylabel ( 'Asse y' ) ax_2 . legend () ax_2 . grid () # Mostro la figura plt . show () Il risultato sar\u00e0 simile a quello mostrato in figura:","title":"8.4.2: Subplot"},{"location":"material/02_libs/08_matplotlib/lecture/#843-rappresentazione-di-un-istogramma","text":"Abbiamo gi\u00e0 parlato degli istogrammi in precedenza. Tuttavia, la loro vera potenza sta nella rappresentazione visiva che offrono, ed in tal senso Matplotlib ci viene in soccorso offrendoci la funzione hist() . Per prima cosa, creiamo un vettore di interi. x = rng . integers ( low = 0 , high = 100 , size = 1000 ) Al solito, creiamo la nostra figura, ed usiamo la funzione hist() passandogli il vettore x creato in precedenza e il parametro density , che ci permetter\u00e0 di normalizzare l'istogramma (ovvero, fare in modo tale che la sommatoria dei singoli bin sia esattamente pari ad 1). fig , ax = plt . subplots () ax . hist ( x , edgecolor = 'black' , linewidth = 1.2 , density = True ) Notiamo anche l'uso dei parametri edgecolor , che permette di impostare il colore del bordo di ciascuna barra dell'istogramma, e linewidth , che consente di specificarne lo spessore. Al solito, usiamo i metodi opportuni per impostare titolo e label degli assi, e mostriamo la figura. ax . set_xlabel ( 'Bin' ) ax . set_ylabel ( 'Conteggio dei singoli elementi' ) ax . set_title ( 'Esempio di istogramma' ) plt . show () Il risultato sar\u00e0 simile a quello mostrato nella figura successiva.","title":"8.4.3: Rappresentazione di un istogramma"},{"location":"material/02_libs/09_pandas/lecture/","text":"9 - Introduzione a Pandas \u00b6 Pandas \u00e8 una delle librerie pi\u00f9 importanti dell'ecosistema SciPy, e viene usata per la lettura ed elaborazione dei dati provenienti da sorgenti di vario tipo, come ad esempio file CSV o Excel, ma anche file di testo e database. Vediamo quindi brevemente come usare la libreria, tenendo presente che ne approfondiremo il funzionamento anche durante le lezioni successive. 9.1 - Installazione e configurazione di Pandas \u00b6 Al solito, provvediamo ad installare Pandas: pip install pandas Cos\u00ec come per le altre librerie, nel prosieguo presupporemo che Pandas sia gi\u00e0 stato importato nel nostro script/notebook: import pandas as pd 9.2 - Pandas e la gestione dei dati \u00b6 Pandas gestisce prevalentemente dati strutturati sotto forma tabellare , ossia simili a quelli comunemente contenuti all'interno dei fogli di calcolo o nei database. Questi dati sono sicuramente tra i pi\u00f9 diffusi ed utilizzati nel contesto dell'analisi dei dati, ovviamente escludendo le immagini: in tal senso, per modellarli, Pandas ci mette a disposizione un'apposita struttura denominata dataframe . I dataframe sono quindi delle strutture atte a contenere dati di ogni tipo. Questi sono normalmente organizzati in righe e colonne, in maniera del tutto analoga a quella in cui sono organizzati i fogli di calcolo ed i database. Importante anche sottolineare come, per convenzione, le singole righe rappresentino i campioni del dataset, mentre le colonne siano associati ai valori assunti dalle diverse caratteristiche, o feature , di ciascun campione. Facciamo un esempio usando il dataset Titanic , che \u00e8 uno tra i pi\u00f9 utilizzati a scopi di sperimentazione. Per prima cosa, generiamo un dataframe rappresentativo dei dati contenuti nel dataset: df = pd . read_csv ( 'titanic.csv' ) Usiamo il metodo head() per mostrare a schermo le prime cinque righe del dataframe. >>> df . head () PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund , Mr . Owen Harris male 22.0 1 0 A / 5 21171 7.2500 NaN S 1 2 1 1 Cumings , Mrs . John Bradley ( Florence Briggs Th ... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen , Miss . Laina female 26.0 0 0 STON / O2 . 3101282 7.9250 NaN S 3 4 1 1 Futrelle , Mrs . Jacques Heath ( Lily May Peel ) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen , Mr . William Henry male 35.0 0 0 373450 8.0500 NaN S Vediamo rapidamente che ad ogni passeggero sono associate delle feature, di cui possiamo inferire il tipo (lo verificheremo a breve): Feature Descrizione Tipo PassengerId Identificativo univoco del passeggero. Intero Survived Stabilisce se il passeggero \u00e8 sopravvissuto. Intero/booleano Pclass Rappresenta la classe del passeggero Intero Name Nome completo del passeggero Stringa Sex Genere del passeggero Stringa Age Et\u00e0 del passeggero Decimale SibSp Crasi di \"Siblings/Spouses\", rappresenta il numero di fratelli/sorelle/coniugi a bordo per ogni passeggero Intero Parch Crasi di \"Parents/Children\", rappresenta il numero di genitori/figli a bordo per ogni passeggero Intero Ticket Rappresenta l'identificativo per il ticket del passeggero. Stringa Tariffa Rappresenta la tariffa pagata dal passeggero. Decimale Cabin Rappresenta la cabina in cui allogiava il passeggero. Stringa Embarked Rappresenta il punto di imbarco del passeggero. Stringa Verifichiamo che le nostre ipotesi sul tipo di dato siano corrette; per farlo, possiamo usare la propriet\u00e0 dtypes del dataframe: >>> df . dtypes PassengerId int64 Survived int64 Pclass int64 Name object Sex object Age float64 SibSp int64 Parch int64 Ticket object Fare float64 Cabin object Embarked object dtype : object Notiamo subito la presenza di tre tipi di colonna, ovvero int64 , float64 e object . Laddove i primi due sono autoesplicativi, merita una particolare menzione il tipo object , che viene associato automaticamente a tutte le stringhe. Suggerimento Normalmente, usare il tipo object comporta diversi problemi nella successiva fase di analisi dei dati. Potrebbe quindi essere una buona idea parametrizzare la funzione read_csv() mediante il parametro dtype , che accetta un dizionario che specifica il tipo di una o pi\u00f9 colonne. Ad esempio, se volessimo specificare che i nomi sono delle stringhe, potremmo usare il tipo string : >>> types = { 'Name' : 'string' } >>> df = pd . read_csv ( 'train.csv' , dtype = types ) >>> df . dtypes # ... Name string # ... Appare chiaro come il dataset ci illustri numerose propriet\u00e0 per ogni passeggero imbarcato. Queste potranno quindi essere utilizzate per un'analisi approfondita della struttura dei dati sotto diversi aspetti e punti di vista; ne parleremo pi\u00f9 estesamente nel seguito. 9.2.1 - Le Series \u00b6 Abbiamo visto come ogni dataframe sia in realt\u00e0 composto da diverse colonne, ciascuna rappresentativa di una feature specifica. Nella pratica, Pandas ci offre un modo per rappresentare singolarmente ciascuna di queste colonne, mediante un oggetto di classe Series . Ad esempio, potremmo estrarre la serie relativa agli identificativi numerici dei passeggeri: names = df [ 'Name' ] names . head () # Output restituito 0 Braund , Mr . Owen Harris 1 Cumings , Mrs . John Bradley ( Florence Briggs Th ... 2 Heikkinen , Miss . Laina 3 Futrelle , Mrs . Jacques Heath ( Lily May Peel ) 4 Allen , Mr . William Henry Name : Name , dtype : object 9.2.1.1 - Accesso agli elementi di una serie \u00b6 Possiamo accedere ad un singolo elemento di una serie mediante una classica procedura di indicizzazione. Notiamo infatti come ogni campione all'interno della serie sia associato ad un indice numerico crescente il cui valore iniziale \u00e8 pari a 0; pertanto, possiamo accedere all' \\(i\\) -mo elemento della serie richiamando l' \\(i-1\\) -mo indice, esattamente come accade per le liste o le sequenze. >>> names [ 0 ] 'Braund, Mr. Owen Harris' Nota L'indicizzazione pu\u00f2 essere anche usata per impostare il valore associato ad uno specifico indice della serie. 9.2.1.2 - Accesso agli elementi del dataframe \u00b6 L'accesso agli elementi del dataframe pu\u00f2 avvenire attraverso diverse modalit\u00e0. In primo luogo, possiamo accedere allo specifico valore di una feature di un dato campione mediante il chained indexing : >>> df [ 'Age' ][ 1 38 In alternativa, \u00e8 possibile usare la funzione loc(row_idx, col) , che permette di accedere al valore assunto dalla feature col per l'elemento in posizione row_idx : >>> df . loc [ 1 , ( 'Age' )] 38.0 La funzione loc() pu\u00f2 operare anche su delle slice di dati: >>> df . loc [ 1 : 5 , ( 'Age' )] 1 38.0 2 26.0 3 35.0 4 35.0 5 NaN o su insiemi di feature: >>> df . loc [ 1 : 5 , ( 'Age' , 'Sex' )] Age Sex 1 38.0 female 2 26.0 female 3 35.0 female 4 35.0 male 5 NaN male Sottolineamo che la funzione loc() opera sugli indici di riga . In questo caso, il nostro dataframe ha degli indici di riga interi, assegnati automaticamente in fase di lettura del dataframe. Nel caso decidessimo di usare una colonna del dataframe come indice, potremmo usare il metodo set_index() : df = df . set_index ( 'Ticket' ) Notiamo che, come al solito, le funzioni lavorano sul valore, e non sulla reference. Di conseguenza, se omettessimo l'assegnazione, df rimarrebbe invariato. Un modo per evitare di usare ogni volta l'operazione di assegnazione \u00e8 quello di impostare il parametro inplace a True : df . set_index ( 'Ticket' , inplace = True ) In alternativa, possiamo decidere di impostare l'indice direttamente nel metodo read_csv() impostando il parametro index_col : df = pd . read_csv ( 'titanic.csv' , index_col = 'Ticket' ) In questo caso, la funzione loc() dovr\u00e0 essere utilizzata usando come parametri di lettura per righe i nuovi indici. Ad esempio: >>> df . loc [ 'STON/O2. 3101282' , 'Name' ] 'Heikkinen, Miss. Laina' Oltre alla funzione loc() Pandas ci mette a disposizione la funzione iloc() , la quale ci offre la possibilit\u00e0 di selezionare un sottoinsieme di campion del dataframe mediante indici interi (da cui la i ): >>> df . iloc [ 2 : 5 , 2 : 4 ] Pclass Name Ticket STON / O2 . 3101282 3 Heikkinen , Miss . Laina 113803 1 Futrelle , Mrs . Jacques Heath ( Lily May Peel ) 373450 3 Allen , Mr . William Henry 9.2.2 - Maschere booleane \u00b6 Supponiamo di voler selezionare soltanto gli uomini maggiorenni presenti nel dataset del Titanic. Per farlo, possiamo usare un'istruzione che implementi delle logiche di tipo booleano: >>> men = df [( df [ 'Age' ] > 18 ) & ( df [ 'Sex' ] == 'male' )] >>> men . head () PassengerId Survived Pclass Name Sex Age 0 1 0 3 Braund , Mr . Owen Harris male 22.0 4 5 0 3 Allen , Mr . William Henry male 35.0 6 7 0 1 McCarthy , Mr . Timothy J male 54.0 12 13 0 3 Saundercock , Mr . William Henry male 20.0 13 14 0 3 Andersson , Mr . Anders Johan male 39.0 Nella pratica, stiamo filtrando il dataset in base all' AND logico tra due condizioni: df['Age'] > 18 : questa condizione genera una maschera booleana che \u00e8 True soltanto se l'et\u00e0 per quel passeggero \u00e8 maggiore di 18 anni; df['Sex'] == 'male' : questa condizione genera una maschera booleana che \u00e8 vera soltanto se il genere del passeggero \u00e8 maschile. 9.2.3 - La funzione groupby \u00b6 Possiamo sfruttare la funzione groupby per raggruppare insiemi di dati (normalmente pertinenti a categorie ). Ad esempio, potremmo raggruppare i passeggeri per genere: >>> df . groupby ([ 'Sex' ]) Possiamo ovviamente estrarre delle statistiche a partire da questi raggruppamenti. Vediamo, ad esempio, l'et\u00e0 media dei passeggeri di sesso femminile e maschile: >>> df . groupby ([ 'Sex' ])[ 'Age' ] . mean () Sex female 27.915709 male 30.726645 Name : Age , dtype : float64 9.3 - Scrittura e lettura dei dataframe \u00b6 9.3.1 - Lettura di dati da sorgenti eterogenee \u00b6 Nel nostro primo esempio abbiamo usato la funzione read_csv per creare un dataframe partendo dai dati memorizzati in un file in formato CSV. Tuttavia, Pandas supporta molti altri formati. Ad esempio, potremmo provare a leggere un file Excel: df = pd . read_excel ( 'dati.xlsx' ) Attenzione Per leggere (e scrivere) da (su) Excel \u00e8 necessario installare la libreria openpyxl ( pip install openpyxl ). In alternativa, pu\u00f2 essere letto un file in formato JSON, oppure ancora direttamente un database: df = pd . read_json ( 'dati.json' ) df = pd . read_sql ( SQL_QUERY ) Esiste un elenco completo delle (numerose) funzioni disponibili, che possono essere individuate sulla reference . In generale, comunque, la sintassi \u00e8 sempre read_*(data_source) , con * da sostituire con il tipo di sorgente dati ( csv , excel , etc.). 9.3.2 - Scrittura di dati su destinazioni eterogenee \u00b6 Possiamo anche scrivere un dataframe su file mediante le funzioni duali alle read_ , che usano il suffisso to_ seguito dall'estensione del file destinazione. Ad esempio, potremmo scrivere un file CSV con il metodo to_csv : df . to_csv ( 'train.xlsx' ) 9.4 - Aggiunta di feature e dati \u00b6 Immaginiamo adesso di voler aggiungere una nuova feature ad un dataframe gi\u00e0 esistente. Per farlo, iniziamo creando un dataframe da zero: >>> df = pd . DataFrame ([ 1 , 2 , 3 , 4 , 5 ], columns = [ 'one' ]) one 0 1 1 2 2 3 3 4 4 5 Possiamo aggiungere una nuova colonna semplicemente usando l'operatore di assegnazione e specificandone il nome: >>> df [ 'two' ] = df [ 'one' ] * 2 one two 0 1 2 1 2 4 2 3 6 3 4 8 4 5 10 Possiamo poi inserire nuovi campioni in coda al dataframe. Per farlo, dovremo prima creare un nuovo dataframe dalle dimensioni coerenti con quello gi\u00e0 esistente, e poi usare la funzione concat() : >>> df_add = pd . DataFrame ([[ 6 , 7 ]], columns = [ 'one' , 'two' ]) >>> df = pd . concat ([ df , df_add ]) one two 0 1 2 1 2 4 2 3 6 3 4 8 4 5 10 0 6 7 Notiamo che la funzione concat() accetta, tra gli altri, il parametro axis . Se questo \u00e8 uguale a zero (come lo \u00e8 di default), la concat() effettua la concatenazione per righe; se \u00e8 pari ad 1, invece, la concatenazione avviene per colonne. Tuttavia, \u00e8 importante sottolineare come la concatenazione avvenga anche nel caso le misure non siano completamente coerenti: infatti, se provassimo ad effettuare una concatenazione per colonne, avremmo un risultato del tipo: >>> pd . concat ([ df , df_add ], axis = 1 ) one two one two 0 1 2 6.0 7.0 1 2 4 NaN NaN 2 3 6 NaN NaN 3 4 8 NaN NaN 4 5 10 NaN NaN I valori relativi alle righe con indice che va da 1 a 4, che ovviamente non saranno presenti, saranno automaticamente impostati a NaN , acronimo di Not a Number . 9.5 - Visualizzazione dei dati in Pandas \u00b6 Pandas ci offre un supporto nativo a Matplotlib per permettere la visualizzazione dei dati contenuti all'interno di un dataframe. In tal senso, possiamo usare la funzione plot() su una serie o su un intero dataframe; ad esempio, potremmo plottare le et\u00e0 dei passeggeri: df [ 'Age' ] . plot () plt . show () ottenendo il risultato mostrato in figura: Possiamo anche fare il plot dell'intero DataFrame : df . plot () plt . show () che risulter\u00e0 nella seguente figura: Ovviamente, \u00e8 possibile usare Pandas anche per fare il plot di altri tipi di grafico, come ad esempio gli istogrammi. Per farlo, si usano le apposite sotto-funzioni di plot : df [ 'Age' ] . plot . hist () plt . show () Il risultato \u00e8 mostrato in figura. Pandas e Seaborn Pandas si integra in maniera naturale anche con la libreria Seaborn, di cui tratteremo nella prossima lezione. 9.6 - Operazioni statistiche sui dataframe \u00b6 Pandas ci mette a disposizione delle funzioni, simili a quelle offerte da NumPy, per calcolare delle statistiche per ciascuna delle colonne presenti in un DataFrame. Ad esempio: >>> df . mean () PassengerId 446.000000 Survived 0.383838 Pclass 2.308642 Age 29.699118 SibSp 0.523008 Parch 0.381594 Fare 32.204208 dtype : float64 Ovviamente, esistono funzioni anche per calcolare varianza ( df.var() ), mediana ( df.median() ), deviazione standard ( df.std() ), e via discorrendo. Particolarmente interessante \u00e8 la funzione describe() , che ci mosta tutte le statistiche pi\u00f9 significative per ognuna delle feature considerate. >>> df . describe () PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25 % 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50 % 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75 % 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200","title":"Dispense"},{"location":"material/02_libs/09_pandas/lecture/#9-introduzione-a-pandas","text":"Pandas \u00e8 una delle librerie pi\u00f9 importanti dell'ecosistema SciPy, e viene usata per la lettura ed elaborazione dei dati provenienti da sorgenti di vario tipo, come ad esempio file CSV o Excel, ma anche file di testo e database. Vediamo quindi brevemente come usare la libreria, tenendo presente che ne approfondiremo il funzionamento anche durante le lezioni successive.","title":"9 - Introduzione a Pandas"},{"location":"material/02_libs/09_pandas/lecture/#91-installazione-e-configurazione-di-pandas","text":"Al solito, provvediamo ad installare Pandas: pip install pandas Cos\u00ec come per le altre librerie, nel prosieguo presupporemo che Pandas sia gi\u00e0 stato importato nel nostro script/notebook: import pandas as pd","title":"9.1 - Installazione e configurazione di Pandas"},{"location":"material/02_libs/09_pandas/lecture/#92-pandas-e-la-gestione-dei-dati","text":"Pandas gestisce prevalentemente dati strutturati sotto forma tabellare , ossia simili a quelli comunemente contenuti all'interno dei fogli di calcolo o nei database. Questi dati sono sicuramente tra i pi\u00f9 diffusi ed utilizzati nel contesto dell'analisi dei dati, ovviamente escludendo le immagini: in tal senso, per modellarli, Pandas ci mette a disposizione un'apposita struttura denominata dataframe . I dataframe sono quindi delle strutture atte a contenere dati di ogni tipo. Questi sono normalmente organizzati in righe e colonne, in maniera del tutto analoga a quella in cui sono organizzati i fogli di calcolo ed i database. Importante anche sottolineare come, per convenzione, le singole righe rappresentino i campioni del dataset, mentre le colonne siano associati ai valori assunti dalle diverse caratteristiche, o feature , di ciascun campione. Facciamo un esempio usando il dataset Titanic , che \u00e8 uno tra i pi\u00f9 utilizzati a scopi di sperimentazione. Per prima cosa, generiamo un dataframe rappresentativo dei dati contenuti nel dataset: df = pd . read_csv ( 'titanic.csv' ) Usiamo il metodo head() per mostrare a schermo le prime cinque righe del dataframe. >>> df . head () PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund , Mr . Owen Harris male 22.0 1 0 A / 5 21171 7.2500 NaN S 1 2 1 1 Cumings , Mrs . John Bradley ( Florence Briggs Th ... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen , Miss . Laina female 26.0 0 0 STON / O2 . 3101282 7.9250 NaN S 3 4 1 1 Futrelle , Mrs . Jacques Heath ( Lily May Peel ) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen , Mr . William Henry male 35.0 0 0 373450 8.0500 NaN S Vediamo rapidamente che ad ogni passeggero sono associate delle feature, di cui possiamo inferire il tipo (lo verificheremo a breve): Feature Descrizione Tipo PassengerId Identificativo univoco del passeggero. Intero Survived Stabilisce se il passeggero \u00e8 sopravvissuto. Intero/booleano Pclass Rappresenta la classe del passeggero Intero Name Nome completo del passeggero Stringa Sex Genere del passeggero Stringa Age Et\u00e0 del passeggero Decimale SibSp Crasi di \"Siblings/Spouses\", rappresenta il numero di fratelli/sorelle/coniugi a bordo per ogni passeggero Intero Parch Crasi di \"Parents/Children\", rappresenta il numero di genitori/figli a bordo per ogni passeggero Intero Ticket Rappresenta l'identificativo per il ticket del passeggero. Stringa Tariffa Rappresenta la tariffa pagata dal passeggero. Decimale Cabin Rappresenta la cabina in cui allogiava il passeggero. Stringa Embarked Rappresenta il punto di imbarco del passeggero. Stringa Verifichiamo che le nostre ipotesi sul tipo di dato siano corrette; per farlo, possiamo usare la propriet\u00e0 dtypes del dataframe: >>> df . dtypes PassengerId int64 Survived int64 Pclass int64 Name object Sex object Age float64 SibSp int64 Parch int64 Ticket object Fare float64 Cabin object Embarked object dtype : object Notiamo subito la presenza di tre tipi di colonna, ovvero int64 , float64 e object . Laddove i primi due sono autoesplicativi, merita una particolare menzione il tipo object , che viene associato automaticamente a tutte le stringhe. Suggerimento Normalmente, usare il tipo object comporta diversi problemi nella successiva fase di analisi dei dati. Potrebbe quindi essere una buona idea parametrizzare la funzione read_csv() mediante il parametro dtype , che accetta un dizionario che specifica il tipo di una o pi\u00f9 colonne. Ad esempio, se volessimo specificare che i nomi sono delle stringhe, potremmo usare il tipo string : >>> types = { 'Name' : 'string' } >>> df = pd . read_csv ( 'train.csv' , dtype = types ) >>> df . dtypes # ... Name string # ... Appare chiaro come il dataset ci illustri numerose propriet\u00e0 per ogni passeggero imbarcato. Queste potranno quindi essere utilizzate per un'analisi approfondita della struttura dei dati sotto diversi aspetti e punti di vista; ne parleremo pi\u00f9 estesamente nel seguito.","title":"9.2 - Pandas e la gestione dei dati"},{"location":"material/02_libs/09_pandas/lecture/#921-le-series","text":"Abbiamo visto come ogni dataframe sia in realt\u00e0 composto da diverse colonne, ciascuna rappresentativa di una feature specifica. Nella pratica, Pandas ci offre un modo per rappresentare singolarmente ciascuna di queste colonne, mediante un oggetto di classe Series . Ad esempio, potremmo estrarre la serie relativa agli identificativi numerici dei passeggeri: names = df [ 'Name' ] names . head () # Output restituito 0 Braund , Mr . Owen Harris 1 Cumings , Mrs . John Bradley ( Florence Briggs Th ... 2 Heikkinen , Miss . Laina 3 Futrelle , Mrs . Jacques Heath ( Lily May Peel ) 4 Allen , Mr . William Henry Name : Name , dtype : object","title":"9.2.1 - Le Series"},{"location":"material/02_libs/09_pandas/lecture/#9211-accesso-agli-elementi-di-una-serie","text":"Possiamo accedere ad un singolo elemento di una serie mediante una classica procedura di indicizzazione. Notiamo infatti come ogni campione all'interno della serie sia associato ad un indice numerico crescente il cui valore iniziale \u00e8 pari a 0; pertanto, possiamo accedere all' \\(i\\) -mo elemento della serie richiamando l' \\(i-1\\) -mo indice, esattamente come accade per le liste o le sequenze. >>> names [ 0 ] 'Braund, Mr. Owen Harris' Nota L'indicizzazione pu\u00f2 essere anche usata per impostare il valore associato ad uno specifico indice della serie.","title":"9.2.1.1 - Accesso agli elementi di una serie"},{"location":"material/02_libs/09_pandas/lecture/#9212-accesso-agli-elementi-del-dataframe","text":"L'accesso agli elementi del dataframe pu\u00f2 avvenire attraverso diverse modalit\u00e0. In primo luogo, possiamo accedere allo specifico valore di una feature di un dato campione mediante il chained indexing : >>> df [ 'Age' ][ 1 38 In alternativa, \u00e8 possibile usare la funzione loc(row_idx, col) , che permette di accedere al valore assunto dalla feature col per l'elemento in posizione row_idx : >>> df . loc [ 1 , ( 'Age' )] 38.0 La funzione loc() pu\u00f2 operare anche su delle slice di dati: >>> df . loc [ 1 : 5 , ( 'Age' )] 1 38.0 2 26.0 3 35.0 4 35.0 5 NaN o su insiemi di feature: >>> df . loc [ 1 : 5 , ( 'Age' , 'Sex' )] Age Sex 1 38.0 female 2 26.0 female 3 35.0 female 4 35.0 male 5 NaN male Sottolineamo che la funzione loc() opera sugli indici di riga . In questo caso, il nostro dataframe ha degli indici di riga interi, assegnati automaticamente in fase di lettura del dataframe. Nel caso decidessimo di usare una colonna del dataframe come indice, potremmo usare il metodo set_index() : df = df . set_index ( 'Ticket' ) Notiamo che, come al solito, le funzioni lavorano sul valore, e non sulla reference. Di conseguenza, se omettessimo l'assegnazione, df rimarrebbe invariato. Un modo per evitare di usare ogni volta l'operazione di assegnazione \u00e8 quello di impostare il parametro inplace a True : df . set_index ( 'Ticket' , inplace = True ) In alternativa, possiamo decidere di impostare l'indice direttamente nel metodo read_csv() impostando il parametro index_col : df = pd . read_csv ( 'titanic.csv' , index_col = 'Ticket' ) In questo caso, la funzione loc() dovr\u00e0 essere utilizzata usando come parametri di lettura per righe i nuovi indici. Ad esempio: >>> df . loc [ 'STON/O2. 3101282' , 'Name' ] 'Heikkinen, Miss. Laina' Oltre alla funzione loc() Pandas ci mette a disposizione la funzione iloc() , la quale ci offre la possibilit\u00e0 di selezionare un sottoinsieme di campion del dataframe mediante indici interi (da cui la i ): >>> df . iloc [ 2 : 5 , 2 : 4 ] Pclass Name Ticket STON / O2 . 3101282 3 Heikkinen , Miss . Laina 113803 1 Futrelle , Mrs . Jacques Heath ( Lily May Peel ) 373450 3 Allen , Mr . William Henry","title":"9.2.1.2 - Accesso agli elementi del dataframe"},{"location":"material/02_libs/09_pandas/lecture/#922-maschere-booleane","text":"Supponiamo di voler selezionare soltanto gli uomini maggiorenni presenti nel dataset del Titanic. Per farlo, possiamo usare un'istruzione che implementi delle logiche di tipo booleano: >>> men = df [( df [ 'Age' ] > 18 ) & ( df [ 'Sex' ] == 'male' )] >>> men . head () PassengerId Survived Pclass Name Sex Age 0 1 0 3 Braund , Mr . Owen Harris male 22.0 4 5 0 3 Allen , Mr . William Henry male 35.0 6 7 0 1 McCarthy , Mr . Timothy J male 54.0 12 13 0 3 Saundercock , Mr . William Henry male 20.0 13 14 0 3 Andersson , Mr . Anders Johan male 39.0 Nella pratica, stiamo filtrando il dataset in base all' AND logico tra due condizioni: df['Age'] > 18 : questa condizione genera una maschera booleana che \u00e8 True soltanto se l'et\u00e0 per quel passeggero \u00e8 maggiore di 18 anni; df['Sex'] == 'male' : questa condizione genera una maschera booleana che \u00e8 vera soltanto se il genere del passeggero \u00e8 maschile.","title":"9.2.2 - Maschere booleane"},{"location":"material/02_libs/09_pandas/lecture/#923-la-funzione-groupby","text":"Possiamo sfruttare la funzione groupby per raggruppare insiemi di dati (normalmente pertinenti a categorie ). Ad esempio, potremmo raggruppare i passeggeri per genere: >>> df . groupby ([ 'Sex' ]) Possiamo ovviamente estrarre delle statistiche a partire da questi raggruppamenti. Vediamo, ad esempio, l'et\u00e0 media dei passeggeri di sesso femminile e maschile: >>> df . groupby ([ 'Sex' ])[ 'Age' ] . mean () Sex female 27.915709 male 30.726645 Name : Age , dtype : float64","title":"9.2.3 - La funzione groupby"},{"location":"material/02_libs/09_pandas/lecture/#93-scrittura-e-lettura-dei-dataframe","text":"","title":"9.3 - Scrittura e lettura dei dataframe"},{"location":"material/02_libs/09_pandas/lecture/#931-lettura-di-dati-da-sorgenti-eterogenee","text":"Nel nostro primo esempio abbiamo usato la funzione read_csv per creare un dataframe partendo dai dati memorizzati in un file in formato CSV. Tuttavia, Pandas supporta molti altri formati. Ad esempio, potremmo provare a leggere un file Excel: df = pd . read_excel ( 'dati.xlsx' ) Attenzione Per leggere (e scrivere) da (su) Excel \u00e8 necessario installare la libreria openpyxl ( pip install openpyxl ). In alternativa, pu\u00f2 essere letto un file in formato JSON, oppure ancora direttamente un database: df = pd . read_json ( 'dati.json' ) df = pd . read_sql ( SQL_QUERY ) Esiste un elenco completo delle (numerose) funzioni disponibili, che possono essere individuate sulla reference . In generale, comunque, la sintassi \u00e8 sempre read_*(data_source) , con * da sostituire con il tipo di sorgente dati ( csv , excel , etc.).","title":"9.3.1 - Lettura di dati da sorgenti eterogenee"},{"location":"material/02_libs/09_pandas/lecture/#932-scrittura-di-dati-su-destinazioni-eterogenee","text":"Possiamo anche scrivere un dataframe su file mediante le funzioni duali alle read_ , che usano il suffisso to_ seguito dall'estensione del file destinazione. Ad esempio, potremmo scrivere un file CSV con il metodo to_csv : df . to_csv ( 'train.xlsx' )","title":"9.3.2 - Scrittura di dati su destinazioni eterogenee"},{"location":"material/02_libs/09_pandas/lecture/#94-aggiunta-di-feature-e-dati","text":"Immaginiamo adesso di voler aggiungere una nuova feature ad un dataframe gi\u00e0 esistente. Per farlo, iniziamo creando un dataframe da zero: >>> df = pd . DataFrame ([ 1 , 2 , 3 , 4 , 5 ], columns = [ 'one' ]) one 0 1 1 2 2 3 3 4 4 5 Possiamo aggiungere una nuova colonna semplicemente usando l'operatore di assegnazione e specificandone il nome: >>> df [ 'two' ] = df [ 'one' ] * 2 one two 0 1 2 1 2 4 2 3 6 3 4 8 4 5 10 Possiamo poi inserire nuovi campioni in coda al dataframe. Per farlo, dovremo prima creare un nuovo dataframe dalle dimensioni coerenti con quello gi\u00e0 esistente, e poi usare la funzione concat() : >>> df_add = pd . DataFrame ([[ 6 , 7 ]], columns = [ 'one' , 'two' ]) >>> df = pd . concat ([ df , df_add ]) one two 0 1 2 1 2 4 2 3 6 3 4 8 4 5 10 0 6 7 Notiamo che la funzione concat() accetta, tra gli altri, il parametro axis . Se questo \u00e8 uguale a zero (come lo \u00e8 di default), la concat() effettua la concatenazione per righe; se \u00e8 pari ad 1, invece, la concatenazione avviene per colonne. Tuttavia, \u00e8 importante sottolineare come la concatenazione avvenga anche nel caso le misure non siano completamente coerenti: infatti, se provassimo ad effettuare una concatenazione per colonne, avremmo un risultato del tipo: >>> pd . concat ([ df , df_add ], axis = 1 ) one two one two 0 1 2 6.0 7.0 1 2 4 NaN NaN 2 3 6 NaN NaN 3 4 8 NaN NaN 4 5 10 NaN NaN I valori relativi alle righe con indice che va da 1 a 4, che ovviamente non saranno presenti, saranno automaticamente impostati a NaN , acronimo di Not a Number .","title":"9.4 - Aggiunta di feature e dati"},{"location":"material/02_libs/09_pandas/lecture/#95-visualizzazione-dei-dati-in-pandas","text":"Pandas ci offre un supporto nativo a Matplotlib per permettere la visualizzazione dei dati contenuti all'interno di un dataframe. In tal senso, possiamo usare la funzione plot() su una serie o su un intero dataframe; ad esempio, potremmo plottare le et\u00e0 dei passeggeri: df [ 'Age' ] . plot () plt . show () ottenendo il risultato mostrato in figura: Possiamo anche fare il plot dell'intero DataFrame : df . plot () plt . show () che risulter\u00e0 nella seguente figura: Ovviamente, \u00e8 possibile usare Pandas anche per fare il plot di altri tipi di grafico, come ad esempio gli istogrammi. Per farlo, si usano le apposite sotto-funzioni di plot : df [ 'Age' ] . plot . hist () plt . show () Il risultato \u00e8 mostrato in figura. Pandas e Seaborn Pandas si integra in maniera naturale anche con la libreria Seaborn, di cui tratteremo nella prossima lezione.","title":"9.5 - Visualizzazione dei dati in Pandas"},{"location":"material/02_libs/09_pandas/lecture/#96-operazioni-statistiche-sui-dataframe","text":"Pandas ci mette a disposizione delle funzioni, simili a quelle offerte da NumPy, per calcolare delle statistiche per ciascuna delle colonne presenti in un DataFrame. Ad esempio: >>> df . mean () PassengerId 446.000000 Survived 0.383838 Pclass 2.308642 Age 29.699118 SibSp 0.523008 Parch 0.381594 Fare 32.204208 dtype : float64 Ovviamente, esistono funzioni anche per calcolare varianza ( df.var() ), mediana ( df.median() ), deviazione standard ( df.std() ), e via discorrendo. Particolarmente interessante \u00e8 la funzione describe() , che ci mosta tutte le statistiche pi\u00f9 significative per ognuna delle feature considerate. >>> df . describe () PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25 % 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50 % 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75 % 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200","title":"9.6 - Operazioni statistiche sui dataframe"},{"location":"material/02_libs/10_seaborn/exercises/","text":"E10 - Visualizzazione dei dati in Seaborn \u00b6 Esercizio E10.1 \u00b6 Visualizzare la distribuzione dell'et\u00e0 dei diversi passeggeri del Titanic in base al loro genere. Visualizzare inoltre il rapporto tra et\u00e0 e numero di fratelli/sorelle/coniugi in base al genere del passeggero. Soluzione S10.1 \u00b6 Per prima cosa, leggiamo il dataframe: df = pd . read_csv ( 'titanic.csv' ) Per visualizzare la distribuzione dell'et\u00e0 dei diversi passeggeri del Titanic in base al loro genere usiamo un displot() : sns . displot ( data = df , x = 'Age' , col = 'Sex' ) Per visualizzare inoltre il rapporto tra et\u00e0 e numero di fratelli/sorelle/coniugi in base al genere del passeggero usiamo un catplot() : sns . catplot ( data = df , kind = 'violin' , x = 'SibSp' , y = 'Age' , hue = 'Sex' , split = True ) Esercizio E10.2 \u00b6 Effettuiamo un' analisi esplorativa del dataset Titanic. In particolare, sfruttiamo Pandas e Seaborn per: verificare la correlazione tra le feature che riteniamo rilevanti, visualizzandola a schermo mediante una heatmap; analizzare la distribuzione statistica delle feature di tipo categorico e numerico; valutare la descrizione statistica delle diverse feature; dedurre empiricamente l'esistenza di eventuali relazioni tra le diverse feature; isolare quattro diversi tipi di soggetti (maschi adulti, femmine adulte, maschi giovani e femmine giovani), ed effettuare le precedenti analisi in maniera separata su ciascuno dei gruppi.","title":"Esercizi"},{"location":"material/02_libs/10_seaborn/exercises/#e10-visualizzazione-dei-dati-in-seaborn","text":"","title":"E10 - Visualizzazione dei dati in Seaborn"},{"location":"material/02_libs/10_seaborn/exercises/#esercizio-e101","text":"Visualizzare la distribuzione dell'et\u00e0 dei diversi passeggeri del Titanic in base al loro genere. Visualizzare inoltre il rapporto tra et\u00e0 e numero di fratelli/sorelle/coniugi in base al genere del passeggero.","title":"Esercizio E10.1"},{"location":"material/02_libs/10_seaborn/exercises/#soluzione-s101","text":"Per prima cosa, leggiamo il dataframe: df = pd . read_csv ( 'titanic.csv' ) Per visualizzare la distribuzione dell'et\u00e0 dei diversi passeggeri del Titanic in base al loro genere usiamo un displot() : sns . displot ( data = df , x = 'Age' , col = 'Sex' ) Per visualizzare inoltre il rapporto tra et\u00e0 e numero di fratelli/sorelle/coniugi in base al genere del passeggero usiamo un catplot() : sns . catplot ( data = df , kind = 'violin' , x = 'SibSp' , y = 'Age' , hue = 'Sex' , split = True )","title":"Soluzione S10.1"},{"location":"material/02_libs/10_seaborn/exercises/#esercizio-e102","text":"Effettuiamo un' analisi esplorativa del dataset Titanic. In particolare, sfruttiamo Pandas e Seaborn per: verificare la correlazione tra le feature che riteniamo rilevanti, visualizzandola a schermo mediante una heatmap; analizzare la distribuzione statistica delle feature di tipo categorico e numerico; valutare la descrizione statistica delle diverse feature; dedurre empiricamente l'esistenza di eventuali relazioni tra le diverse feature; isolare quattro diversi tipi di soggetti (maschi adulti, femmine adulte, maschi giovani e femmine giovani), ed effettuare le precedenti analisi in maniera separata su ciascuno dei gruppi.","title":"Esercizio E10.2"},{"location":"material/02_libs/10_seaborn/lecture/","text":"10 - Visualizzazione di dati in Seaborn \u00b6 Seaborn \u00e8 una libreria che estende Matplotlib aggiungendone diverse funzionalit\u00e0, tutte nell'ottica della data analysis, e sulla scia di quello che abbiamo presentato in Pandas nella lezione precedente. Ci\u00f2 permette quindi di mantenere un'interfaccia molto simile a quella di Matplotlib, estendendone al contempo le possibilit\u00e0. Vediamo qualche esempio. 10.1 - Installazione della libreria \u00b6 Come in ogni altro caso, partiamo dall'installazione della libreria: pip install seaborn Una volta installata, potremo importarla utilizzando un alias: import seaborn as sns 10.1 - Lettura dei dati \u00b6 Abbiamo detto che Seaborn \u00e8 utile specialmente nel momento in cui si vogliono valutare visiamente le relazioni che intercorrono tra diverse feature presenti all'interno di un dataset. In tal senso, proviamo innanzitutto a caricare un insieme di dati, affidandoci al metodo load_dataset() , che estrae uno dei dataset gi\u00e0 presenti nella libreria. Ad esempio: tips = load_dataset ( 'tips' ) I dataset L'elenco dei dataset supportati da Seaborn \u00e8 presente a questo indirizzo . Ispezionando il tipo di tips possiamo scoprire che si tratta di un dataframe; di conseguenza, possiamo esplorarne liberamente la struttura utilizzando Pandas. In particolare, vediamo che questi sono organizzati secondo la seguente tabella: >>> tips . head () total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 La struttura della tabella \u00e8 la seguente: ogni riga \u00e8 associata ad una specifica ordinazione; le colonne sono associate rispettivamente a conto ( total_bill ), mancia ( tip ), genere ( sex ), fumatore (smoker), giorno ( day ), orario ( time ) e numero di attendenti ( size ). 10.1.1 - Visualizzare le relazioni tra dati \u00b6 Seaborn ci offre la funzione relplot() che ci permette di analizzare velocemente diversi aspetti inclusi del dataset. Ad esempio, potremmo vedere come cambiano contro e mancia al variare della giornata: sns . relplot ( data = tips , x = 'total_bill' , y = 'tip' , col = 'day' ) Notiamo che abbiamo passato al parametro data il valore tips , indicando quindi la sorgente dei dati. Metteremo poi sull'asse delle ascisse il conto totale, mentre su quello delle ascisse la mancia ricevuta. In ultimo, il parametro col ci permette di generare tanti grafici quanti sono i diversi valori presenti nella tabella day , ognuno dei quali rappresenter\u00e0 ovviamente l'andamento dei conti e delle mance per quello specifico giorno. Un altro esperimento \u00e8 quello che vede valutare la differenza tra conto e mance pagati da uomini e donne. In questo caso, inoltre, andiamo ad aumentare la dimensione del punto in maniera direttamente proporzionale alla mancia percepita. sns . relplot ( data = tips , x = 'total_bill' , y = 'tip' , col = 'sex' , size = 'tip' ) Una funzione simile alla relplot() \u00e8 la lmplot() , che permette anche di mostrare un'approssimazione ai minimi quadrati dei dati. Ad esempio: sns . lmplot ( data = tips , x = 'total_bill' , y = 'tip' , col = 'time' , hue = 'day' ) 10.2 - Analisi della distribuzione dati \u00b6 Possiamo anche effettuare un'analisi della distribuzione delle variabili all'interno del nostro dataset. In tal senso, la funzione displot() ci permette di vedere come si vanno a distribuire i dati in base a determinate condizioni mediante l'uso di un istogramma. Ad esempio, potremmo visualizzare la distribuzione dei clienti in base al loro genere ed al momento della giornata in cui effettuano la consumazione: sns . displot ( data = tips , x = 'sex' , col = 'time' , kde = True ) Specificando il parametro kde , \u00e8 possibile ottenere un'approssimazione della distribuzione mediante kernel density estimation, come mostrato nella figura seguente. 10.3 - Plot di dati categorici \u00b6 Seaborn offre anche dei plot specializzati per la creazione e visualizzazione di dati (o feature) di tipo categorico , ovvero dati appartenenti ad una tra diverse possibili categorie. In tal senso, un esempio di feature categorica \u00e8 il genere dei clienti del ristorante, che nel dataset sono soltanto uomini o donne. I plot di questo tipo possono essere generati mediante la funzione catplot() , delegata alla definizione di plot a diversi livelli di granularit\u00e0, come ad esempio i violin plot . sns . catplot ( data = tips , kind = 'violin' , x = 'day' , y = 'tip' , hue = 'sex' , split = True ) In particolare, il grafico mostrato in figura descrive la distribuzione delle mance giorno per giorno al variare del genere del cliente. Catplot con dati non categorici In realt\u00e0, \u00e8 possibile usare la catplot() con dati non categorici, come numeri interi. Tuttavia, vi \u00e8 il rischio (o meglio, la certezza ) che il risultato sia non interpretabile , in quanto la funzione assegner\u00e0 una categoria ad ogni possibile valore assunto dalla feature di riferimento, il che ovviamente comporter\u00e0 l'illeggibilit\u00e0 del grafico nel caso di valori reali. 10.4 - Heatmap \u00b6 Un'ultima funzione che vale la pena menzionare \u00e8 quella che ci permette di visualizzare le heatmap , ovvero delle strutture grafiche che ci permettono di visualizzare rapidamente gli intervalli in cui ricadono i valori di diversi tipi di matrici. Questa funzione \u00e8, per l'appunto, chiamata heatmap() , e richiede in ingresso almeno il parametro relativo alla matrice da cui sar\u00e0 estratta la figura. Ad esempio: ar = np . array ([[ 5 , 12 ], [ 4 , 3 ]]) sns . heatmap ( ar , cmap = 'jet' , annot = True , xticklabels = False , yticklabels = False ) Nella precedente invocazione della funzione heatmap() specifichiamo i parametri indicati in modo da passare un array (o similari) come primo argomento, seguito da una colormap , ovvero i colori da utilizzare. Specifichiamo inoltre che vogliamo inserire i valori dell'array su ciascuna delle celle dell'heatmap (mediante il parametro annot ) e che non vogliamo visualizzare i label sugli assi \\(x\\) e \\(y\\) ( xticklabels ed yticklabels rispettivamente). Otterremo questo risultato:","title":"Dispense"},{"location":"material/02_libs/10_seaborn/lecture/#10-visualizzazione-di-dati-in-seaborn","text":"Seaborn \u00e8 una libreria che estende Matplotlib aggiungendone diverse funzionalit\u00e0, tutte nell'ottica della data analysis, e sulla scia di quello che abbiamo presentato in Pandas nella lezione precedente. Ci\u00f2 permette quindi di mantenere un'interfaccia molto simile a quella di Matplotlib, estendendone al contempo le possibilit\u00e0. Vediamo qualche esempio.","title":"10 - Visualizzazione di dati in Seaborn"},{"location":"material/02_libs/10_seaborn/lecture/#101-installazione-della-libreria","text":"Come in ogni altro caso, partiamo dall'installazione della libreria: pip install seaborn Una volta installata, potremo importarla utilizzando un alias: import seaborn as sns","title":"10.1 - Installazione della libreria"},{"location":"material/02_libs/10_seaborn/lecture/#101-lettura-dei-dati","text":"Abbiamo detto che Seaborn \u00e8 utile specialmente nel momento in cui si vogliono valutare visiamente le relazioni che intercorrono tra diverse feature presenti all'interno di un dataset. In tal senso, proviamo innanzitutto a caricare un insieme di dati, affidandoci al metodo load_dataset() , che estrae uno dei dataset gi\u00e0 presenti nella libreria. Ad esempio: tips = load_dataset ( 'tips' ) I dataset L'elenco dei dataset supportati da Seaborn \u00e8 presente a questo indirizzo . Ispezionando il tipo di tips possiamo scoprire che si tratta di un dataframe; di conseguenza, possiamo esplorarne liberamente la struttura utilizzando Pandas. In particolare, vediamo che questi sono organizzati secondo la seguente tabella: >>> tips . head () total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 La struttura della tabella \u00e8 la seguente: ogni riga \u00e8 associata ad una specifica ordinazione; le colonne sono associate rispettivamente a conto ( total_bill ), mancia ( tip ), genere ( sex ), fumatore (smoker), giorno ( day ), orario ( time ) e numero di attendenti ( size ).","title":"10.1 - Lettura dei dati"},{"location":"material/02_libs/10_seaborn/lecture/#1011-visualizzare-le-relazioni-tra-dati","text":"Seaborn ci offre la funzione relplot() che ci permette di analizzare velocemente diversi aspetti inclusi del dataset. Ad esempio, potremmo vedere come cambiano contro e mancia al variare della giornata: sns . relplot ( data = tips , x = 'total_bill' , y = 'tip' , col = 'day' ) Notiamo che abbiamo passato al parametro data il valore tips , indicando quindi la sorgente dei dati. Metteremo poi sull'asse delle ascisse il conto totale, mentre su quello delle ascisse la mancia ricevuta. In ultimo, il parametro col ci permette di generare tanti grafici quanti sono i diversi valori presenti nella tabella day , ognuno dei quali rappresenter\u00e0 ovviamente l'andamento dei conti e delle mance per quello specifico giorno. Un altro esperimento \u00e8 quello che vede valutare la differenza tra conto e mance pagati da uomini e donne. In questo caso, inoltre, andiamo ad aumentare la dimensione del punto in maniera direttamente proporzionale alla mancia percepita. sns . relplot ( data = tips , x = 'total_bill' , y = 'tip' , col = 'sex' , size = 'tip' ) Una funzione simile alla relplot() \u00e8 la lmplot() , che permette anche di mostrare un'approssimazione ai minimi quadrati dei dati. Ad esempio: sns . lmplot ( data = tips , x = 'total_bill' , y = 'tip' , col = 'time' , hue = 'day' )","title":"10.1.1 - Visualizzare le relazioni tra dati"},{"location":"material/02_libs/10_seaborn/lecture/#102-analisi-della-distribuzione-dati","text":"Possiamo anche effettuare un'analisi della distribuzione delle variabili all'interno del nostro dataset. In tal senso, la funzione displot() ci permette di vedere come si vanno a distribuire i dati in base a determinate condizioni mediante l'uso di un istogramma. Ad esempio, potremmo visualizzare la distribuzione dei clienti in base al loro genere ed al momento della giornata in cui effettuano la consumazione: sns . displot ( data = tips , x = 'sex' , col = 'time' , kde = True ) Specificando il parametro kde , \u00e8 possibile ottenere un'approssimazione della distribuzione mediante kernel density estimation, come mostrato nella figura seguente.","title":"10.2 - Analisi della distribuzione dati"},{"location":"material/02_libs/10_seaborn/lecture/#103-plot-di-dati-categorici","text":"Seaborn offre anche dei plot specializzati per la creazione e visualizzazione di dati (o feature) di tipo categorico , ovvero dati appartenenti ad una tra diverse possibili categorie. In tal senso, un esempio di feature categorica \u00e8 il genere dei clienti del ristorante, che nel dataset sono soltanto uomini o donne. I plot di questo tipo possono essere generati mediante la funzione catplot() , delegata alla definizione di plot a diversi livelli di granularit\u00e0, come ad esempio i violin plot . sns . catplot ( data = tips , kind = 'violin' , x = 'day' , y = 'tip' , hue = 'sex' , split = True ) In particolare, il grafico mostrato in figura descrive la distribuzione delle mance giorno per giorno al variare del genere del cliente. Catplot con dati non categorici In realt\u00e0, \u00e8 possibile usare la catplot() con dati non categorici, come numeri interi. Tuttavia, vi \u00e8 il rischio (o meglio, la certezza ) che il risultato sia non interpretabile , in quanto la funzione assegner\u00e0 una categoria ad ogni possibile valore assunto dalla feature di riferimento, il che ovviamente comporter\u00e0 l'illeggibilit\u00e0 del grafico nel caso di valori reali.","title":"10.3 - Plot di dati categorici"},{"location":"material/02_libs/10_seaborn/lecture/#104-heatmap","text":"Un'ultima funzione che vale la pena menzionare \u00e8 quella che ci permette di visualizzare le heatmap , ovvero delle strutture grafiche che ci permettono di visualizzare rapidamente gli intervalli in cui ricadono i valori di diversi tipi di matrici. Questa funzione \u00e8, per l'appunto, chiamata heatmap() , e richiede in ingresso almeno il parametro relativo alla matrice da cui sar\u00e0 estratta la figura. Ad esempio: ar = np . array ([[ 5 , 12 ], [ 4 , 3 ]]) sns . heatmap ( ar , cmap = 'jet' , annot = True , xticklabels = False , yticklabels = False ) Nella precedente invocazione della funzione heatmap() specifichiamo i parametri indicati in modo da passare un array (o similari) come primo argomento, seguito da una colormap , ovvero i colori da utilizzare. Specifichiamo inoltre che vogliamo inserire i valori dell'array su ciascuna delle celle dell'heatmap (mediante il parametro annot ) e che non vogliamo visualizzare i label sugli assi \\(x\\) e \\(y\\) ( xticklabels ed yticklabels rispettivamente). Otterremo questo risultato:","title":"10.4 - Heatmap"},{"location":"material/02_libs/11_scipy/exercises/","text":"E11 - Introduzione a SciPy \u00b6 Esercizio E11.1 \u00b6 Scrivere una funzione che restituisca True se la matrice passata in ingresso \u00e8 invertibile, False altrimenti. Usare SciPy. Soluzione S10.1 \u00b6 Ecco una possibile soluzione: from scipy import linalg def invertibile ( mat ): \"\"\" Usiamo un operatore ternario. Il risultato \u00e8 analogo alla seguente: if linalg.det(mat) != 0.: return True else: return False \"\"\" return True if linalg . det ( mat ) != 0. else False Esercizio E11.2 \u00b6 Scrivere una classe che, incorporando la funzione precedente, permetta di invertire una matrice. Soluzione S11.2 \u00b6 Ecco una possibile soluzione: from scipy import linalg import warnings class InversoreMatrici (): def __init__ ( self , mat ): self . mat = mat self . invertibilita = mat @property def mat ( self ): return self . __mat @mat . setter def mat ( self , value ): if value is None : raise ValueError ( 'La matrice non pu\u00f2 essere nulla' ) self . __mat = value @property def inv ( self ): return self . __inv @inv . setter def inv ( self , value ): if value is None : raise ValueError ( \"L'inversa non pu\u00f2 essere nulla\" ) self . __inv = value @property def invertibilita ( self ): return self . __invertibilita @invertibilita . setter def invertibilita ( self , value ): if value is None : raise ValueError ( \"La determinazione dell'invertibilit\u00e0 non pu\u00f2 essere nulla\" ) self . __invertibilita = True if linalg . det ( value ) != 0. else False def inverti ( self ): if self . invertibilita : self . inv = linalg . inv ( self . mat ) else : warnings . warn ( 'La matrice non \u00e8 invertibile' ) a = np . array ([[ 1 , 2 ], [ 2 , 5 ]]) i = InversoreMatrici ( a ) i . inverti () i . inv","title":"Esercizi"},{"location":"material/02_libs/11_scipy/exercises/#e11-introduzione-a-scipy","text":"","title":"E11 - Introduzione a SciPy"},{"location":"material/02_libs/11_scipy/exercises/#esercizio-e111","text":"Scrivere una funzione che restituisca True se la matrice passata in ingresso \u00e8 invertibile, False altrimenti. Usare SciPy.","title":"Esercizio E11.1"},{"location":"material/02_libs/11_scipy/exercises/#soluzione-s101","text":"Ecco una possibile soluzione: from scipy import linalg def invertibile ( mat ): \"\"\" Usiamo un operatore ternario. Il risultato \u00e8 analogo alla seguente: if linalg.det(mat) != 0.: return True else: return False \"\"\" return True if linalg . det ( mat ) != 0. else False","title":"Soluzione S10.1"},{"location":"material/02_libs/11_scipy/exercises/#esercizio-e112","text":"Scrivere una classe che, incorporando la funzione precedente, permetta di invertire una matrice.","title":"Esercizio E11.2"},{"location":"material/02_libs/11_scipy/exercises/#soluzione-s112","text":"Ecco una possibile soluzione: from scipy import linalg import warnings class InversoreMatrici (): def __init__ ( self , mat ): self . mat = mat self . invertibilita = mat @property def mat ( self ): return self . __mat @mat . setter def mat ( self , value ): if value is None : raise ValueError ( 'La matrice non pu\u00f2 essere nulla' ) self . __mat = value @property def inv ( self ): return self . __inv @inv . setter def inv ( self , value ): if value is None : raise ValueError ( \"L'inversa non pu\u00f2 essere nulla\" ) self . __inv = value @property def invertibilita ( self ): return self . __invertibilita @invertibilita . setter def invertibilita ( self , value ): if value is None : raise ValueError ( \"La determinazione dell'invertibilit\u00e0 non pu\u00f2 essere nulla\" ) self . __invertibilita = True if linalg . det ( value ) != 0. else False def inverti ( self ): if self . invertibilita : self . inv = linalg . inv ( self . mat ) else : warnings . warn ( 'La matrice non \u00e8 invertibile' ) a = np . array ([[ 1 , 2 ], [ 2 , 5 ]]) i = InversoreMatrici ( a ) i . inverti () i . inv","title":"Soluzione S11.2"},{"location":"material/02_libs/11_scipy/lecture/","text":"11 - L'ecosistema SciPy \u00b6 Ai lettori pi\u00f9 attenti pu\u00f2 apparire evidente come tutte le librerie viste finora facciano parte di una sorta di \"ecosistema\" pensato per permettere un'interazione tra tipi e classi il quanto pi\u00f9 possibile \"semplice\" e coesa. Questo \u00e8 dovuto al fatto che librerie come NumPy, Matplotlib, Pandas e Seaborn fanno tutte parte di un unico ecosistema chiamato SciPy , pensato per dare delle fondamenta comuni su cui costruire l'intera disciplina del calcolo scientifico in Python. Tuttavia, abbiamo omesso una delle librerie fondamentali di questo ecosistema, talmente importante che prende il nome del framework stesso: ovviamente, stiamo parlando della libreria SciPy. 11.1 - La libreria SciPy \u00b6 La libreria SciPy presenta un vastissimo insieme di algoritmi e funzioni matematiche costruite a partire dagli oggetti definiti da NumPy. Al solito, la libreria va installata usando, ad esempio pip : pip install scipy In questa giocoforza brevissima introduzione, vedremo alcune delle potenzialit\u00e0 di SciPy, basandoci su un paio di casi d'uso (pi\u00f9 o meno) reali. 11.2 - Validazione empirica di due distribuzioni \u00b6 Proviamo a vedere come viene visualizzato il valore (teorico) assunto da due distribuzioni di probabilit\u00e0 \"classiche\", ovvero la distribuzione uniforme e quella normale. Vediamo come comparare visivamente il valore teorico assunto da due distribuzioni di probabilit\u00e0 \"standard\" (ovvero la uniforme e la normale) e l'istogramma ottenuto a partire da un elevato numero di elementi generati casualmente ma appartenenti a quella distribuzione. In primis, iniziamo importando i moduli norm ed uniform dal package stats , atti a modellare tutte le istruzioni riguardanti le distribuzioni normali ed uniformi: from scipy.stats import norm , uniform Generiamo adesso 100 campioni equidistanziati e compresi tra l'1 ed il 99 percentile delle distribuzioni: x_1 = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 100 ) x_2 = np . linspace ( uniform . ppf ( 0.01 ), uniform . ppf ( 0.99 ), 100 ) Stiamo usando la funzione linspace() per generare dei campioni equidistanti tra loro, compresi tra dist.ppf(0.01) e dist.ppf(0.99) . L'oggetto dist pu\u00f2 essere sia norm che uniform , mentre ppf(0.01) rappresenta l'1-percentile della distribuzione (e, analogamente, ppf(0.99) rappresenta il 99-percentile). In parole povere, stiamo generando cento campioni equidistanti tra l'1-percentile ed il 99-percentile della distribuzione dist . Successivamente, utilizziamo la funzione rvs() per generare casualmente un \"gran\" numero di valori che per\u00f2 siano distribuiti secondo le due distribuzioni considerate: r_1 = norm . rvs ( size = 1000 ) r_2 = uniform . rvs ( size = 1000 ) A questo punto, possiamo plottare l'istogramma dei valori r_i , e verificare che segua la distribuzione di probabilit\u00e0 pdf(x) per ciascuno dei due tipi di distribuzione. Ricordiamo di inserire il valore density=True per normalizzare l'istogramma. Il risultato dovrebbe essere simile a quello mostrato in figura: 11.3 - Calcolo del determinante e dell'inversa \u00b6 SciPy offre anche la possibilit\u00e0 di effettuare calcoli algebrici grazie ad un numero di funzioni molto pi\u00f9 elevato rispetto a quelle presenti in NumPy. Per fare un rapido esempio, vediamo come \u00e8 possibile calcolare il determinante e l'inversa di una matrice. from scipy import linalg # ... matrice mat creata sotto forma di array NumPy # Determinante d = linalg . det ( mat ) # Inversa i = linalg . inv ( mat ) Nota E' molto semplice notare come la sintassi richiami quella di NumPy e, in realt\u00e0, anche il funzionamento sia il medesimo, per cui \u00e8 possibile usare indifferentemente entrambe le librerie. Dove SciPy \"spicca\" \u00e8 in tutte quelle funzioni che non sono presenti in NumPy. 11.4 - Filtraggio di un segnale \u00b6 SciPy ha al suo interno diverse librerie per l'elaborazione dei segnali a diverse dimensionalit\u00e0. Per fare un esempio, proviamo ad utilizzare un filtro di Savitzky-Golay su un array monodimensionale mediante la funzione savgol_filter() . Creiamo un array casuale mediante NumPy: noisy = np . random . normal ( 0 , 1 , size = ( 100 ,)) Filtriamo questo segnale usando un filtro di Savitzky-Golay con finestra di lunghezza pari a 7 campioni e mediante un polinomio approssimante di secondo grado: from scipy.signal import savgol_filter filtered = savgol_filter ( noisy , 7 , 2 ) Creiamo il plot: plt . plot ( np . arange ( 100 ), noisy , alpha = 0.5 , label = 'Segnale originale' ) plt . plot ( np . arange ( 100 ), filtered , label = 'Segnale filtrato' ) plt . grid () plt . legend () plt . show () Otterremo un risultato simile a quello mostrato in figura:","title":"Dispense"},{"location":"material/02_libs/11_scipy/lecture/#11-lecosistema-scipy","text":"Ai lettori pi\u00f9 attenti pu\u00f2 apparire evidente come tutte le librerie viste finora facciano parte di una sorta di \"ecosistema\" pensato per permettere un'interazione tra tipi e classi il quanto pi\u00f9 possibile \"semplice\" e coesa. Questo \u00e8 dovuto al fatto che librerie come NumPy, Matplotlib, Pandas e Seaborn fanno tutte parte di un unico ecosistema chiamato SciPy , pensato per dare delle fondamenta comuni su cui costruire l'intera disciplina del calcolo scientifico in Python. Tuttavia, abbiamo omesso una delle librerie fondamentali di questo ecosistema, talmente importante che prende il nome del framework stesso: ovviamente, stiamo parlando della libreria SciPy.","title":"11 - L'ecosistema SciPy"},{"location":"material/02_libs/11_scipy/lecture/#111-la-libreria-scipy","text":"La libreria SciPy presenta un vastissimo insieme di algoritmi e funzioni matematiche costruite a partire dagli oggetti definiti da NumPy. Al solito, la libreria va installata usando, ad esempio pip : pip install scipy In questa giocoforza brevissima introduzione, vedremo alcune delle potenzialit\u00e0 di SciPy, basandoci su un paio di casi d'uso (pi\u00f9 o meno) reali.","title":"11.1 - La libreria SciPy"},{"location":"material/02_libs/11_scipy/lecture/#112-validazione-empirica-di-due-distribuzioni","text":"Proviamo a vedere come viene visualizzato il valore (teorico) assunto da due distribuzioni di probabilit\u00e0 \"classiche\", ovvero la distribuzione uniforme e quella normale. Vediamo come comparare visivamente il valore teorico assunto da due distribuzioni di probabilit\u00e0 \"standard\" (ovvero la uniforme e la normale) e l'istogramma ottenuto a partire da un elevato numero di elementi generati casualmente ma appartenenti a quella distribuzione. In primis, iniziamo importando i moduli norm ed uniform dal package stats , atti a modellare tutte le istruzioni riguardanti le distribuzioni normali ed uniformi: from scipy.stats import norm , uniform Generiamo adesso 100 campioni equidistanziati e compresi tra l'1 ed il 99 percentile delle distribuzioni: x_1 = np . linspace ( norm . ppf ( 0.01 ), norm . ppf ( 0.99 ), 100 ) x_2 = np . linspace ( uniform . ppf ( 0.01 ), uniform . ppf ( 0.99 ), 100 ) Stiamo usando la funzione linspace() per generare dei campioni equidistanti tra loro, compresi tra dist.ppf(0.01) e dist.ppf(0.99) . L'oggetto dist pu\u00f2 essere sia norm che uniform , mentre ppf(0.01) rappresenta l'1-percentile della distribuzione (e, analogamente, ppf(0.99) rappresenta il 99-percentile). In parole povere, stiamo generando cento campioni equidistanti tra l'1-percentile ed il 99-percentile della distribuzione dist . Successivamente, utilizziamo la funzione rvs() per generare casualmente un \"gran\" numero di valori che per\u00f2 siano distribuiti secondo le due distribuzioni considerate: r_1 = norm . rvs ( size = 1000 ) r_2 = uniform . rvs ( size = 1000 ) A questo punto, possiamo plottare l'istogramma dei valori r_i , e verificare che segua la distribuzione di probabilit\u00e0 pdf(x) per ciascuno dei due tipi di distribuzione. Ricordiamo di inserire il valore density=True per normalizzare l'istogramma. Il risultato dovrebbe essere simile a quello mostrato in figura:","title":"11.2 - Validazione empirica di due distribuzioni"},{"location":"material/02_libs/11_scipy/lecture/#113-calcolo-del-determinante-e-dellinversa","text":"SciPy offre anche la possibilit\u00e0 di effettuare calcoli algebrici grazie ad un numero di funzioni molto pi\u00f9 elevato rispetto a quelle presenti in NumPy. Per fare un rapido esempio, vediamo come \u00e8 possibile calcolare il determinante e l'inversa di una matrice. from scipy import linalg # ... matrice mat creata sotto forma di array NumPy # Determinante d = linalg . det ( mat ) # Inversa i = linalg . inv ( mat ) Nota E' molto semplice notare come la sintassi richiami quella di NumPy e, in realt\u00e0, anche il funzionamento sia il medesimo, per cui \u00e8 possibile usare indifferentemente entrambe le librerie. Dove SciPy \"spicca\" \u00e8 in tutte quelle funzioni che non sono presenti in NumPy.","title":"11.3 - Calcolo del determinante e dell'inversa"},{"location":"material/02_libs/11_scipy/lecture/#114-filtraggio-di-un-segnale","text":"SciPy ha al suo interno diverse librerie per l'elaborazione dei segnali a diverse dimensionalit\u00e0. Per fare un esempio, proviamo ad utilizzare un filtro di Savitzky-Golay su un array monodimensionale mediante la funzione savgol_filter() . Creiamo un array casuale mediante NumPy: noisy = np . random . normal ( 0 , 1 , size = ( 100 ,)) Filtriamo questo segnale usando un filtro di Savitzky-Golay con finestra di lunghezza pari a 7 campioni e mediante un polinomio approssimante di secondo grado: from scipy.signal import savgol_filter filtered = savgol_filter ( noisy , 7 , 2 ) Creiamo il plot: plt . plot ( np . arange ( 100 ), noisy , alpha = 0.5 , label = 'Segnale originale' ) plt . plot ( np . arange ( 100 ), filtered , label = 'Segnale filtrato' ) plt . grid () plt . legend () plt . show () Otterremo un risultato simile a quello mostrato in figura:","title":"11.4 - Filtraggio di un segnale"},{"location":"material/03_ml_sklearn/12_intro_ml/lecture/","text":"12 - Introduzione al machine learning \u00b6 Il machine learning \u00e8 alla base di alcune tra le pi\u00f9 importanti tecnologie odierne. Le sue applicazioni sono molteplici: si va dagli strumenti di traduzione automatica fino ai veicoli autonomi, passando per sistemi di videosorveglianza e software per scrivere codice. In pratica, l'avvento del machine learning ha offerto un modo alternativo, e pi\u00f9 efficace , di risolvere problemi estremamente complessi. Volendo riassumere il concetto alla base del machine learning, potremmo dire che questo rappresenta il procedimento che insegna ad un software, chiamato modello , a fare predizioni significative a partire da un insieme di dati. In altri termini: Modello di machine learning Un modello di machine learning rappresenta la relazione matematica intercorrente tra i dati che il sistema derivante utilizza per effettuare predizioni. Come esempio, immaginiamo di creare un software che effettui la predizione del quantitativo di pioggia che cadr\u00e0 in una zona. Per farlo, possiamo usare due approcci: nell'approccio tradizionale , creeremo una rappresentazione fisica dell'atmosfera e della superficie terrestre, risolvendo equazioni estremamente complesse come le Navier-Stokes; nell'approccio basato sul machine learning , daremo ad un modello un quantitativo adeguato (e, molto spesso, enorme ) di dati riguardanti le condizioni meteorologiche, fino a che il modello stesso non apprender\u00e0 le relazioni sottostanti i diversi pattern di feature meteorologiche che permettono di produrre diversi quantitativi di pioggia. In entrambi i casi, una volta completata l'implementazione (per l'approccio tradizionale) o l'addestramento (per l'approccio basato su machine learning) passeremo al software i dati sulla condizione meteorologica attuale, per poi predire il quantitativo di pioggia previsto. 12.1 - Tipi di sistemi di machine learning \u00b6 I sistemi di machine learning ricadono in tre diverse categorie, distinte sulla base di come \"apprendono\" a fare determinate predizioni. 12.1.1 - Sistemi ad apprendimento supervisionato \u00b6 I sistemi ad apprendimento supervisionato ( supervised learning ) effettuano una predizione dopo aver appreso le relazioni intercorrenti tra un numero pi\u00f9 o meno grande di dati ed i corrispondenti valori da predire. Per intenderci, un sistema di questo tipo \u00e8 un po' come uno studente di matematica che, dopo aver appreso i metodi per la risoluzione di un problema di analisi mediante la risoluzione di un gran numero degli stessi, si prepara a sostenere l'esame. Perch\u00e9 supervisionato? L'appellativo supervisionato deriva dal fatto che \u00e8 (di solito) un esperto di dominio a fornire al sistema i dati con i risultati corretti. I pi\u00f9 importanti approcci all'apprendimento supervisionato sono la regressione e la classificazione . 12.1.1.1 - Modelli di regressione \u00b6 Un modello di regressione predice un valore numerico. Ad esempio, un modello meteorologico di regressione potrebbe predire il quantitativo di pioggia in millimetri, mentre un altro modello di regressione potrebbe valutare l'andamento dei prezzi delle propriet\u00e0 immobiliari sulla base di dati come i metri quadri, la posizione e le caratteristiche della casa, nonch\u00e9 la situazione attuale dei mercati finanziario ed immobiliare. 12.1.1.2 - Modelli di classificazione \u00b6 A differenza dei modelli di regressione, il cui output \u00e8 rappresentato da un numero, i modelli di classificazione restituiscono in uscita un valore che stabilisce la possibilit\u00e0 che un certo campione appartenga ad una data categoria. Ad esempio, un modello di classificazione potrebbe essere usato per predire se un'email \u00e8 un messaggio di spam, o se una foto contiene invece un gatto o un cane. Esistono due macrocategorie di modelli di classificazione, ovvero quelli binari e quelli multiclasse . In particolare, i modelli di classificazione binaria distinguono esclusivamente tra due valori: ad esempio, un modello di classificazione delle email potrebbe indicare se il messaggio \u00e8 di spam o meno. I modelli di classificazione multiclasse invece riescono a distinguere tra pi\u00f9 classi: ad esempio, il nostro modello di riconoscimento delle foto potrebbe riconoscere oggetti di \"classe\" gatto, cane, gallina ed oca. 12.1.2 - Sistemi ad apprendimento non supervisionato \u00b6 I sistemi di apprendimento non supervisionato compiono delle predizioni a partire da dati che non contengono alcuna informazione sulla classe di appartenenza o sul valore di regressione. In pratica, i modelli non supervisionati hanno il compito di identificare pattern significativi direttamente nei dati , senza alcun \"indizio\" a priori, ma limitandosi ad inferire automaticamente le proprie regole. Algoritmi comunemente utilizzati in tal senso sono quelli di clustering , nei quali il modello individua come i dati vanno a \"disporsi\" utilizzando delle regole basate su distanze o capacit\u00e0 di \"agglomerarsi\". Il clustering differisce dagli algoritmi supervisionati, ed in particolare dalla classificazione, principalmente perch\u00e9 le categorie non sono definite a priori da un esperto di dominio. Ad esempio, un algoritmo di clustering potrebbe raggruppare i campioni in un dataset meteo sulla base esclusivamente delle temperature, rivelando delle suddivisioni che definiscono le diverse stagioni, oppure ancora gli orari del giorno. Sar\u00e0 poi nostro compito \"provare\" a dare un nome a questi cluster sulla base della nostra interpretazione del dataset. 12.1.3 - Sistemi di reinforcement learning \u00b6 I sistemi di reinforcement learning effettuano delle predizioni a partire da ricompense o penalit\u00e0 basate sulle azioni effettuate da un agente all'interno di un ambiente . Sulla base di queste osservazioni, il sistema di reinforcement learning genera una policy che definisce la strategia migliore per raggiungere lo scopo prefissato. Le applicazioni dei sistemi di questo tipo sono varie, e spaziano dall'addestramento dei robot per svolgere task anche complessi, alla creazione di programmi come Alpha Go che sfidino (e battano) gli umani al gioco del Go.","title":"Dispense"},{"location":"material/03_ml_sklearn/12_intro_ml/lecture/#12-introduzione-al-machine-learning","text":"Il machine learning \u00e8 alla base di alcune tra le pi\u00f9 importanti tecnologie odierne. Le sue applicazioni sono molteplici: si va dagli strumenti di traduzione automatica fino ai veicoli autonomi, passando per sistemi di videosorveglianza e software per scrivere codice. In pratica, l'avvento del machine learning ha offerto un modo alternativo, e pi\u00f9 efficace , di risolvere problemi estremamente complessi. Volendo riassumere il concetto alla base del machine learning, potremmo dire che questo rappresenta il procedimento che insegna ad un software, chiamato modello , a fare predizioni significative a partire da un insieme di dati. In altri termini: Modello di machine learning Un modello di machine learning rappresenta la relazione matematica intercorrente tra i dati che il sistema derivante utilizza per effettuare predizioni. Come esempio, immaginiamo di creare un software che effettui la predizione del quantitativo di pioggia che cadr\u00e0 in una zona. Per farlo, possiamo usare due approcci: nell'approccio tradizionale , creeremo una rappresentazione fisica dell'atmosfera e della superficie terrestre, risolvendo equazioni estremamente complesse come le Navier-Stokes; nell'approccio basato sul machine learning , daremo ad un modello un quantitativo adeguato (e, molto spesso, enorme ) di dati riguardanti le condizioni meteorologiche, fino a che il modello stesso non apprender\u00e0 le relazioni sottostanti i diversi pattern di feature meteorologiche che permettono di produrre diversi quantitativi di pioggia. In entrambi i casi, una volta completata l'implementazione (per l'approccio tradizionale) o l'addestramento (per l'approccio basato su machine learning) passeremo al software i dati sulla condizione meteorologica attuale, per poi predire il quantitativo di pioggia previsto.","title":"12 - Introduzione al machine learning"},{"location":"material/03_ml_sklearn/12_intro_ml/lecture/#121-tipi-di-sistemi-di-machine-learning","text":"I sistemi di machine learning ricadono in tre diverse categorie, distinte sulla base di come \"apprendono\" a fare determinate predizioni.","title":"12.1 - Tipi di sistemi di machine learning"},{"location":"material/03_ml_sklearn/12_intro_ml/lecture/#1211-sistemi-ad-apprendimento-supervisionato","text":"I sistemi ad apprendimento supervisionato ( supervised learning ) effettuano una predizione dopo aver appreso le relazioni intercorrenti tra un numero pi\u00f9 o meno grande di dati ed i corrispondenti valori da predire. Per intenderci, un sistema di questo tipo \u00e8 un po' come uno studente di matematica che, dopo aver appreso i metodi per la risoluzione di un problema di analisi mediante la risoluzione di un gran numero degli stessi, si prepara a sostenere l'esame. Perch\u00e9 supervisionato? L'appellativo supervisionato deriva dal fatto che \u00e8 (di solito) un esperto di dominio a fornire al sistema i dati con i risultati corretti. I pi\u00f9 importanti approcci all'apprendimento supervisionato sono la regressione e la classificazione .","title":"12.1.1 - Sistemi ad apprendimento supervisionato"},{"location":"material/03_ml_sklearn/12_intro_ml/lecture/#12111-modelli-di-regressione","text":"Un modello di regressione predice un valore numerico. Ad esempio, un modello meteorologico di regressione potrebbe predire il quantitativo di pioggia in millimetri, mentre un altro modello di regressione potrebbe valutare l'andamento dei prezzi delle propriet\u00e0 immobiliari sulla base di dati come i metri quadri, la posizione e le caratteristiche della casa, nonch\u00e9 la situazione attuale dei mercati finanziario ed immobiliare.","title":"12.1.1.1 - Modelli di regressione"},{"location":"material/03_ml_sklearn/12_intro_ml/lecture/#12112-modelli-di-classificazione","text":"A differenza dei modelli di regressione, il cui output \u00e8 rappresentato da un numero, i modelli di classificazione restituiscono in uscita un valore che stabilisce la possibilit\u00e0 che un certo campione appartenga ad una data categoria. Ad esempio, un modello di classificazione potrebbe essere usato per predire se un'email \u00e8 un messaggio di spam, o se una foto contiene invece un gatto o un cane. Esistono due macrocategorie di modelli di classificazione, ovvero quelli binari e quelli multiclasse . In particolare, i modelli di classificazione binaria distinguono esclusivamente tra due valori: ad esempio, un modello di classificazione delle email potrebbe indicare se il messaggio \u00e8 di spam o meno. I modelli di classificazione multiclasse invece riescono a distinguere tra pi\u00f9 classi: ad esempio, il nostro modello di riconoscimento delle foto potrebbe riconoscere oggetti di \"classe\" gatto, cane, gallina ed oca.","title":"12.1.1.2 - Modelli di classificazione"},{"location":"material/03_ml_sklearn/12_intro_ml/lecture/#1212-sistemi-ad-apprendimento-non-supervisionato","text":"I sistemi di apprendimento non supervisionato compiono delle predizioni a partire da dati che non contengono alcuna informazione sulla classe di appartenenza o sul valore di regressione. In pratica, i modelli non supervisionati hanno il compito di identificare pattern significativi direttamente nei dati , senza alcun \"indizio\" a priori, ma limitandosi ad inferire automaticamente le proprie regole. Algoritmi comunemente utilizzati in tal senso sono quelli di clustering , nei quali il modello individua come i dati vanno a \"disporsi\" utilizzando delle regole basate su distanze o capacit\u00e0 di \"agglomerarsi\". Il clustering differisce dagli algoritmi supervisionati, ed in particolare dalla classificazione, principalmente perch\u00e9 le categorie non sono definite a priori da un esperto di dominio. Ad esempio, un algoritmo di clustering potrebbe raggruppare i campioni in un dataset meteo sulla base esclusivamente delle temperature, rivelando delle suddivisioni che definiscono le diverse stagioni, oppure ancora gli orari del giorno. Sar\u00e0 poi nostro compito \"provare\" a dare un nome a questi cluster sulla base della nostra interpretazione del dataset.","title":"12.1.2 - Sistemi ad apprendimento non supervisionato"},{"location":"material/03_ml_sklearn/12_intro_ml/lecture/#1213-sistemi-di-reinforcement-learning","text":"I sistemi di reinforcement learning effettuano delle predizioni a partire da ricompense o penalit\u00e0 basate sulle azioni effettuate da un agente all'interno di un ambiente . Sulla base di queste osservazioni, il sistema di reinforcement learning genera una policy che definisce la strategia migliore per raggiungere lo scopo prefissato. Le applicazioni dei sistemi di questo tipo sono varie, e spaziano dall'addestramento dei robot per svolgere task anche complessi, alla creazione di programmi come Alpha Go che sfidino (e battano) gli umani al gioco del Go.","title":"12.1.3 - Sistemi di reinforcement learning"},{"location":"material/03_ml_sklearn/13_framing/lecture/","text":"13 - Definire un problema di machine learning \u00b6 Il primo passo nella risoluzione di un problema di machine learning \u00e8, per l'apponto, definirlo . In pratica, dovremo analizzare il problema, isolando gli elementi essenziali da utilizzare per la sua risoluzione: determineremo la fattibilit\u00e0 del problema, fornendo un insieme chiaro di obiettivi e criteri per la sua risoluzione. 13.1 - Determinare l'obiettivo \u00b6 Partiamo nella definizione del problema determinando il nostro obiettivo, ovvero definendo ci\u00f2 che si vuole ottenere a valle della risoluzione del problema. Ad esempio, potremmo voler calcolare le precipitazioni orarie in una determinata zona, oppure vorremmo definire un modo di individuare automaticamente lo spam in un'applicazione email, o ancora identificare delle transazioni fraudolente in applicazioni di tipo bancario. Questo passo \u00e8 fondamentale per un motivo: infatti, a volte il machine learning \u00e8 visto come uno strumento \"universale\", in grado di risolvere qualsiasi problema a cui viene applicato. In realt\u00e0, questo non \u00e8 vero, ed il machine learning \u00e8 applicabile solo a determinati problemi, i quali alle volte possono essere anche risolti mediante approcci meno complessi. Una volta verificato che il problema pu\u00f2 essere risolto mediante approcci di machine learning, dovremo stabilire quale sia l'esatta natura del task che vogliamo portare avanti. Mantenendoci al caso precedente: Applicazione Obiettivo del problema Output del modello Previsioni meteo Calcolare le precipitazioni orarie in una determinata zona Predizione delle precipitazione orarie Spam detector Individuare lo spam Alert per un possibile spam Prevenzione bancaria Identificare transazioni fraudolente Blocco transazioni sospette 13.2 - Comprendere i dati \u00b6 La disponibilit\u00e0 di dati per l'analisi \u00e8 alla base del machine learning. Per effettuare delle predizioni efficaci, infatti, abbiamo bisogno di usare dati dotati di un certo potere predittivo. In particolare, i dati devono essere: abbondanti : pi\u00f9 esempi rilevanti abbiamo a disposizione, migliore sar\u00e0 il nostro algoritmo risolutivo; consistenti : i dati devono essere raccolti usando criteri e strumenti ben determinati e coerenti. Ad esempio, un algoritmo meteo beneficier\u00e0 di dati raccolti ogni mese per cento anni, piuttosto che di dati raccolti lungo lo stesso arco di tempo ma soltanto nel mese di luglio; affidabili : occorre valutare la sorgente dei nostri dati: siamo in grado di comprenderla e ritenerla affidabile, oppure \u00e8 soltanto parzialmente sotto il nostro controllo? disponibili : dobbiamo assicurarci che i dati siano disponibili e completamente accessibili. Infatti, qualora ci siano delle parti del dataset parzialmente omesse, potrebbe essere preferibile trascurarle completamente in fase di analisi; corretti : molto spesso vi \u00e8 una percentuale di dati con feature o label non corrette. Per quanto possibile, questi dati andrebbero isolati e rimossi in fase di preprocessing; rappresentativi : il dataset dovrebbe rappresentare in maniera completa il fenomeno sottostante, riflettendone accuratamente aspetti e caratteristiche. Utilizzare un dataset non rappresentativo inficier\u00e0 negativamente le performance predittive del modello. 13.3 - Scegliere il modello \u00b6 L'ultimo step \u00e8 la scelta del tipo di modello da utilizzare, valutando ad esempio tra classificazione, regressione e clustering. Per la nostra applicazione meteo, ad esempio, predire il quantitativo di pioggia che cadr\u00e0 in un determinato luogo \u00e8 un chiaro problema di regressione, nel senso che date \\(n\\) variabili indipendenti cercheremo di predire una variabile dipendente in uscita. Regressione univariata e multivariata In questo caso, la regressione si dice univariata a causa del fatto che si sta predicendo un'unica variabile dipendente. Se provassimo a predire (ad esempio) anche la temperatura, avremmo a che fare con una regressione multivariata . Nel caso dell'applicazione mail, dato che stiamo cercando di valutare se un messaggio \u00e8 classificabile o meno come spam, avremo a che fare con un problema di classificazione binaria. Una volta determinato il tipo di problema, dovremo scegliere l'algoritmo da utilizzare e, in ultimo, la metrica con cui valutare i risultati. In particolare, quest'ultimo valore dipende molto dall'ambito applicativo: se, ad esempio, un errore del 10% potrebbe non essere estremamente importante nell'applicazione mail, questo diventerebbe estremamente rilevante e potenzialmente disastroso nell'individuazione di transazioni fraudolente.","title":"Dispense"},{"location":"material/03_ml_sklearn/13_framing/lecture/#13-definire-un-problema-di-machine-learning","text":"Il primo passo nella risoluzione di un problema di machine learning \u00e8, per l'apponto, definirlo . In pratica, dovremo analizzare il problema, isolando gli elementi essenziali da utilizzare per la sua risoluzione: determineremo la fattibilit\u00e0 del problema, fornendo un insieme chiaro di obiettivi e criteri per la sua risoluzione.","title":"13 - Definire un problema di machine learning"},{"location":"material/03_ml_sklearn/13_framing/lecture/#131-determinare-lobiettivo","text":"Partiamo nella definizione del problema determinando il nostro obiettivo, ovvero definendo ci\u00f2 che si vuole ottenere a valle della risoluzione del problema. Ad esempio, potremmo voler calcolare le precipitazioni orarie in una determinata zona, oppure vorremmo definire un modo di individuare automaticamente lo spam in un'applicazione email, o ancora identificare delle transazioni fraudolente in applicazioni di tipo bancario. Questo passo \u00e8 fondamentale per un motivo: infatti, a volte il machine learning \u00e8 visto come uno strumento \"universale\", in grado di risolvere qualsiasi problema a cui viene applicato. In realt\u00e0, questo non \u00e8 vero, ed il machine learning \u00e8 applicabile solo a determinati problemi, i quali alle volte possono essere anche risolti mediante approcci meno complessi. Una volta verificato che il problema pu\u00f2 essere risolto mediante approcci di machine learning, dovremo stabilire quale sia l'esatta natura del task che vogliamo portare avanti. Mantenendoci al caso precedente: Applicazione Obiettivo del problema Output del modello Previsioni meteo Calcolare le precipitazioni orarie in una determinata zona Predizione delle precipitazione orarie Spam detector Individuare lo spam Alert per un possibile spam Prevenzione bancaria Identificare transazioni fraudolente Blocco transazioni sospette","title":"13.1 - Determinare l'obiettivo"},{"location":"material/03_ml_sklearn/13_framing/lecture/#132-comprendere-i-dati","text":"La disponibilit\u00e0 di dati per l'analisi \u00e8 alla base del machine learning. Per effettuare delle predizioni efficaci, infatti, abbiamo bisogno di usare dati dotati di un certo potere predittivo. In particolare, i dati devono essere: abbondanti : pi\u00f9 esempi rilevanti abbiamo a disposizione, migliore sar\u00e0 il nostro algoritmo risolutivo; consistenti : i dati devono essere raccolti usando criteri e strumenti ben determinati e coerenti. Ad esempio, un algoritmo meteo beneficier\u00e0 di dati raccolti ogni mese per cento anni, piuttosto che di dati raccolti lungo lo stesso arco di tempo ma soltanto nel mese di luglio; affidabili : occorre valutare la sorgente dei nostri dati: siamo in grado di comprenderla e ritenerla affidabile, oppure \u00e8 soltanto parzialmente sotto il nostro controllo? disponibili : dobbiamo assicurarci che i dati siano disponibili e completamente accessibili. Infatti, qualora ci siano delle parti del dataset parzialmente omesse, potrebbe essere preferibile trascurarle completamente in fase di analisi; corretti : molto spesso vi \u00e8 una percentuale di dati con feature o label non corrette. Per quanto possibile, questi dati andrebbero isolati e rimossi in fase di preprocessing; rappresentativi : il dataset dovrebbe rappresentare in maniera completa il fenomeno sottostante, riflettendone accuratamente aspetti e caratteristiche. Utilizzare un dataset non rappresentativo inficier\u00e0 negativamente le performance predittive del modello.","title":"13.2 - Comprendere i dati"},{"location":"material/03_ml_sklearn/13_framing/lecture/#133-scegliere-il-modello","text":"L'ultimo step \u00e8 la scelta del tipo di modello da utilizzare, valutando ad esempio tra classificazione, regressione e clustering. Per la nostra applicazione meteo, ad esempio, predire il quantitativo di pioggia che cadr\u00e0 in un determinato luogo \u00e8 un chiaro problema di regressione, nel senso che date \\(n\\) variabili indipendenti cercheremo di predire una variabile dipendente in uscita. Regressione univariata e multivariata In questo caso, la regressione si dice univariata a causa del fatto che si sta predicendo un'unica variabile dipendente. Se provassimo a predire (ad esempio) anche la temperatura, avremmo a che fare con una regressione multivariata . Nel caso dell'applicazione mail, dato che stiamo cercando di valutare se un messaggio \u00e8 classificabile o meno come spam, avremo a che fare con un problema di classificazione binaria. Una volta determinato il tipo di problema, dovremo scegliere l'algoritmo da utilizzare e, in ultimo, la metrica con cui valutare i risultati. In particolare, quest'ultimo valore dipende molto dall'ambito applicativo: se, ad esempio, un errore del 10% potrebbe non essere estremamente importante nell'applicazione mail, questo diventerebbe estremamente rilevante e potenzialmente disastroso nell'individuazione di transazioni fraudolente.","title":"13.3 - Scegliere il modello"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/","text":"14 - Preparazione dei dati \u00b6 Nella lezione precedente abbiamo evidenziato come uno dei passi fondamentali per il machine learning sia quello di determinare i dati sui quali il modello deve essere addestrato: per ottenere buone predizioni, dovremo costruire un dataset , ed eventualmente effettuare delle opportune trasformazioni sui dati. Queste operazioni sono normalmente riassumibili nei concetti di campionamento e preparazione dei dati . 14.1 - Il campionamento \u00b6 Il primo problema da affrontare \u00e8 la raccolta dei dati, che servir\u00e0 ovviamente a generare il nostro dataset. In questa fase, dovremo partire affrontando due aspetti: la dimensione e la qualit\u00e0 dei dati che abbiamo raccolto. 14.1.1 - Dimensione del dataset \u00b6 Non vi \u00e8 una regola vera e propria per determinare il quantitativo di dati sufficiente per addestrare adeguatamente un modello. In generale, tuttavia, potremmo dire che il modello deve essere addestrato su un quantitativo di dati che sia maggiore di almeno un ordine di grandezza rispetto al numero dei parametri dello stesso. A scopo puramente esemplificativo, usando una rete neurale con \\(100\\) neuroni dovremmo indicativamente avere almeno \\(1000\\) campioni a disposizione. 14.1.2 - Qualit\u00e0 del dataset \u00b6 Parafrasando un vecchio adagio, la quantit\u00e0 \u00e8 niente senza qualit\u00e0 . Nell'ambito della data science, ci\u00f2 implica che avere a disposizione grandi quantit\u00e0 di dati non basta se questi non sono anche significativi nella caratterizzazione del fenomeno sotto osservazione. Per comprendere questo concetto, possiamo usare un approccio empirico. Immaginiamo di voler creare il nostro solito modello di predizione delle precipitazioni, e di dover scegliere per addestrarlo tra due dataset. Il dataset A contiene i campionamenti ogni \\(15\\) minuti dei valori di temperatura e di pressione degli ultimi \\(100\\) anni, ma soltanto per il mese di luglio, mentre il dataset B contiene un unico valore giornaliero, ma preso per tutti i mesi dell'anno. E' facile calcolare che il numero di campioni del dataset A \u00e8 pari a \\(4 \\cdot 24 \\cdot 31 \\cdot 100 = 297.600\\) valori, mentre quello per il dataset B \u00e8 pari a \\(365 \\cdot 100 = 36.500\\) . Tuttavia, la qualit\u00e0 del dataset B \u00e8 migliore rispetto a quella del dataset A: infatti, nonostante quest'ultimo abbia quasi il decuplo dei dati, sar\u00e0 praticamente inutile per la stima delle precipitazioni in inverno, primavera o autunno. 14.2 - Preparazione dei dati \u00b6 Una volta completata la procedura di campionamento, dovremo passare ad effettuare la preparazione dei dati. Il primo step \u00e8 in molti casi trascurato, ma \u00e8 di vitale importanza nel caso si stiano utilizzando delle informazioni in qualche modo sensibili, come ad esempio informazioni legate alle condizioni sanitarie di diversi pazienti. In questi casi, infatti, \u00e8 strettamente necessario provvedere all' anonimizzazione dei dati, rimuovendo tutte le informazioni definite come personally identifiable ( PII ). Una volta completato questo passaggio, potremo passare alle azioni maggiormente rilevanti dal punto di vista scientifico. 14.2.1 - Pulizia dei dati \u00b6 Abbiamo in precedenza sottolineato come l'affidabilit\u00e0 del dataset sia essenziale a garantire performance ottimali del modello addestrato. In tal senso, occorre determinare diversi fattori, tra cui: errori nel labelling : bisogna valutare a grandi linee se il lavoro svolto dall'essere umano nell'etichettatura \u00e8 accettabile, o se questa procedura \u00e8 stata soggetta ad errori di natura grossolana; rumorosit\u00e0 del dataset : \u00e8 importante valutare se i dati sono affetti da rumore. Ad esempio, le letture di un sensore potrebbero essere tutte quante affette da offset o bias o, nel caso peggiore, essere causate da lettori non pi\u00f9 tarati e quindi inutilizzabili; dati mancanti : potrebbe darsi che i valori di alcune feature non siano disponibili per alcuni campioni; valori contrastanti o duplicati : ad esempio, ci potrebbero essere parti di dataset in cui una lettura di temperatura avviene in gradi Kelvin, ed altre in cui la lettura avviene in gradi centigradi, oppure ancora ci potrebbero essere valori duplicati a causa di errori nell'I/O del sensore. In tutti questi casi, va scelta una strategia di pulizia: in certe situazioni potrebbe essere sufficiente eliminare un campione, oppure effettuare un'operazione di filling a partire dalla restante parte del dataset, o ancora, in casi estremi, si potrebbe eliminare completamente la feature interessata da rumore. 14.2.2 - Sbilanciamento del dataset \u00b6 E' possibile che un dataset abbia diverse proporzioni nei raggruppamenti dei dati. Anche se questo fenomeno pu\u00f2 interessare ogni insieme di dati, \u00e8 maggiormente evidente nei problemi di classificazione, nei quali abbiamo un feedback immediato sulle differenti proporzioni grazie proprio alla presenza delle label per le classi. In particolare, avremo due tipi di \"suddivisioni\": le classi maggioritarie saranno quelle con il maggior numero di campioni, mentre quelle minoritarie saranno prevedibilmente quelle con a disposizione un numero limitato di dati. Un dataset in cui sussiste questa ineguaglianza \u00e8 detto sbilanciato . E' possibile quantificare approssimativamente lo sbilanciamento del dataset. In tal senso, possiamo rifarci alla seguente tabella:\u00ec Grado di sblianciamento \\(\\%\\) di campioni di classi minoritarie Leggero 20-40 \\(\\%\\) del datset moderato 1-20 \\(\\%\\) del dataset Estremo < 1 \\(\\%\\) del dataset 14.2.2.1 - Influenza dello sbilanciamento \u00b6 Per capire qual \u00e8 il problema legato allo sbilanciamento del dataset, immaginiamo di dover creare un modello che individui una mail di spam. Per farlo, usiamo un dataset con la seguente proporzione: Mail non spam Mail spam Numero di immagini \\(5\\) \\(995\\) Percentuale \\(0.5 \\%\\) \\(99.5 \\%\\) Il problema sta nel fatto che un numero cos\u00ec esiguo di mail di spam far\u00e0 s\u00ec che il modello spenda la maggior parte dell'addestramento su mail normali, non imparando quindi a riconoscere i casi di spam. Per fare un parallelismo con il nostro cervello, se vedessimo 995 immagini di penne, e solo 5 di matite, \u00e8 probabile che non saremmo in grado di distinguere una matita da una penna perch\u00e9, semplicemente, non sapremmo come \u00e8 fatta una matita . 14.2.2.2 - Downsampling ed upweighting \u00b6 Un modo efficace per gestire situazioni in cui il dataset \u00e8 sbilanciato \u00e8 quello di utilizzare tecniche di data balancing . Ne esistono di diverse, pi\u00f9 o meno efficaci; tuttavia, la pi\u00f9 semplice \u00e8 quella di rimuovere un certo numero di campioni di classe maggioritaria ( downsampling ), dando agli esempi sottocampionati un peso maggiore nell'addestramento ( upweighting ). In pratica, se scegliessimo di mantenere soltanto il \\(10 \\%\\) delle mail non-spam, avremmo circa \\(99\\) campioni. Ci\u00f2 porter\u00e0 il rapporto tra le mail di spam e quelle non di spam a circa il \\(5 \\%\\) , passando da una situazione di sbilanciamento estremo ad una di sbilanciamento moderato. A valle di questa operazione, dovremmo dare maggior peso ai campioni delle mail non-spam, usando un fattore tendenzialmente pari a quello che abbiamo usato in fase di downsampling. Nella pratica, ogni mail non-spam avr\u00e0 un peso dieci volte superiore a quello che avrebbe avuto se si fosse utilizzato il dataset iniziale. 14.2.3 - Trasformazione dei dati \u00b6 Il passo successivo nella preparazione dei dati \u00e8 quello di trasformare alcuni valori. In tal senso, possiamo operare per due ragioni principali. La prima \u00e8 che siano necessarie delle trasformazioni obbligatorie volte a garantire la compatibilit\u00e0 dei dati, come ad esempio: convertire feature non numeriche in numeriche : in pratica, non possiamo effettuare operazioni sensate tra interi e stringhe, per cui dovremmo trovarci ad individuare un modo per favorire il confronto; ridimensionare gli input ad una dimensione fissa : alcuni modelli, come ad esempio le reti neurali, prevedono un numero fisso di nodi di input, per cui i dati in ingresso devono avere sempre la stessa dimensione. La seconda \u00e8 legata invece a delle trasformazioni opzionali, che ottimizzano l'addestramento del modello. Ad esempio, potremmo dover effettuare la normalizzazione dei dati numerici, ovvero portarli tutti all'interno di una stessa scala di valori, normalmente compresa tra \\(0\\) ed \\(1\\) o tra \\(-1\\) ed \\(1\\) . Vediamo pi\u00f9 nel dettaglio alcune possibilit\u00e0. 14.2.3.1 - Trasformazione dei dati numerici \u00b6 Abbiamo detto in precedenza che potremmo voler applicare delle normalizzazioni a dei dati numerici per migliorare le performance del nostro modello. Per comprenderne il motivo, immaginiamo di avere un dataset che comprende feature per et\u00e0 (che possiamo presupporre assuma valori da \\(0\\) a \\(100\\) ) e stipendio (che possiamo presupporre assuma valori da \\(10.000\\) a \\(100.000\\) \u20ac). Quando andiamo ad utilizzare questi valori in algoritmi che effettuano delle operazioni tra feature, l'et\u00e0 diventer\u00e0 presto trascurabile rispetto allo stipendio, che \u00e8 di due o tre ordini di grandezza superiore, per cui il modello si trover\u00e0 a prediligere quest'ultimo in fase di analisi. Ci\u00f2 implica quindi la necessit\u00e0 di arrivare ad una \"base comune\" a partire dalla quale operare. Le principali tecniche di normalizzazione disponibili sono quattro. 14.2.3.1.1 - Scaling \u00b6 Lo scaling prevede la conversione dei valori assunti da una feature in un range che va di solito tra \\([0, 1]\\) o \\([-1, 1]\\) . La formula dello scaling \u00e8 la seguente: \\[ y = \\frac{(x - x_{min})}{(x_{max} - x_{min})} \\] 14.2.3.1.2 - Clipping \u00b6 Pu\u00f2 capitare che il dataset contenga degli outlier , ovvero dei campioni che divergono notevolmente dalle caratteristiche statistiche del dataset. In questo caso, potremmo limitarci a rimuovere completamente tali valori mediante soglie statistiche, come i range interquartili in caso di distribuzione parametrica, o i classici \\(3 \\sigma\\) in caso di distribuzione normale. 14.2.3.1.3 - Trasformazione logaritmica \u00b6 Un'altra possibilit\u00e0 \u00e8 quella di convertire i nostri valori in scala logaritmica, comprimendo un range ampio in uno pi\u00f9 piccolo usando la funzione logaritmo: \\[ y = Log(x) \\] 14.2.3.1.4 - Z-score \u00b6 Un ultimo tipo di trasformazione prevede l'uso dello z-score , che prevede una riformulazione dei valori assunti dalla feature per fare in modo che questi aderiscano ad una distribuzione a media nulla e deviazione tandard unitaria. Per calcolarlo, si usa la seguente formula: \\[ y = \\frac{x - \\mu}{\\sigma} \\] dove \\(\\mu\\) \u00e8 la media della distribuzione dei nostri dati, mentre \\(\\sigma\\) ne \u00e8 chiaramente la varianza. 14.2.3.2 - Trasformazione dei dati categorici \u00b6 Alcune delle nostre feature possono assumere esclusivamente valori discreti . Ad esempio, le nostre immagini potrebbero raffigurare diverse razze di cani, oppure il campo \"localit\u00e0\" potrebbe riportare il codice postale. Queste feature sono conosciute come feature categoriche , ed i valori ad esse associate possono essere sia stringhe sia numeri. Le feature categoriche di tipo numerico Spesso, dobbiamo trattare feature categoriche che contengono valori numerici. Per fare un esempio, consideriamo il codice postale, che \u00e8 un numero. Se lo si rappresentasse come una feature di tipo numerico, il nostro modello potrebbe interpretare la distanza tra Bari (70126) e Taranto (74121) come pari a \\(3.995\\) , il che non avrebbe ovviamente alcun senso. Per essere trattate, comunque, le feature categoriche hanno rappresentazioni di tipo numerico, mantenendo il riferimento al significato categorico e discreto . Per comprendere le implicazioni di questo concetto, immaginiamo i giorni della settimana. Il modo pi\u00f9 semplice per passare da una rappresentazione puramente categorica ad una numerica \u00e8 quella di usare un numero: Giorno Rappresentazione Luned\u00ec 1 Marted\u00ec 2 Mercoled\u00ec 3 Gioved\u00ec 4 Venerd\u00ec 5 Sabato 6 Domenica 7 In questa maniera creeremo un \"dizionario\", nel quale potremo accedere ad una chiave (la rappresentazione) che rappresenter\u00e0 un determinato valore (il giorno). Sulle feature categoriche trasformate A valle di questa trasformazione, la differenza aritmetica tra domenica e sabato continua ad avere un senso alquanto limitato, e comunque relativo ad un generico concetto di distanza . Un altro modo di rappresentare le feature categoriche \u00e8 mediante una rappresentazione sparsa , detta anche one-hot encoding , nella quale ogni valore \u00e8 rappresentato da un vettore \\(V\\) di lunghezza \\(m\\) , con \\(m\\) numero di categorie possibili. In questo caso, tutti i valori di \\(V\\) saranno pari a \\(0\\) , tranne quello rappresentativo del valore attualmente assunto dalla feature, che sar\u00e0 pari ad \\(1\\) . Ad esempio, la rappresentazione sparsa del luned\u00ec \u00e8 data da: lunedi = np . array ([ 1 0 0 0 0 0 0 ]) mentre quella del gioved\u00ec: giovedi = np . array ([ 0 0 0 1 0 0 0 ]) 14.2.4 - Suddivisione dei dati \u00b6 L'ultimo passo nella preparazione del dataset \u00e8 quello della suddivisione dei dati. In particolare, si destinano un certo quantitativo di dati per l'addestramento del modello, delegando la restante parte alla validazione dei risultati ottenuti; ci\u00f2 \u00e8 legato alla volont\u00e0 di verificare la capacit\u00e0 di generalizzazione del modello, ovvero a quanto \u00e8 in grado di \"funzionare\" il nostro algoritmo in caso di analisi di dati su cui non \u00e8 stato addestrato. Un rapporto molto usato in tal senso \u00e8 quello che prevede che il \\(70\\%\\) dei dati sia usato per l'addestramento, mentre il restante \\(30\\%\\) per la validazione dei risultati ottenuti.","title":"Dispense"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#14-preparazione-dei-dati","text":"Nella lezione precedente abbiamo evidenziato come uno dei passi fondamentali per il machine learning sia quello di determinare i dati sui quali il modello deve essere addestrato: per ottenere buone predizioni, dovremo costruire un dataset , ed eventualmente effettuare delle opportune trasformazioni sui dati. Queste operazioni sono normalmente riassumibili nei concetti di campionamento e preparazione dei dati .","title":"14 - Preparazione dei dati"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#141-il-campionamento","text":"Il primo problema da affrontare \u00e8 la raccolta dei dati, che servir\u00e0 ovviamente a generare il nostro dataset. In questa fase, dovremo partire affrontando due aspetti: la dimensione e la qualit\u00e0 dei dati che abbiamo raccolto.","title":"14.1 - Il campionamento"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#1411-dimensione-del-dataset","text":"Non vi \u00e8 una regola vera e propria per determinare il quantitativo di dati sufficiente per addestrare adeguatamente un modello. In generale, tuttavia, potremmo dire che il modello deve essere addestrato su un quantitativo di dati che sia maggiore di almeno un ordine di grandezza rispetto al numero dei parametri dello stesso. A scopo puramente esemplificativo, usando una rete neurale con \\(100\\) neuroni dovremmo indicativamente avere almeno \\(1000\\) campioni a disposizione.","title":"14.1.1 - Dimensione del dataset"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#1412-qualita-del-dataset","text":"Parafrasando un vecchio adagio, la quantit\u00e0 \u00e8 niente senza qualit\u00e0 . Nell'ambito della data science, ci\u00f2 implica che avere a disposizione grandi quantit\u00e0 di dati non basta se questi non sono anche significativi nella caratterizzazione del fenomeno sotto osservazione. Per comprendere questo concetto, possiamo usare un approccio empirico. Immaginiamo di voler creare il nostro solito modello di predizione delle precipitazioni, e di dover scegliere per addestrarlo tra due dataset. Il dataset A contiene i campionamenti ogni \\(15\\) minuti dei valori di temperatura e di pressione degli ultimi \\(100\\) anni, ma soltanto per il mese di luglio, mentre il dataset B contiene un unico valore giornaliero, ma preso per tutti i mesi dell'anno. E' facile calcolare che il numero di campioni del dataset A \u00e8 pari a \\(4 \\cdot 24 \\cdot 31 \\cdot 100 = 297.600\\) valori, mentre quello per il dataset B \u00e8 pari a \\(365 \\cdot 100 = 36.500\\) . Tuttavia, la qualit\u00e0 del dataset B \u00e8 migliore rispetto a quella del dataset A: infatti, nonostante quest'ultimo abbia quasi il decuplo dei dati, sar\u00e0 praticamente inutile per la stima delle precipitazioni in inverno, primavera o autunno.","title":"14.1.2 - Qualit\u00e0 del dataset"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#142-preparazione-dei-dati","text":"Una volta completata la procedura di campionamento, dovremo passare ad effettuare la preparazione dei dati. Il primo step \u00e8 in molti casi trascurato, ma \u00e8 di vitale importanza nel caso si stiano utilizzando delle informazioni in qualche modo sensibili, come ad esempio informazioni legate alle condizioni sanitarie di diversi pazienti. In questi casi, infatti, \u00e8 strettamente necessario provvedere all' anonimizzazione dei dati, rimuovendo tutte le informazioni definite come personally identifiable ( PII ). Una volta completato questo passaggio, potremo passare alle azioni maggiormente rilevanti dal punto di vista scientifico.","title":"14.2 - Preparazione dei dati"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#1421-pulizia-dei-dati","text":"Abbiamo in precedenza sottolineato come l'affidabilit\u00e0 del dataset sia essenziale a garantire performance ottimali del modello addestrato. In tal senso, occorre determinare diversi fattori, tra cui: errori nel labelling : bisogna valutare a grandi linee se il lavoro svolto dall'essere umano nell'etichettatura \u00e8 accettabile, o se questa procedura \u00e8 stata soggetta ad errori di natura grossolana; rumorosit\u00e0 del dataset : \u00e8 importante valutare se i dati sono affetti da rumore. Ad esempio, le letture di un sensore potrebbero essere tutte quante affette da offset o bias o, nel caso peggiore, essere causate da lettori non pi\u00f9 tarati e quindi inutilizzabili; dati mancanti : potrebbe darsi che i valori di alcune feature non siano disponibili per alcuni campioni; valori contrastanti o duplicati : ad esempio, ci potrebbero essere parti di dataset in cui una lettura di temperatura avviene in gradi Kelvin, ed altre in cui la lettura avviene in gradi centigradi, oppure ancora ci potrebbero essere valori duplicati a causa di errori nell'I/O del sensore. In tutti questi casi, va scelta una strategia di pulizia: in certe situazioni potrebbe essere sufficiente eliminare un campione, oppure effettuare un'operazione di filling a partire dalla restante parte del dataset, o ancora, in casi estremi, si potrebbe eliminare completamente la feature interessata da rumore.","title":"14.2.1 - Pulizia dei dati"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#1422-sbilanciamento-del-dataset","text":"E' possibile che un dataset abbia diverse proporzioni nei raggruppamenti dei dati. Anche se questo fenomeno pu\u00f2 interessare ogni insieme di dati, \u00e8 maggiormente evidente nei problemi di classificazione, nei quali abbiamo un feedback immediato sulle differenti proporzioni grazie proprio alla presenza delle label per le classi. In particolare, avremo due tipi di \"suddivisioni\": le classi maggioritarie saranno quelle con il maggior numero di campioni, mentre quelle minoritarie saranno prevedibilmente quelle con a disposizione un numero limitato di dati. Un dataset in cui sussiste questa ineguaglianza \u00e8 detto sbilanciato . E' possibile quantificare approssimativamente lo sbilanciamento del dataset. In tal senso, possiamo rifarci alla seguente tabella:\u00ec Grado di sblianciamento \\(\\%\\) di campioni di classi minoritarie Leggero 20-40 \\(\\%\\) del datset moderato 1-20 \\(\\%\\) del dataset Estremo < 1 \\(\\%\\) del dataset","title":"14.2.2 - Sbilanciamento del dataset"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#14221-influenza-dello-sbilanciamento","text":"Per capire qual \u00e8 il problema legato allo sbilanciamento del dataset, immaginiamo di dover creare un modello che individui una mail di spam. Per farlo, usiamo un dataset con la seguente proporzione: Mail non spam Mail spam Numero di immagini \\(5\\) \\(995\\) Percentuale \\(0.5 \\%\\) \\(99.5 \\%\\) Il problema sta nel fatto che un numero cos\u00ec esiguo di mail di spam far\u00e0 s\u00ec che il modello spenda la maggior parte dell'addestramento su mail normali, non imparando quindi a riconoscere i casi di spam. Per fare un parallelismo con il nostro cervello, se vedessimo 995 immagini di penne, e solo 5 di matite, \u00e8 probabile che non saremmo in grado di distinguere una matita da una penna perch\u00e9, semplicemente, non sapremmo come \u00e8 fatta una matita .","title":"14.2.2.1 - Influenza dello sbilanciamento"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#14222-downsampling-ed-upweighting","text":"Un modo efficace per gestire situazioni in cui il dataset \u00e8 sbilanciato \u00e8 quello di utilizzare tecniche di data balancing . Ne esistono di diverse, pi\u00f9 o meno efficaci; tuttavia, la pi\u00f9 semplice \u00e8 quella di rimuovere un certo numero di campioni di classe maggioritaria ( downsampling ), dando agli esempi sottocampionati un peso maggiore nell'addestramento ( upweighting ). In pratica, se scegliessimo di mantenere soltanto il \\(10 \\%\\) delle mail non-spam, avremmo circa \\(99\\) campioni. Ci\u00f2 porter\u00e0 il rapporto tra le mail di spam e quelle non di spam a circa il \\(5 \\%\\) , passando da una situazione di sbilanciamento estremo ad una di sbilanciamento moderato. A valle di questa operazione, dovremmo dare maggior peso ai campioni delle mail non-spam, usando un fattore tendenzialmente pari a quello che abbiamo usato in fase di downsampling. Nella pratica, ogni mail non-spam avr\u00e0 un peso dieci volte superiore a quello che avrebbe avuto se si fosse utilizzato il dataset iniziale.","title":"14.2.2.2 - Downsampling ed upweighting"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#1423-trasformazione-dei-dati","text":"Il passo successivo nella preparazione dei dati \u00e8 quello di trasformare alcuni valori. In tal senso, possiamo operare per due ragioni principali. La prima \u00e8 che siano necessarie delle trasformazioni obbligatorie volte a garantire la compatibilit\u00e0 dei dati, come ad esempio: convertire feature non numeriche in numeriche : in pratica, non possiamo effettuare operazioni sensate tra interi e stringhe, per cui dovremmo trovarci ad individuare un modo per favorire il confronto; ridimensionare gli input ad una dimensione fissa : alcuni modelli, come ad esempio le reti neurali, prevedono un numero fisso di nodi di input, per cui i dati in ingresso devono avere sempre la stessa dimensione. La seconda \u00e8 legata invece a delle trasformazioni opzionali, che ottimizzano l'addestramento del modello. Ad esempio, potremmo dover effettuare la normalizzazione dei dati numerici, ovvero portarli tutti all'interno di una stessa scala di valori, normalmente compresa tra \\(0\\) ed \\(1\\) o tra \\(-1\\) ed \\(1\\) . Vediamo pi\u00f9 nel dettaglio alcune possibilit\u00e0.","title":"14.2.3 - Trasformazione dei dati"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#14231-trasformazione-dei-dati-numerici","text":"Abbiamo detto in precedenza che potremmo voler applicare delle normalizzazioni a dei dati numerici per migliorare le performance del nostro modello. Per comprenderne il motivo, immaginiamo di avere un dataset che comprende feature per et\u00e0 (che possiamo presupporre assuma valori da \\(0\\) a \\(100\\) ) e stipendio (che possiamo presupporre assuma valori da \\(10.000\\) a \\(100.000\\) \u20ac). Quando andiamo ad utilizzare questi valori in algoritmi che effettuano delle operazioni tra feature, l'et\u00e0 diventer\u00e0 presto trascurabile rispetto allo stipendio, che \u00e8 di due o tre ordini di grandezza superiore, per cui il modello si trover\u00e0 a prediligere quest'ultimo in fase di analisi. Ci\u00f2 implica quindi la necessit\u00e0 di arrivare ad una \"base comune\" a partire dalla quale operare. Le principali tecniche di normalizzazione disponibili sono quattro.","title":"14.2.3.1 - Trasformazione dei dati numerici"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#142311-scaling","text":"Lo scaling prevede la conversione dei valori assunti da una feature in un range che va di solito tra \\([0, 1]\\) o \\([-1, 1]\\) . La formula dello scaling \u00e8 la seguente: \\[ y = \\frac{(x - x_{min})}{(x_{max} - x_{min})} \\]","title":"14.2.3.1.1 - Scaling"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#142312-clipping","text":"Pu\u00f2 capitare che il dataset contenga degli outlier , ovvero dei campioni che divergono notevolmente dalle caratteristiche statistiche del dataset. In questo caso, potremmo limitarci a rimuovere completamente tali valori mediante soglie statistiche, come i range interquartili in caso di distribuzione parametrica, o i classici \\(3 \\sigma\\) in caso di distribuzione normale.","title":"14.2.3.1.2 - Clipping"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#142313-trasformazione-logaritmica","text":"Un'altra possibilit\u00e0 \u00e8 quella di convertire i nostri valori in scala logaritmica, comprimendo un range ampio in uno pi\u00f9 piccolo usando la funzione logaritmo: \\[ y = Log(x) \\]","title":"14.2.3.1.3 - Trasformazione logaritmica"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#142314-z-score","text":"Un ultimo tipo di trasformazione prevede l'uso dello z-score , che prevede una riformulazione dei valori assunti dalla feature per fare in modo che questi aderiscano ad una distribuzione a media nulla e deviazione tandard unitaria. Per calcolarlo, si usa la seguente formula: \\[ y = \\frac{x - \\mu}{\\sigma} \\] dove \\(\\mu\\) \u00e8 la media della distribuzione dei nostri dati, mentre \\(\\sigma\\) ne \u00e8 chiaramente la varianza.","title":"14.2.3.1.4 - Z-score"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#14232-trasformazione-dei-dati-categorici","text":"Alcune delle nostre feature possono assumere esclusivamente valori discreti . Ad esempio, le nostre immagini potrebbero raffigurare diverse razze di cani, oppure il campo \"localit\u00e0\" potrebbe riportare il codice postale. Queste feature sono conosciute come feature categoriche , ed i valori ad esse associate possono essere sia stringhe sia numeri. Le feature categoriche di tipo numerico Spesso, dobbiamo trattare feature categoriche che contengono valori numerici. Per fare un esempio, consideriamo il codice postale, che \u00e8 un numero. Se lo si rappresentasse come una feature di tipo numerico, il nostro modello potrebbe interpretare la distanza tra Bari (70126) e Taranto (74121) come pari a \\(3.995\\) , il che non avrebbe ovviamente alcun senso. Per essere trattate, comunque, le feature categoriche hanno rappresentazioni di tipo numerico, mantenendo il riferimento al significato categorico e discreto . Per comprendere le implicazioni di questo concetto, immaginiamo i giorni della settimana. Il modo pi\u00f9 semplice per passare da una rappresentazione puramente categorica ad una numerica \u00e8 quella di usare un numero: Giorno Rappresentazione Luned\u00ec 1 Marted\u00ec 2 Mercoled\u00ec 3 Gioved\u00ec 4 Venerd\u00ec 5 Sabato 6 Domenica 7 In questa maniera creeremo un \"dizionario\", nel quale potremo accedere ad una chiave (la rappresentazione) che rappresenter\u00e0 un determinato valore (il giorno). Sulle feature categoriche trasformate A valle di questa trasformazione, la differenza aritmetica tra domenica e sabato continua ad avere un senso alquanto limitato, e comunque relativo ad un generico concetto di distanza . Un altro modo di rappresentare le feature categoriche \u00e8 mediante una rappresentazione sparsa , detta anche one-hot encoding , nella quale ogni valore \u00e8 rappresentato da un vettore \\(V\\) di lunghezza \\(m\\) , con \\(m\\) numero di categorie possibili. In questo caso, tutti i valori di \\(V\\) saranno pari a \\(0\\) , tranne quello rappresentativo del valore attualmente assunto dalla feature, che sar\u00e0 pari ad \\(1\\) . Ad esempio, la rappresentazione sparsa del luned\u00ec \u00e8 data da: lunedi = np . array ([ 1 0 0 0 0 0 0 ]) mentre quella del gioved\u00ec: giovedi = np . array ([ 0 0 0 1 0 0 0 ])","title":"14.2.3.2 - Trasformazione dei dati categorici"},{"location":"material/03_ml_sklearn/14_data_prep/lecture/#1424-suddivisione-dei-dati","text":"L'ultimo passo nella preparazione del dataset \u00e8 quello della suddivisione dei dati. In particolare, si destinano un certo quantitativo di dati per l'addestramento del modello, delegando la restante parte alla validazione dei risultati ottenuti; ci\u00f2 \u00e8 legato alla volont\u00e0 di verificare la capacit\u00e0 di generalizzazione del modello, ovvero a quanto \u00e8 in grado di \"funzionare\" il nostro algoritmo in caso di analisi di dati su cui non \u00e8 stato addestrato. Un rapporto molto usato in tal senso \u00e8 quello che prevede che il \\(70\\%\\) dei dati sia usato per l'addestramento, mentre il restante \\(30\\%\\) per la validazione dei risultati ottenuti.","title":"14.2.4 - Suddivisione dei dati"},{"location":"material/03_ml_sklearn/15_intro_sklearn/exercises/","text":"E15 - Introduzione a Scikit Learn \u00b6 Esercizio E15.1 \u00b6 Analizziamo i dati del dataset Titanic, e prepariamoli per un'elaborazione successiva. In particolare, proviamo a: rimuovere le feature rumorose o non necessarie; normalizzare le feature numeriche nell'intervallo \\([0, 1]\\) ; convertire le feature categoriche in valori numerici. Facciamolo utilizzando Scikit Learn.","title":"Esercizi"},{"location":"material/03_ml_sklearn/15_intro_sklearn/exercises/#e15-introduzione-a-scikit-learn","text":"","title":"E15 - Introduzione a Scikit Learn"},{"location":"material/03_ml_sklearn/15_intro_sklearn/exercises/#esercizio-e151","text":"Analizziamo i dati del dataset Titanic, e prepariamoli per un'elaborazione successiva. In particolare, proviamo a: rimuovere le feature rumorose o non necessarie; normalizzare le feature numeriche nell'intervallo \\([0, 1]\\) ; convertire le feature categoriche in valori numerici. Facciamolo utilizzando Scikit Learn.","title":"Esercizio E15.1"},{"location":"material/03_ml_sklearn/15_intro_sklearn/lecture/","text":"15 - Introduzione (breve) a Scikit Learn \u00b6 Scikit Learn \u00e8 una tra le librerie per il machine learning pi\u00f9 utilizzate in Python. Ci\u00f2 avviene principalmente per tre fattori: il supporto ad un numero molto elevato di algoritmi di machine learning; la semplicit\u00e0 di utilizzo della libreria; la perfetta integrazione con NumPy e Pandas. Partiamo quindi nella nostra discussione sulla libreria da una panoramica ad ampio spettro delle potenzialit\u00e0 della stessa. Come di consueto, per\u00f2, la prima cosa da fare \u00e8 installare la libreria. Per farlo, spostiamoci (eventualmente) nell'ambiente virtuale usato per il corso, ed usiamo il seguente comando: pip install scikit-learn 15.1 - Stimatori e transformer \u00b6 Scikit Learn si basa su due concetti fondamentali, ovvero quelli di estimator (stimatore) e di transformer (traducibile maccaronicamente come trasformatore di dati ). In particolare, un estimator \u00e8 l'implementazione di uno specifico algoritmo di machine learning, mentre un transformer \u00e8 un algoritmo che effettua delle trasformazioni sui dati. Ad esempio, le istanze delle classi RandomForestClassifier e DBSCAN sono degli estimator, mentre quelle della classe StandardScaler sono dei transformer. Questa suddivisione permette di implementare un'interfaccia comune, la quale offre nella maggior parte dei casi i metodi fit e transform per, rispettivamente, effettuare l'addestramento e la trasformazione dei dati. Tuttavia, \u00e8 importante notare come ogni stimatore e transformer abbiano parametri specifici e dipendenti dalla natura dell'algoritmo utilizzato; ogni algoritmo, inoltre, andr\u00e0 verificato secondo delle opportune metriche , che permettono di definire, in termini percentuali o assoluti, l'accuratezza dell'algoritmo utilizzato. 15.2 - Preprocessing \u00b6 Abbiamo visto come spesso sia necessario effettuare delle operazioni di preprocessing sui dati. In tal senso, gli strumenti che utilizzeremo maggiormente saranno tre: la funzione train_test_split , utile a suddividere il dataset in un insieme di training ed uno di test; gli imputer come SimpleImputer() transformer che ci permettono di assegnare eventuali valori mancanti all'interno del dataset; i transformer veri e propri per le operazioni di categorizzazione e normalizzazione.","title":"Dispense"},{"location":"material/03_ml_sklearn/15_intro_sklearn/lecture/#15-introduzione-breve-a-scikit-learn","text":"Scikit Learn \u00e8 una tra le librerie per il machine learning pi\u00f9 utilizzate in Python. Ci\u00f2 avviene principalmente per tre fattori: il supporto ad un numero molto elevato di algoritmi di machine learning; la semplicit\u00e0 di utilizzo della libreria; la perfetta integrazione con NumPy e Pandas. Partiamo quindi nella nostra discussione sulla libreria da una panoramica ad ampio spettro delle potenzialit\u00e0 della stessa. Come di consueto, per\u00f2, la prima cosa da fare \u00e8 installare la libreria. Per farlo, spostiamoci (eventualmente) nell'ambiente virtuale usato per il corso, ed usiamo il seguente comando: pip install scikit-learn","title":"15 - Introduzione (breve) a Scikit Learn"},{"location":"material/03_ml_sklearn/15_intro_sklearn/lecture/#151-stimatori-e-transformer","text":"Scikit Learn si basa su due concetti fondamentali, ovvero quelli di estimator (stimatore) e di transformer (traducibile maccaronicamente come trasformatore di dati ). In particolare, un estimator \u00e8 l'implementazione di uno specifico algoritmo di machine learning, mentre un transformer \u00e8 un algoritmo che effettua delle trasformazioni sui dati. Ad esempio, le istanze delle classi RandomForestClassifier e DBSCAN sono degli estimator, mentre quelle della classe StandardScaler sono dei transformer. Questa suddivisione permette di implementare un'interfaccia comune, la quale offre nella maggior parte dei casi i metodi fit e transform per, rispettivamente, effettuare l'addestramento e la trasformazione dei dati. Tuttavia, \u00e8 importante notare come ogni stimatore e transformer abbiano parametri specifici e dipendenti dalla natura dell'algoritmo utilizzato; ogni algoritmo, inoltre, andr\u00e0 verificato secondo delle opportune metriche , che permettono di definire, in termini percentuali o assoluti, l'accuratezza dell'algoritmo utilizzato.","title":"15.1 - Stimatori e transformer"},{"location":"material/03_ml_sklearn/15_intro_sklearn/lecture/#152-preprocessing","text":"Abbiamo visto come spesso sia necessario effettuare delle operazioni di preprocessing sui dati. In tal senso, gli strumenti che utilizzeremo maggiormente saranno tre: la funzione train_test_split , utile a suddividere il dataset in un insieme di training ed uno di test; gli imputer come SimpleImputer() transformer che ci permettono di assegnare eventuali valori mancanti all'interno del dataset; i transformer veri e propri per le operazioni di categorizzazione e normalizzazione.","title":"15.2 - Preprocessing"},{"location":"material/03_ml_sklearn/16_lin_reg/exercises/","text":"E16 - Apprendimento supervisionato e regressione lineare \u00b6 Esercizio E16.1 \u00b6 Proviamo ad operare sul dataset Tips di Seaborn, effettuando una regressione lineare che riguardi le mance ed il conto totale. Per farlo, usiamo un oggetto di classe LinearRegression() messo a disposizione dal package linear_model di Scikit Learn. Valutiamo lo score \\(R^2\\) ottenuto, e mostriamo a schermo i risultati dell'interpolazione, assieme al coefficiente angolare ed all'intercetta ottenuti. Esercizio E16.2 \u00b6 L'algoritmo RANSAC (RANdom SAmple Consensus) permette di effettuare una regressione in quattro step. Per prima cosa, viene scelto un sottoinsieme dei dati iniziali. Viene stimato un modello a partire dal sottoinsieme considerato nel punto 1. Tutti i dati sono classificati come inlier o outlier sulla base di un valore di soglia. Se il modello ha un numero di outlier inferiore a quello estrapolato dal modello all'iterazione precedente, viene aggiornato il \"modello migliore\", e si passa all'iterazione successiva. Proviamo ad effettuare poi un'interpolazione mediante un oggetto di classe RANSACRegression() , e confrontiamo i risultati ottenuti in precedenza in tre modi: tramite un plot; valutando lo score; valutando i valori di coefficiente ed intercetta del modello usato. Proviamo infine ad eseguire due volte il RANSAC, e verifichiamo che i risultati ottenuti siano differenti. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizi"},{"location":"material/03_ml_sklearn/16_lin_reg/exercises/#e16-apprendimento-supervisionato-e-regressione-lineare","text":"","title":"E16 - Apprendimento supervisionato e regressione lineare"},{"location":"material/03_ml_sklearn/16_lin_reg/exercises/#esercizio-e161","text":"Proviamo ad operare sul dataset Tips di Seaborn, effettuando una regressione lineare che riguardi le mance ed il conto totale. Per farlo, usiamo un oggetto di classe LinearRegression() messo a disposizione dal package linear_model di Scikit Learn. Valutiamo lo score \\(R^2\\) ottenuto, e mostriamo a schermo i risultati dell'interpolazione, assieme al coefficiente angolare ed all'intercetta ottenuti.","title":"Esercizio E16.1"},{"location":"material/03_ml_sklearn/16_lin_reg/exercises/#esercizio-e162","text":"L'algoritmo RANSAC (RANdom SAmple Consensus) permette di effettuare una regressione in quattro step. Per prima cosa, viene scelto un sottoinsieme dei dati iniziali. Viene stimato un modello a partire dal sottoinsieme considerato nel punto 1. Tutti i dati sono classificati come inlier o outlier sulla base di un valore di soglia. Se il modello ha un numero di outlier inferiore a quello estrapolato dal modello all'iterazione precedente, viene aggiornato il \"modello migliore\", e si passa all'iterazione successiva. Proviamo ad effettuare poi un'interpolazione mediante un oggetto di classe RANSACRegression() , e confrontiamo i risultati ottenuti in precedenza in tre modi: tramite un plot; valutando lo score; valutando i valori di coefficiente ed intercetta del modello usato. Proviamo infine ad eseguire due volte il RANSAC, e verifichiamo che i risultati ottenuti siano differenti. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizio E16.2"},{"location":"material/03_ml_sklearn/16_lin_reg/lecture/","text":"16 - Apprendimento supervisionato: la regressione lineare \u00b6 Quelli di apprendimento supervisionato sono probabilmente tra i sistemi di machine learning pi\u00f9 diffusi, soprattutto a causa dei numerosi casi d'uso disponibili. Abbiamo detto che esistono fondamentalmente due tipi di tecniche di apprendimento supervisionato, ovvero regressione e classificazione . Vediamole maggiormente nel dettaglio. 16.1 - Un esempio di regressione \u00b6 A tutti noi \u00e8 intuitivamente noto che i millimetri di pioggia che cadono sono in qualche modo correlati alle temperature medie che abbiamo durante la giornata. Immaginiamo quindi di avere un dataset che contenga al suo interno i dati medi sui millimetri di pioggia degli ultimi dieci anni per undici valori differenti di temperatura. Se provassimo a visualizzare i dati mediante un relplot() , otterremmo la seguente figura. Proviamo ad usare la funzione lmplot() che, ricordiamo, effettua una regressione tra i dati. 16.2 - Rappresentazione analitica del modello \u00b6 Notiamo subito che, come prevedibile, i millimetri di pioggia attesi diminuiscono all'aumentare della temperatura, andando a definire una sorta di relazione lineare tra i dati sull'asse delle ascisse (ovvero i gradi) e quelli sull'asse delle ordinate (ovvero la pioggia). Ovviamente, la retta di regressione non tocca direttamente tutti i punti, ma li approssima . Possiamo quindi dire che la relazione tra gradi e mm di pioggia \u00e8 riconducibile ad una forma del tipo: \\[ y = mx + b \\] dove: \\(y\\) sono i millimetri di pioggia medi caduti nell'arco di tutte le giornate con un dato valore medio di temperatura; \\(x\\) \u00e8 il valore medio di temperatura; \\(m\\) \u00e8 il coefficiente angolare della retta di regressione; \\(b\\) \u00e8 l'incercetta della retta di regressione. Questa notazione analitica si traduce in un modello usando la seguente notazione: \\[ y' = b + w_1 x_1 \\] dove: \\(y'\\) \u00e8 l'output predetto dal modello; \\(b\\) \u00e8 il bias , equivalente al concetto analitico di intercetta; \\(w_1\\) \u00e8 il peso della prima feature, equivalente al concetto analitico di coefficiente angolare; \\(x_1\\) \u00e8 il valore di ingresso assunto dalla prima feature. Per inferire un nuovo valore di \\(y'\\) ci baster\u00e0 quindi cambiare il valore assunto da \\(x1\\) . In pratica, potremo prevedere che per una temperatura di 8 gradi, avremo un valore di precipitazioni pari a 25 mm, mentre per una temperatura di 32 gradi il valore di precipitazioni sar\u00e0 pari a 0. Nota In questo caso, abbiamo presupposto che vi sia un'unica variabile indipendente, o feature, a determinare il valore dell'output. Esistono ovviamente casi pi\u00f9 complessi, nei quali il valore di \\(y'\\) \u00e8 determinato a partire da pi\u00f9 feature come \\(y' = b + w_1 x_1 + \\ldots + w_n x_n\\) . 16.3 - Addestramento e funzione di costo \u00b6 Addestrare un modello significa fare in modo che determini dei valori ottimali per tutti i pesi ed i bias a partire dagli esempi dotati di label. Per determinare tali valori, i modelli ad apprendimento supervisionato provano ad esaminare iterativamente tutti i campioni presenti nel set di addestramento alla ricerca di un modo per minimizzare un costo , il quale rappresenta una certa penalit\u00e0 assegnata al modello in caso di predizione errata. In pratica, il costo (o, in inglese, loss ) \u00e8 un numero che determina se la predizione effettuata dal modello su un singolo \u00e8 stata pi\u00f9 o meno conforme alla label assegnata. In caso di predizione perfetta, la loss \u00e8 pari a \\(0\\) ; tuttavia, nel caso la predizione sia sbagliata, la loss sar\u00e0 tanto pi\u00f9 grande quanto pi\u00f9 il valore predetto sar\u00e0 divergente dal valore atteso. Proviamo ad interpretare graficamente questo concetto, riferendoci ai modelli di regressione: In particolare, nella figura precedente, le frecce rappresentano la loss, mentre il segmento blu rappresenta la predizione. Appare evidente come il secondo esempio abbia una loss complessiva inferiore rispetto al primo. Per calcolare la loss complessiva del modello su un insieme di campioni \u00e8 possibile utilizzare una funzione di costo , o loss function . Esistono molteplici esempi di funzioni di costo; tuttavia, uno dei pi\u00f9 semplici da comprendere \u00e8 l' errore quadratico medio , calcolato a partire dalla seguente formula: \\[ MSE = \\frac{1}{N} \\sum_{(x, y) \\in D} (y - y')^2 \\] Nella formula precedente: \\((x, y)\\) \u00e8 una coppia di feature e label; \\(y'\\) \u00e8 il valore predetto della label a partire dall'applicazione del modello; \\(D\\) \u00e8 il nostro dataset etichettato; \\(N\\) \u00e8 il numero di campioni prensenti in \\(D\\) . In pratica, l'MSE \u00e8 tanto pi\u00f9 alto quanto maggiore \u00e8 la distanza quadratica complessiva tra ogni label \"vera\" ed il rispettivo valore predetto dall'algoritmo di machine learning. Nel caso precedente, \u00e8 chiaro come l'MSE sia maggiore per la prima approssimazione rispetto alla seconda. 16.5 - Addestramento iterativo \u00b6 Gli algoritmi di machine learning tendono ad essere addestrati seguendo un approccio iterativo, che prevede che al termine di ciascuna iterazione i valori dei pesi siano aggiornati in maniera da ridurre ulteriormente il valore della funzione di costo. Questo \u00e8 riassumibile nel seguente schema: In pratica, durante l'addestramento, ad ogni iterazione il modello effettua una predizione sulle feature. Questa predizione viene comparata con la label, e la loss viene calcolata. I pesi sono quindi aggiornati in base ad una determinata regola di ottimizzazione , ed il ciclo si ripete. Nota Le iterazioni non sono infinite : normalmente, si imposta un numero preciso di epoche di training , oppure si aspetta che l'algoritmo arrivi ad una sorta di \"convergenza\", nella quale il valore della loss non decresce ulteriormente. 16.5.1 - Ottimizzazione della loss \u00b6 Abbiamo in precedenza accennato al fatto che l'aggiornamento dei pesi segue una certa regola di ottimizzazione volta a minimizzare la loss. Ne esistono diverse versioni, ma in generale si rifanno al concetto di discesa di gradiente , illustrato nella seguente immagine. Spieghiamo brevemente cosa accade guardando da sinistra verso destra. Possiamo immaginare la funzione che modella la nostra loss come una sorta di paraboloide, dotato di un valore minimo prossimo allo zero che viene raggiunto in corrispondenza di una determinata combinazione dei valori dei pesi. Ipotizzando di trovarci all'inizio dell'addestramento nella situazione raffigurata nella figura a sinistra, ovvero con dei pesi nel ramo sinistro del paraboloide, il nostro obiettivo sar\u00e0 quello di muoverci verso \"destra\", ovvero verso il minimo globale della funzione. Per farlo, intuitivamente, dovremo valutare la derivata o, nel caso di funzioni ad \\(n\\) dimensioni, con \\(n\\) numero di feature, il gradiente della nostra funzione di costo, ed aggiornare i pesi in maniera tale che questo assuma, alla successiva iterazione, un valore inferiore. Questo aggiornamento ci porta alla figura centrale, in cui vediamo che il gradiente si \u00e8 spostato dal punto rosso al punto blu. In questa iterazione dovremo ancora aumentare il valore dei pesi affinch\u00e8 il valore della funzione di costo diminuisca, portandoci quindi nella situazione raffigurata nella figura a destra. In quest'ultima situazione vedremo che il segno del gradiente sar\u00e0 diventato positivo, in quanto ci troveremo su una parte ascendente del paraboloide; di conseguenza, dovremo diminuire i pesi per far convergere l'algoritmo. Learning rate Il \"quantitativo\" di cui sono aggiornati i pesi \u00e8 spesso denotato come learning rate . Un learning rate troppo basso porta ad una convergenza molto lenta dell'algoritmo, che potrebbe \"esaurire\" le iterazioni prima di arrivare al minimo della funzione di costo. Un learning rate eccessivamente altro potrebbe invece fare in modo che l'algoritmo \"salti\" da una parte all'altra del minimo, non arrivando neanche in questo caso a convergenza. Minimi locali Il nostro banale esempio presuppone che la funzione di costo non abbia alcun minimo locale. Ci\u00f2 non \u00e8 ovviamente vero, e delle scelte sbagliate in termini di punto di partenza o learning rate potrebbero farci finire all'interno di un minimo locale, impedendoci di arrivare a convergenza. 16.5.2 - Overfitting e regolarizzazione \u00b6 Alle volte, accade che il nostro modello sia in grado di arrivare ad una loss estremamente bassa sui dati di training, ma che tuttavia inizia ad aumentare sui dati di validazione, un po' come nella figura successiva: Ci\u00f2 pu\u00f2 accadere per diversi motivi, come errori nei parametri di addestramento o dati non ben bilanciati. Ad ogni modo, questo fenomeno prende il nome di overfitting , e comporta che il modello, che si comporta benissimo sui dati di training, non riesca a generalizzare , comportandosi in maniera meno egregia sui dati di validazione. L'overfitting si manifesta all'aumentare delle epoche di training, quando il nostro modello diventa sempre pi\u00f9 \"complesso\", ed apprende sempre meglio a caratterizzare relazioni di complessit\u00e0 crescente intercorrenti tra feature e label. Per arginare il fenomeno dell'overfitting, oltre ad agire sui dati e sui parametri del modello, si inserisce spesso un termine di regolarizzazione , che tende a penalizzare un modello in grado di caratterizzare relazioni eccessivamente complesse. Il termine di regolarizzazione interviene direttamente sul valore trattato dall'ottimizzatore, che non avr\u00e0 pi\u00f9 come unico obiettivo quello di minimizzare la loss, ma quello di minimizzare congiuntamente la loss e la complessit\u00e0 del modello ottenuto . Una funzione di regolarizzazione molto usata \u00e8 la regolarizzazione \\(L_2\\) , definita come la somma dei quadrati dei pesi associati alle feature: \\[ L_2 = ||w||_2^2 = w_1^2 + w_2^2 + \\ldots + w_n^2 \\] Minimizzare questo termine significa dare meno \"importanza\" ad alcuni pesi che inficiano la complessit\u00e0 totale del modello. Se, ad esempio, avessimo i seguenti pesi: \\[ {w_1 = 0.1, w_2 = 0.025, w_3 = 0.4, w_4 = 10} \\] il termine di regolarizzazione \\(L_2\\) diverrebbe pari a: \\[ L_2 = 0.01 + 0,000625 + 0.16 + 100 \\sim 100.17 \\] E' evidente come la maggior parte del contributo sia data dal quarto peso, per cui risulta essere necessario diminuirne l'influenza nel modello allo scopo di bilanciare l'overfitting. 16.6 La regressione lineare in Scikit Learn \u00b6 La regressione lineare in Scikit Learn \u00e8 implementata mediante gli oggetti di classe LinearRegression() contenuti all'interno del package linear_model delal libreria. Oggetti di questo tipo sono degli estimator, e funzionano in questo modo: import numpy as np from sklearn.linear_model import LinearRegression reg = LinearRegression () data = np . array ([[ 0 , 0 ], [ 1 , 1 ], [ 2 , 2 ]]) reg . fit ( data ) Nel codice precedente stiamo creando un oggetto di classe LinearRegression() ed un array NumPy chiamato genericamente data . Per effettuare l'addestramento del nostro modello, dovremo chiamare il metodo fit di reg passandogli data ; fatto questo, l'istanza reg sar\u00e0 stata regolarmente addestrata, e sar\u00e0 pronta per effettuare le predizioni. In tal senso, dovremo usare il metodo predict() : reg . predict ([[ 4 , 4 ]]) Per accedere ai parametri dello stimatore (ovvero al coefficiente angolare ed all'intercetta) dovremo usare gli attributi coef_ ed intercept_ : reg . coef_ La classe LinearRegression() ci mette a disposizione anche il metodo score() , che ci permette di ottenere il coefficiente \\(R^2\\) ottenuto dal modello di regressione. Questo \u00e8 pari a: \\[ R^2 = (1 - \\frac{u}{v}) \\] dove: \\(u\\) \u00e8 pari alla sommatoria dei quadrati dei residui , ovvero \\(\\sum (y - y')^2\\) ; \\(v\\) \u00e8 pari alla sommatoria della differenza tra i valori veri ed il valor medio, ovvero \\(\\sum (y - \\mu(y))^2\\) . Conoscere il valore di \\(R^2\\) \u00e8 importante per avere un'idea della bont\u00e0 del modello. Nel caso ideale, infatti, questo valore \u00e8 \\(1\\) , mentre valori inferiori (o addirittura negativi) rappresentano delle possibili criticit\u00e0 del modello. Intervalli di confidenza Scikit Learn non fornisce un intervallo di confidenza per le predizioni ottenute; pi\u00f9 informazioni su questa scelta di design qui . Tuttavia, \u00e8 possibile implementare questa funzionalit\u00e0 usando NumPy, come descritto qui , o in alternativa usare il package Statsmodels.","title":"Dispense"},{"location":"material/03_ml_sklearn/16_lin_reg/lecture/#16-apprendimento-supervisionato-la-regressione-lineare","text":"Quelli di apprendimento supervisionato sono probabilmente tra i sistemi di machine learning pi\u00f9 diffusi, soprattutto a causa dei numerosi casi d'uso disponibili. Abbiamo detto che esistono fondamentalmente due tipi di tecniche di apprendimento supervisionato, ovvero regressione e classificazione . Vediamole maggiormente nel dettaglio.","title":"16 - Apprendimento supervisionato: la regressione lineare"},{"location":"material/03_ml_sklearn/16_lin_reg/lecture/#161-un-esempio-di-regressione","text":"A tutti noi \u00e8 intuitivamente noto che i millimetri di pioggia che cadono sono in qualche modo correlati alle temperature medie che abbiamo durante la giornata. Immaginiamo quindi di avere un dataset che contenga al suo interno i dati medi sui millimetri di pioggia degli ultimi dieci anni per undici valori differenti di temperatura. Se provassimo a visualizzare i dati mediante un relplot() , otterremmo la seguente figura. Proviamo ad usare la funzione lmplot() che, ricordiamo, effettua una regressione tra i dati.","title":"16.1 - Un esempio di regressione"},{"location":"material/03_ml_sklearn/16_lin_reg/lecture/#162-rappresentazione-analitica-del-modello","text":"Notiamo subito che, come prevedibile, i millimetri di pioggia attesi diminuiscono all'aumentare della temperatura, andando a definire una sorta di relazione lineare tra i dati sull'asse delle ascisse (ovvero i gradi) e quelli sull'asse delle ordinate (ovvero la pioggia). Ovviamente, la retta di regressione non tocca direttamente tutti i punti, ma li approssima . Possiamo quindi dire che la relazione tra gradi e mm di pioggia \u00e8 riconducibile ad una forma del tipo: \\[ y = mx + b \\] dove: \\(y\\) sono i millimetri di pioggia medi caduti nell'arco di tutte le giornate con un dato valore medio di temperatura; \\(x\\) \u00e8 il valore medio di temperatura; \\(m\\) \u00e8 il coefficiente angolare della retta di regressione; \\(b\\) \u00e8 l'incercetta della retta di regressione. Questa notazione analitica si traduce in un modello usando la seguente notazione: \\[ y' = b + w_1 x_1 \\] dove: \\(y'\\) \u00e8 l'output predetto dal modello; \\(b\\) \u00e8 il bias , equivalente al concetto analitico di intercetta; \\(w_1\\) \u00e8 il peso della prima feature, equivalente al concetto analitico di coefficiente angolare; \\(x_1\\) \u00e8 il valore di ingresso assunto dalla prima feature. Per inferire un nuovo valore di \\(y'\\) ci baster\u00e0 quindi cambiare il valore assunto da \\(x1\\) . In pratica, potremo prevedere che per una temperatura di 8 gradi, avremo un valore di precipitazioni pari a 25 mm, mentre per una temperatura di 32 gradi il valore di precipitazioni sar\u00e0 pari a 0. Nota In questo caso, abbiamo presupposto che vi sia un'unica variabile indipendente, o feature, a determinare il valore dell'output. Esistono ovviamente casi pi\u00f9 complessi, nei quali il valore di \\(y'\\) \u00e8 determinato a partire da pi\u00f9 feature come \\(y' = b + w_1 x_1 + \\ldots + w_n x_n\\) .","title":"16.2 - Rappresentazione analitica del modello"},{"location":"material/03_ml_sklearn/16_lin_reg/lecture/#163-addestramento-e-funzione-di-costo","text":"Addestrare un modello significa fare in modo che determini dei valori ottimali per tutti i pesi ed i bias a partire dagli esempi dotati di label. Per determinare tali valori, i modelli ad apprendimento supervisionato provano ad esaminare iterativamente tutti i campioni presenti nel set di addestramento alla ricerca di un modo per minimizzare un costo , il quale rappresenta una certa penalit\u00e0 assegnata al modello in caso di predizione errata. In pratica, il costo (o, in inglese, loss ) \u00e8 un numero che determina se la predizione effettuata dal modello su un singolo \u00e8 stata pi\u00f9 o meno conforme alla label assegnata. In caso di predizione perfetta, la loss \u00e8 pari a \\(0\\) ; tuttavia, nel caso la predizione sia sbagliata, la loss sar\u00e0 tanto pi\u00f9 grande quanto pi\u00f9 il valore predetto sar\u00e0 divergente dal valore atteso. Proviamo ad interpretare graficamente questo concetto, riferendoci ai modelli di regressione: In particolare, nella figura precedente, le frecce rappresentano la loss, mentre il segmento blu rappresenta la predizione. Appare evidente come il secondo esempio abbia una loss complessiva inferiore rispetto al primo. Per calcolare la loss complessiva del modello su un insieme di campioni \u00e8 possibile utilizzare una funzione di costo , o loss function . Esistono molteplici esempi di funzioni di costo; tuttavia, uno dei pi\u00f9 semplici da comprendere \u00e8 l' errore quadratico medio , calcolato a partire dalla seguente formula: \\[ MSE = \\frac{1}{N} \\sum_{(x, y) \\in D} (y - y')^2 \\] Nella formula precedente: \\((x, y)\\) \u00e8 una coppia di feature e label; \\(y'\\) \u00e8 il valore predetto della label a partire dall'applicazione del modello; \\(D\\) \u00e8 il nostro dataset etichettato; \\(N\\) \u00e8 il numero di campioni prensenti in \\(D\\) . In pratica, l'MSE \u00e8 tanto pi\u00f9 alto quanto maggiore \u00e8 la distanza quadratica complessiva tra ogni label \"vera\" ed il rispettivo valore predetto dall'algoritmo di machine learning. Nel caso precedente, \u00e8 chiaro come l'MSE sia maggiore per la prima approssimazione rispetto alla seconda.","title":"16.3 - Addestramento e funzione di costo"},{"location":"material/03_ml_sklearn/16_lin_reg/lecture/#165-addestramento-iterativo","text":"Gli algoritmi di machine learning tendono ad essere addestrati seguendo un approccio iterativo, che prevede che al termine di ciascuna iterazione i valori dei pesi siano aggiornati in maniera da ridurre ulteriormente il valore della funzione di costo. Questo \u00e8 riassumibile nel seguente schema: In pratica, durante l'addestramento, ad ogni iterazione il modello effettua una predizione sulle feature. Questa predizione viene comparata con la label, e la loss viene calcolata. I pesi sono quindi aggiornati in base ad una determinata regola di ottimizzazione , ed il ciclo si ripete. Nota Le iterazioni non sono infinite : normalmente, si imposta un numero preciso di epoche di training , oppure si aspetta che l'algoritmo arrivi ad una sorta di \"convergenza\", nella quale il valore della loss non decresce ulteriormente.","title":"16.5 - Addestramento iterativo"},{"location":"material/03_ml_sklearn/16_lin_reg/lecture/#1651-ottimizzazione-della-loss","text":"Abbiamo in precedenza accennato al fatto che l'aggiornamento dei pesi segue una certa regola di ottimizzazione volta a minimizzare la loss. Ne esistono diverse versioni, ma in generale si rifanno al concetto di discesa di gradiente , illustrato nella seguente immagine. Spieghiamo brevemente cosa accade guardando da sinistra verso destra. Possiamo immaginare la funzione che modella la nostra loss come una sorta di paraboloide, dotato di un valore minimo prossimo allo zero che viene raggiunto in corrispondenza di una determinata combinazione dei valori dei pesi. Ipotizzando di trovarci all'inizio dell'addestramento nella situazione raffigurata nella figura a sinistra, ovvero con dei pesi nel ramo sinistro del paraboloide, il nostro obiettivo sar\u00e0 quello di muoverci verso \"destra\", ovvero verso il minimo globale della funzione. Per farlo, intuitivamente, dovremo valutare la derivata o, nel caso di funzioni ad \\(n\\) dimensioni, con \\(n\\) numero di feature, il gradiente della nostra funzione di costo, ed aggiornare i pesi in maniera tale che questo assuma, alla successiva iterazione, un valore inferiore. Questo aggiornamento ci porta alla figura centrale, in cui vediamo che il gradiente si \u00e8 spostato dal punto rosso al punto blu. In questa iterazione dovremo ancora aumentare il valore dei pesi affinch\u00e8 il valore della funzione di costo diminuisca, portandoci quindi nella situazione raffigurata nella figura a destra. In quest'ultima situazione vedremo che il segno del gradiente sar\u00e0 diventato positivo, in quanto ci troveremo su una parte ascendente del paraboloide; di conseguenza, dovremo diminuire i pesi per far convergere l'algoritmo. Learning rate Il \"quantitativo\" di cui sono aggiornati i pesi \u00e8 spesso denotato come learning rate . Un learning rate troppo basso porta ad una convergenza molto lenta dell'algoritmo, che potrebbe \"esaurire\" le iterazioni prima di arrivare al minimo della funzione di costo. Un learning rate eccessivamente altro potrebbe invece fare in modo che l'algoritmo \"salti\" da una parte all'altra del minimo, non arrivando neanche in questo caso a convergenza. Minimi locali Il nostro banale esempio presuppone che la funzione di costo non abbia alcun minimo locale. Ci\u00f2 non \u00e8 ovviamente vero, e delle scelte sbagliate in termini di punto di partenza o learning rate potrebbero farci finire all'interno di un minimo locale, impedendoci di arrivare a convergenza.","title":"16.5.1 - Ottimizzazione della loss"},{"location":"material/03_ml_sklearn/16_lin_reg/lecture/#1652-overfitting-e-regolarizzazione","text":"Alle volte, accade che il nostro modello sia in grado di arrivare ad una loss estremamente bassa sui dati di training, ma che tuttavia inizia ad aumentare sui dati di validazione, un po' come nella figura successiva: Ci\u00f2 pu\u00f2 accadere per diversi motivi, come errori nei parametri di addestramento o dati non ben bilanciati. Ad ogni modo, questo fenomeno prende il nome di overfitting , e comporta che il modello, che si comporta benissimo sui dati di training, non riesca a generalizzare , comportandosi in maniera meno egregia sui dati di validazione. L'overfitting si manifesta all'aumentare delle epoche di training, quando il nostro modello diventa sempre pi\u00f9 \"complesso\", ed apprende sempre meglio a caratterizzare relazioni di complessit\u00e0 crescente intercorrenti tra feature e label. Per arginare il fenomeno dell'overfitting, oltre ad agire sui dati e sui parametri del modello, si inserisce spesso un termine di regolarizzazione , che tende a penalizzare un modello in grado di caratterizzare relazioni eccessivamente complesse. Il termine di regolarizzazione interviene direttamente sul valore trattato dall'ottimizzatore, che non avr\u00e0 pi\u00f9 come unico obiettivo quello di minimizzare la loss, ma quello di minimizzare congiuntamente la loss e la complessit\u00e0 del modello ottenuto . Una funzione di regolarizzazione molto usata \u00e8 la regolarizzazione \\(L_2\\) , definita come la somma dei quadrati dei pesi associati alle feature: \\[ L_2 = ||w||_2^2 = w_1^2 + w_2^2 + \\ldots + w_n^2 \\] Minimizzare questo termine significa dare meno \"importanza\" ad alcuni pesi che inficiano la complessit\u00e0 totale del modello. Se, ad esempio, avessimo i seguenti pesi: \\[ {w_1 = 0.1, w_2 = 0.025, w_3 = 0.4, w_4 = 10} \\] il termine di regolarizzazione \\(L_2\\) diverrebbe pari a: \\[ L_2 = 0.01 + 0,000625 + 0.16 + 100 \\sim 100.17 \\] E' evidente come la maggior parte del contributo sia data dal quarto peso, per cui risulta essere necessario diminuirne l'influenza nel modello allo scopo di bilanciare l'overfitting.","title":"16.5.2 - Overfitting e regolarizzazione"},{"location":"material/03_ml_sklearn/16_lin_reg/lecture/#166-la-regressione-lineare-in-scikit-learn","text":"La regressione lineare in Scikit Learn \u00e8 implementata mediante gli oggetti di classe LinearRegression() contenuti all'interno del package linear_model delal libreria. Oggetti di questo tipo sono degli estimator, e funzionano in questo modo: import numpy as np from sklearn.linear_model import LinearRegression reg = LinearRegression () data = np . array ([[ 0 , 0 ], [ 1 , 1 ], [ 2 , 2 ]]) reg . fit ( data ) Nel codice precedente stiamo creando un oggetto di classe LinearRegression() ed un array NumPy chiamato genericamente data . Per effettuare l'addestramento del nostro modello, dovremo chiamare il metodo fit di reg passandogli data ; fatto questo, l'istanza reg sar\u00e0 stata regolarmente addestrata, e sar\u00e0 pronta per effettuare le predizioni. In tal senso, dovremo usare il metodo predict() : reg . predict ([[ 4 , 4 ]]) Per accedere ai parametri dello stimatore (ovvero al coefficiente angolare ed all'intercetta) dovremo usare gli attributi coef_ ed intercept_ : reg . coef_ La classe LinearRegression() ci mette a disposizione anche il metodo score() , che ci permette di ottenere il coefficiente \\(R^2\\) ottenuto dal modello di regressione. Questo \u00e8 pari a: \\[ R^2 = (1 - \\frac{u}{v}) \\] dove: \\(u\\) \u00e8 pari alla sommatoria dei quadrati dei residui , ovvero \\(\\sum (y - y')^2\\) ; \\(v\\) \u00e8 pari alla sommatoria della differenza tra i valori veri ed il valor medio, ovvero \\(\\sum (y - \\mu(y))^2\\) . Conoscere il valore di \\(R^2\\) \u00e8 importante per avere un'idea della bont\u00e0 del modello. Nel caso ideale, infatti, questo valore \u00e8 \\(1\\) , mentre valori inferiori (o addirittura negativi) rappresentano delle possibili criticit\u00e0 del modello. Intervalli di confidenza Scikit Learn non fornisce un intervallo di confidenza per le predizioni ottenute; pi\u00f9 informazioni su questa scelta di design qui . Tuttavia, \u00e8 possibile implementare questa funzionalit\u00e0 usando NumPy, come descritto qui , o in alternativa usare il package Statsmodels.","title":"16.6 La regressione lineare in Scikit Learn"},{"location":"material/03_ml_sklearn/17_logistic/exercises/","text":"E17 - Regressione logistica \u00b6 Esercizio E17.1 \u00b6 Continuiamo ad operare sul dataset Tips di Seaborn. In particolare, scegliamo come label il giorno , e come feature sulle quali operare il conto totale , la mancia e la dimensione del tavolo . Addestriamo un classificatore a determinare qual \u00e8 il giorno pi\u00f9 probabile sulla base delle feature selezionate. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizi"},{"location":"material/03_ml_sklearn/17_logistic/exercises/#e17-regressione-logistica","text":"","title":"E17 - Regressione logistica"},{"location":"material/03_ml_sklearn/17_logistic/exercises/#esercizio-e171","text":"Continuiamo ad operare sul dataset Tips di Seaborn. In particolare, scegliamo come label il giorno , e come feature sulle quali operare il conto totale , la mancia e la dimensione del tavolo . Addestriamo un classificatore a determinare qual \u00e8 il giorno pi\u00f9 probabile sulla base delle feature selezionate. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizio E17.1"},{"location":"material/03_ml_sklearn/17_logistic/lecture/","text":"17 - Modelli supervisionati: regressione logistica \u00b6 Esistono diversi problemi, tra cui quelli di classificazione multiclasse, che richiedono che l'uscita del sistema sia una stima di probabilit\u00e0; per far questo, la regressione logistica \u00e8 lo strumento \"principe\" da utilizzare. Per comprenderne il funzionamento, supponiamo di creare un modello di regressione logistica che predica la probabilit\u00e0 che una mail ricevuta da un indirizzo sconosciuto sia di spam. Chiameremo questa probabilit\u00e0 come: \\[ p(mail|unknown) \\] Se il modello afferma che la probabilit\u00e0 \\(p(mail|unknown) = 0.05\\) , allora, su \\(100\\) mail ricevute da indirizzi sconosciuti, \\(5\\) saranno di spam: \\[ spam = p(mail|unknown) \\cdot mail_rec = 0.05 * 100 = 5 \\] Questo \u00e8 un esempio di utilizzo della probabilit\u00e0 as is . In molti casi, tuttavia, mapperemo l'output della soluzione su un problema di classificazione binario, nel quale l'obiettivo \u00e8 predire correttamente uno di due possibili label (in questo caso, spam o non spam ). 17.1 - La funzione sigmoidale \u00b6 Ci si potrebbe chiedere come un modello per la regressione logistica sia in grado di asicurarsi che l'uscita ricada sempre nell'intervallo tra \\(0\\) ed \\(1\\) . In tal senso, questo \u00e8 assicurato dall'uso della funzione sigmoidale , definita come segue: \\[ y = \\frac{1}{1+e^{-z}} \\] la cui formulazione grafica \u00e8 la seguente: Nell'espressione precedente, notiamo che: \\(y\\) \u00e8 l'uscita della regressione logistica; \\(z\\) \u00e8 pari, per un generico modello lineare, a \\(b + w_1 x_1 + \\ldots + w_N z_N\\) . 17.2 - Funzione di costo \u00b6 La funzione di costo per la funzione logistica \u00e8 chiamata log loss , ed \u00e8 espressa come: \\[ LogLoss = \\sum_{(x, y) \\in D} -y log(y') - (1 - y) log (1 - y') \\] dove: \\((x, y)\\) sono le coppie date da feature e label nel dataset \\(D\\) ; \\(y\\) \u00e8 la label vera per un dato insieme di feature; \\(y'\\) \u00e8 il valore predetto. 17.3 - Regressione logistica in Scikit Learn \u00b6 In Scikit Learn la regressione logistica \u00e8 implementata mediante la classe LogisticRegression() . E' importante sottolineare come la regressione logistica, nonostante il nome, si comporti a tutti gli effetti come un classificatore : di conseguenza, l'output del modello sar\u00e0 una classe, e non un valore di regressione.","title":"Dispense"},{"location":"material/03_ml_sklearn/17_logistic/lecture/#17-modelli-supervisionati-regressione-logistica","text":"Esistono diversi problemi, tra cui quelli di classificazione multiclasse, che richiedono che l'uscita del sistema sia una stima di probabilit\u00e0; per far questo, la regressione logistica \u00e8 lo strumento \"principe\" da utilizzare. Per comprenderne il funzionamento, supponiamo di creare un modello di regressione logistica che predica la probabilit\u00e0 che una mail ricevuta da un indirizzo sconosciuto sia di spam. Chiameremo questa probabilit\u00e0 come: \\[ p(mail|unknown) \\] Se il modello afferma che la probabilit\u00e0 \\(p(mail|unknown) = 0.05\\) , allora, su \\(100\\) mail ricevute da indirizzi sconosciuti, \\(5\\) saranno di spam: \\[ spam = p(mail|unknown) \\cdot mail_rec = 0.05 * 100 = 5 \\] Questo \u00e8 un esempio di utilizzo della probabilit\u00e0 as is . In molti casi, tuttavia, mapperemo l'output della soluzione su un problema di classificazione binario, nel quale l'obiettivo \u00e8 predire correttamente uno di due possibili label (in questo caso, spam o non spam ).","title":"17 - Modelli supervisionati: regressione logistica"},{"location":"material/03_ml_sklearn/17_logistic/lecture/#171-la-funzione-sigmoidale","text":"Ci si potrebbe chiedere come un modello per la regressione logistica sia in grado di asicurarsi che l'uscita ricada sempre nell'intervallo tra \\(0\\) ed \\(1\\) . In tal senso, questo \u00e8 assicurato dall'uso della funzione sigmoidale , definita come segue: \\[ y = \\frac{1}{1+e^{-z}} \\] la cui formulazione grafica \u00e8 la seguente: Nell'espressione precedente, notiamo che: \\(y\\) \u00e8 l'uscita della regressione logistica; \\(z\\) \u00e8 pari, per un generico modello lineare, a \\(b + w_1 x_1 + \\ldots + w_N z_N\\) .","title":"17.1 - La funzione sigmoidale"},{"location":"material/03_ml_sklearn/17_logistic/lecture/#172-funzione-di-costo","text":"La funzione di costo per la funzione logistica \u00e8 chiamata log loss , ed \u00e8 espressa come: \\[ LogLoss = \\sum_{(x, y) \\in D} -y log(y') - (1 - y) log (1 - y') \\] dove: \\((x, y)\\) sono le coppie date da feature e label nel dataset \\(D\\) ; \\(y\\) \u00e8 la label vera per un dato insieme di feature; \\(y'\\) \u00e8 il valore predetto.","title":"17.2 - Funzione di costo"},{"location":"material/03_ml_sklearn/17_logistic/lecture/#173-regressione-logistica-in-scikit-learn","text":"In Scikit Learn la regressione logistica \u00e8 implementata mediante la classe LogisticRegression() . E' importante sottolineare come la regressione logistica, nonostante il nome, si comporti a tutti gli effetti come un classificatore : di conseguenza, l'output del modello sar\u00e0 una classe, e non un valore di regressione.","title":"17.3 - Regressione logistica in Scikit Learn"},{"location":"material/03_ml_sklearn/18_metrics/exercises/","text":"E18 - Metriche \u00b6 Esercizio E18.1 \u00b6 Consideriamo il regressore logistico usato nell' esercizio 17.1 . Valutiamo per prima cosa i risultati ottenuti in termini di accuratezza, precisione e recall usando le apposite funzioni di Scikit Learn. Utilizziamo anche la funzione classification_report() per ottenere un report completo dell'esito del classificatore. Esercizio E18.2 \u00b6 Proviamo adesso a verificare come variano i valori di accuratezza, precisione e recall per diversi valori della soglia di decisione. In tal senso: semplifichiamo il problema riducendolo ad una classificazione binaria, e quindi considerando come label la colonna time ; utilizziamo il metodo predict_proba(X) del LogisticRegressor() . Esercizio E18.3 \u00b6 Consideriamo il regressore lineare usato nell' esercizio 16.1 . Valutiamo i risultati ottenuti in termini di MSE, MAPE ed \\(R^2\\) . Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizi"},{"location":"material/03_ml_sklearn/18_metrics/exercises/#e18-metriche","text":"","title":"E18 - Metriche"},{"location":"material/03_ml_sklearn/18_metrics/exercises/#esercizio-e181","text":"Consideriamo il regressore logistico usato nell' esercizio 17.1 . Valutiamo per prima cosa i risultati ottenuti in termini di accuratezza, precisione e recall usando le apposite funzioni di Scikit Learn. Utilizziamo anche la funzione classification_report() per ottenere un report completo dell'esito del classificatore.","title":"Esercizio E18.1"},{"location":"material/03_ml_sklearn/18_metrics/exercises/#esercizio-e182","text":"Proviamo adesso a verificare come variano i valori di accuratezza, precisione e recall per diversi valori della soglia di decisione. In tal senso: semplifichiamo il problema riducendolo ad una classificazione binaria, e quindi considerando come label la colonna time ; utilizziamo il metodo predict_proba(X) del LogisticRegressor() .","title":"Esercizio E18.2"},{"location":"material/03_ml_sklearn/18_metrics/exercises/#esercizio-e183","text":"Consideriamo il regressore lineare usato nell' esercizio 16.1 . Valutiamo i risultati ottenuti in termini di MSE, MAPE ed \\(R^2\\) . Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizio E18.3"},{"location":"material/03_ml_sklearn/18_metrics/lecture/","text":"18 - Metriche \u00b6 Abbiamo visto come la regressione logistca restituisca una probabilit\u00e0, che grazie alla classe LogisticRegression() viene automaticamente convertita in un valore relativo ad una classe. Torniamo al nostro spam detector. Un modello di regressione logistica che restituisca una probabilit\u00e0 \\(p = 0.999\\) ci sta dicendo che, molto probabilmente, questo \u00e8 di spam; di converso, se il modello restituisce \\(p = 0.003\\) allora \u00e8 molto probabile che il messaggio non sia spam. Cosa accade per\u00f2 nel caso in cui \\(p = 0.505\\) ? 18.1 - Soglia di decisione \u00b6 L'esempio precedente ci fa comprendere come per passare da una probabilit\u00e0 ad una classe sia necessario definire una soglia di decisione : un valore oltre questa soglia indicher\u00e0, ad esempio, che la mail ricevuta \u00e8 di spam, mentre uno al di sotto della soglia ci suggerir\u00e0 che non lo \u00e8. Ovviamente, la tentazione potrebbe essere quella di presupporre che la soglia di decisione sia sempre pari a \\(0.5\\) : questo, ovviamente, non \u00e8 vero, in quanto la soglia dipende dal problema, ed \u00e8 un valore che bisogna stabilire in base al problema affrontato. Introduciamo alcune metriche che possono essere usate in tal senso. 18.2 - Metriche per i classificatori \u00b6 Continuiamo a concentrarci sul caso della classificazione dello spam, ed introduciamo il concetto di classe positiva e classe negativa . In particolare, la classe positiva sar\u00e0 rappresentata da tutte le mail di spam, mentre la classe negativa sar\u00e0 rappresentata dalle mail non spam. In tal senso, le predizioni del modello potranno essere di quattro tipi: nel primo caso, il modello classificher\u00e0 correttamente una mail di spam . In questo caso, si parla di vero positivo , o true positive (TP); nel secondo caso, il modello classificher\u00e0 correttamente una mail legittima . In questo caso, si parla di vero negativo , o true negative (TN); nel terzo caso, il modello classificher\u00e0 una mail di spam come legittima . In questo caso, si parla di falso negativo , o false negative (FN); nel quarto caso, il modello classificher\u00e0 una mail legittima come di spam . In questo caso, si parla di falso positivo , o false positive (FP). In pratica, un TP (TN) si ha quando il modello predice correttamente la classe positiva (negativa), mentre un FP (FN) si ha quando il modello predice in maniera non corretta la classe positiva (negativa). 18.2.1 - Accuratezza \u00b6 L' accuratezza \u00e8 la prima metrica che vedremo per la valutazione dei modelli di classificazione. Informalmente, possiamo definirla come la percentuale di predizioni corrette effettuate dal nostro modello, e definirla come: \\[ AC = \\frac{C}{T} \\] dove \\(C\\) \u00e8 il numero totale di predizioni corrette, mentre \\(T\\) \u00e8 il numero totale di predizioni. Nel caso della classificazione binaria, possiamo calcolare l'accuratezza come segue: \\[ AC = \\frac{TP + TN}{TP + TN + FP + FN} \\] Immaginiamo ad esempio di aver ricevuto \\(100\\) email, tra cui \\(10\\) di spam. Il nostro spam detector ha individuato correttamente \\(5\\) messaggi di spam, e classificato per sbaglio come spam \\(5\\) messaggi legittimi. Allora: \\[ AC = \\frac{TP+TN}{TP+TN+FP+FN}=\\frac{5+85}{5+85+5+5} \\] In questo caso, l'accuratezza del modello \u00e8 pari a \\(0.90\\) , o del \\(90\\%\\) , il che significa che il nostro modello \u00e8 in grado di fare \\(90\\) predizioni su \\(100\\) . Buon risultato, giusto? In realt\u00e0, non necessariamente. Infatti, delle mail che abbiamo ricevuto, \\(90\\) sono legittime, e \\(10\\) di spam. Questo significa che il modello \u00e8 stato in grado di individuare soltanto il \\(50\\%\\) dello spam ricevuto, ed ha inoltre classificato un buon \\(7\\%\\) delle email legittime come spam. Tra cui, prevedibilmente, quella che ci comunicava notizie di vitale importanza. In sostanza, il nostro modello ha un'efficacia \"vera e propria\" al pi\u00f9 in un caso su due. Di conseguenza, l'accuratezza non ci racconta \"tutta la storia\" quando lavoriamo su un dataset sbilanciato come questo, dove vi \u00e8 una disparit\u00e0 significativa tra la classe positiva e quella negativa. 18.2.1.1 - Accuratezza in Scikit Learn \u00b6 L'accuratezza delle predizioni effettuate da un classificatore \u00e8 ottenuta in Scikit Learn utilizzando il metodo accuracy_score() . Ad esempio: from sklearn.metrics import accuracy_score clf = LogisticRegression ( max_iter = 1000 ) clf . fit ( X_train , y_train ) y_pred = clf . predict ( X_test ) accuracy_score ( y_test , y_pred ) 18.2.2 - La precisione \u00b6 La precisione \u00e8 una metrica che prova a risolvere alcuni dei problemi dell'accuratezza valutando quale sia la proporzione di valori per la classe positiva identificati correttamente. La definizione analitica della precisione \u00e8 la seguente: \\[ PR = \\frac{TP}{TP+FP} \\] In pratica, riferendoci al nostro solito esempio, la precisione \u00e8 data dal rapporto tra le mail di spam riconosciute come tali e la somma tra queste e le mail legittime riconosciute come spam. Provando a calcolarla: \\[ PR = \\frac{5}{5+5} = 0.5 \\] Il modello ha quindi una precisione del \\(50\\%\\) nel riconoscere una mail di spam. 18.2.2.1 - Precisione in Scikit Learn \u00b6 La precisione delle predizioni effettuate da un classificatore \u00e8 ottenuta in Scikit Learn utilizzando il metodo precision_score() . Ad esempio: from sklearn.metrics import precision_score precision_score ( y_test , y_pred ) 18.2.3 - Il recall \u00b6 Il recall , traducibile in italiano come richiamo , verifica la porzione di veri positivi correttamente identificata dall'algoritmo, ed \u00e8 espresso come: \\[ R = \\frac{TP}{TP+FN} \\] Nel nostro caso, il recall sar\u00e0 quindi dato dal rapporto tra le mail correttamente indicate come spam e la somma tra le stesse e quelle erroneamente indicate come legittime. Va da s\u00e8 che anche in questo caso possiamo calcolarlo: \\[ R = \\frac{5}{5+5} \\] Cos\u00ec come la precisione, il recall \u00e8 pari a \\(0.5\\) , ovvero \u00e8 del \\(50\\%\\) . 18.2.3.1 - Recall in Scikit Learn \u00b6 Ovviamente, anche il recall ha una rappresentazione in Scikit Learn mediante la funzione recall_score() : from sklearn.metrics import recall_score recall_score ( y_test , y_pred ) 18.3 - Tuning della soglia di decisione \u00b6 Per valutare l'effiacia del modello dobbiamo esaminare congiuntamente la precisione ed il recall. Sfortunatamente, questi due valori sono spesso in contrapposizione: spesso, infatti, migliorare la precisione riduce il recall, e viceversa. Per comprendere empiricamente questo concetto, facciamo un esempio con il nostro spam detector, immaginando di aver impostato la soglia di decisione a \\(0.6\\) . I risultati sono mostrati nella figura successiva. Calcoliamo la precisione e il recall in questo caso: \\[ P = \\frac{TP}{TP+FP}=\\frac{4}{4+1} = 0.8 \\\\ R = \\frac{TP}{TP+FN}=\\frac{4}{4+2} = 0.66 \\] Proviamo ad aumentare la soglia di decisione, portandola al \\(75\\%\\) . \\[ P = \\frac{TP}{TP+FP}=\\frac{3}{3} = 1 \\\\ R = \\frac{TP}{TP+FN}=\\frac{3}{3+3} = 0.5 \\] Proviamo infine a diminuire la soglia di decisione, portandola al \\(50%\\) . \\[ P = \\frac{TP}{TP+FP}=\\frac{4}{4+3} \\approx 0.57 \\\\ R = \\frac{TP}{TP+FN}=\\frac{4}{4+2} = 0.66 \\] Come possiamo vedere, la soglia di detection agisce su precisione e recall; non \u00e8 per\u00f2 possibile aumentarli contemporaneamente, per cui occorre scegliere un valore tale per cui, ad esempio, si massimizzi la media. La realt\u00e0 \u00e8 che, per\u00f2, dipende sempre dall'applicazione: se non abbiamo paura di perdere mail legittime, allora possiamo abbassare la soglia di decisione, aumentando il recall; viceversa, se siamo disposti ad eliminare manualmente un po' di spam, potremo alzare la soglia di decisione, aumentando la precisione. 18.4 - Metriche per i regressori \u00b6 Definiamo brevemente alcune delle metriche che \u00e8 possibile utilizzare per la valutazione delle performance di un modello di regressione. 18.4.1 - Mean Squared Error (MSE) \u00b6 Abbiamo gi\u00e0 visto questa metrica quando abbiamo parlato della regressione. L'errore quadratico medio \u00e8 definito come: \\[ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y}_i)^2 \\] Questo errore, implementato in Scikit Learn dalla funzione mean_squared_error() , permette di tenere conto di eventuali errori negativi e positivi, ma viene influenzato dalla grandezza assoluta delle variabili. In altre parole, un errore dell' \\(1\\%\\) su un valore \\(y=100\\) sar\u00e0 pi\u00f9 influente di un errore del \\(50\\%\\) su un valore \\(y=1\\) . Ovviamente, tanto \u00e8 minore l'MSE, tanto \u00e8 migliore il modello considerato. 18.4.2 - Mean Absolute Percentage Error (MAPE) \u00b6 Il mean absolute percentage error viene calcolato mediante il rapporto tra il valore assoluto della differenza tra i valori veri e quelli predetti dal regressore e i valori veri stessi. Tale rapporto viene quindi mediato sull'insieme dei campioni, e ne viene dedotta la percentuale. La formula \u00e8 la seguente: \\[ MAPE = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y_i - \\hat{y}_i|}{max (\\epsilon, y_i)} \\% \\] Il MAPE \u00e8 implementato in Scikit Learn mediante la funzione mean_average_percentage_error() . Il vantaggio principale derivante dall'uso del MAPE sta nel fatto che l'uso del valore assoluto elimina eventuali annullamenti derivanti da contributi di segno opposto. Inoltre, la presenza del valore vero a denominatore fa in modo che la metrica sia sensibile agli errori relativi. Anche in questo caso, un valore di MAPE basso indica un'ottima approssimazione. 18.4.3 - \\(R^2\\) e varianza spiegata \u00b6 Il valore \\(R^2\\) determina la proporzione della varianza del valore vero che viene spiegata dal modello. In pratica, ci permette di definire quanta della variabilit\u00e0 del fenomeno (ovvero, del modo in cui il fenomeno combina le \\(n\\) variabili indipendenti per ottenere le \\(m\\) variabili dipendenti) viene correttamente caratterizzata attraverso il modello considerato. Il valore di \\(R^2\\) pu\u00f2 oscillare tra \\(1\\) e \\(- \\infty\\) , ovvero tra la modellazione completa dell'intera variabilit\u00e0 del fenomeno ed un modello totalmente incorrelato allo stesso. Il valore di \\(R^2\\) \u00e8 definito come: \\[ R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i-avg(y_i))} \\] con: \\[ avg(y_i) = \\frac{1}{n} \\sum_{i=1}^n y_i \\] Il valore \\(R^2\\) \u00e8 modellato in Scikit Learn mediante la funzione r2_score() , ed in alcuni casi \u00e8 anche presente come metodo all'interno degli stimatori stessi.","title":"Dispense"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#18-metriche","text":"Abbiamo visto come la regressione logistca restituisca una probabilit\u00e0, che grazie alla classe LogisticRegression() viene automaticamente convertita in un valore relativo ad una classe. Torniamo al nostro spam detector. Un modello di regressione logistica che restituisca una probabilit\u00e0 \\(p = 0.999\\) ci sta dicendo che, molto probabilmente, questo \u00e8 di spam; di converso, se il modello restituisce \\(p = 0.003\\) allora \u00e8 molto probabile che il messaggio non sia spam. Cosa accade per\u00f2 nel caso in cui \\(p = 0.505\\) ?","title":"18 - Metriche"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#181-soglia-di-decisione","text":"L'esempio precedente ci fa comprendere come per passare da una probabilit\u00e0 ad una classe sia necessario definire una soglia di decisione : un valore oltre questa soglia indicher\u00e0, ad esempio, che la mail ricevuta \u00e8 di spam, mentre uno al di sotto della soglia ci suggerir\u00e0 che non lo \u00e8. Ovviamente, la tentazione potrebbe essere quella di presupporre che la soglia di decisione sia sempre pari a \\(0.5\\) : questo, ovviamente, non \u00e8 vero, in quanto la soglia dipende dal problema, ed \u00e8 un valore che bisogna stabilire in base al problema affrontato. Introduciamo alcune metriche che possono essere usate in tal senso.","title":"18.1 - Soglia di decisione"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#182-metriche-per-i-classificatori","text":"Continuiamo a concentrarci sul caso della classificazione dello spam, ed introduciamo il concetto di classe positiva e classe negativa . In particolare, la classe positiva sar\u00e0 rappresentata da tutte le mail di spam, mentre la classe negativa sar\u00e0 rappresentata dalle mail non spam. In tal senso, le predizioni del modello potranno essere di quattro tipi: nel primo caso, il modello classificher\u00e0 correttamente una mail di spam . In questo caso, si parla di vero positivo , o true positive (TP); nel secondo caso, il modello classificher\u00e0 correttamente una mail legittima . In questo caso, si parla di vero negativo , o true negative (TN); nel terzo caso, il modello classificher\u00e0 una mail di spam come legittima . In questo caso, si parla di falso negativo , o false negative (FN); nel quarto caso, il modello classificher\u00e0 una mail legittima come di spam . In questo caso, si parla di falso positivo , o false positive (FP). In pratica, un TP (TN) si ha quando il modello predice correttamente la classe positiva (negativa), mentre un FP (FN) si ha quando il modello predice in maniera non corretta la classe positiva (negativa).","title":"18.2 - Metriche per i classificatori"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#1821-accuratezza","text":"L' accuratezza \u00e8 la prima metrica che vedremo per la valutazione dei modelli di classificazione. Informalmente, possiamo definirla come la percentuale di predizioni corrette effettuate dal nostro modello, e definirla come: \\[ AC = \\frac{C}{T} \\] dove \\(C\\) \u00e8 il numero totale di predizioni corrette, mentre \\(T\\) \u00e8 il numero totale di predizioni. Nel caso della classificazione binaria, possiamo calcolare l'accuratezza come segue: \\[ AC = \\frac{TP + TN}{TP + TN + FP + FN} \\] Immaginiamo ad esempio di aver ricevuto \\(100\\) email, tra cui \\(10\\) di spam. Il nostro spam detector ha individuato correttamente \\(5\\) messaggi di spam, e classificato per sbaglio come spam \\(5\\) messaggi legittimi. Allora: \\[ AC = \\frac{TP+TN}{TP+TN+FP+FN}=\\frac{5+85}{5+85+5+5} \\] In questo caso, l'accuratezza del modello \u00e8 pari a \\(0.90\\) , o del \\(90\\%\\) , il che significa che il nostro modello \u00e8 in grado di fare \\(90\\) predizioni su \\(100\\) . Buon risultato, giusto? In realt\u00e0, non necessariamente. Infatti, delle mail che abbiamo ricevuto, \\(90\\) sono legittime, e \\(10\\) di spam. Questo significa che il modello \u00e8 stato in grado di individuare soltanto il \\(50\\%\\) dello spam ricevuto, ed ha inoltre classificato un buon \\(7\\%\\) delle email legittime come spam. Tra cui, prevedibilmente, quella che ci comunicava notizie di vitale importanza. In sostanza, il nostro modello ha un'efficacia \"vera e propria\" al pi\u00f9 in un caso su due. Di conseguenza, l'accuratezza non ci racconta \"tutta la storia\" quando lavoriamo su un dataset sbilanciato come questo, dove vi \u00e8 una disparit\u00e0 significativa tra la classe positiva e quella negativa.","title":"18.2.1 - Accuratezza"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#18211-accuratezza-in-scikit-learn","text":"L'accuratezza delle predizioni effettuate da un classificatore \u00e8 ottenuta in Scikit Learn utilizzando il metodo accuracy_score() . Ad esempio: from sklearn.metrics import accuracy_score clf = LogisticRegression ( max_iter = 1000 ) clf . fit ( X_train , y_train ) y_pred = clf . predict ( X_test ) accuracy_score ( y_test , y_pred )","title":"18.2.1.1 - Accuratezza in Scikit Learn"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#1822-la-precisione","text":"La precisione \u00e8 una metrica che prova a risolvere alcuni dei problemi dell'accuratezza valutando quale sia la proporzione di valori per la classe positiva identificati correttamente. La definizione analitica della precisione \u00e8 la seguente: \\[ PR = \\frac{TP}{TP+FP} \\] In pratica, riferendoci al nostro solito esempio, la precisione \u00e8 data dal rapporto tra le mail di spam riconosciute come tali e la somma tra queste e le mail legittime riconosciute come spam. Provando a calcolarla: \\[ PR = \\frac{5}{5+5} = 0.5 \\] Il modello ha quindi una precisione del \\(50\\%\\) nel riconoscere una mail di spam.","title":"18.2.2 - La precisione"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#18221-precisione-in-scikit-learn","text":"La precisione delle predizioni effettuate da un classificatore \u00e8 ottenuta in Scikit Learn utilizzando il metodo precision_score() . Ad esempio: from sklearn.metrics import precision_score precision_score ( y_test , y_pred )","title":"18.2.2.1 - Precisione in Scikit Learn"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#1823-il-recall","text":"Il recall , traducibile in italiano come richiamo , verifica la porzione di veri positivi correttamente identificata dall'algoritmo, ed \u00e8 espresso come: \\[ R = \\frac{TP}{TP+FN} \\] Nel nostro caso, il recall sar\u00e0 quindi dato dal rapporto tra le mail correttamente indicate come spam e la somma tra le stesse e quelle erroneamente indicate come legittime. Va da s\u00e8 che anche in questo caso possiamo calcolarlo: \\[ R = \\frac{5}{5+5} \\] Cos\u00ec come la precisione, il recall \u00e8 pari a \\(0.5\\) , ovvero \u00e8 del \\(50\\%\\) .","title":"18.2.3 - Il recall"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#18231-recall-in-scikit-learn","text":"Ovviamente, anche il recall ha una rappresentazione in Scikit Learn mediante la funzione recall_score() : from sklearn.metrics import recall_score recall_score ( y_test , y_pred )","title":"18.2.3.1 - Recall in Scikit Learn"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#183-tuning-della-soglia-di-decisione","text":"Per valutare l'effiacia del modello dobbiamo esaminare congiuntamente la precisione ed il recall. Sfortunatamente, questi due valori sono spesso in contrapposizione: spesso, infatti, migliorare la precisione riduce il recall, e viceversa. Per comprendere empiricamente questo concetto, facciamo un esempio con il nostro spam detector, immaginando di aver impostato la soglia di decisione a \\(0.6\\) . I risultati sono mostrati nella figura successiva. Calcoliamo la precisione e il recall in questo caso: \\[ P = \\frac{TP}{TP+FP}=\\frac{4}{4+1} = 0.8 \\\\ R = \\frac{TP}{TP+FN}=\\frac{4}{4+2} = 0.66 \\] Proviamo ad aumentare la soglia di decisione, portandola al \\(75\\%\\) . \\[ P = \\frac{TP}{TP+FP}=\\frac{3}{3} = 1 \\\\ R = \\frac{TP}{TP+FN}=\\frac{3}{3+3} = 0.5 \\] Proviamo infine a diminuire la soglia di decisione, portandola al \\(50%\\) . \\[ P = \\frac{TP}{TP+FP}=\\frac{4}{4+3} \\approx 0.57 \\\\ R = \\frac{TP}{TP+FN}=\\frac{4}{4+2} = 0.66 \\] Come possiamo vedere, la soglia di detection agisce su precisione e recall; non \u00e8 per\u00f2 possibile aumentarli contemporaneamente, per cui occorre scegliere un valore tale per cui, ad esempio, si massimizzi la media. La realt\u00e0 \u00e8 che, per\u00f2, dipende sempre dall'applicazione: se non abbiamo paura di perdere mail legittime, allora possiamo abbassare la soglia di decisione, aumentando il recall; viceversa, se siamo disposti ad eliminare manualmente un po' di spam, potremo alzare la soglia di decisione, aumentando la precisione.","title":"18.3 - Tuning della soglia di decisione"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#184-metriche-per-i-regressori","text":"Definiamo brevemente alcune delle metriche che \u00e8 possibile utilizzare per la valutazione delle performance di un modello di regressione.","title":"18.4 - Metriche per i regressori"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#1841-mean-squared-error-mse","text":"Abbiamo gi\u00e0 visto questa metrica quando abbiamo parlato della regressione. L'errore quadratico medio \u00e8 definito come: \\[ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y}_i)^2 \\] Questo errore, implementato in Scikit Learn dalla funzione mean_squared_error() , permette di tenere conto di eventuali errori negativi e positivi, ma viene influenzato dalla grandezza assoluta delle variabili. In altre parole, un errore dell' \\(1\\%\\) su un valore \\(y=100\\) sar\u00e0 pi\u00f9 influente di un errore del \\(50\\%\\) su un valore \\(y=1\\) . Ovviamente, tanto \u00e8 minore l'MSE, tanto \u00e8 migliore il modello considerato.","title":"18.4.1 - Mean Squared Error (MSE)"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#1842-mean-absolute-percentage-error-mape","text":"Il mean absolute percentage error viene calcolato mediante il rapporto tra il valore assoluto della differenza tra i valori veri e quelli predetti dal regressore e i valori veri stessi. Tale rapporto viene quindi mediato sull'insieme dei campioni, e ne viene dedotta la percentuale. La formula \u00e8 la seguente: \\[ MAPE = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y_i - \\hat{y}_i|}{max (\\epsilon, y_i)} \\% \\] Il MAPE \u00e8 implementato in Scikit Learn mediante la funzione mean_average_percentage_error() . Il vantaggio principale derivante dall'uso del MAPE sta nel fatto che l'uso del valore assoluto elimina eventuali annullamenti derivanti da contributi di segno opposto. Inoltre, la presenza del valore vero a denominatore fa in modo che la metrica sia sensibile agli errori relativi. Anche in questo caso, un valore di MAPE basso indica un'ottima approssimazione.","title":"18.4.2 - Mean Absolute Percentage Error (MAPE)"},{"location":"material/03_ml_sklearn/18_metrics/lecture/#1843-r2-e-varianza-spiegata","text":"Il valore \\(R^2\\) determina la proporzione della varianza del valore vero che viene spiegata dal modello. In pratica, ci permette di definire quanta della variabilit\u00e0 del fenomeno (ovvero, del modo in cui il fenomeno combina le \\(n\\) variabili indipendenti per ottenere le \\(m\\) variabili dipendenti) viene correttamente caratterizzata attraverso il modello considerato. Il valore di \\(R^2\\) pu\u00f2 oscillare tra \\(1\\) e \\(- \\infty\\) , ovvero tra la modellazione completa dell'intera variabilit\u00e0 del fenomeno ed un modello totalmente incorrelato allo stesso. Il valore di \\(R^2\\) \u00e8 definito come: \\[ R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i-avg(y_i))} \\] con: \\[ avg(y_i) = \\frac{1}{n} \\sum_{i=1}^n y_i \\] Il valore \\(R^2\\) \u00e8 modellato in Scikit Learn mediante la funzione r2_score() , ed in alcuni casi \u00e8 anche presente come metodo all'interno degli stimatori stessi.","title":"18.4.3 - \\(R^2\\) e varianza spiegata"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/exercises/","text":"E19 - Classificatori e regressori \u00b6 Esercizio E19.1 \u00b6 Operiamo sul problema visto nell' esercizio 17.1 usando un albero decisionale, un random forest ed un multilayer perceptron. Compariamo i risultati in termini di precisione, recall ed accuracy. Esercizio E19.2 \u00b6 Operiamo sul problema visto nell' esercizio 16.1 usando un albero decisionale, un random forest ed un multilayer perceptron. Compariamo i risultati in termini di errore quadratico medio usando la funzione mean_squared_error del package sklearn.metrics . Esercizio E19.3 \u00b6 Esploriamo i risultati ottenuti dall'albero decisionale nell' esercizio E19.1 . Per farlo, usiamo il metodo plot_tree del package sklearn.tree . Esercizio E19.4 \u00b6 Proviamo a variare leggermente alcuni parametri per i classificatori ed i regressori usati negli esercizi precedenti. Confrontiamo i risultati ottenuti nei termini delle metriche viste in precedenza. Soluzione Le soluzioni a questi esercizi sono contenute in questo notebook .","title":"Esercizi"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/exercises/#e19-classificatori-e-regressori","text":"","title":"E19 - Classificatori e regressori"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/exercises/#esercizio-e191","text":"Operiamo sul problema visto nell' esercizio 17.1 usando un albero decisionale, un random forest ed un multilayer perceptron. Compariamo i risultati in termini di precisione, recall ed accuracy.","title":"Esercizio E19.1"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/exercises/#esercizio-e192","text":"Operiamo sul problema visto nell' esercizio 16.1 usando un albero decisionale, un random forest ed un multilayer perceptron. Compariamo i risultati in termini di errore quadratico medio usando la funzione mean_squared_error del package sklearn.metrics .","title":"Esercizio E19.2"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/exercises/#esercizio-e193","text":"Esploriamo i risultati ottenuti dall'albero decisionale nell' esercizio E19.1 . Per farlo, usiamo il metodo plot_tree del package sklearn.tree .","title":"Esercizio E19.3"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/exercises/#esercizio-e194","text":"Proviamo a variare leggermente alcuni parametri per i classificatori ed i regressori usati negli esercizi precedenti. Confrontiamo i risultati ottenuti nei termini delle metriche viste in precedenza. Soluzione Le soluzioni a questi esercizi sono contenute in questo notebook .","title":"Esercizio E19.4"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/lecture/","text":"19 - Classificatori e regressori \u00b6 19.1 - Alberi decisionali \u00b6 Gli alberi decisionali creano un modello che predice una classe o un valore in output a partire da regole di tipo binario inferite dalle feature dei dati. Per far questo, utilizzano una tecnica chiamata recursive partitioning : in pratica, l'insieme di test viene suddiviso imponendo delle soglie sulle diverse variabili, le quali saranno modificate fino a che tutti i dati appartenenti ad una certa classe (o con valori simili di regressione) ricadono all'interno di uno stesso sottoinsieme. Gli alberi decisionali sono facili da interpretare, in quanto rappresentano una serie di regole binarie: un esempio \u00e8 mostrato nella seguente figura. Inoltre, non richiedono particolari accortezze in fase di preparazione dei dati, ed hanno una complessit\u00e0 computazionale di tipo logaritmico (e quindi abbastanza bassa). Dall'altro lato, per\u00f2, sono spesso soggetti ad overfitting, ed inoltre non assicurano una predizione continua, ma piuttosto un'approssimazione lineare a tratti. Scikit Learn implementa due versioni degli alberi decisionali: la prima \u00e8 dedicata alla classificazione, ed \u00e8 chiamata DecisionTreeClassifier() , mentre la seconda \u00e8 orientata alla regressione ed \u00e8 chiamata DecisionTreeRegressor() . 19.2 - Random forest \u00b6 I random forest sono dei metodi ensemble basati su alberi decisionali. Un metodo ensemble (letteralmente \"insieme\") permette di combinare i risultati provenienti da diversi algoritmi, ottenendo in generale risultati migliori. In particolare, il random forest sfrutta un insieme di alberi decisionali, ognuno dei quali modellato su un sottoinsieme di dati e feature presenti nel set di training; i risultati provenienti da ciascuno degli alberi saranno poi mediati e combinati. La presenza di queste due componenti di casualit\u00e0 permette di raggiungere un obiettivo ben preciso, ovvero diminuire l'overfitting proprio di un singolo albero decisionale, ottenendo un modello generalmente migliore. Anche per il random forest esistono due versioni, ovvero quella dedicata alla regressione ( RandomForestRegressor() ) e quella dedicata alla classificazione ( RandomForestClassifier() ). 19.3 - Multilayer perceptron \u00b6 Un multilayer perceptron \u00e8 il pi\u00f9 semplice modello di rete neurale che \u00e8 possibile concepire. Nella pratica, \u00e8 un algoritmo che considera una relazione del tipo: \\[ f:\\mathbb{R}^m \\rightarrow \\mathbb{R}^o \\] dove \\(m\\) \u00e8 il numero di input ed \\(o\\) \u00e8 il numero di dimensioni per l'output. Ad esempio, se avessimo un insieme di feature \\(X=x_1, x_2, \\ldots, x_m\\) ed un'output \\(y\\) , sia esso una classe o un valore di regressione, il multilayer perceptron apprender\u00e0 una funzione \\(f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^1\\) . Una rappresentazione del multilayer perceptron \u00e8 mostrata nella seguente figura. Nella pratica, il layer di input (a sinistra) consiste di un insieme di neuroni, uno per ogni feature. Ogni neurone nello strato nascosto trasforma i valori del layer precedente con una sommatoria pesata \\(w_1 x_1 + \\ldots + w_m x_m\\) seguita da una funzione di attivazione non lineare del tipo \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\) . La funzione di attivazione Le funzioni di attivazioni pi\u00f9 usate sono state per lungo tempo le sigmoidali e le loro varianti. Vedremo in seguito come negli ultimi anni quelle maggiormente gettonate siano diventate le ReLU. Nell'ultimo layer, infine, i valori ricevuti dal layer nascosto sono sommati e combinati nell'output. Ovviamente, Scikit Learn offre due varianti dell'algoritmo, quella per la classificazione ( MLPClassifier() ) e quella per la regressione ( MLPRegressor() )","title":"Dispense"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/lecture/#19-classificatori-e-regressori","text":"","title":"19 - Classificatori e regressori"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/lecture/#191-alberi-decisionali","text":"Gli alberi decisionali creano un modello che predice una classe o un valore in output a partire da regole di tipo binario inferite dalle feature dei dati. Per far questo, utilizzano una tecnica chiamata recursive partitioning : in pratica, l'insieme di test viene suddiviso imponendo delle soglie sulle diverse variabili, le quali saranno modificate fino a che tutti i dati appartenenti ad una certa classe (o con valori simili di regressione) ricadono all'interno di uno stesso sottoinsieme. Gli alberi decisionali sono facili da interpretare, in quanto rappresentano una serie di regole binarie: un esempio \u00e8 mostrato nella seguente figura. Inoltre, non richiedono particolari accortezze in fase di preparazione dei dati, ed hanno una complessit\u00e0 computazionale di tipo logaritmico (e quindi abbastanza bassa). Dall'altro lato, per\u00f2, sono spesso soggetti ad overfitting, ed inoltre non assicurano una predizione continua, ma piuttosto un'approssimazione lineare a tratti. Scikit Learn implementa due versioni degli alberi decisionali: la prima \u00e8 dedicata alla classificazione, ed \u00e8 chiamata DecisionTreeClassifier() , mentre la seconda \u00e8 orientata alla regressione ed \u00e8 chiamata DecisionTreeRegressor() .","title":"19.1 - Alberi decisionali"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/lecture/#192-random-forest","text":"I random forest sono dei metodi ensemble basati su alberi decisionali. Un metodo ensemble (letteralmente \"insieme\") permette di combinare i risultati provenienti da diversi algoritmi, ottenendo in generale risultati migliori. In particolare, il random forest sfrutta un insieme di alberi decisionali, ognuno dei quali modellato su un sottoinsieme di dati e feature presenti nel set di training; i risultati provenienti da ciascuno degli alberi saranno poi mediati e combinati. La presenza di queste due componenti di casualit\u00e0 permette di raggiungere un obiettivo ben preciso, ovvero diminuire l'overfitting proprio di un singolo albero decisionale, ottenendo un modello generalmente migliore. Anche per il random forest esistono due versioni, ovvero quella dedicata alla regressione ( RandomForestRegressor() ) e quella dedicata alla classificazione ( RandomForestClassifier() ).","title":"19.2 - Random forest"},{"location":"material/03_ml_sklearn/19_classifiers_regressors/lecture/#193-multilayer-perceptron","text":"Un multilayer perceptron \u00e8 il pi\u00f9 semplice modello di rete neurale che \u00e8 possibile concepire. Nella pratica, \u00e8 un algoritmo che considera una relazione del tipo: \\[ f:\\mathbb{R}^m \\rightarrow \\mathbb{R}^o \\] dove \\(m\\) \u00e8 il numero di input ed \\(o\\) \u00e8 il numero di dimensioni per l'output. Ad esempio, se avessimo un insieme di feature \\(X=x_1, x_2, \\ldots, x_m\\) ed un'output \\(y\\) , sia esso una classe o un valore di regressione, il multilayer perceptron apprender\u00e0 una funzione \\(f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^1\\) . Una rappresentazione del multilayer perceptron \u00e8 mostrata nella seguente figura. Nella pratica, il layer di input (a sinistra) consiste di un insieme di neuroni, uno per ogni feature. Ogni neurone nello strato nascosto trasforma i valori del layer precedente con una sommatoria pesata \\(w_1 x_1 + \\ldots + w_m x_m\\) seguita da una funzione di attivazione non lineare del tipo \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\) . La funzione di attivazione Le funzioni di attivazioni pi\u00f9 usate sono state per lungo tempo le sigmoidali e le loro varianti. Vedremo in seguito come negli ultimi anni quelle maggiormente gettonate siano diventate le ReLU. Nell'ultimo layer, infine, i valori ricevuti dal layer nascosto sono sommati e combinati nell'output. Ovviamente, Scikit Learn offre due varianti dell'algoritmo, quella per la classificazione ( MLPClassifier() ) e quella per la regressione ( MLPRegressor() )","title":"19.3 - Multilayer perceptron"},{"location":"material/03_ml_sklearn/20_clustering/exercises/","text":"E20 - Il clustering \u00b6 Esercizio E20.1 \u00b6 Il dataset Iris contiene i dati riguardanti lunghezza ed ampiezza di steli e petali per tre classi di fiori, ed \u00e8 uno dei dataset \"standard\" per l'analisi dei dati nel machine learning. In tal senso, usiamo il metodo load_iris del package datasets di Scikit Learn per caricarlo. Una volta caricato in memoria, proviamo ad effettuare un primo clustering usando l'algoritmo k-means con 3 cluster. Esercizio E20.2 \u00b6 Verificare il valore di magnitudine e cardinalit\u00e0 per i cluster identificati nell'esercizio precedente. Esercizio E20.3 \u00b6 Valutiamo il valore migliore per il numero di cluster da utilizzare per il K-means utilizzando il dataset Iris e l'approccio empirico discusso a lezione. Usiamo valori per il clustering compresi tra 2 e 4. Esercizio E20.4 \u00b6 Il K-Means parte da alcune ipotesi sulla natura dei diversi cluster in cui sono organizzati i dati, ovvero che questi siano: isotropi , e quindi che abbiano una forma \"identica\" in tutte le direzioni; ad eguale varianza , e quindi che non vi siano dei cluster di varianza sensibilmente superiore o inferiore alla varianza media dell'insieme degli stessi; ad eguale cardinalit\u00e0 , e quindi che il numero di campioni per i diversi cluster sia all'incirca costante. Verifichiamo questi assunti su dati generati dal metodo make_blobs() . Note Per ottenere l'anisotropia, potete applicare una rotazione all'insieme dei dati. Questa pu\u00f2 essere definita in questo modo: t = np . tan ( np . radians ( 60 )) rot = np . array ([[ 1 , t ], [ 0 , 1 ]]) X_an = X . dot ( rot ) Per ottenere dei cluster a diversa cardinalit\u00e0, proviamo a selezionare diversi sottoinsiemi dei dati originari in base al loro valore di y . Esercizio E20.5 \u00b6 Proviamo ad utilizzare l'algoritmo DBSCAN, implementato mediante la classe DBSCAN() del package cluster , nelle tre diverse situazioni ispirate dall'esercizio precedente. Soluzione Le soluzioni a questi esercizi sono contenute in questo notebook .","title":"Esercizi"},{"location":"material/03_ml_sklearn/20_clustering/exercises/#e20-il-clustering","text":"","title":"E20 - Il clustering"},{"location":"material/03_ml_sklearn/20_clustering/exercises/#esercizio-e201","text":"Il dataset Iris contiene i dati riguardanti lunghezza ed ampiezza di steli e petali per tre classi di fiori, ed \u00e8 uno dei dataset \"standard\" per l'analisi dei dati nel machine learning. In tal senso, usiamo il metodo load_iris del package datasets di Scikit Learn per caricarlo. Una volta caricato in memoria, proviamo ad effettuare un primo clustering usando l'algoritmo k-means con 3 cluster.","title":"Esercizio E20.1"},{"location":"material/03_ml_sklearn/20_clustering/exercises/#esercizio-e202","text":"Verificare il valore di magnitudine e cardinalit\u00e0 per i cluster identificati nell'esercizio precedente.","title":"Esercizio E20.2"},{"location":"material/03_ml_sklearn/20_clustering/exercises/#esercizio-e203","text":"Valutiamo il valore migliore per il numero di cluster da utilizzare per il K-means utilizzando il dataset Iris e l'approccio empirico discusso a lezione. Usiamo valori per il clustering compresi tra 2 e 4.","title":"Esercizio E20.3"},{"location":"material/03_ml_sklearn/20_clustering/exercises/#esercizio-e204","text":"Il K-Means parte da alcune ipotesi sulla natura dei diversi cluster in cui sono organizzati i dati, ovvero che questi siano: isotropi , e quindi che abbiano una forma \"identica\" in tutte le direzioni; ad eguale varianza , e quindi che non vi siano dei cluster di varianza sensibilmente superiore o inferiore alla varianza media dell'insieme degli stessi; ad eguale cardinalit\u00e0 , e quindi che il numero di campioni per i diversi cluster sia all'incirca costante. Verifichiamo questi assunti su dati generati dal metodo make_blobs() . Note Per ottenere l'anisotropia, potete applicare una rotazione all'insieme dei dati. Questa pu\u00f2 essere definita in questo modo: t = np . tan ( np . radians ( 60 )) rot = np . array ([[ 1 , t ], [ 0 , 1 ]]) X_an = X . dot ( rot ) Per ottenere dei cluster a diversa cardinalit\u00e0, proviamo a selezionare diversi sottoinsiemi dei dati originari in base al loro valore di y .","title":"Esercizio E20.4"},{"location":"material/03_ml_sklearn/20_clustering/exercises/#esercizio-e205","text":"Proviamo ad utilizzare l'algoritmo DBSCAN, implementato mediante la classe DBSCAN() del package cluster , nelle tre diverse situazioni ispirate dall'esercizio precedente. Soluzione Le soluzioni a questi esercizi sono contenute in questo notebook .","title":"Esercizio E20.5"},{"location":"material/03_ml_sklearn/20_clustering/lecture/","text":"20 - Il clustering \u00b6 Il clustering \u00e8 l'operazione di categorizzare dei campioni in un dataset senza che questi abbiano necessariamente un'etichetta determinata a priori. Per fare un esempio, potremmo suddividere i nostri album musicali sulla base delle sonorit\u00e0 ispirate dal loro ascolto: in questo caso, non ci staremmo affidando ad una certa \"etichetta\", come ad esempio l'anno di produzione o l'artista, ma ad un concetto molto pi\u00f9 \"empirico\", ovvero la vicinanza o meno dell'album ai nostri gusti musicali. Ovviamente, dato che nel clustering i campioni non considerano una label, stiamo parlando di apprendimento non supervisionato. Se i campioni fossero etichettati, avremmo una normale procedura di classificazione. Il clustering pu\u00f2 avere numerose applicazioni: ad esempio, potrebbe essere usato per segmentare il mercato mediante dei profili di clientela simili, oppure per suddividere le immagini in zone simili, o ancora per individuare delle anomalie all'interno di un insieme di dati. Una volta che il clustering \u00e8 completo, ad ogni cluster viene assegnato un certo identificativo , che ci permette in qualche modo di \"condensare\" e \"riassumere\" le informazioni dell'intero cluster. Quest'assegnazione pu\u00f2 anche essere usata come ingresso ad altri sistemi di machine learning, ad esempio di classificazione, che possono usare l'identificativo assegnato come una vera e propria label. 20.1 - Tipi di clustering \u00b6 La scelta di un algoritmo di clustering deve essere condotta sulla base della scalabilit\u00e0 dello stesso. Infatti, laddove alcuni algoritmi di clustering confrontano tra loro ogni possibile coppia di dati, con una complessit\u00e0 \\(O(n^2)\\) per \\(n\\) campioni, altri, come il k-means, effettuano un numero molto pi\u00f9 limitato di operazioni, ottenendo una complessit\u00e0 nell'ordine di \\(O(n)\\) , il che cambia radicalmente la situazione nel caso di dataset con milioni di campioni. Tuttavia, ogni algoritmo ha anche diversi vantaggi e svantaggi che devono essere valutati sulla base dell'applicazione scelta. In generale, abbiamo quattro diverse categorie di clustering: nel centroid-based clustering , i dati sono organizzati secondo la loro distanza da dei centroidi , ovvero dei campioni considerati come \"base\" per ciascun cluster. Questo tipo di algoritmi risulta essere mediamente efficace, ma \u00e8 sensibile alle condizioni iniziali ed alla presenza di eventuali outliers; nel density-based clustering , i dati sono organizzati in aree ad alta densit\u00e0. Ci\u00f2 permette la connessione di cluster di forma arbitraria, e facilita inoltre l'individuazione di outlier, che per definizione sono nelle zone a minore densit\u00e0 di campioni. Possono per\u00f2 essere sensibili a dataset con densit\u00e0 variabile ed alta dimensionalit\u00e0; nel distribution-based clustering , si suppone che i dati abbiano distribuzione gaussiana, e siano quindi suddivisibili come tali. Questo tipo di algoritmi non \u00e8 efficiente se non si conosce a priori il tipo di distribuzione dei dati; nello hierarchical clustering viene creato un albero a partire dai dati. Questo tipo di clustering \u00e8 particolarmente efficace nel caso si trattino certi tipi di dati, come ad esempio le tassonomie, e prevede che possa essere selezionato un numero ridotto di cluster tagliando l'albero al giusto livello. 20.2 - Workflow del clustering \u00b6 L'esecuzione di un algoritmo di clustering prevede tre step: nel primo, dobbiamo preparare i dati , effettuando le operazioni che abbiamo visto in precedenza per la classificazione e la regressione; nel secondo, dovremo definire una metrica di similarit\u00e0 ; nel terzo, eseguiremo l'algoritmo vero e proprio. Concentriamoci per un attimo sul secondo step. Definire una metrica di similarit\u00e0 significa nella pratica stabilire quando due campioni risultano essere simili tra loro. In tal senso, \u00e8 possibile operare in due modi: la metrica pu\u00f2 essere scelta manualmente , ovvero scegliendo le feature da considerare nella valutazione della distanza tra i campioni; oppure, la metrica pu\u00f2 essere scelta in maniera automatica a partire da un embedding , ovvero da una rappresentazione a dimensionalit\u00e0 ridotta del dato iniziale. Nel primo caso questo avviene in modo abbastanza intuitivo: se, ad esempio, volessimo suddividere un insieme di scarpe in base a taglia e prezzo, potremmo considerare la distanza euclidea come rappresentativa dello \"spazio\" che intercorre tra due campioni. Questo approccio, tuttavia, \u00e8 efficace soltanto nel caso di campioni a bassa dimensionalit\u00e0. Il secondo caso \u00e8 invece preferibile nel momento in cui si vanno a considerare dei dati ad alta dimensionalit\u00e0: infatti, in queste situazioni si rischia di incorrere nel fenomeno della curse of dimensionality , che rende difficile distinguere tra due campioni differenti, per cui si tende ad estrarre delle rappresentazioni \"ridotte\" dei dati a partire dalle quali applicare il concetto di distanza. 20.3 - Applicazione di un algoritmo di clustering: il K-Means \u00b6 Vediamo adesso come usare il pi\u00f9 conosciuto ed utilizzato algoritmo di clustering, ovvero il k-means , algoritmo centroid-based che raggruppa i campioni in \\(k\\) diversi cluster assegnando ogni dato in base alla distanza dal centroide del cluster stesso. Il k-means ha diverse ipotesi alla base, tra cui la pi\u00f9 restrittiva \u00e8 una, ovvero quella legata alla conoscenza del numero iniziale di cluster \\(k\\) . Una volta fissato questo valore, l'algoritmo lavora in tre step successivi: al primo step, l'algoritmo sceglie casualmente \\(k\\) centroidi tra i diversi dati a disposizione; al secondo step, l'algoritmo assegna ogni punto al centroide pi\u00f9 vicino, definendo i \\(k\\) cluster iniziali; al terzo step, l'algoritmo ricalcola il centroide considerando il valore medio di tutti i punti del cluster, e ritorna allo step 2. Il k-means proseguir\u00e0 fino a che i cluster calcolati al punto 2 non saranno stabili o, nei casi pi\u00f9 complessi, fino a che non sar\u00e0 raggiunto il numero massimo di iterazioni impostato in fase di inizializzazione. In figura possiamo osservare una spiegazione visiva del funzionamento dell'algoritmo. Figura 20.1 - Step dell'algoritmo K-Means 20.4 - Clustering in Scikit Learn \u00b6 Per implementare un algoritmo di clustering in Scikit Learn dovremo fare affidamento sulla classe KMeans() , utilizzabile come segue: from sklearn.cluster import KMeans import numpy as np X = np . array ([[ 1 , 2 ], [ 2 , 1 ], [ 5 , 2 ], [ 3 , 3 ]]) cl = KMeans () cl . fit ( X ) 20.5 - Scelta del valore ottimale di cluster \u00b6 La scelta del valore ottimale di \\(k\\) \u00e8 un procedimento emnpirico, in quanto non abbiamo a disposizione delle vere e proprie label per la verifica dell'uscita dell'algoritmo. In tal senso, abbiamo a disposizione sia delle metriche, che vedremo in seguito, sia degli approcci pi\u00f9 qualitativi, che dipendono dai concetti di cardinalit\u00e0 e magnitudine del clustering. In particolare, per cardinalit\u00e0 si intende il numero di campioni per ogni cluster, mentre per magnitudine la somma delle distanze di tutti i campioni in un cluster dal centroide. Immaginiamo di essere in un caso come quello descritto nella seguente figura. Figura 20.2 - Rapporto tra cardinalit\u00e0 e magnitudine dei cluster Prevedibilmente, il rapporto tra cardinalit\u00e0 e magnitudine dovrebbe essere all'incirca lineare. Quindi, come si pu\u00f2 vedere dalla figura precedente, ci potrebbe essere qualcosa che non va con il cluster \\(4\\) . A questo punto, avendo valutato empiricamente la possibile presenza di un problema qualitativo con il clustering, possiamo provare ad eseguire l'algoritmo per un valore crescente di \\(k\\) . Proviamo a plottare questo valore in rapporto alla somma delle magnitudini del risultato, che diminuir\u00e0 all'aumentare di \\(k\\) ; un valore ottimale per \\(k\\) \u00e8 quello che si ottiene quando questo grafico tende a stabilizzarsi, ad esempio considerando il valore per cui la derivata diventa maggiore di -1 (e quindi l'angolo della funzione dei \\(k\\) \u00e8 maggiore di \\(135\u00b0\\) ). Figura 20.3 - Rapporto tra il numero dei cluster e la magnitudine 20.6 - Un altro algoritmo: il DBSCAN \u00b6 Il DBSCAN \u00e8 un algoritmo di clustering di tipo agglomerativo density-based che opera considerando due parametri principali: la distanza massima \\(\\epsilon\\) per considerare due punti come appartenenti allo stesso cluster; il numero minimo di campioni \\(m\\) per il quale \u00e8 possibile definire un cluster. Nella pratica, il DBSCAN seleziona un campione casuale tra quelli non visitati, e valuta se ci sono \\(m\\) campioni all'interno della distanza \\(\\epsilon\\) , nel qual caso si ha un core point . In alternativa, se il numero di campioni presenti in \\(\\epsilon\\) \u00e8 minore di \\(m\\) , ma comunque maggiore di 0, i campioni si dicono \\(density reachable\\) e, se connessi ad un core point , appartengono allo stesso cluster. Infine, se non vi sono campioni presenti in \\(\\epsilon\\) , allora il punto \u00e8 isolato, ed \u00e8 interpretato come un outlier. Un'interpretazione visiva \u00e8 quella proposta in figura; in particolare, i punti in rosso definiscono diversi core points , i punti in giallo sono density reachable , e quindi fanno parte dello stesso cluster dei core points, mentre \\(N\\) \u00e8 un outlier. Figura 20.4 - Algoritmo DBSCAN. Di Chire - Opera propria, CC BY-SA 3.0, Wikipedia","title":"Dispense"},{"location":"material/03_ml_sklearn/20_clustering/lecture/#20-il-clustering","text":"Il clustering \u00e8 l'operazione di categorizzare dei campioni in un dataset senza che questi abbiano necessariamente un'etichetta determinata a priori. Per fare un esempio, potremmo suddividere i nostri album musicali sulla base delle sonorit\u00e0 ispirate dal loro ascolto: in questo caso, non ci staremmo affidando ad una certa \"etichetta\", come ad esempio l'anno di produzione o l'artista, ma ad un concetto molto pi\u00f9 \"empirico\", ovvero la vicinanza o meno dell'album ai nostri gusti musicali. Ovviamente, dato che nel clustering i campioni non considerano una label, stiamo parlando di apprendimento non supervisionato. Se i campioni fossero etichettati, avremmo una normale procedura di classificazione. Il clustering pu\u00f2 avere numerose applicazioni: ad esempio, potrebbe essere usato per segmentare il mercato mediante dei profili di clientela simili, oppure per suddividere le immagini in zone simili, o ancora per individuare delle anomalie all'interno di un insieme di dati. Una volta che il clustering \u00e8 completo, ad ogni cluster viene assegnato un certo identificativo , che ci permette in qualche modo di \"condensare\" e \"riassumere\" le informazioni dell'intero cluster. Quest'assegnazione pu\u00f2 anche essere usata come ingresso ad altri sistemi di machine learning, ad esempio di classificazione, che possono usare l'identificativo assegnato come una vera e propria label.","title":"20 - Il clustering"},{"location":"material/03_ml_sklearn/20_clustering/lecture/#201-tipi-di-clustering","text":"La scelta di un algoritmo di clustering deve essere condotta sulla base della scalabilit\u00e0 dello stesso. Infatti, laddove alcuni algoritmi di clustering confrontano tra loro ogni possibile coppia di dati, con una complessit\u00e0 \\(O(n^2)\\) per \\(n\\) campioni, altri, come il k-means, effettuano un numero molto pi\u00f9 limitato di operazioni, ottenendo una complessit\u00e0 nell'ordine di \\(O(n)\\) , il che cambia radicalmente la situazione nel caso di dataset con milioni di campioni. Tuttavia, ogni algoritmo ha anche diversi vantaggi e svantaggi che devono essere valutati sulla base dell'applicazione scelta. In generale, abbiamo quattro diverse categorie di clustering: nel centroid-based clustering , i dati sono organizzati secondo la loro distanza da dei centroidi , ovvero dei campioni considerati come \"base\" per ciascun cluster. Questo tipo di algoritmi risulta essere mediamente efficace, ma \u00e8 sensibile alle condizioni iniziali ed alla presenza di eventuali outliers; nel density-based clustering , i dati sono organizzati in aree ad alta densit\u00e0. Ci\u00f2 permette la connessione di cluster di forma arbitraria, e facilita inoltre l'individuazione di outlier, che per definizione sono nelle zone a minore densit\u00e0 di campioni. Possono per\u00f2 essere sensibili a dataset con densit\u00e0 variabile ed alta dimensionalit\u00e0; nel distribution-based clustering , si suppone che i dati abbiano distribuzione gaussiana, e siano quindi suddivisibili come tali. Questo tipo di algoritmi non \u00e8 efficiente se non si conosce a priori il tipo di distribuzione dei dati; nello hierarchical clustering viene creato un albero a partire dai dati. Questo tipo di clustering \u00e8 particolarmente efficace nel caso si trattino certi tipi di dati, come ad esempio le tassonomie, e prevede che possa essere selezionato un numero ridotto di cluster tagliando l'albero al giusto livello.","title":"20.1 - Tipi di clustering"},{"location":"material/03_ml_sklearn/20_clustering/lecture/#202-workflow-del-clustering","text":"L'esecuzione di un algoritmo di clustering prevede tre step: nel primo, dobbiamo preparare i dati , effettuando le operazioni che abbiamo visto in precedenza per la classificazione e la regressione; nel secondo, dovremo definire una metrica di similarit\u00e0 ; nel terzo, eseguiremo l'algoritmo vero e proprio. Concentriamoci per un attimo sul secondo step. Definire una metrica di similarit\u00e0 significa nella pratica stabilire quando due campioni risultano essere simili tra loro. In tal senso, \u00e8 possibile operare in due modi: la metrica pu\u00f2 essere scelta manualmente , ovvero scegliendo le feature da considerare nella valutazione della distanza tra i campioni; oppure, la metrica pu\u00f2 essere scelta in maniera automatica a partire da un embedding , ovvero da una rappresentazione a dimensionalit\u00e0 ridotta del dato iniziale. Nel primo caso questo avviene in modo abbastanza intuitivo: se, ad esempio, volessimo suddividere un insieme di scarpe in base a taglia e prezzo, potremmo considerare la distanza euclidea come rappresentativa dello \"spazio\" che intercorre tra due campioni. Questo approccio, tuttavia, \u00e8 efficace soltanto nel caso di campioni a bassa dimensionalit\u00e0. Il secondo caso \u00e8 invece preferibile nel momento in cui si vanno a considerare dei dati ad alta dimensionalit\u00e0: infatti, in queste situazioni si rischia di incorrere nel fenomeno della curse of dimensionality , che rende difficile distinguere tra due campioni differenti, per cui si tende ad estrarre delle rappresentazioni \"ridotte\" dei dati a partire dalle quali applicare il concetto di distanza.","title":"20.2 - Workflow del clustering"},{"location":"material/03_ml_sklearn/20_clustering/lecture/#203-applicazione-di-un-algoritmo-di-clustering-il-k-means","text":"Vediamo adesso come usare il pi\u00f9 conosciuto ed utilizzato algoritmo di clustering, ovvero il k-means , algoritmo centroid-based che raggruppa i campioni in \\(k\\) diversi cluster assegnando ogni dato in base alla distanza dal centroide del cluster stesso. Il k-means ha diverse ipotesi alla base, tra cui la pi\u00f9 restrittiva \u00e8 una, ovvero quella legata alla conoscenza del numero iniziale di cluster \\(k\\) . Una volta fissato questo valore, l'algoritmo lavora in tre step successivi: al primo step, l'algoritmo sceglie casualmente \\(k\\) centroidi tra i diversi dati a disposizione; al secondo step, l'algoritmo assegna ogni punto al centroide pi\u00f9 vicino, definendo i \\(k\\) cluster iniziali; al terzo step, l'algoritmo ricalcola il centroide considerando il valore medio di tutti i punti del cluster, e ritorna allo step 2. Il k-means proseguir\u00e0 fino a che i cluster calcolati al punto 2 non saranno stabili o, nei casi pi\u00f9 complessi, fino a che non sar\u00e0 raggiunto il numero massimo di iterazioni impostato in fase di inizializzazione. In figura possiamo osservare una spiegazione visiva del funzionamento dell'algoritmo. Figura 20.1 - Step dell'algoritmo K-Means","title":"20.3 - Applicazione di un algoritmo di clustering: il K-Means"},{"location":"material/03_ml_sklearn/20_clustering/lecture/#204-clustering-in-scikit-learn","text":"Per implementare un algoritmo di clustering in Scikit Learn dovremo fare affidamento sulla classe KMeans() , utilizzabile come segue: from sklearn.cluster import KMeans import numpy as np X = np . array ([[ 1 , 2 ], [ 2 , 1 ], [ 5 , 2 ], [ 3 , 3 ]]) cl = KMeans () cl . fit ( X )","title":"20.4 - Clustering in Scikit Learn"},{"location":"material/03_ml_sklearn/20_clustering/lecture/#205-scelta-del-valore-ottimale-di-cluster","text":"La scelta del valore ottimale di \\(k\\) \u00e8 un procedimento emnpirico, in quanto non abbiamo a disposizione delle vere e proprie label per la verifica dell'uscita dell'algoritmo. In tal senso, abbiamo a disposizione sia delle metriche, che vedremo in seguito, sia degli approcci pi\u00f9 qualitativi, che dipendono dai concetti di cardinalit\u00e0 e magnitudine del clustering. In particolare, per cardinalit\u00e0 si intende il numero di campioni per ogni cluster, mentre per magnitudine la somma delle distanze di tutti i campioni in un cluster dal centroide. Immaginiamo di essere in un caso come quello descritto nella seguente figura. Figura 20.2 - Rapporto tra cardinalit\u00e0 e magnitudine dei cluster Prevedibilmente, il rapporto tra cardinalit\u00e0 e magnitudine dovrebbe essere all'incirca lineare. Quindi, come si pu\u00f2 vedere dalla figura precedente, ci potrebbe essere qualcosa che non va con il cluster \\(4\\) . A questo punto, avendo valutato empiricamente la possibile presenza di un problema qualitativo con il clustering, possiamo provare ad eseguire l'algoritmo per un valore crescente di \\(k\\) . Proviamo a plottare questo valore in rapporto alla somma delle magnitudini del risultato, che diminuir\u00e0 all'aumentare di \\(k\\) ; un valore ottimale per \\(k\\) \u00e8 quello che si ottiene quando questo grafico tende a stabilizzarsi, ad esempio considerando il valore per cui la derivata diventa maggiore di -1 (e quindi l'angolo della funzione dei \\(k\\) \u00e8 maggiore di \\(135\u00b0\\) ). Figura 20.3 - Rapporto tra il numero dei cluster e la magnitudine","title":"20.5 - Scelta del valore ottimale di cluster"},{"location":"material/03_ml_sklearn/20_clustering/lecture/#206-un-altro-algoritmo-il-dbscan","text":"Il DBSCAN \u00e8 un algoritmo di clustering di tipo agglomerativo density-based che opera considerando due parametri principali: la distanza massima \\(\\epsilon\\) per considerare due punti come appartenenti allo stesso cluster; il numero minimo di campioni \\(m\\) per il quale \u00e8 possibile definire un cluster. Nella pratica, il DBSCAN seleziona un campione casuale tra quelli non visitati, e valuta se ci sono \\(m\\) campioni all'interno della distanza \\(\\epsilon\\) , nel qual caso si ha un core point . In alternativa, se il numero di campioni presenti in \\(\\epsilon\\) \u00e8 minore di \\(m\\) , ma comunque maggiore di 0, i campioni si dicono \\(density reachable\\) e, se connessi ad un core point , appartengono allo stesso cluster. Infine, se non vi sono campioni presenti in \\(\\epsilon\\) , allora il punto \u00e8 isolato, ed \u00e8 interpretato come un outlier. Un'interpretazione visiva \u00e8 quella proposta in figura; in particolare, i punti in rosso definiscono diversi core points , i punti in giallo sono density reachable , e quindi fanno parte dello stesso cluster dei core points, mentre \\(N\\) \u00e8 un outlier. Figura 20.4 - Algoritmo DBSCAN. Di Chire - Opera propria, CC BY-SA 3.0, Wikipedia","title":"20.6 - Un altro algoritmo: il DBSCAN"},{"location":"material/03_ml_sklearn/21_cls_metrics/exercises/","text":"E21 - Metriche di clustering \u00b6 Esercizio E21.1 \u00b6 Ricreiamo le condizioni sperimentali degli esercizi E20.4 ed E20.5 . Stavolta, per\u00f2, valutiamo le performance di ogni algoritmo utilizzando l'ARI ed il silhouette score. Inoltre, proviamo a vedere cosa accade per i seguenti parametri: per il K-Means, facciamo variare il numero di cluster tra 2 e 5 ; per il DBSCAN, assegnamo ad \\(\\epsilon\\) i valori 0.5 o 1.0 , ed a min_samples i valori 5 e 10 . Per ognuno dei due algoritmi, infine, riportiamo a schermo solo i valori dei parametri per i quali le metriche assumono valore massimo. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizi"},{"location":"material/03_ml_sklearn/21_cls_metrics/exercises/#e21-metriche-di-clustering","text":"","title":"E21 - Metriche di clustering"},{"location":"material/03_ml_sklearn/21_cls_metrics/exercises/#esercizio-e211","text":"Ricreiamo le condizioni sperimentali degli esercizi E20.4 ed E20.5 . Stavolta, per\u00f2, valutiamo le performance di ogni algoritmo utilizzando l'ARI ed il silhouette score. Inoltre, proviamo a vedere cosa accade per i seguenti parametri: per il K-Means, facciamo variare il numero di cluster tra 2 e 5 ; per il DBSCAN, assegnamo ad \\(\\epsilon\\) i valori 0.5 o 1.0 , ed a min_samples i valori 5 e 10 . Per ognuno dei due algoritmi, infine, riportiamo a schermo solo i valori dei parametri per i quali le metriche assumono valore massimo. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizio E21.1"},{"location":"material/03_ml_sklearn/21_cls_metrics/lecture/","text":"21 - Metriche di clustering \u00b6 Cos\u00ec come per la regressione e la classificazione, esistono delle metriche appositamente progettate per valutare la qualit\u00e0 dei risultati ottenuti da un algoritmo di clustering. Nello specifico, valuteremo l' adjusted rand index ed il silhouette score . 21.1 - Adjusted Rand Index \u00b6 Sia \\(C\\) l'insieme dei cluster \"veri\" assegnati ad un certo dataset, e \\(K\\) l'insieme dei cluster assegnati a valle dell'applicazione di un algoritmo di clustering. Allora definiamo l'indice di Rand come: \\[ RI = \\frac{a + b}{C_2^n} \\] dove: \\(a\\) \u00e8 il numero di coppie di campioni che appartengono allo stesso cluster sia in \\(C\\) sia a valle dell'assegnazione \\(K\\) ; \\(b\\) \u00e8 il numero di coppie di campioni che non appartengono allo stesso cluster sia in \\(C\\) sia a valle dell'assegnazione \\(K\\) ; \\(C_2^n\\) \u00e8 il numero totale di coppie di campioni presenti nel dataset. In pratica, se: \\[ s = [s_1, s_2, s_3, s_4, s_5] \\\\ C = [s_1, s_2], [s_3, s_4, s_5] \\\\ K = [s_1, s_2, s_3], [s_4, s_5] \\\\ \\] allora: \\[ a = |(s_1, s_2), (s_4, s_5)| = 2 \\\\ b = |(s_1, s_4), (s_1, s_5), (s_4, s_2), (s_5, s_2)| = 4 \\\\ C_2^n = \\frac{|s|*|s-1|}{2} = 5 * 2 = 10 \\] Di conseguenza, \\(RI=\\frac{6}{10}=0.6\\) . Si pu\u00f2 dimostrare non \u00e8 garantito che l'indice di Rand assuma valore vicino allo zero a seguito di un'assegnazione completamente casuale dei cluster da parte dell'algoritmo. Possiamo quindi tenere conto dell'aspettazione \\(E[RI]\\) di ottenere un'assegnazione casuale mediante l' indice di Rand modificato : \\[ ARI = \\frac{RI - E[RI]}{max(RI) - E[RI]} \\] In Scikit Learn, l'indice di Rand modificato \u00e8 ottenuto usando la funzione adjusted_rand_score() del package metrics . Il valore ottimale dell'ARI \u00e8 pari proprio ad 1, caso in cui il clustering \u00e8 riuscito a predire correttamente tutte le classi dei singoli campioni. Valori prossimi allo zero o negativi (fino a -1) contraddistinguono invece labeling non corretti. Una metrica di questo tipo ha l'ovvio vantaggio di essere facilmente interpretabile, oltre che di non essere collegata ad uno specifico algoritmo di clustering. Tuttavia, vi \u00e8 una criticit\u00e0 indotta dalla necessit\u00e0 di conoscere a priori il labeling esatto dei campioni (il che, quindi, potrebbe farci propendere per l'uso di un algoritmo di classificazione). 21.2 - Silhouette Score \u00b6 A differenza dell'ARI, il silhouette score non richiede la conoscenza aprioristica delle label vere; per valutare la qualit\u00e0 del clustering, invece, questa metrica si affida a valutazioni sulla separazione dei cluster, ottenendo un valore tanto pi\u00f9 alto quanto questi sono tra di loro ben separati e definiti. In particolare, il silhouete score per un singolo campione \u00e8 definito come: \\[ s = \\frac{b-a}{max(a, b)} \\] dove: \\(a\\) \u00e8 la distanza media tra un campione e tutti gli altri campioni appartenenti allo stesso cluster; \\(b\\) \u00e8 la distanza media tra un campione e tutti gli altri campioni appartenenti al cluster pi\u00f9 vicino. Questa metrica, implementata grazie alla funzione silhouette_score() del package metrics , \u00e8 anch'essa di facile interpretazione, in quanto pu\u00f2 assumere valori compresi nell'intervallo \\([-1, 1]\\) , con: valori prossimi a \\(-1\\) che indicano un clustering non corretto; valori prossimi allo \\(0\\) che indicano cluster sovrapposti; valori prossimi a \\(+1\\) che indicano cluster densi e ben suddivisi. Uno svantaggio del silhouette score \u00e8 che, in generale, pu\u00f2 variare in base all'algoritmo utilizzato.","title":"Dispense"},{"location":"material/03_ml_sklearn/21_cls_metrics/lecture/#21-metriche-di-clustering","text":"Cos\u00ec come per la regressione e la classificazione, esistono delle metriche appositamente progettate per valutare la qualit\u00e0 dei risultati ottenuti da un algoritmo di clustering. Nello specifico, valuteremo l' adjusted rand index ed il silhouette score .","title":"21 - Metriche di clustering"},{"location":"material/03_ml_sklearn/21_cls_metrics/lecture/#211-adjusted-rand-index","text":"Sia \\(C\\) l'insieme dei cluster \"veri\" assegnati ad un certo dataset, e \\(K\\) l'insieme dei cluster assegnati a valle dell'applicazione di un algoritmo di clustering. Allora definiamo l'indice di Rand come: \\[ RI = \\frac{a + b}{C_2^n} \\] dove: \\(a\\) \u00e8 il numero di coppie di campioni che appartengono allo stesso cluster sia in \\(C\\) sia a valle dell'assegnazione \\(K\\) ; \\(b\\) \u00e8 il numero di coppie di campioni che non appartengono allo stesso cluster sia in \\(C\\) sia a valle dell'assegnazione \\(K\\) ; \\(C_2^n\\) \u00e8 il numero totale di coppie di campioni presenti nel dataset. In pratica, se: \\[ s = [s_1, s_2, s_3, s_4, s_5] \\\\ C = [s_1, s_2], [s_3, s_4, s_5] \\\\ K = [s_1, s_2, s_3], [s_4, s_5] \\\\ \\] allora: \\[ a = |(s_1, s_2), (s_4, s_5)| = 2 \\\\ b = |(s_1, s_4), (s_1, s_5), (s_4, s_2), (s_5, s_2)| = 4 \\\\ C_2^n = \\frac{|s|*|s-1|}{2} = 5 * 2 = 10 \\] Di conseguenza, \\(RI=\\frac{6}{10}=0.6\\) . Si pu\u00f2 dimostrare non \u00e8 garantito che l'indice di Rand assuma valore vicino allo zero a seguito di un'assegnazione completamente casuale dei cluster da parte dell'algoritmo. Possiamo quindi tenere conto dell'aspettazione \\(E[RI]\\) di ottenere un'assegnazione casuale mediante l' indice di Rand modificato : \\[ ARI = \\frac{RI - E[RI]}{max(RI) - E[RI]} \\] In Scikit Learn, l'indice di Rand modificato \u00e8 ottenuto usando la funzione adjusted_rand_score() del package metrics . Il valore ottimale dell'ARI \u00e8 pari proprio ad 1, caso in cui il clustering \u00e8 riuscito a predire correttamente tutte le classi dei singoli campioni. Valori prossimi allo zero o negativi (fino a -1) contraddistinguono invece labeling non corretti. Una metrica di questo tipo ha l'ovvio vantaggio di essere facilmente interpretabile, oltre che di non essere collegata ad uno specifico algoritmo di clustering. Tuttavia, vi \u00e8 una criticit\u00e0 indotta dalla necessit\u00e0 di conoscere a priori il labeling esatto dei campioni (il che, quindi, potrebbe farci propendere per l'uso di un algoritmo di classificazione).","title":"21.1 - Adjusted Rand Index"},{"location":"material/03_ml_sklearn/21_cls_metrics/lecture/#212-silhouette-score","text":"A differenza dell'ARI, il silhouette score non richiede la conoscenza aprioristica delle label vere; per valutare la qualit\u00e0 del clustering, invece, questa metrica si affida a valutazioni sulla separazione dei cluster, ottenendo un valore tanto pi\u00f9 alto quanto questi sono tra di loro ben separati e definiti. In particolare, il silhouete score per un singolo campione \u00e8 definito come: \\[ s = \\frac{b-a}{max(a, b)} \\] dove: \\(a\\) \u00e8 la distanza media tra un campione e tutti gli altri campioni appartenenti allo stesso cluster; \\(b\\) \u00e8 la distanza media tra un campione e tutti gli altri campioni appartenenti al cluster pi\u00f9 vicino. Questa metrica, implementata grazie alla funzione silhouette_score() del package metrics , \u00e8 anch'essa di facile interpretazione, in quanto pu\u00f2 assumere valori compresi nell'intervallo \\([-1, 1]\\) , con: valori prossimi a \\(-1\\) che indicano un clustering non corretto; valori prossimi allo \\(0\\) che indicano cluster sovrapposti; valori prossimi a \\(+1\\) che indicano cluster densi e ben suddivisi. Uno svantaggio del silhouette score \u00e8 che, in generale, pu\u00f2 variare in base all'algoritmo utilizzato.","title":"21.2 - Silhouette Score"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/exercises/","text":"E22 - Scikit Learn: Tips & Tricks \u00b6 Esercizio E22.1 \u00b6 Creiamo una pipeline di processing che, dati i dati relativi a conto e mance del dataset tips , e come label il giorno dello stesso, calcoli l'ARI a valle dell'applicazione di un'operazione di scaling prima dell'algoritmo di clustering. Successivamente, provare a modificare il numero di cluster, ricalcolando l'ARI. Esercizio E22.2 \u00b6 Usiamo un column transformer per filtrare e pre-elaborare i dati contenuti nel dataset Titanic. In particolare, selezioniamo i dati relativi all'et\u00e0 ed alla tariffa dei passeggeri, riempiendo eventuali dati mancanti e scalandoli nel range [0, 1] , e codifichiamo i dati relativi alla sopravvivenza, classe, genere e porta di imbarcazione del passeggero mediante un OrdinalEncoder() seguito da un imputer. Una volta completato il transformer, usiamolo in una pipeline di clustering. Esercizio E22.3 \u00b6 Utilizziamo la tecnica della grid search per automatizzare la prova fatta nell'esercizio E22.1. In particolare, andiamo a variare il numero di cluster ( n_clusters ) tra 3 ed 8 e l'algoritmo usato dal kmeans ( algorithm ) tra lloyd ed elkan . Esercizio E22.4 \u00b6 Utilizziamo la tecnica di feature selection pi\u00f9 semplice offerta da Scikit Learn, ovvero VarianceThreshold() per effettuare una procedura di feature selection sul dataset Titanic. In tal senso, integriamo tale procedura nel transformer per i dati di tipo numerico usati nell'esercizio E22.2, e proviamo ad effettuare una grid search impostando due threshold (0 e 0.05) e facendo variare il numero di cluster tra 3 ed 8. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizi"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/exercises/#e22-scikit-learn-tips-tricks","text":"","title":"E22 - Scikit Learn: Tips &amp; Tricks"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/exercises/#esercizio-e221","text":"Creiamo una pipeline di processing che, dati i dati relativi a conto e mance del dataset tips , e come label il giorno dello stesso, calcoli l'ARI a valle dell'applicazione di un'operazione di scaling prima dell'algoritmo di clustering. Successivamente, provare a modificare il numero di cluster, ricalcolando l'ARI.","title":"Esercizio E22.1"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/exercises/#esercizio-e222","text":"Usiamo un column transformer per filtrare e pre-elaborare i dati contenuti nel dataset Titanic. In particolare, selezioniamo i dati relativi all'et\u00e0 ed alla tariffa dei passeggeri, riempiendo eventuali dati mancanti e scalandoli nel range [0, 1] , e codifichiamo i dati relativi alla sopravvivenza, classe, genere e porta di imbarcazione del passeggero mediante un OrdinalEncoder() seguito da un imputer. Una volta completato il transformer, usiamolo in una pipeline di clustering.","title":"Esercizio E22.2"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/exercises/#esercizio-e223","text":"Utilizziamo la tecnica della grid search per automatizzare la prova fatta nell'esercizio E22.1. In particolare, andiamo a variare il numero di cluster ( n_clusters ) tra 3 ed 8 e l'algoritmo usato dal kmeans ( algorithm ) tra lloyd ed elkan .","title":"Esercizio E22.3"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/exercises/#esercizio-e224","text":"Utilizziamo la tecnica di feature selection pi\u00f9 semplice offerta da Scikit Learn, ovvero VarianceThreshold() per effettuare una procedura di feature selection sul dataset Titanic. In tal senso, integriamo tale procedura nel transformer per i dati di tipo numerico usati nell'esercizio E22.2, e proviamo ad effettuare una grid search impostando due threshold (0 e 0.05) e facendo variare il numero di cluster tra 3 ed 8. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizio E22.4"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/lecture/","text":"22 - Scikit Learn: Tips & Tricks \u00b6 In questa lezione vedremo alcuni concetti e costrutti che ci permettono di semplificarci la vita nell'utilizzo di Scikit Learn. 22.1 - Pipeline \u00b6 Finora abbiamo visto gli algoritmi come dei singoli blocchi a s\u00e8 stanti. Tuttavia, spesso questo non corrisponde a ci\u00f2 che accade nella realt\u00e0: infatti, si parla di processing pipeline , intesa come sequenza di pi\u00f9 algoritmi e metodi da applicare per trasformare dei dati e portarli all'elaborazione. In tal senso, potremmo decidere di applicare \"a cascata\" diversi transformer e stimatori oppure, in maniera pi\u00f9 compatta, sfruttare le potenzialit\u00e0 offerte dalla classe Pipeline() di Scikit Learn. Per capire come questa funziona, facciamo un esempio. Immaginiamo di fare in modo che ad un algoritmo di clustering segua uno step di normalizzazione usando uno standard scaler. Abbiamo in tal senso due possibilit\u00e0. La prima \u00e8 quella di usare il transformer e lo stimatore in cascata: from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler X , y = make_blobs () scaler = StandardScaler () X_new = scaler . fit_transform ( X ) kmeans = KMeans () kmeans . fit_predict ( X_new ) L'altra \u00e8 quella di utilizzare un oggetto di tipo Pipeline() , che ci permette di indicare i singoli step da seguire nella nostra linea di elaborazione dati: from sklearn.pipeline import Pipeline pipe = Pipeline ( steps = [ ( 'scaler' , StandardScaler ()), ( 'kmeans' , KMeans ()) ]) pipe . fit_predict ( X ) Nota Notiamo come l'oggetto di classe Pipeline() implementi nativamente i metodi fit , predict e transform (pi\u00f9 altri) per permettere l'uso di pipeline in cui lo step finale \u00e8 un transformer o uno stimatore. 22.1.1 - Accesso e modifica dei parametri degli stimatori \u00b6 Possiamo accedere ed impostare manualmente i parametri dei singoli stimatori e transformer dall'interno della pipeline. Questo pu\u00f2 essere fatto al momento dell'inizializzazione: pipe = Pipeline ( steps = [ ( 'scaler' , MinMaxScaler ( range = ( 0 , 2 ))), ( 'kmeans' , KMeans ( n_clusters = 3 )) ]) oppure successivamente mediante il comando set_params e la notazione stimatore__parametro : pipe . set_params ( kmeans__n_clusters = 4 ) Possiamo anche accedere al singolo stimatore come se la pipeline fosse un dizionario: pipe [ 'kmeans' ] 22.2 - ColumnTransformer \u00b6 Abbiamo visto come i dataset che utilizziamo non abbiano sempre dati di tipo uniforme (ad esempio, tutti numerici o categorici), e che alle volte sia necessario applicare differenti trasformazioni a feature differenti. Cos\u00ec come la pipeline di processing, anche questa operazione pu\u00f2 essere svolta una feature alla volta o, in maniera estremamente pi\u00f9 semplice , creando un oggetto di classe ColumnTransformer() . Immaginiamo ad esempio di dover trasformare tutte le colonne del dataset tips, normalizzando quelle numeriche ed usando un OrdinalEncoder() per le categoriche. Per farlo, possiamo usare un ColumnTransformer() in questo modo: import seaborn as sns from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OrdinalEncoder , StandardScaler tips = sns . load_dataset ( 'tips' ) ct = ColumnTransformer ( [( 'scaler' , StandardScaler (), [ 'total_bill' , 'tip' ]), ( 'encoder' , OrdinalEncoder (), [ 'sex' , 'smoker' , 'day' , 'time' ])], remainder = 'passthrough' ) ct . fit ( tips ) ct . transform ( tips ) Nota Il ColumnTransformer() lavora sui dataframe. Possiamo anche combinare il concetto di ColumnTransformer() con quello di Pipeline() , usando al posto di un singolo transformer una pipeline di transformer. Ad esempio, se volessimo assegnare i dati mancanti usando un SimpleImputer() prima dello scaling, potremmo usare una pipeline da passare al transformer: from sklearn.impute import Imputer numerical_transformer = Pipeline ( [( 'imputer' , SimpleImputer ()), ( 'scaler' , StandardScaler ())] ) ct = ColumnTransformer ( [( 'scaler' , numerical_transformer , [ 'total_bill' , 'tip' ]), ( 'encoder' , OrdinalEncoder (), [ 'sex' , 'smoker' , 'day' , 'time' ])], remainder = 'passthrough' ) Nota Ovviamente, anche un ColumnTransformer() pu\u00f2 essere usato in una pipeline! 22.3 - Crossvalidazione \u00b6 Quando abbiamo parlato di preparazione dei dati abbiamo visto come sia necessario suddividere gli stessi in due insiemi , ovvero quello di training e quello di validazione, allo scopo di assicurarsi che il modello sia in grado di generalizzare. Tuttavia, questa procedura spesso non \u00e8 sufficiente, perch\u00e9 all'interno dei dati di training o testing, anche se scelti con meccanismi casuali, \u00e8 possibile che siano presenti dei particolari meccanismi di generazione propri di quel sottinsieme . In altre parole, c'\u00e8 il rischio che l'algoritmo vada comunque in overfitting sui dati di training! Per ovviare a questa evenienza, molto spesso non ci si limita ad un'unica iterazione di addestramento, ma si effettua la procedura chiamata \\(k\\) -fold cross validation . La cross validation consiste di \\(k\\) iterazioni, e prevede che l'intero set di dati sia suddiviso in \\(k\\) porzioni, ognuna delle quali sar\u00e0 usata ad ogni iterazione come test set, mentre la restante parte del dataset sar\u00e0 utilizzata per il training. Dopo \\(k\\) iterazioni, i risultati saranno quindi mediati tra loro, ed il modello risulter\u00e0 essere pi\u00f9 robusto all'overfitting. Questa procedura \u00e8 descritta graficamente nella seguente immagine (presa direttamente dalla documentazione di Scikit Learn). Ovviamente, in Scikit Learn \u00e8 possibile effettuare la procedura di cross validazione utilizzando la funzione cross_validate() , che permette di effettuare \\(k\\) round di cross-validazione di un algoritmo su un dataset. from sklearn.model_selection import cross_validate kmeans = KMeans () cross_validate ( kmeans , X , y , scoring = [ 'adjusted_rand_score' , 'adjusted_mutual_info_score' ]) Se ci si vuole concentrare su un'unica metrica, si pu\u00f2 usare la funzione cross_val_score() : from sklearn.model_selection import cross_val_score cross_val_score ( kmeans , X , y , scoring = 'adjusted_rand_score' ) Infine, possiamo usare la funzione cross_val_predict() per ottenere i valori predetti a valle della cross-validazione: from sklearn.model_selection import cross_val_predict cross_val_predict ( kmeans , X , y ) 22.4 - Ottimizzazione degli iperparametri \u00b6 Le funzioni che abbiamo visto finora hanno due problemi. Il primo \u00e8 che le funzioni di cross-validazione non restituiscono uno stimatore fittato, mentre il secondo \u00e8 che la valutazione di diversi iperparametri \u00e8 delegata manualmente all'utente. Per ovviare a questi problemi, esistono delle tecniche di ottimizzazione degli iperparametri , che permettono di esplorare una serie di valori per ogni parametro dell'algoritmo selezionato, valutando la combinazione degli stessi che produce i risultati migliori. 22.4.1 - Tecniche di ottimizzazione degli iperparametri: la grid search \u00b6 Esistono numerose tecniche di ottimizzazione degli iperparametri; in questo frangente, valuteremo la pi\u00f9 semplice di tutte, ovvero la grid search (traducibile in italiano con \"ricerca a griglia\"). La tecnica della grid search \u00e8 schematizzata nella precedente immagine. Immaginiamo per semplicit\u00e0 che il nostro algoritmo abbia soltanto due possibili parametri, e che tutti i possibili valori che vogliamo esplorare possano essere disposti lungo una griglia. All'interno di questa, i valori di una metrica a nostra scelta (ad esempio, l'accuratezza) si andranno a disporre secondo picchi e valli; il nostro obiettivo, in questo specifico caso, \u00e8 ovviamente trovare il massimo picco della funzione che correla i valori dei parametri con l'accuratezza. Per farlo, la grid search prova ad eseguire l'algoritmo sui dati di training per ogni combinazione di parametri , fino a trovare la migliore possibile. Scikit Learn implementa oggetti di classe GridSearchCV() , che prendono uno stimatore (o una pipeline), effettuano la crossvalidazione sui dati, e restituiscono quello con i parametri migliori dopo l'addestramento. Ad esempio: from sklearn.model_selection import GridSearchCV parameters = { 'n_clusters' : [ 3 , 4 , 5 ] } kmeans = KMeans () est = GridSearchCV ( kmeans , parameters ) est . fit ( X ) E' anche possibile usare la grid search in combinazione con con una pipeline. from sklearn.model_selection import GridSearchCV from sklearn.decomposition import PCA pipe = Pipeline ([ ( 'pca' , PCA ()), ( 'kmeans' , KMeans ()) ]) param_grid = { 'pca__n_components' : [ 2 , 3 ], 'kmeans__n_clusters' : [ 3 , 4 , 5 ] } est = GridSearchCV ( pipe , param_grid ) est . fit ( X ) 22.5 - Feature selection \u00b6 Concludiamo con un breve cenno alle tecniche di feature selection, che ci permettono di isolare le feature maggiormente significative all'interno del nostro dataset, scartando le altre. In tal senso, Scikit Learn ci offre un intero package, ovvero il feature_selection , operante in tal senso.","title":"Dispense"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/lecture/#22-scikit-learn-tips-tricks","text":"In questa lezione vedremo alcuni concetti e costrutti che ci permettono di semplificarci la vita nell'utilizzo di Scikit Learn.","title":"22 - Scikit Learn: Tips &amp; Tricks"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/lecture/#221-pipeline","text":"Finora abbiamo visto gli algoritmi come dei singoli blocchi a s\u00e8 stanti. Tuttavia, spesso questo non corrisponde a ci\u00f2 che accade nella realt\u00e0: infatti, si parla di processing pipeline , intesa come sequenza di pi\u00f9 algoritmi e metodi da applicare per trasformare dei dati e portarli all'elaborazione. In tal senso, potremmo decidere di applicare \"a cascata\" diversi transformer e stimatori oppure, in maniera pi\u00f9 compatta, sfruttare le potenzialit\u00e0 offerte dalla classe Pipeline() di Scikit Learn. Per capire come questa funziona, facciamo un esempio. Immaginiamo di fare in modo che ad un algoritmo di clustering segua uno step di normalizzazione usando uno standard scaler. Abbiamo in tal senso due possibilit\u00e0. La prima \u00e8 quella di usare il transformer e lo stimatore in cascata: from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler X , y = make_blobs () scaler = StandardScaler () X_new = scaler . fit_transform ( X ) kmeans = KMeans () kmeans . fit_predict ( X_new ) L'altra \u00e8 quella di utilizzare un oggetto di tipo Pipeline() , che ci permette di indicare i singoli step da seguire nella nostra linea di elaborazione dati: from sklearn.pipeline import Pipeline pipe = Pipeline ( steps = [ ( 'scaler' , StandardScaler ()), ( 'kmeans' , KMeans ()) ]) pipe . fit_predict ( X ) Nota Notiamo come l'oggetto di classe Pipeline() implementi nativamente i metodi fit , predict e transform (pi\u00f9 altri) per permettere l'uso di pipeline in cui lo step finale \u00e8 un transformer o uno stimatore.","title":"22.1 - Pipeline"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/lecture/#2211-accesso-e-modifica-dei-parametri-degli-stimatori","text":"Possiamo accedere ed impostare manualmente i parametri dei singoli stimatori e transformer dall'interno della pipeline. Questo pu\u00f2 essere fatto al momento dell'inizializzazione: pipe = Pipeline ( steps = [ ( 'scaler' , MinMaxScaler ( range = ( 0 , 2 ))), ( 'kmeans' , KMeans ( n_clusters = 3 )) ]) oppure successivamente mediante il comando set_params e la notazione stimatore__parametro : pipe . set_params ( kmeans__n_clusters = 4 ) Possiamo anche accedere al singolo stimatore come se la pipeline fosse un dizionario: pipe [ 'kmeans' ]","title":"22.1.1 - Accesso e modifica dei parametri degli stimatori"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/lecture/#222-columntransformer","text":"Abbiamo visto come i dataset che utilizziamo non abbiano sempre dati di tipo uniforme (ad esempio, tutti numerici o categorici), e che alle volte sia necessario applicare differenti trasformazioni a feature differenti. Cos\u00ec come la pipeline di processing, anche questa operazione pu\u00f2 essere svolta una feature alla volta o, in maniera estremamente pi\u00f9 semplice , creando un oggetto di classe ColumnTransformer() . Immaginiamo ad esempio di dover trasformare tutte le colonne del dataset tips, normalizzando quelle numeriche ed usando un OrdinalEncoder() per le categoriche. Per farlo, possiamo usare un ColumnTransformer() in questo modo: import seaborn as sns from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OrdinalEncoder , StandardScaler tips = sns . load_dataset ( 'tips' ) ct = ColumnTransformer ( [( 'scaler' , StandardScaler (), [ 'total_bill' , 'tip' ]), ( 'encoder' , OrdinalEncoder (), [ 'sex' , 'smoker' , 'day' , 'time' ])], remainder = 'passthrough' ) ct . fit ( tips ) ct . transform ( tips ) Nota Il ColumnTransformer() lavora sui dataframe. Possiamo anche combinare il concetto di ColumnTransformer() con quello di Pipeline() , usando al posto di un singolo transformer una pipeline di transformer. Ad esempio, se volessimo assegnare i dati mancanti usando un SimpleImputer() prima dello scaling, potremmo usare una pipeline da passare al transformer: from sklearn.impute import Imputer numerical_transformer = Pipeline ( [( 'imputer' , SimpleImputer ()), ( 'scaler' , StandardScaler ())] ) ct = ColumnTransformer ( [( 'scaler' , numerical_transformer , [ 'total_bill' , 'tip' ]), ( 'encoder' , OrdinalEncoder (), [ 'sex' , 'smoker' , 'day' , 'time' ])], remainder = 'passthrough' ) Nota Ovviamente, anche un ColumnTransformer() pu\u00f2 essere usato in una pipeline!","title":"22.2 - ColumnTransformer"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/lecture/#223-crossvalidazione","text":"Quando abbiamo parlato di preparazione dei dati abbiamo visto come sia necessario suddividere gli stessi in due insiemi , ovvero quello di training e quello di validazione, allo scopo di assicurarsi che il modello sia in grado di generalizzare. Tuttavia, questa procedura spesso non \u00e8 sufficiente, perch\u00e9 all'interno dei dati di training o testing, anche se scelti con meccanismi casuali, \u00e8 possibile che siano presenti dei particolari meccanismi di generazione propri di quel sottinsieme . In altre parole, c'\u00e8 il rischio che l'algoritmo vada comunque in overfitting sui dati di training! Per ovviare a questa evenienza, molto spesso non ci si limita ad un'unica iterazione di addestramento, ma si effettua la procedura chiamata \\(k\\) -fold cross validation . La cross validation consiste di \\(k\\) iterazioni, e prevede che l'intero set di dati sia suddiviso in \\(k\\) porzioni, ognuna delle quali sar\u00e0 usata ad ogni iterazione come test set, mentre la restante parte del dataset sar\u00e0 utilizzata per il training. Dopo \\(k\\) iterazioni, i risultati saranno quindi mediati tra loro, ed il modello risulter\u00e0 essere pi\u00f9 robusto all'overfitting. Questa procedura \u00e8 descritta graficamente nella seguente immagine (presa direttamente dalla documentazione di Scikit Learn). Ovviamente, in Scikit Learn \u00e8 possibile effettuare la procedura di cross validazione utilizzando la funzione cross_validate() , che permette di effettuare \\(k\\) round di cross-validazione di un algoritmo su un dataset. from sklearn.model_selection import cross_validate kmeans = KMeans () cross_validate ( kmeans , X , y , scoring = [ 'adjusted_rand_score' , 'adjusted_mutual_info_score' ]) Se ci si vuole concentrare su un'unica metrica, si pu\u00f2 usare la funzione cross_val_score() : from sklearn.model_selection import cross_val_score cross_val_score ( kmeans , X , y , scoring = 'adjusted_rand_score' ) Infine, possiamo usare la funzione cross_val_predict() per ottenere i valori predetti a valle della cross-validazione: from sklearn.model_selection import cross_val_predict cross_val_predict ( kmeans , X , y )","title":"22.3 - Crossvalidazione"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/lecture/#224-ottimizzazione-degli-iperparametri","text":"Le funzioni che abbiamo visto finora hanno due problemi. Il primo \u00e8 che le funzioni di cross-validazione non restituiscono uno stimatore fittato, mentre il secondo \u00e8 che la valutazione di diversi iperparametri \u00e8 delegata manualmente all'utente. Per ovviare a questi problemi, esistono delle tecniche di ottimizzazione degli iperparametri , che permettono di esplorare una serie di valori per ogni parametro dell'algoritmo selezionato, valutando la combinazione degli stessi che produce i risultati migliori.","title":"22.4 - Ottimizzazione degli iperparametri"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/lecture/#2241-tecniche-di-ottimizzazione-degli-iperparametri-la-grid-search","text":"Esistono numerose tecniche di ottimizzazione degli iperparametri; in questo frangente, valuteremo la pi\u00f9 semplice di tutte, ovvero la grid search (traducibile in italiano con \"ricerca a griglia\"). La tecnica della grid search \u00e8 schematizzata nella precedente immagine. Immaginiamo per semplicit\u00e0 che il nostro algoritmo abbia soltanto due possibili parametri, e che tutti i possibili valori che vogliamo esplorare possano essere disposti lungo una griglia. All'interno di questa, i valori di una metrica a nostra scelta (ad esempio, l'accuratezza) si andranno a disporre secondo picchi e valli; il nostro obiettivo, in questo specifico caso, \u00e8 ovviamente trovare il massimo picco della funzione che correla i valori dei parametri con l'accuratezza. Per farlo, la grid search prova ad eseguire l'algoritmo sui dati di training per ogni combinazione di parametri , fino a trovare la migliore possibile. Scikit Learn implementa oggetti di classe GridSearchCV() , che prendono uno stimatore (o una pipeline), effettuano la crossvalidazione sui dati, e restituiscono quello con i parametri migliori dopo l'addestramento. Ad esempio: from sklearn.model_selection import GridSearchCV parameters = { 'n_clusters' : [ 3 , 4 , 5 ] } kmeans = KMeans () est = GridSearchCV ( kmeans , parameters ) est . fit ( X ) E' anche possibile usare la grid search in combinazione con con una pipeline. from sklearn.model_selection import GridSearchCV from sklearn.decomposition import PCA pipe = Pipeline ([ ( 'pca' , PCA ()), ( 'kmeans' , KMeans ()) ]) param_grid = { 'pca__n_components' : [ 2 , 3 ], 'kmeans__n_clusters' : [ 3 , 4 , 5 ] } est = GridSearchCV ( pipe , param_grid ) est . fit ( X )","title":"22.4.1 - Tecniche di ottimizzazione degli iperparametri: la grid search"},{"location":"material/03_ml_sklearn/22_sklearn_tricks/lecture/#225-feature-selection","text":"Concludiamo con un breve cenno alle tecniche di feature selection, che ci permettono di isolare le feature maggiormente significative all'interno del nostro dataset, scartando le altre. In tal senso, Scikit Learn ci offre un intero package, ovvero il feature_selection , operante in tal senso.","title":"22.5 - Feature selection"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/","text":"Esercitazione 23 \u00b6 E23.1 \u00b6 Caricare i dataset Diabetes (a scopo di regressione) ed Iris (a scopo di classificazione). Visualizzarne rapidamente la struttura. E23.2 \u00b6 Studiare le feature e la distribuzione delle stesse del dataset Diabetes. E23.3 \u00b6 Effettuare un'analisi delle cross-correlazioni presenti tra le feature del dataset Diabetes. Utilizzare il \\(\\tau\\) di Kendall. E23.4 \u00b6 Isolare le \\(k\\) feature pi\u00f9 importanti del dataset Diabetes. Per farlo, utilizzare un oggetto di classe SelectKBest() scegliendo la metrica pi\u00f9 appropriata tra mutual_info_regression e mutual_info_classif . E23.5 \u00b6 Comparare i risultati di regressione ottenuti da un regressore lineare e da un albero decisionale in termini di MAPE. E23.6 \u00b6 Provare ad eseguire una GridSearchCV sull'albero decisionale, e valutare se i risultati in termini di MAPE migliorano. E23.7 \u00b6 Ripetere gli stessi step degli esercizi precedenti per il dataset Iris. E23.8 \u00b6 Comparare, per mezzo di due pipeline, i risultati ottenuti effettuando il clustering mediante algoritmo KMeans sui dati di Iris con e senza feature selection. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizi"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/#esercitazione-23","text":"","title":"Esercitazione 23"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/#e231","text":"Caricare i dataset Diabetes (a scopo di regressione) ed Iris (a scopo di classificazione). Visualizzarne rapidamente la struttura.","title":"E23.1"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/#e232","text":"Studiare le feature e la distribuzione delle stesse del dataset Diabetes.","title":"E23.2"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/#e233","text":"Effettuare un'analisi delle cross-correlazioni presenti tra le feature del dataset Diabetes. Utilizzare il \\(\\tau\\) di Kendall.","title":"E23.3"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/#e234","text":"Isolare le \\(k\\) feature pi\u00f9 importanti del dataset Diabetes. Per farlo, utilizzare un oggetto di classe SelectKBest() scegliendo la metrica pi\u00f9 appropriata tra mutual_info_regression e mutual_info_classif .","title":"E23.4"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/#e235","text":"Comparare i risultati di regressione ottenuti da un regressore lineare e da un albero decisionale in termini di MAPE.","title":"E23.5"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/#e236","text":"Provare ad eseguire una GridSearchCV sull'albero decisionale, e valutare se i risultati in termini di MAPE migliorano.","title":"E23.6"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/#e237","text":"Ripetere gli stessi step degli esercizi precedenti per il dataset Iris.","title":"E23.7"},{"location":"material/03_ml_sklearn/23_sklearn_ex/exercises/#e238","text":"Comparare, per mezzo di due pipeline, i risultati ottenuti effettuando il clustering mediante algoritmo KMeans sui dati di Iris con e senza feature selection. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"E23.8"},{"location":"material/04_nn_tflow/24_nn/lecture/","text":"25 - Anatomia di una rete neurale \u00b6 Le reti neurali sono ormai sulla bocca di tutti: chiunque le usa per risolvere con successo ogni tipo di problema. Tuttavia, il passo giusto prima di usarle \u00e8 quello di comprendere effettivamente a cosa servono: in tal senso, introduciamo il concetto di problema non lineare . 25.1 - Problemi non lineari \u00b6 Diamo un'occhiata al dataset rappresentato nella figura 25.1. Figura 25.1 - Dataset non lineare Questo dataset \u00e8 evidentemente non lineare : in pratica, questo significa che non esiste un algoritmo in grado di separare in maniera lineare le due classi tra loro o, in termini matematici, non esiste un modello in forma: \\[ y = ax_1 + bx_2 + c \\] che permetta di determinare \\(y\\) a partire dalle feature \\(x_1\\) ed \\(x_2\\) . Numero di feature Ovviamente, se il numero di feature fosse pi\u00f9 elevato, il sommatore lineare dovrebbe considerare un maggior numero di variabili indipendenti. I lettori pi\u00f9 audaci potrebbero provare ad usare delle approssimazioni lineari a tratti. Costoro dovrebbero considerare la figura 25.2. Figura 25.2 - Dataset *estremamente* non lineare 25.2 - Reti neurali e problemi non lineari \u00b6 Per capire come le reti neurali ci aiutano a modellare un problema non lineare, visualizziamo un semplice sommatore pesato. Figura 25.3 - Un sommatore lineare In questo semplice caso abbiamo tre input ed un output. Proviamo ad aggiungere un ulteriore strato. Figura 25.4 - Un sommatore lineare a pi\u00f9 strati Lo strato che abbiamo aggiunto \u00e8 detto nascosto , e rappresenta una serie di valori intermedi considerati dal sommatore nel calcolo dell'uscita. Quest'ultima non sar\u00e0 pi\u00f9 una somma pesata degli input, ma una somma pesata dei valori in uscita dallo strato nascosto , che a loro volta sono dipendenti dall'input. Tuttavia, il modello rimane lineare : potremo aggiungere un numero arbitrario di strati nascosti, ma questo sar\u00e0 sempre vero, a meno che non si usi una particolare funzione, detta di attivazione . 25.2.1 - Funzioni di attivazione \u00b6 La modellazione di un problema non lineare prevede l'introduzione di (appunto) non linearit\u00e0 all'interno del modello. Nella pratica, potremo inserire delle opportune funzioni non lineari tra i diversi strati della rete. Queste funzioni sono dette di attivazione . Figura 25.5 - Semplice rete neurale con funzioni di attivazione Ovviamente, con un maggior numero di strati nascosti, l'impatto delle non linearit\u00e0 diventa maggiore: in questo modo, saremo in grado di inferire delle relazioni anche molto complesse tra gli input e gli output (predetti). Le funzioni di attivazione pi\u00f9 utilizzate in passato erano di tipo sigmoidale (simili, per intenderci, alla funzione che abbiamo visto in uscita alla regressione logistica). Attualmente, le funzioni pi\u00f9 usate sono le rectified linear unit , o ReLU , che hanno risultati comparabili in termini di accuratezza del modello alla sigmoidale, ma risultano essere significativamente meno complesse dal punto di vista computazionale. Le ReLU sono espresse dalla seguente funzione: \\[ y = max(0, x) \\] che graficamente si traduce in una forma espressa come: Figura 25. - ReLU ReLU nella pratica In pratica, una ReLU \"fa passare\" soltanto i valori positivi, portando a zero tutti quelli negativi. Riassumendo: una rete neurale \u00e8 data da un insieme di nodi, o neuroni , organizzati in uno o pi\u00f9 strati ; ogni neurone \u00e8 connesso a quelli dello strato successivo mediante dei pesi , che rappresentano la \"forza\" della connessione; esiste una funzione di attivazione che trasforma l'uscita di ogni neurone verso lo strato successivo inserendo delle non linearit\u00e0.","title":"Dispense"},{"location":"material/04_nn_tflow/24_nn/lecture/#25-anatomia-di-una-rete-neurale","text":"Le reti neurali sono ormai sulla bocca di tutti: chiunque le usa per risolvere con successo ogni tipo di problema. Tuttavia, il passo giusto prima di usarle \u00e8 quello di comprendere effettivamente a cosa servono: in tal senso, introduciamo il concetto di problema non lineare .","title":"25 - Anatomia di una rete neurale"},{"location":"material/04_nn_tflow/24_nn/lecture/#251-problemi-non-lineari","text":"Diamo un'occhiata al dataset rappresentato nella figura 25.1. Figura 25.1 - Dataset non lineare Questo dataset \u00e8 evidentemente non lineare : in pratica, questo significa che non esiste un algoritmo in grado di separare in maniera lineare le due classi tra loro o, in termini matematici, non esiste un modello in forma: \\[ y = ax_1 + bx_2 + c \\] che permetta di determinare \\(y\\) a partire dalle feature \\(x_1\\) ed \\(x_2\\) . Numero di feature Ovviamente, se il numero di feature fosse pi\u00f9 elevato, il sommatore lineare dovrebbe considerare un maggior numero di variabili indipendenti. I lettori pi\u00f9 audaci potrebbero provare ad usare delle approssimazioni lineari a tratti. Costoro dovrebbero considerare la figura 25.2. Figura 25.2 - Dataset *estremamente* non lineare","title":"25.1 - Problemi non lineari"},{"location":"material/04_nn_tflow/24_nn/lecture/#252-reti-neurali-e-problemi-non-lineari","text":"Per capire come le reti neurali ci aiutano a modellare un problema non lineare, visualizziamo un semplice sommatore pesato. Figura 25.3 - Un sommatore lineare In questo semplice caso abbiamo tre input ed un output. Proviamo ad aggiungere un ulteriore strato. Figura 25.4 - Un sommatore lineare a pi\u00f9 strati Lo strato che abbiamo aggiunto \u00e8 detto nascosto , e rappresenta una serie di valori intermedi considerati dal sommatore nel calcolo dell'uscita. Quest'ultima non sar\u00e0 pi\u00f9 una somma pesata degli input, ma una somma pesata dei valori in uscita dallo strato nascosto , che a loro volta sono dipendenti dall'input. Tuttavia, il modello rimane lineare : potremo aggiungere un numero arbitrario di strati nascosti, ma questo sar\u00e0 sempre vero, a meno che non si usi una particolare funzione, detta di attivazione .","title":"25.2 - Reti neurali e problemi non lineari"},{"location":"material/04_nn_tflow/24_nn/lecture/#2521-funzioni-di-attivazione","text":"La modellazione di un problema non lineare prevede l'introduzione di (appunto) non linearit\u00e0 all'interno del modello. Nella pratica, potremo inserire delle opportune funzioni non lineari tra i diversi strati della rete. Queste funzioni sono dette di attivazione . Figura 25.5 - Semplice rete neurale con funzioni di attivazione Ovviamente, con un maggior numero di strati nascosti, l'impatto delle non linearit\u00e0 diventa maggiore: in questo modo, saremo in grado di inferire delle relazioni anche molto complesse tra gli input e gli output (predetti). Le funzioni di attivazione pi\u00f9 utilizzate in passato erano di tipo sigmoidale (simili, per intenderci, alla funzione che abbiamo visto in uscita alla regressione logistica). Attualmente, le funzioni pi\u00f9 usate sono le rectified linear unit , o ReLU , che hanno risultati comparabili in termini di accuratezza del modello alla sigmoidale, ma risultano essere significativamente meno complesse dal punto di vista computazionale. Le ReLU sono espresse dalla seguente funzione: \\[ y = max(0, x) \\] che graficamente si traduce in una forma espressa come: Figura 25. - ReLU ReLU nella pratica In pratica, una ReLU \"fa passare\" soltanto i valori positivi, portando a zero tutti quelli negativi. Riassumendo: una rete neurale \u00e8 data da un insieme di nodi, o neuroni , organizzati in uno o pi\u00f9 strati ; ogni neurone \u00e8 connesso a quelli dello strato successivo mediante dei pesi , che rappresentano la \"forza\" della connessione; esiste una funzione di attivazione che trasforma l'uscita di ogni neurone verso lo strato successivo inserendo delle non linearit\u00e0.","title":"25.2.1 - Funzioni di attivazione"},{"location":"material/04_nn_tflow/25_intro_tflow/lecture/","text":"25 - Introduzione a TensorFlow (e Keras) \u00b6 Attenzione Dispensa in fase di stesura.","title":"Dispense"},{"location":"material/04_nn_tflow/25_intro_tflow/lecture/#25-introduzione-a-tensorflow-e-keras","text":"Attenzione Dispensa in fase di stesura.","title":"25 - Introduzione a TensorFlow (e Keras)"},{"location":"material/04_nn_tflow/26_dropout_reg/exercises/","text":"E26 - Overfitting e regolarizzazione \u00b6 Esercizio E26.1 \u00b6 Proviamo ad utilizzare il dataset IMDB movie da Keras per addestrare una rete neurale con la seguente struttura: _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= input ( Dense ) ( None, 8 ) 8008 dense_1 ( Dense ) ( None, 8 ) 72 classification ( Dense ) ( None, 1 ) 9 _________________________________________________________________ Proviamo ad aggiungere una regolarizzazione ed un dropout sul secondo layer, e compariamo i risultati ottenuti. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizi"},{"location":"material/04_nn_tflow/26_dropout_reg/exercises/#e26-overfitting-e-regolarizzazione","text":"","title":"E26 - Overfitting e regolarizzazione"},{"location":"material/04_nn_tflow/26_dropout_reg/exercises/#esercizio-e261","text":"Proviamo ad utilizzare il dataset IMDB movie da Keras per addestrare una rete neurale con la seguente struttura: _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= input ( Dense ) ( None, 8 ) 8008 dense_1 ( Dense ) ( None, 8 ) 72 classification ( Dense ) ( None, 1 ) 9 _________________________________________________________________ Proviamo ad aggiungere una regolarizzazione ed un dropout sul secondo layer, e compariamo i risultati ottenuti. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizio E26.1"},{"location":"material/04_nn_tflow/26_dropout_reg/lecture/","text":"26 - Overfitting e regolarizzazione \u00b6 Nella lezione precedente abbiamo limitato il training delle reti neurali a 10 epoche, usando come metrica di test esclusivamente l'accuratezza sui dati di training. Tuttavia, se proseguissimo nel training e valutassimo anche l'accuratezza sui dati di validazione, noteremmo che ad una certa epoca questa raggiunger\u00e0 un picco, per poi iniziare a stagnare o, in alcuni casi, diminuire : in altre parole, il nostro modello andr\u00e0 incontro ad overfitting . Gestire correttamente questa situazione \u00e8 molto importante: infatti, ottenere un'elevata accuratezza sui dati di training non \u00e8 importante quanto sviluppare un modello in grado di generalizzare su dati che non ha visto durante l'addestramento. L'opposto dell'overfitting \u00e8, abbastanza prevedibilmente, l' underfitting , situazione che si verifica quando vi \u00e8 ancora la possibilit\u00e0 di migliorare il modello. Di solito, l'underfitting si verifica nel momento in cui un modello non \u00e8 abbastanza descrittivo, oppure quando non \u00e8 stato addestrato per un numero di epoche sufficienti, non permettendo alla rete di caratterizzare i pattern di rilievo presenti nei dati. Ovviamente, trovare i parametri ottimali di addestramento significa trovare un equilibrio tra overfitting ed underfitting. In primis, infatti, \u00e8 necessario scegliere accuratamente il numero di epoche di addestramento, per evitare una scarsa (o al contrario eccessiva) adesione del modello ai dati. Inoltre, \u00e8 necessario verificare che i dati di training utilizzati siano adeguati, seguendo magari le indicazioni date in precedenza; infine, qualora questi passi siano gi\u00e0 stati compiuti, pu\u00f2 essere necessario utilizzare delle tecniche di regolarizzazione . 26.1 - Strategie di regolarizzazione \u00b6 Abbiamo gi\u00e0 discusso delle strategie di regolarizzazione quando abbiamo visto la regressione logistica. In pratica, per comprendere il motivo per cui si usa la regolarizzazione possiamo usare il concetto di rasoio di Occam , ovvero, date due possibili spiegazioni per lo stesso fenomeno, quella che con tutta probabilit\u00e0 lo descrive in maniera migliore \u00e8 anche la pi\u00f9 semplice o, in altre parole, quella che assume il minor numero di ipotesi. Questo concetto si applica anche ad un modello di rete neurale: data un'architettura, esistono diverse combinazioni di valori di pesi che possono spiegare i dati, ed in generale le combinazioni pi\u00f9 semplici corrono meno il rischio di andare in overfitting se comparati a quelli pi\u00f9 complessi. Dalla precedente affermazione discende che un modo comune di mitigare l'overfitting del modello ai dati \u00e8 inserire degli opportuni vincoli sulla complessit\u00e0 della rete, \"forzando\" i pesi ad assumere valori ridotti, e rendendo implicitamente la distribuzione di detti valori maggiormente uniforme. Questo procedimento, chiamato weight regularization , \u00e8 ottenuto aggiungendo alla funzione di costo della rete un termine direttamente proporzionale al valore del peso. Di solito, si utilizzano due tecniche di regolarizzazione: nella regolarizzazione L1 il costo aggiunto \u00e8 proporzionale al valore assoluto dei coefficienti dei pesi, ovvero alla norma \\(L^1\\) ; nella regolarizzazione L2 il costo aggiunto \u00e8 proporzionale al quadrato del valore dei coefficienti dei pesi, ovvero alla norma \\(L^2\\) . Nota In generale, la regolarizzazione L1 favorisce la \"sparsit\u00e0\" dei dati, forzando il valore di alcuni pesi a \\(0\\) . Ci\u00f2 non avviene con la regolarizzazione L2. In Keras, possiamo aggiungere un parametro di regolarizzazione usando il package regularizers ed il parametro kernel_regularizers del layer da regolarizzare: from keras import regularizers layers . Dense ( 64 , activation = 'relu' , kernel_regularizers = regularizers . l2 ( 0.001 )) In questo caso, stiamo usando un valore di regolarizzazione pari a \\(0.001\\) , il che significa che ogni peso del layer regolarizzato aggiunger\u00e0 un valore pari a \\(0.001 \\cdot w_i^2\\) al costo totale della rete, con \\(w_i\\) valore del peso dell' \\(i\\) -mo coefficiente. 26.2 - Dropout \u00b6 Un altro metodo di regolarizzazione molto diffuso \u00e8 dato dall'uso di uno strato di dropout . L'idea alla base del dropout sta nel fatto che ogni nodo della rete deve restituire in output delle feature utili a prescindere da quelle restituite dagli altri nodi. Per ottenere questo risultato, si fa in modo che un certo numero di neuroni (scelti in maniera casuale ad ogni iterazione) dello strato precedente venga ignorato dal layer che implementa il dropout. In questo modo, il layer modifica ad ogni iterazione la sua connettivit\u00e0, ottenendo in un certo senso un diverso \"punto di vista\" sui dati stessi: in tal senso, il dropout aggiunge in maniera artificiosa del rumore sul processo di apprendimento , forzando una maggiore o minore importanza delle connessioni a seconda dei nodi scartati, ed evitando quindi delle situazioni dove i layer di rete tendono ad adattarsi vicendevolmente per \"correggere\" gli errori di predizione. Di conseguenza, il modello acquisisce maggiore capacit\u00e0 di generalizzazione, visto e considerato che ogni neurone isoler\u00e0 delle feature in maniera indipendente dagli altri. Per quello che riguarda gli iperparametri usati dal layer di dropout, il pi\u00f9 importante \u00e8 quello che specifica la probabilit\u00e0 con la quale gli output dello strato precedente vengono scartati. Un valore comune in tal senso \u00e8 \\(0.5\\) per gli strati nascosti, e \\(0.8\\) per lo strato di input.","title":"Dispense"},{"location":"material/04_nn_tflow/26_dropout_reg/lecture/#26-overfitting-e-regolarizzazione","text":"Nella lezione precedente abbiamo limitato il training delle reti neurali a 10 epoche, usando come metrica di test esclusivamente l'accuratezza sui dati di training. Tuttavia, se proseguissimo nel training e valutassimo anche l'accuratezza sui dati di validazione, noteremmo che ad una certa epoca questa raggiunger\u00e0 un picco, per poi iniziare a stagnare o, in alcuni casi, diminuire : in altre parole, il nostro modello andr\u00e0 incontro ad overfitting . Gestire correttamente questa situazione \u00e8 molto importante: infatti, ottenere un'elevata accuratezza sui dati di training non \u00e8 importante quanto sviluppare un modello in grado di generalizzare su dati che non ha visto durante l'addestramento. L'opposto dell'overfitting \u00e8, abbastanza prevedibilmente, l' underfitting , situazione che si verifica quando vi \u00e8 ancora la possibilit\u00e0 di migliorare il modello. Di solito, l'underfitting si verifica nel momento in cui un modello non \u00e8 abbastanza descrittivo, oppure quando non \u00e8 stato addestrato per un numero di epoche sufficienti, non permettendo alla rete di caratterizzare i pattern di rilievo presenti nei dati. Ovviamente, trovare i parametri ottimali di addestramento significa trovare un equilibrio tra overfitting ed underfitting. In primis, infatti, \u00e8 necessario scegliere accuratamente il numero di epoche di addestramento, per evitare una scarsa (o al contrario eccessiva) adesione del modello ai dati. Inoltre, \u00e8 necessario verificare che i dati di training utilizzati siano adeguati, seguendo magari le indicazioni date in precedenza; infine, qualora questi passi siano gi\u00e0 stati compiuti, pu\u00f2 essere necessario utilizzare delle tecniche di regolarizzazione .","title":"26 - Overfitting e regolarizzazione"},{"location":"material/04_nn_tflow/26_dropout_reg/lecture/#261-strategie-di-regolarizzazione","text":"Abbiamo gi\u00e0 discusso delle strategie di regolarizzazione quando abbiamo visto la regressione logistica. In pratica, per comprendere il motivo per cui si usa la regolarizzazione possiamo usare il concetto di rasoio di Occam , ovvero, date due possibili spiegazioni per lo stesso fenomeno, quella che con tutta probabilit\u00e0 lo descrive in maniera migliore \u00e8 anche la pi\u00f9 semplice o, in altre parole, quella che assume il minor numero di ipotesi. Questo concetto si applica anche ad un modello di rete neurale: data un'architettura, esistono diverse combinazioni di valori di pesi che possono spiegare i dati, ed in generale le combinazioni pi\u00f9 semplici corrono meno il rischio di andare in overfitting se comparati a quelli pi\u00f9 complessi. Dalla precedente affermazione discende che un modo comune di mitigare l'overfitting del modello ai dati \u00e8 inserire degli opportuni vincoli sulla complessit\u00e0 della rete, \"forzando\" i pesi ad assumere valori ridotti, e rendendo implicitamente la distribuzione di detti valori maggiormente uniforme. Questo procedimento, chiamato weight regularization , \u00e8 ottenuto aggiungendo alla funzione di costo della rete un termine direttamente proporzionale al valore del peso. Di solito, si utilizzano due tecniche di regolarizzazione: nella regolarizzazione L1 il costo aggiunto \u00e8 proporzionale al valore assoluto dei coefficienti dei pesi, ovvero alla norma \\(L^1\\) ; nella regolarizzazione L2 il costo aggiunto \u00e8 proporzionale al quadrato del valore dei coefficienti dei pesi, ovvero alla norma \\(L^2\\) . Nota In generale, la regolarizzazione L1 favorisce la \"sparsit\u00e0\" dei dati, forzando il valore di alcuni pesi a \\(0\\) . Ci\u00f2 non avviene con la regolarizzazione L2. In Keras, possiamo aggiungere un parametro di regolarizzazione usando il package regularizers ed il parametro kernel_regularizers del layer da regolarizzare: from keras import regularizers layers . Dense ( 64 , activation = 'relu' , kernel_regularizers = regularizers . l2 ( 0.001 )) In questo caso, stiamo usando un valore di regolarizzazione pari a \\(0.001\\) , il che significa che ogni peso del layer regolarizzato aggiunger\u00e0 un valore pari a \\(0.001 \\cdot w_i^2\\) al costo totale della rete, con \\(w_i\\) valore del peso dell' \\(i\\) -mo coefficiente.","title":"26.1 - Strategie di regolarizzazione"},{"location":"material/04_nn_tflow/26_dropout_reg/lecture/#262-dropout","text":"Un altro metodo di regolarizzazione molto diffuso \u00e8 dato dall'uso di uno strato di dropout . L'idea alla base del dropout sta nel fatto che ogni nodo della rete deve restituire in output delle feature utili a prescindere da quelle restituite dagli altri nodi. Per ottenere questo risultato, si fa in modo che un certo numero di neuroni (scelti in maniera casuale ad ogni iterazione) dello strato precedente venga ignorato dal layer che implementa il dropout. In questo modo, il layer modifica ad ogni iterazione la sua connettivit\u00e0, ottenendo in un certo senso un diverso \"punto di vista\" sui dati stessi: in tal senso, il dropout aggiunge in maniera artificiosa del rumore sul processo di apprendimento , forzando una maggiore o minore importanza delle connessioni a seconda dei nodi scartati, ed evitando quindi delle situazioni dove i layer di rete tendono ad adattarsi vicendevolmente per \"correggere\" gli errori di predizione. Di conseguenza, il modello acquisisce maggiore capacit\u00e0 di generalizzazione, visto e considerato che ogni neurone isoler\u00e0 delle feature in maniera indipendente dagli altri. Per quello che riguarda gli iperparametri usati dal layer di dropout, il pi\u00f9 importante \u00e8 quello che specifica la probabilit\u00e0 con la quale gli output dello strato precedente vengono scartati. Un valore comune in tal senso \u00e8 \\(0.5\\) per gli strati nascosti, e \\(0.8\\) per lo strato di input.","title":"26.2 - Dropout"},{"location":"material/04_nn_tflow/27_tflow_images/exercises/","text":"E27 - Convolutional Neural Networks in TensorFlow e Keras \u00b6 Esercizio E27.1 \u00b6 Creare un modello di rete neurale composto in questo modo: _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= conv_1 ( Conv2D ) ( None, 30 , 30 , 32 ) 896 pooling_1 ( MaxPooling2D ) ( None, 15 , 15 , 32 ) 0 conv_2 ( Conv2D ) ( None, 13 , 13 , 32 ) 9248 dropout ( Dropout ) ( None, 13 , 13 , 32 ) 0 flatten ( Flatten ) ( None, 5408 ) 0 classification ( Dense ) ( None, 10 ) 54090 _________________________________________________________________ Questo modello deve essere in grado di classificare le immagini presenti nel dataset CIFAR10 . Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizi"},{"location":"material/04_nn_tflow/27_tflow_images/exercises/#e27-convolutional-neural-networks-in-tensorflow-e-keras","text":"","title":"E27 - Convolutional Neural Networks in TensorFlow e Keras"},{"location":"material/04_nn_tflow/27_tflow_images/exercises/#esercizio-e271","text":"Creare un modello di rete neurale composto in questo modo: _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= conv_1 ( Conv2D ) ( None, 30 , 30 , 32 ) 896 pooling_1 ( MaxPooling2D ) ( None, 15 , 15 , 32 ) 0 conv_2 ( Conv2D ) ( None, 13 , 13 , 32 ) 9248 dropout ( Dropout ) ( None, 13 , 13 , 32 ) 0 flatten ( Flatten ) ( None, 5408 ) 0 classification ( Dense ) ( None, 10 ) 54090 _________________________________________________________________ Questo modello deve essere in grado di classificare le immagini presenti nel dataset CIFAR10 . Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizio E27.1"},{"location":"material/04_nn_tflow/27_tflow_images/lecture/","text":"27 - Convolutional Neural Networks in TensorFlow e Keras \u00b6 Una delle applicazioni pi\u00f9 diffuse del deep learning riguarda il riconoscimento degli oggetti presenti all'interno di un'immagine. In tal senso, \u00e8 necessario introdurre due ulteriori layer, che rappresentano la base per le cosiddette convolutional neural network . Queste reti (che chiameremo per brevit\u00e0 CNN) sono specializzate nel lavorare su immagini, ma possono anche essere usate per segnali monodimensionali (come la voce) o tridimensionali (come le nuvole di punti). Le CNN assumono una rilevanza fondamentale nel moderno deep learning: \u00e8 grazie a loro se il campo del deep learning \u00e8 diventato mainstream nel mondo della ricerca, il che ha portato ad un interesse e, conseguentemente, ad avanzamenti impensabili in un ridottissimo lasso di tempo di soli dieci anni. Oggigiorno, le CNN vengono utilizzate in ogni ambito che preveda l'elaborazione di dati bidimensionali, dal riconoscimento facciale all'individuazione e caratterizzazione delle targhe degli autoveicoli in transito; conoscerle, quindi, \u00e8 imprescindibile. 27.1 - Layer convoluzionale \u00b6 I layer convoluzionali sono alla base del funzionamento delle CNN. Il concetto alla base di questo tipo di layer \u00e8 la convoluzione che, nel contesto del deep learning, \u00e8 un'operazione lineare che prevede la moltiplicazione di un insieme di pesi, chiamato filtro , con una piccola porzione (o finestra ) dell'immagine considerata, seguita ovviamente da una funzione di attivazione. Questo processo \u00e8 analogo a quello che avviene in una tradizionale rete neurale. Il filtro ha dimensioni volutamente inferiori rispetto a quelle dell'immagine da convolvere, tipicamente nell'ordine di \\(3 \\times 3\\) o \\(5 \\times 5\\) pixel; la convoluzione del filtro per ogni finestra dell'immagine \u00e8 inoltre assimilabile ad un prodotto scalare, per cui viene restituito sempre un unico valore per ogni finestra convoluta. In tal senso, il filtro viene \"fatto scorrere\" dall'alto in basso, da sinistra verso destra, anche su finestre sovrapposte, che scorrono quindi a passo di un pixel. Nota Tecnicamente, quindi, l'operazione definita come \"convoluzione\" \u00e8 in realt\u00e0 una cross-correlazione. Applicare sistematicamente lo stesso filtro su tutte le finestre possibili dell'immagine, anche sovrapposte, \u00e8 un'idea alquanto potente: infatti, un filtro viene opportunamente tarato per riconoscere uno specifico tipo di feature, come un bordo o una forma, che potr\u00e0 essere trovata ovunque nell'immagine grazie allo scorrimento, ottenendo la cosiddetta invarianza alla traslazione . Abbiamo accennato al fatto che l'output dell'applicazione di un filtro su di una finestra dell'immagine \u00e8 un prodotto scalare. Di conseguenza, man mano cheil filtro scorre, viene creato un array bidimensionale di valori, che viene indicato come mappa delle feature , o feature map . Sar\u00e0 proprio questa, e non l'immagine iniziale, ad essere passata al layer successivo. 26.3 - Layer di pooling \u00b6 Abbiamo visto come i layer convoluzionali creino delle feature map che \"sintetizzano\" la presenza di determinate feature all'interno di un input. Un limite di queste mappature sta per\u00f2 nel fatto che registrano la posizione precisa della feature individuata all'interno dell'input: ci\u00f2 significa quindi che anche piccole variazioni nella posizione di una feature risulter\u00e0 in una feature map completamente differente, il che comporta un'estrema sensibilit\u00e0 della rete neurale a piccole trasformazioni dell'immagine di input. Per risolvere questo problema, si utilizza un approccio chiamato sottocampionamento : in pratica, si ricava una versione a pi\u00f9 bassa risoluzione del segnale di ingresso, evidenziando di conseguenza gli elementi pi\u00f9 importanti, e scartando i piccoli dettagli non rilevanti nel task di classificazione. Per far questo, viene utilizzato un layer di pooling , applicato a cascata rispetto a quello convoluzionale. Il layer di pooling non fa altro che applicare un filtro, di solito di dimensioni \\(2 \\times 2\\) e con un passo di \\(2\\) pixel (quindi senza sovrapposizioni), che applica una funzione di sottocampionamento, scegliendo quindi un unico pixel tra quelli presenti nel filtro. Due funzioni di pooling molto comuni sono le seguenti: average pooling : questo filtro associa ad ogni finestra dell'immagine in input il valore medio presente nella finestra; max pooling : questo filtro associa ad ogni finestra dell'immagine in input il valore massimo presente nella finestra. Ottenendo quindi una versione \"sottocampionata\" dell'input, si raggiunge la cosiddetta invarianza a traslazioni locali , ovvero una sorta di \"insensibilit\u00e0\" del modello a traslazioni o rotazioni di entit\u00e0 minima.","title":"Dispense"},{"location":"material/04_nn_tflow/27_tflow_images/lecture/#27-convolutional-neural-networks-in-tensorflow-e-keras","text":"Una delle applicazioni pi\u00f9 diffuse del deep learning riguarda il riconoscimento degli oggetti presenti all'interno di un'immagine. In tal senso, \u00e8 necessario introdurre due ulteriori layer, che rappresentano la base per le cosiddette convolutional neural network . Queste reti (che chiameremo per brevit\u00e0 CNN) sono specializzate nel lavorare su immagini, ma possono anche essere usate per segnali monodimensionali (come la voce) o tridimensionali (come le nuvole di punti). Le CNN assumono una rilevanza fondamentale nel moderno deep learning: \u00e8 grazie a loro se il campo del deep learning \u00e8 diventato mainstream nel mondo della ricerca, il che ha portato ad un interesse e, conseguentemente, ad avanzamenti impensabili in un ridottissimo lasso di tempo di soli dieci anni. Oggigiorno, le CNN vengono utilizzate in ogni ambito che preveda l'elaborazione di dati bidimensionali, dal riconoscimento facciale all'individuazione e caratterizzazione delle targhe degli autoveicoli in transito; conoscerle, quindi, \u00e8 imprescindibile.","title":"27 - Convolutional Neural Networks in TensorFlow e Keras"},{"location":"material/04_nn_tflow/27_tflow_images/lecture/#271-layer-convoluzionale","text":"I layer convoluzionali sono alla base del funzionamento delle CNN. Il concetto alla base di questo tipo di layer \u00e8 la convoluzione che, nel contesto del deep learning, \u00e8 un'operazione lineare che prevede la moltiplicazione di un insieme di pesi, chiamato filtro , con una piccola porzione (o finestra ) dell'immagine considerata, seguita ovviamente da una funzione di attivazione. Questo processo \u00e8 analogo a quello che avviene in una tradizionale rete neurale. Il filtro ha dimensioni volutamente inferiori rispetto a quelle dell'immagine da convolvere, tipicamente nell'ordine di \\(3 \\times 3\\) o \\(5 \\times 5\\) pixel; la convoluzione del filtro per ogni finestra dell'immagine \u00e8 inoltre assimilabile ad un prodotto scalare, per cui viene restituito sempre un unico valore per ogni finestra convoluta. In tal senso, il filtro viene \"fatto scorrere\" dall'alto in basso, da sinistra verso destra, anche su finestre sovrapposte, che scorrono quindi a passo di un pixel. Nota Tecnicamente, quindi, l'operazione definita come \"convoluzione\" \u00e8 in realt\u00e0 una cross-correlazione. Applicare sistematicamente lo stesso filtro su tutte le finestre possibili dell'immagine, anche sovrapposte, \u00e8 un'idea alquanto potente: infatti, un filtro viene opportunamente tarato per riconoscere uno specifico tipo di feature, come un bordo o una forma, che potr\u00e0 essere trovata ovunque nell'immagine grazie allo scorrimento, ottenendo la cosiddetta invarianza alla traslazione . Abbiamo accennato al fatto che l'output dell'applicazione di un filtro su di una finestra dell'immagine \u00e8 un prodotto scalare. Di conseguenza, man mano cheil filtro scorre, viene creato un array bidimensionale di valori, che viene indicato come mappa delle feature , o feature map . Sar\u00e0 proprio questa, e non l'immagine iniziale, ad essere passata al layer successivo.","title":"27.1 - Layer convoluzionale"},{"location":"material/04_nn_tflow/27_tflow_images/lecture/#263-layer-di-pooling","text":"Abbiamo visto come i layer convoluzionali creino delle feature map che \"sintetizzano\" la presenza di determinate feature all'interno di un input. Un limite di queste mappature sta per\u00f2 nel fatto che registrano la posizione precisa della feature individuata all'interno dell'input: ci\u00f2 significa quindi che anche piccole variazioni nella posizione di una feature risulter\u00e0 in una feature map completamente differente, il che comporta un'estrema sensibilit\u00e0 della rete neurale a piccole trasformazioni dell'immagine di input. Per risolvere questo problema, si utilizza un approccio chiamato sottocampionamento : in pratica, si ricava una versione a pi\u00f9 bassa risoluzione del segnale di ingresso, evidenziando di conseguenza gli elementi pi\u00f9 importanti, e scartando i piccoli dettagli non rilevanti nel task di classificazione. Per far questo, viene utilizzato un layer di pooling , applicato a cascata rispetto a quello convoluzionale. Il layer di pooling non fa altro che applicare un filtro, di solito di dimensioni \\(2 \\times 2\\) e con un passo di \\(2\\) pixel (quindi senza sovrapposizioni), che applica una funzione di sottocampionamento, scegliendo quindi un unico pixel tra quelli presenti nel filtro. Due funzioni di pooling molto comuni sono le seguenti: average pooling : questo filtro associa ad ogni finestra dell'immagine in input il valore medio presente nella finestra; max pooling : questo filtro associa ad ogni finestra dell'immagine in input il valore massimo presente nella finestra. Ottenendo quindi una versione \"sottocampionata\" dell'input, si raggiunge la cosiddetta invarianza a traslazioni locali , ovvero una sorta di \"insensibilit\u00e0\" del modello a traslazioni o rotazioni di entit\u00e0 minima.","title":"26.3 - Layer di pooling"},{"location":"material/04_nn_tflow/28_tricks/exercises/","text":"E28 - TensorFlow & Keras: Tips & Tricks \u00b6 E28.1 \u00b6 Scarichiamo il dataset flowers . Per farlo, usiamo il seguente codice: import pathlib from tensorflow import keras dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\" data_dir = keras . utils . get_file ( origin = dataset_url , extract = True ) data_dir = pathlib . Path ( data_dir ) . parent data_dir = pathlib . Path ( data_dir , 'flower_photos' ) Utilizziamo i metodi di Keras per caricare il dataset in memoria ed addestrare un modello per la classificazione di questo tipo: _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= rescaling_1 ( Rescaling ) ( None, 150 , 150 , 3 ) 0 conv_1 ( Conv2D ) ( None, 148 , 148 , 32 ) 896 max_pool_1 ( MaxPooling2D ) ( None, 74 , 74 , 32 ) 0 conv_2 ( Conv2D ) ( None, 72 , 72 , 32 ) 9248 max_pool_2 ( MaxPooling2D ) ( None, 36 , 36 , 32 ) 0 dropout ( Dropout ) ( None, 36 , 36 , 32 ) 0 flatten_1 ( Flatten ) ( None, 41472 ) 0 classification ( Dense ) ( None, 5 ) 207365 _________________________________________________________________ Inferiamo il numero di classi del dataset (ovvero la X nel precedente sommario) usando l'attributo class_names del dataset ottenuto da image_dataset_from_directory . Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook . E28.2 \u00b6 Utilizziamo gli opportuni callback per terminare l'addestramento del modello visto nell'esercizio 1 dopo 3 epoche nelle quali l'accuracy di validazione non migliora per pi\u00f9 di 0.01 . Visualizziamo inoltre i risultati ottenuti in TensorBoard. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook . E28.3 \u00b6 Scarichiamo il dataset Stack Overflow mediante il seguente codice: import pathlib from tensorflow import keras dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\" data_dir = keras . utils . get_file ( origin = dataset_url , extract = True , cache_subdir = 'datasets/stack_overflow' ) data_dir = pathlib . Path ( data_dir ) . parent train_dir = pathlib . Path ( data_dir , 'train' ) test_dir = pathlib . Path ( data_dir , 'test' ) Utilizziamo i metodi di Keras per caricare il dataset in memoria ed addestrare un modello per la classificazione di questo tipo: _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= dense_1 ( Dense ) ( None, 64 ) 64064 dense_2 ( Dense ) ( None, 4 ) 260 _________________________________________________________________ Usiamo gli stessi callback utilizzati in precedenza. Inferiamo il numero di classi del dataset (ovvero la X nel precedente sommario) usando l'attributo class_names del dataset ottenuto da text_dataset_from_directory . Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"Esercizi"},{"location":"material/04_nn_tflow/28_tricks/exercises/#e28-tensorflow-keras-tips-tricks","text":"","title":"E28 - TensorFlow &amp; Keras: Tips &amp; Tricks"},{"location":"material/04_nn_tflow/28_tricks/exercises/#e281","text":"Scarichiamo il dataset flowers . Per farlo, usiamo il seguente codice: import pathlib from tensorflow import keras dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\" data_dir = keras . utils . get_file ( origin = dataset_url , extract = True ) data_dir = pathlib . Path ( data_dir ) . parent data_dir = pathlib . Path ( data_dir , 'flower_photos' ) Utilizziamo i metodi di Keras per caricare il dataset in memoria ed addestrare un modello per la classificazione di questo tipo: _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= rescaling_1 ( Rescaling ) ( None, 150 , 150 , 3 ) 0 conv_1 ( Conv2D ) ( None, 148 , 148 , 32 ) 896 max_pool_1 ( MaxPooling2D ) ( None, 74 , 74 , 32 ) 0 conv_2 ( Conv2D ) ( None, 72 , 72 , 32 ) 9248 max_pool_2 ( MaxPooling2D ) ( None, 36 , 36 , 32 ) 0 dropout ( Dropout ) ( None, 36 , 36 , 32 ) 0 flatten_1 ( Flatten ) ( None, 41472 ) 0 classification ( Dense ) ( None, 5 ) 207365 _________________________________________________________________ Inferiamo il numero di classi del dataset (ovvero la X nel precedente sommario) usando l'attributo class_names del dataset ottenuto da image_dataset_from_directory . Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"E28.1"},{"location":"material/04_nn_tflow/28_tricks/exercises/#e282","text":"Utilizziamo gli opportuni callback per terminare l'addestramento del modello visto nell'esercizio 1 dopo 3 epoche nelle quali l'accuracy di validazione non migliora per pi\u00f9 di 0.01 . Visualizziamo inoltre i risultati ottenuti in TensorBoard. Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"E28.2"},{"location":"material/04_nn_tflow/28_tricks/exercises/#e283","text":"Scarichiamo il dataset Stack Overflow mediante il seguente codice: import pathlib from tensorflow import keras dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\" data_dir = keras . utils . get_file ( origin = dataset_url , extract = True , cache_subdir = 'datasets/stack_overflow' ) data_dir = pathlib . Path ( data_dir ) . parent train_dir = pathlib . Path ( data_dir , 'train' ) test_dir = pathlib . Path ( data_dir , 'test' ) Utilizziamo i metodi di Keras per caricare il dataset in memoria ed addestrare un modello per la classificazione di questo tipo: _________________________________________________________________ Layer ( type ) Output Shape Param # ================================================================= dense_1 ( Dense ) ( None, 64 ) 64064 dense_2 ( Dense ) ( None, 4 ) 260 _________________________________________________________________ Usiamo gli stessi callback utilizzati in precedenza. Inferiamo il numero di classi del dataset (ovvero la X nel precedente sommario) usando l'attributo class_names del dataset ottenuto da text_dataset_from_directory . Soluzione La soluzione a questo esercizio \u00e8 contenuta in questo notebook .","title":"E28.3"},{"location":"material/04_nn_tflow/28_tricks/lecture/","text":"28 - TensorFlow & Keras: Tips & Tricks \u00b6 28.1 - Dataset \u00b6 I dati che abbiamo utilizzato finora erano organizzati sotto forma di array NumPy. Tuttavia, per dataset di grosse dimensioni, potrebbe essere necessario utilizzare degli oggetti di tipo Dataset . In tal senso, Keras ci mette a disposizione diverse tecniche; vediamone alcuni. 28.1.1 - Immagini \u00b6 Per caricare un dataset di immagini a partire da una cartella, possiamo usare la funzione image_dataset_from_directory , che ha una sintassi di questo tipo: train = tf . keras . utils . image_dataset_from_directory ( data_dir , validation_split = 0.2 , subset = 'training' , image_size = ( img_height , img_width ), batch_size = batch_size , seed = seed ) val = tf . keras . utils . image_dataset_from_directory ( data_dir , validation_split = 0.2 , subset = 'validation' , image_size = ( img_height , img_width ), batch_size = batch_size , seed = seed ) Nel precedente esempio: data_dir \u00e8 la cartella dove sono presenti i dati; validation_split indica quanti dati usare per la validazione. Il valore deve essere coerente tra il dataset di train e quello di validazione; subset indica se il dataset \u00e8 indirizzato al training o alla validazione; image_size rappresenta la dimensione (in pixel) dell'immagine; batch_size rappresenta la dimensione del batch di dati da usare; seed \u00e8 un parametro che ci assicura la coerenza tra le immagini scelte per il training e quelle scelte per il test. La cartella data_dir deve essere organizzata come segue: data_dir/ ...class1/ ......1.png ......2.png ...class2/ ......1.png ......2.png ......3.png A questo punto possiamo passare train e val direttamente al metodo fit del nostro modello. model . fit ( train , validation_data = val , epochs = 10 ) Nota Un accorgimento utile a migliorare le prestazioni della nostra rete \u00e8 quello di inserire un layer di rescaling qualora si abbia a che fare con immagini a colori. Infatti, i canali RGB possono assumere valori all'interno del range \\([0, 255]\\) , mentre \u00e8 consigliabile usare per una rete neurale valori compresi nell'intervallo \\([0, 1]\\) . Keras ci mette a disposizione un apposito layer: py tf.keras.layers.Rescaling(1./255) 28.1.2 - Testo \u00b6 Keras offre un metodo simile per creare un dataset a partire da un insieme di file di testo, utilizzando il metodo text_dataset_from_directory . Analogamente al metodo usato per le immagini, text_dataset_from_directory si aspetta una cartella in una certa forma: data_dir/ ...class1/ ......1.txt ......2.txt ...class2/ ......1.txt ......2.txt ......3.txt Per caricare il nostro dataset possiamo usare una forma analoga a quella delle immagini: train = text_dataset_from_direcotry ( data_dir , batch_size = batch_size , validation_split = 0.2 , subset = 'training' , seed = seed ) val = text_dataset_from_directory ( data_dir , batch_size = batch_size , validation_split = 0.2 , subset = 'validation' , seed = seed ) I parametri sono esattamente gli stessi, a meno dell'assenza del parametro image_size . 28.1.2.1 - preparazione dati testuali per il trainng \u00b6 Rispetto alle immagini, i dati testuali richiedono tre ulteriori operazioni, ovvero: standardizzazione : si tratta di una procedura di preprocessing sul testo, che consiste tipicamente nella rimozione della punteggiatura. Di default, questa operazione converte l'intero testo in minuscolo e rimuove la punteggiatura; tokenizzazione : si tratta di una procedura di suddivisione delle stringhe in token . Ad esempio, si pu\u00f2 suddividere una frase nelle singole parole. Di default, questa operazione suddivide i token in base allo spazio; vettorizzazione : si tratta della procedura di conversione dei token in valori numerici trattabili da un modello di rete neurale. Di default, il metodo di vettorizzazione \u00e8 int . Questi tre step sono gestiti in automatico da un layer chiamato TextVectorization . Procediamo quindi a creare un layer di TextVectorization utilizzando una vettorizzazione binaria : vectorize_layer = TextVectorization ( max_tokens = VOCAB_SIZE , output_mode = 'binary' ) In particolare, max_tokens permette di stabilire il numero massimo di vocaboli consentiti, mentre l' output_mode indica la modalit\u00e0 con cui sar\u00e0 gestita la sequenza vettorizzat. A questo punto, occorre creare il dataset che sar\u00e0 effettivamente passato al primo layer della rete neurale. In tal senso, dobbiamo tenere conto che i dataset che abbiamo creato mediante text_dataset_from_directory non sono ancora stati vettorizzati, ed inoltre le singole coppie campione/label non sono accessibili mediante le tecniche standard di indicizzazione. Ci\u00f2 \u00e8 legato al fatto che le funzioni *_dataset_from_directory creano un oggetto di tipo BatchDataset , usato da TensorFlow per ottimizzare il caricamento in memoria di dataset di grosse dimensioni. Di conseguenza, dovremo innanzitutto estrarre il testo senza considerare le singole label . Per farlo, possiamo usare la funzione map() del nostro dataset: train_text = train . map ( lambda text , labels : text ) La funzione map() non fa altro che applicare all'intero iterabile la funzione passata come argomento. In tal senso, il parametro passato altro non \u00e8 se non una lambda function , ovvero una funzione anonima che assume una forma sintattica del tipo: lambda args : expression e quindi applica l'espressione a valle dei : agli argomenti passati. In questo caso, stiamo semplicemente facendo in modo che tutte le coppie testo/label siano \"mappate\" sul semplice testo. Una volta estratto il testo, dovremo chiamare il metodo adapt del layer di vettorizzazione in modo tale da creare il vocabolario che associ un determinato token numerico a ciascuna stringa. vectorize_layer . adapt ( train_text ) Potremo quindi procedere ad integrare il layer di vettorizzazione all'interno del nostro modello. In tal senso, dovremo assicurarci che il modello abbia un input di forma (1,) e tipo stringa, facendo in modo che la rete abbia un'unica stringa in input per ciascun batch: model . add ( keras . Input ( shape = ( 1 ,), dtype = tf . string )) 28.1.3 - Array NumPy \u00b6 Nel caso di un array NumPy, occorre utilizzare il metodo from_tensor_slices : train = from_tensor_slices (( x_train , y_train )) val = from_tensor_slices (( x_test , y_test )) 28.2 - Callback \u00b6 Un callback \u00e8 un'azione che il modello pu\u00f2 effettuare durante diverse fasi dell'apprendimento. Keras ne offre di numerosi, che possono essere utilizzati ad esempio per monitorare le metriche che abbiamo scelto per la valutazione del modello, o salvare lo stesso su disco. Per usare i callback, dovremo crearne una lista, e passarla al parametro callbacks usato dal metodo fit() del nostro modello. Proviamo, ad esempio, a creare un insieme di callback che permetta di salvare i pesi del modello con una certa frequenza, e che termini il training se lo stesso sta andando in overfitting. Per prima cosa, creiamo un oggetto di tipo ModelCheckpoint , che ci permette di salvare i pesi del modello: mc_callback = keras . callbacks . ModelCheckpoint ( filepath = path_to_checkpoints , save_weights_only = True , monitor = 'val_acc' , save_best_only = True ) In particolare: filepath indica il percorso del file nel quale salveremo i checkpoint; save_weights_only istruisce Keras a salvare soltanto i pesi del modello, riducendo lo spazio occupato in memoria; monitor indica la metrica da monitorare; save_best_only istruisce Keras a salvare soltanto il modello \"migliore\", trascurando quelli ottenuti durante le altre iterazioni. Proviamo poi a creare un oggetto di tipo EarlyStopping , che ci permette di terminare l'addestramento qualora la metrica monitorata non presenti miglioramenti tra un'epoca e l'altra. Ad esempio: es_callback = keras . callbacks . EarlyStopping ( monitor = 'val_acc' , min_delta = 0.1 , patience = 3 , restore_best_weights = True ) Nel codice precedente: monitor indica la metrica da monitorare; min_delta indica il valore minimo da considerare migliorativo per la metrica; patience indica il numero di epoche dopo il quale il training viene interrotto in assenza di miglioramenti; restore_best_weights indica se ripristinare i valori migliori ottenuti per i parametri dopo il termine dell'addestramento, o se usare gli ultimi. Aggiungiamo infine un ultimo callback, da utilizzare per permettere di visualizzare i risultati del nostro training su un tool di visualizzazione chiamato TensorBoard. tb_callback = TensorBoard () Per TensorBoard, possiamo lasciare i parametri al loro valore di default. La reference completa \u00e8 disponibile sulla documentazione ufficiale . Possiamo adesso specificare i callback da utilizzare passando le precedenti variabili sotto forma di lista al metodo fit() del nostro modello. 28.3 - Transfer learning e fine tuning \u00b6 Le reti neurali, e soprattutto le CNN, presentano un'interessante caratteristica, ovvero quella di apprendere delle feature di carattere generico nei loro primi strati, andandosi a specializzare man mano che si va in profondit\u00e0 nella rete. Partendo da questa considerazione \u00e8 stata elaborata la tecnica del transfer learning , che consiste nel prendere il modello addestrato su un problema e riconfigurarlo per risolverne uno nuovo (ma, ovviamente, simile: ad esempio, un tool che permette di valutare la razza di un gatto pu\u00f2 essere usato per distinguere tra leopardi e tigri). Questa tecnica permette anche di addestrare un numero limitato di parametri, per cui \u00e8 possibile utilizzarla quando si ha a che fare con dataset di dimensioni limitate. In tal senso, il transfer learning segue di solito questi step: consideriamo i layer ed i pesi di un modello addestrato in precedenza; effettuiamo il freezing (congelamento) di questi layer, fissando i valori dei pesi; creiamo ed inseriamo alcuni layer successivi a quelli congelati per adattarli al nostro problema; addestriamo i nuovi layer sul nostro problema. Opzionalmente, \u00e8 possibile effettuare un passaggio di fine tuning , \"sbloccando\" il modello ottenuto in precedenza e riaddestrandolo sull'intero dataset con un basso learning rate.","title":"Dispense"},{"location":"material/04_nn_tflow/28_tricks/lecture/#28-tensorflow-keras-tips-tricks","text":"","title":"28 - TensorFlow &amp; Keras: Tips &amp; Tricks"},{"location":"material/04_nn_tflow/28_tricks/lecture/#281-dataset","text":"I dati che abbiamo utilizzato finora erano organizzati sotto forma di array NumPy. Tuttavia, per dataset di grosse dimensioni, potrebbe essere necessario utilizzare degli oggetti di tipo Dataset . In tal senso, Keras ci mette a disposizione diverse tecniche; vediamone alcuni.","title":"28.1 - Dataset"},{"location":"material/04_nn_tflow/28_tricks/lecture/#2811-immagini","text":"Per caricare un dataset di immagini a partire da una cartella, possiamo usare la funzione image_dataset_from_directory , che ha una sintassi di questo tipo: train = tf . keras . utils . image_dataset_from_directory ( data_dir , validation_split = 0.2 , subset = 'training' , image_size = ( img_height , img_width ), batch_size = batch_size , seed = seed ) val = tf . keras . utils . image_dataset_from_directory ( data_dir , validation_split = 0.2 , subset = 'validation' , image_size = ( img_height , img_width ), batch_size = batch_size , seed = seed ) Nel precedente esempio: data_dir \u00e8 la cartella dove sono presenti i dati; validation_split indica quanti dati usare per la validazione. Il valore deve essere coerente tra il dataset di train e quello di validazione; subset indica se il dataset \u00e8 indirizzato al training o alla validazione; image_size rappresenta la dimensione (in pixel) dell'immagine; batch_size rappresenta la dimensione del batch di dati da usare; seed \u00e8 un parametro che ci assicura la coerenza tra le immagini scelte per il training e quelle scelte per il test. La cartella data_dir deve essere organizzata come segue: data_dir/ ...class1/ ......1.png ......2.png ...class2/ ......1.png ......2.png ......3.png A questo punto possiamo passare train e val direttamente al metodo fit del nostro modello. model . fit ( train , validation_data = val , epochs = 10 ) Nota Un accorgimento utile a migliorare le prestazioni della nostra rete \u00e8 quello di inserire un layer di rescaling qualora si abbia a che fare con immagini a colori. Infatti, i canali RGB possono assumere valori all'interno del range \\([0, 255]\\) , mentre \u00e8 consigliabile usare per una rete neurale valori compresi nell'intervallo \\([0, 1]\\) . Keras ci mette a disposizione un apposito layer: py tf.keras.layers.Rescaling(1./255)","title":"28.1.1 - Immagini"},{"location":"material/04_nn_tflow/28_tricks/lecture/#2812-testo","text":"Keras offre un metodo simile per creare un dataset a partire da un insieme di file di testo, utilizzando il metodo text_dataset_from_directory . Analogamente al metodo usato per le immagini, text_dataset_from_directory si aspetta una cartella in una certa forma: data_dir/ ...class1/ ......1.txt ......2.txt ...class2/ ......1.txt ......2.txt ......3.txt Per caricare il nostro dataset possiamo usare una forma analoga a quella delle immagini: train = text_dataset_from_direcotry ( data_dir , batch_size = batch_size , validation_split = 0.2 , subset = 'training' , seed = seed ) val = text_dataset_from_directory ( data_dir , batch_size = batch_size , validation_split = 0.2 , subset = 'validation' , seed = seed ) I parametri sono esattamente gli stessi, a meno dell'assenza del parametro image_size .","title":"28.1.2 - Testo"},{"location":"material/04_nn_tflow/28_tricks/lecture/#28121-preparazione-dati-testuali-per-il-trainng","text":"Rispetto alle immagini, i dati testuali richiedono tre ulteriori operazioni, ovvero: standardizzazione : si tratta di una procedura di preprocessing sul testo, che consiste tipicamente nella rimozione della punteggiatura. Di default, questa operazione converte l'intero testo in minuscolo e rimuove la punteggiatura; tokenizzazione : si tratta di una procedura di suddivisione delle stringhe in token . Ad esempio, si pu\u00f2 suddividere una frase nelle singole parole. Di default, questa operazione suddivide i token in base allo spazio; vettorizzazione : si tratta della procedura di conversione dei token in valori numerici trattabili da un modello di rete neurale. Di default, il metodo di vettorizzazione \u00e8 int . Questi tre step sono gestiti in automatico da un layer chiamato TextVectorization . Procediamo quindi a creare un layer di TextVectorization utilizzando una vettorizzazione binaria : vectorize_layer = TextVectorization ( max_tokens = VOCAB_SIZE , output_mode = 'binary' ) In particolare, max_tokens permette di stabilire il numero massimo di vocaboli consentiti, mentre l' output_mode indica la modalit\u00e0 con cui sar\u00e0 gestita la sequenza vettorizzat. A questo punto, occorre creare il dataset che sar\u00e0 effettivamente passato al primo layer della rete neurale. In tal senso, dobbiamo tenere conto che i dataset che abbiamo creato mediante text_dataset_from_directory non sono ancora stati vettorizzati, ed inoltre le singole coppie campione/label non sono accessibili mediante le tecniche standard di indicizzazione. Ci\u00f2 \u00e8 legato al fatto che le funzioni *_dataset_from_directory creano un oggetto di tipo BatchDataset , usato da TensorFlow per ottimizzare il caricamento in memoria di dataset di grosse dimensioni. Di conseguenza, dovremo innanzitutto estrarre il testo senza considerare le singole label . Per farlo, possiamo usare la funzione map() del nostro dataset: train_text = train . map ( lambda text , labels : text ) La funzione map() non fa altro che applicare all'intero iterabile la funzione passata come argomento. In tal senso, il parametro passato altro non \u00e8 se non una lambda function , ovvero una funzione anonima che assume una forma sintattica del tipo: lambda args : expression e quindi applica l'espressione a valle dei : agli argomenti passati. In questo caso, stiamo semplicemente facendo in modo che tutte le coppie testo/label siano \"mappate\" sul semplice testo. Una volta estratto il testo, dovremo chiamare il metodo adapt del layer di vettorizzazione in modo tale da creare il vocabolario che associ un determinato token numerico a ciascuna stringa. vectorize_layer . adapt ( train_text ) Potremo quindi procedere ad integrare il layer di vettorizzazione all'interno del nostro modello. In tal senso, dovremo assicurarci che il modello abbia un input di forma (1,) e tipo stringa, facendo in modo che la rete abbia un'unica stringa in input per ciascun batch: model . add ( keras . Input ( shape = ( 1 ,), dtype = tf . string ))","title":"28.1.2.1 - preparazione dati testuali per il trainng"},{"location":"material/04_nn_tflow/28_tricks/lecture/#2813-array-numpy","text":"Nel caso di un array NumPy, occorre utilizzare il metodo from_tensor_slices : train = from_tensor_slices (( x_train , y_train )) val = from_tensor_slices (( x_test , y_test ))","title":"28.1.3 - Array NumPy"},{"location":"material/04_nn_tflow/28_tricks/lecture/#282-callback","text":"Un callback \u00e8 un'azione che il modello pu\u00f2 effettuare durante diverse fasi dell'apprendimento. Keras ne offre di numerosi, che possono essere utilizzati ad esempio per monitorare le metriche che abbiamo scelto per la valutazione del modello, o salvare lo stesso su disco. Per usare i callback, dovremo crearne una lista, e passarla al parametro callbacks usato dal metodo fit() del nostro modello. Proviamo, ad esempio, a creare un insieme di callback che permetta di salvare i pesi del modello con una certa frequenza, e che termini il training se lo stesso sta andando in overfitting. Per prima cosa, creiamo un oggetto di tipo ModelCheckpoint , che ci permette di salvare i pesi del modello: mc_callback = keras . callbacks . ModelCheckpoint ( filepath = path_to_checkpoints , save_weights_only = True , monitor = 'val_acc' , save_best_only = True ) In particolare: filepath indica il percorso del file nel quale salveremo i checkpoint; save_weights_only istruisce Keras a salvare soltanto i pesi del modello, riducendo lo spazio occupato in memoria; monitor indica la metrica da monitorare; save_best_only istruisce Keras a salvare soltanto il modello \"migliore\", trascurando quelli ottenuti durante le altre iterazioni. Proviamo poi a creare un oggetto di tipo EarlyStopping , che ci permette di terminare l'addestramento qualora la metrica monitorata non presenti miglioramenti tra un'epoca e l'altra. Ad esempio: es_callback = keras . callbacks . EarlyStopping ( monitor = 'val_acc' , min_delta = 0.1 , patience = 3 , restore_best_weights = True ) Nel codice precedente: monitor indica la metrica da monitorare; min_delta indica il valore minimo da considerare migliorativo per la metrica; patience indica il numero di epoche dopo il quale il training viene interrotto in assenza di miglioramenti; restore_best_weights indica se ripristinare i valori migliori ottenuti per i parametri dopo il termine dell'addestramento, o se usare gli ultimi. Aggiungiamo infine un ultimo callback, da utilizzare per permettere di visualizzare i risultati del nostro training su un tool di visualizzazione chiamato TensorBoard. tb_callback = TensorBoard () Per TensorBoard, possiamo lasciare i parametri al loro valore di default. La reference completa \u00e8 disponibile sulla documentazione ufficiale . Possiamo adesso specificare i callback da utilizzare passando le precedenti variabili sotto forma di lista al metodo fit() del nostro modello.","title":"28.2 - Callback"},{"location":"material/04_nn_tflow/28_tricks/lecture/#283-transfer-learning-e-fine-tuning","text":"Le reti neurali, e soprattutto le CNN, presentano un'interessante caratteristica, ovvero quella di apprendere delle feature di carattere generico nei loro primi strati, andandosi a specializzare man mano che si va in profondit\u00e0 nella rete. Partendo da questa considerazione \u00e8 stata elaborata la tecnica del transfer learning , che consiste nel prendere il modello addestrato su un problema e riconfigurarlo per risolverne uno nuovo (ma, ovviamente, simile: ad esempio, un tool che permette di valutare la razza di un gatto pu\u00f2 essere usato per distinguere tra leopardi e tigri). Questa tecnica permette anche di addestrare un numero limitato di parametri, per cui \u00e8 possibile utilizzarla quando si ha a che fare con dataset di dimensioni limitate. In tal senso, il transfer learning segue di solito questi step: consideriamo i layer ed i pesi di un modello addestrato in precedenza; effettuiamo il freezing (congelamento) di questi layer, fissando i valori dei pesi; creiamo ed inseriamo alcuni layer successivi a quelli congelati per adattarli al nostro problema; addestriamo i nuovi layer sul nostro problema. Opzionalmente, \u00e8 possibile effettuare un passaggio di fine tuning , \"sbloccando\" il modello ottenuto in precedenza e riaddestrandolo sull'intero dataset con un basso learning rate.","title":"28.3 - Transfer learning e fine tuning"},{"location":"material/05_time_series/29_ts/lecture/","text":"29 - Analisi di serie temporali \u00b6 A differenza dei dati identicamente ed indipendentemente distribuiti , le serie temporali sono delle sequenze i cui parametri variano con il variare del tempo.","title":"29 - Analisi di serie temporali"},{"location":"material/05_time_series/29_ts/lecture/#29-analisi-di-serie-temporali","text":"A differenza dei dati identicamente ed indipendentemente distribuiti , le serie temporali sono delle sequenze i cui parametri variano con il variare del tempo.","title":"29 - Analisi di serie temporali"},{"location":"material/appendix/01_python_vs_code/guide/","text":"Appendice A - Configurazione dell'ambiente di sviluppo Python \u00b6 Installazione di Python \u00b6 Andare al seguente indirizzo , e selezionare la versione adatta al proprio sistema operativo. Iniziare la procedura di installazione (ad esempio, in Windows, cliccando sull'eseguibile appena scaricato). E' fortemente consigliato aggiungere Python al proprio PATH spuntando l'opportuna casella durante l'installazione , come mostrato in figura. Una volta completata la procedura di installazione, aprire uno shell (ad esempio, il prompt dei comandi), e digitare python . Se tutto \u00e8 andato per il verso giusto, apparir\u00e0 una schermata simile alla successiva. Installazione di Visual Studio Code \u00b6 Andare al seguente indirizzo , e selezionare la versione adatta al proprio sistema operativo. Seguire la procedura di installazione mostrata a schermo. E' anche in questo caso consigliata l'aggiunta di Visual Studio Code al path, come mostrato in figura.","title":"A - Installazione di Python e Visual Studio Code"},{"location":"material/appendix/01_python_vs_code/guide/#appendice-a-configurazione-dellambiente-di-sviluppo-python","text":"","title":"Appendice A - Configurazione dell'ambiente di sviluppo Python"},{"location":"material/appendix/01_python_vs_code/guide/#installazione-di-python","text":"Andare al seguente indirizzo , e selezionare la versione adatta al proprio sistema operativo. Iniziare la procedura di installazione (ad esempio, in Windows, cliccando sull'eseguibile appena scaricato). E' fortemente consigliato aggiungere Python al proprio PATH spuntando l'opportuna casella durante l'installazione , come mostrato in figura. Una volta completata la procedura di installazione, aprire uno shell (ad esempio, il prompt dei comandi), e digitare python . Se tutto \u00e8 andato per il verso giusto, apparir\u00e0 una schermata simile alla successiva.","title":"Installazione di Python"},{"location":"material/appendix/01_python_vs_code/guide/#installazione-di-visual-studio-code","text":"Andare al seguente indirizzo , e selezionare la versione adatta al proprio sistema operativo. Seguire la procedura di installazione mostrata a schermo. E' anche in questo caso consigliata l'aggiunta di Visual Studio Code al path, come mostrato in figura.","title":"Installazione di Visual Studio Code"},{"location":"material/appendix/02_libraries/lecture/","text":"Appendice B: Installazione di una libreria Python \u00b6 Per installare una libreria Python abbiamo a disposizione diverse opzioni. Vediamole nel dettaglio, immaginando di voler installare la libreria NumPy. Opzione A: utilizzare pip \u00b6 La prima opzione, e probabilmente quella maggiormente utilizzata, \u00e8 utilizzare il package manager (ovvero, il gestore di pacchetti) integrato in Python, chiamato pip . Per farlo, apriamo un terminale assicurandoci di avere i diritti di amministratore; in Linux, dovremo usare l'istruzione sudo , a meno che non siamo utenti rott, mentre in Windows ci baster\u00e0 aprire la shell come amministratori. Una volta aperto il terminale, dovremo scrivere: pip install numpy Installare una libreria in questo modo \u00e8 sicuramente molto semplice, ma porta con s\u00e8 uno svantaggio: infatti, l'installazione della stessa avviene globalmente , ovvero risulta essere valida per l'intera macchina. Ci\u00f2 potrebbe non sembrare rilevante; tuttavia, in ben determinate situazioni, si pu\u00f2 rendere necessario installare particolari combinazioni di versioni di librerie, per usufruire di funzionalit\u00e0 successivamente deprecate o, al contrario, non presenti in versioni antecedenti. In tal senso, se installiamo una certa libreria globalmente, tutti i nostri programmi dovranno necessariamente utilizzare quella libreria in quella specifica versione, il che ci pu\u00f2 vincolare fortemente a lungo andare. Opzione B: utilizzare pip ed un ambiente virtuale \u00b6 Un'altra opzione \u00e8 quella di utilizzare pip in un opportuno ambiente virtuale . Quest'ultimo altro non \u00e8 se non un ambiente \"separato\" all'interno del nostro calcolatore, nel quale andremo ad inserire tutte le librerie che utilizzeremo per i progetti da inserire all'interno dell'ambiente (con le versioni specifiche). Per utilizzare questa opzione, dovremo innanzitutto creare un ambiente virtuale. Per farlo, dobbiamo usare un'opportuna libreria Python, che dovremo installare globalmente mediante pip . Nota L'installazione globale delle librerie per la gestione dell'ambiente virtuale \u00e8 strettamente necessaria, e non contraddice il principio descritto nelle righe precedenti: infatti, l'idea \u00e8 che si possa creare un ambiente virtuale in qualsiasi momento . Installiamo quindi la libreria virtualenvwrapper , o l'equivalente porting per Windows virtualenvwrapper-win : ===\"Linux\" pip install virtualenvwrapper ===\"Windows\" pip install virtualenvwrapper-win Una volta completata l'installazione, utilizzeremo il comando mkvirtualenv , seguito da un nome a nostra scelta, per creare l'ambiente virtuale. Ad esempio: mkvirtualenv pcs Noteremo che, a sinistra del terminale, sar\u00e0 apparsa la scritta (pcs) : ( pcs ) current_working_directory/ Questo ci indica che siamo all'interno del nostro ambiente virtuale. Procediamo adesso all'installazione della libreria NumPy mediante pip : ( pcs ) current_working_directory/ pip install numpy In questo modo, avremo installato NumPy esclusivamente all'interno del nostro ambiente virtuale. Per verificarlo, basta eseguire l'istruzione pip freeze , che restituisce tutte le librerie presenti nell'ambiente in cui siamo attualmente, assieme alle loro versioni. Il file requirements.txt Pratica comune \u00e8 quella di memorizzare tutte le librerie presenti in un ambiente virtuale in un file chiamato requirements.txt . Cos\u00ec facendo, un altro programmatore sar\u00e0 in grado di \"clonare\" il nostro ambiente virtuale. Per salvare il file requirements.txt , dovremo usare i seguenti comandi: pip freeze > requirements.txt Per creare un ambiente virtuale come descritto dal file dei requisiti, invece, dovremo eseguire: pip install -r requirements.txt dove il flag -r sta per recursively , ed indica a pip di installare in maniera ricorsiva le librerie indicate nel file requirements.txt . Opzione C: utilizzare una distribuzione di Python per il calcolo scientifico \u00b6 La terza opzione \u00e8 quella di utilizzare una distribuzione di Python specificamente pensata per il calcolo scientifico, come Anaconda . In questo caso, baster\u00e0 scaricare l'installer dal sito ufficiale e seguire la normale procedura di installazione. Il vantaggio di utilizzare una distribuzione di questo tipo sta nel fatto che avremo a disposizione di default la maggior parte delle librerie utilizzate nel calcolo scientifico. Tuttavia, occorre tenere in considerazione il fatto che la libreria \u00e8 specificamente pensata soltanto per scopi scientifici, per cui dovremo considerarlo qualora intendessimo utilizzare Python per progetti di altro tipo. Il package manager di Anaconda Nel caso si decida di optare per l'uso di Anaconda, \u00e8 importante ricordare che questa distribuzione ha un suo package manager, chiamato conda . Questo andr\u00e0 a sostituire pip nell'installazione delle librerie non presenti nella distribuzione. Opzione D: utilizzare un package manager come pipenv \u00b6 L'ultima opzione, che \u00e8 anche quella suggerita in caso di utilizzo professionale ed eterogeneo di Python, \u00e8 quella di affidarsi ad un package manager evoluto, come pipenv . Questo package manager, infatti, automatizza e semplifica la creazione di un ambiente virtuale, combinando la stessa con l'utilizzo di pip in pochi, semplici comandi; in generale, quindi, il tool ci fornisce un'interfaccia utente molto pi\u00f9 snella, ed inoltre si occupa autonomamente di selezionare le ultime versioni disponibili per i package che utilizziamo. Per utillizzare pipenv , dovremo per prima cosa installarlo globalmente sulla nostra macchina mediante pip : pip install pipenv Una volta installato, andiamo nella cartella dove vogliamo creare il nostro progetto, che ricordiamo includer\u00e0 soltanto la libreria NumPy, e scriviamo: pipenv install numpy Vedremo che, al termine della procedura, saranno stati generati due file: il primo, chiamato Pipfile , avr\u00e0 al suo interno tutte le dipendenze che abbiamo aggiunto al nostro progetto, mentre il secondo, Pipfile.lock , conterr\u00e0 delle informazioni dettagliate sulle librerie usate, incluse versioni, repository, e via discorrendo. Tuttavia, pipenv non si limita a creare questi due file, ma provvede anche a definire, in maniera automatica, un nuovo ambiente virtuale, all'interno del quale saranno (ovviamente) memorizzate tutte le librerie installate per il nostro progetto. Per accedere all'ambiente virtuale, dovremo usare il comando pipenv shell , mentre per eseguire un comando senza accedere all'ambiente virtuale dovremo usare il comando pipenv run seguito dal comando che vogliamo eseguire. Ad esempio, se volessimo lanciare un ipotetico script run.py accedendo all'ambiente virtuale, dovremmo scrivere: pipenv shell python run.py Se non volessimo accedere all'ambiente virtuale, invece, dovremmo scrivere: pipenv run python run.py","title":"B - Le librerie in Python"},{"location":"material/appendix/02_libraries/lecture/#appendice-b-installazione-di-una-libreria-python","text":"Per installare una libreria Python abbiamo a disposizione diverse opzioni. Vediamole nel dettaglio, immaginando di voler installare la libreria NumPy.","title":"Appendice B: Installazione di una libreria Python"},{"location":"material/appendix/02_libraries/lecture/#opzione-a-utilizzare-pip","text":"La prima opzione, e probabilmente quella maggiormente utilizzata, \u00e8 utilizzare il package manager (ovvero, il gestore di pacchetti) integrato in Python, chiamato pip . Per farlo, apriamo un terminale assicurandoci di avere i diritti di amministratore; in Linux, dovremo usare l'istruzione sudo , a meno che non siamo utenti rott, mentre in Windows ci baster\u00e0 aprire la shell come amministratori. Una volta aperto il terminale, dovremo scrivere: pip install numpy Installare una libreria in questo modo \u00e8 sicuramente molto semplice, ma porta con s\u00e8 uno svantaggio: infatti, l'installazione della stessa avviene globalmente , ovvero risulta essere valida per l'intera macchina. Ci\u00f2 potrebbe non sembrare rilevante; tuttavia, in ben determinate situazioni, si pu\u00f2 rendere necessario installare particolari combinazioni di versioni di librerie, per usufruire di funzionalit\u00e0 successivamente deprecate o, al contrario, non presenti in versioni antecedenti. In tal senso, se installiamo una certa libreria globalmente, tutti i nostri programmi dovranno necessariamente utilizzare quella libreria in quella specifica versione, il che ci pu\u00f2 vincolare fortemente a lungo andare.","title":"Opzione A: utilizzare pip"},{"location":"material/appendix/02_libraries/lecture/#opzione-b-utilizzare-pip-ed-un-ambiente-virtuale","text":"Un'altra opzione \u00e8 quella di utilizzare pip in un opportuno ambiente virtuale . Quest'ultimo altro non \u00e8 se non un ambiente \"separato\" all'interno del nostro calcolatore, nel quale andremo ad inserire tutte le librerie che utilizzeremo per i progetti da inserire all'interno dell'ambiente (con le versioni specifiche). Per utilizzare questa opzione, dovremo innanzitutto creare un ambiente virtuale. Per farlo, dobbiamo usare un'opportuna libreria Python, che dovremo installare globalmente mediante pip . Nota L'installazione globale delle librerie per la gestione dell'ambiente virtuale \u00e8 strettamente necessaria, e non contraddice il principio descritto nelle righe precedenti: infatti, l'idea \u00e8 che si possa creare un ambiente virtuale in qualsiasi momento . Installiamo quindi la libreria virtualenvwrapper , o l'equivalente porting per Windows virtualenvwrapper-win : ===\"Linux\" pip install virtualenvwrapper ===\"Windows\" pip install virtualenvwrapper-win Una volta completata l'installazione, utilizzeremo il comando mkvirtualenv , seguito da un nome a nostra scelta, per creare l'ambiente virtuale. Ad esempio: mkvirtualenv pcs Noteremo che, a sinistra del terminale, sar\u00e0 apparsa la scritta (pcs) : ( pcs ) current_working_directory/ Questo ci indica che siamo all'interno del nostro ambiente virtuale. Procediamo adesso all'installazione della libreria NumPy mediante pip : ( pcs ) current_working_directory/ pip install numpy In questo modo, avremo installato NumPy esclusivamente all'interno del nostro ambiente virtuale. Per verificarlo, basta eseguire l'istruzione pip freeze , che restituisce tutte le librerie presenti nell'ambiente in cui siamo attualmente, assieme alle loro versioni. Il file requirements.txt Pratica comune \u00e8 quella di memorizzare tutte le librerie presenti in un ambiente virtuale in un file chiamato requirements.txt . Cos\u00ec facendo, un altro programmatore sar\u00e0 in grado di \"clonare\" il nostro ambiente virtuale. Per salvare il file requirements.txt , dovremo usare i seguenti comandi: pip freeze > requirements.txt Per creare un ambiente virtuale come descritto dal file dei requisiti, invece, dovremo eseguire: pip install -r requirements.txt dove il flag -r sta per recursively , ed indica a pip di installare in maniera ricorsiva le librerie indicate nel file requirements.txt .","title":"Opzione B: utilizzare pip ed un ambiente virtuale"},{"location":"material/appendix/02_libraries/lecture/#opzione-c-utilizzare-una-distribuzione-di-python-per-il-calcolo-scientifico","text":"La terza opzione \u00e8 quella di utilizzare una distribuzione di Python specificamente pensata per il calcolo scientifico, come Anaconda . In questo caso, baster\u00e0 scaricare l'installer dal sito ufficiale e seguire la normale procedura di installazione. Il vantaggio di utilizzare una distribuzione di questo tipo sta nel fatto che avremo a disposizione di default la maggior parte delle librerie utilizzate nel calcolo scientifico. Tuttavia, occorre tenere in considerazione il fatto che la libreria \u00e8 specificamente pensata soltanto per scopi scientifici, per cui dovremo considerarlo qualora intendessimo utilizzare Python per progetti di altro tipo. Il package manager di Anaconda Nel caso si decida di optare per l'uso di Anaconda, \u00e8 importante ricordare che questa distribuzione ha un suo package manager, chiamato conda . Questo andr\u00e0 a sostituire pip nell'installazione delle librerie non presenti nella distribuzione.","title":"Opzione C: utilizzare una distribuzione di Python per il calcolo scientifico"},{"location":"material/appendix/02_libraries/lecture/#opzione-d-utilizzare-un-package-manager-come-pipenv","text":"L'ultima opzione, che \u00e8 anche quella suggerita in caso di utilizzo professionale ed eterogeneo di Python, \u00e8 quella di affidarsi ad un package manager evoluto, come pipenv . Questo package manager, infatti, automatizza e semplifica la creazione di un ambiente virtuale, combinando la stessa con l'utilizzo di pip in pochi, semplici comandi; in generale, quindi, il tool ci fornisce un'interfaccia utente molto pi\u00f9 snella, ed inoltre si occupa autonomamente di selezionare le ultime versioni disponibili per i package che utilizziamo. Per utillizzare pipenv , dovremo per prima cosa installarlo globalmente sulla nostra macchina mediante pip : pip install pipenv Una volta installato, andiamo nella cartella dove vogliamo creare il nostro progetto, che ricordiamo includer\u00e0 soltanto la libreria NumPy, e scriviamo: pipenv install numpy Vedremo che, al termine della procedura, saranno stati generati due file: il primo, chiamato Pipfile , avr\u00e0 al suo interno tutte le dipendenze che abbiamo aggiunto al nostro progetto, mentre il secondo, Pipfile.lock , conterr\u00e0 delle informazioni dettagliate sulle librerie usate, incluse versioni, repository, e via discorrendo. Tuttavia, pipenv non si limita a creare questi due file, ma provvede anche a definire, in maniera automatica, un nuovo ambiente virtuale, all'interno del quale saranno (ovviamente) memorizzate tutte le librerie installate per il nostro progetto. Per accedere all'ambiente virtuale, dovremo usare il comando pipenv shell , mentre per eseguire un comando senza accedere all'ambiente virtuale dovremo usare il comando pipenv run seguito dal comando che vogliamo eseguire. Ad esempio, se volessimo lanciare un ipotetico script run.py accedendo all'ambiente virtuale, dovremmo scrivere: pipenv shell python run.py Se non volessimo accedere all'ambiente virtuale, invece, dovremmo scrivere: pipenv run python run.py","title":"Opzione D: utilizzare un package manager come pipenv"},{"location":"material/appendix/03_scope/lecture/","text":"Appendice C - Ambito di una variabile \u00b6 All'interno di un programma ogni variabile ha una sorta di \"ciclo di vita\", che ne prevede la creazione, utilizzo e, infine, distruzione. L'intero script ha un ambito definito come globale : ci\u00f2 significa che tutte le variabili specificate nel corpo \"principale\" dello script hanno validit\u00e0 in tutto il nostro codice. Le singole funzioni, invece, definiscono un ambito locale , creato alla chiamata della funzione, e distrutto al termine della stessa. Facciamo un esempio. Definiamo una funzione calcolo_voto_accesso_laurea che accetta in ingresso un argomento, ovvero la lista con i voti degli esami. def calcolo_voto_accesso_laurea ( voti_esami ): somma_voti = 0 for voto in voti_esami : somma_voti += voto voto_medio = somma_voti / len ( voti_esami ) voto_accesso = voto_medio / 3 * 11 return voto_accesso Proviamo a chiamarla. lista_voti = [ 18 , 20 , 19 , 30 , 24 , 30 ] print ( 'Il voto di accesso \u00e8: ' , calcolo_voto_accesso_laurea ( lista_voti )) A schermo vedremo: Il voto di accesso \u00e8 : 86.16666666666666 C1 - Prima modifica \u00b6 Facciamo una prima modifica: lista_voti = [ 18 , 20 , 19 , 30 , 24 , 30 ] def calcolo_voto_accesso_laurea ( voti_esami ): print ( f 'La lista dei voti \u00e8: { lista_voti } ' ) somma_voti = 0 for voto in voti_esami : somma_voti += voto voto_medio = somma_voti / len ( voti_esami ) voto_accesso = voto_medio / 3 * 11 return voto_accesso print ( 'Il voto di accesso \u00e8: ' , calcolo_voto_accesso_laurea ( lista_voti )) Adesso vedremo a schermo due valori: La lista dei voti \u00e8 : [ 18 , 20 , 19 , 30 , 24 , 30 ] Il voto di accesso \u00e8 : 86.16666666666666 C2 - Seconda modifica \u00b6 Proviamo a modificare ancora il codice: lista_voti = [ 18 , 20 , 19 , 30 , 24 , 30 ] def calcolo_voto_accesso_laurea ( voti_esami ): somma_voti = 0 for voto in voti_esami : somma_voti += voto voto_medio = somma_voti / len ( voti_esami ) voto_accesso = voto_medio / 3 * 11 return voto_accesso print ( 'Il voto medio \u00e8: ' , voto_medio ) print ( 'Il voto di accesso \u00e8: ' , calcolo_voto_accesso_laurea ( lista_voti )) Adesso vedremo a schermo il seguente risultato: Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > NameError : name 'voto_medio' is not defined Il voto di accesso \u00e8 : 86.16666666666666 Cosa \u00e8 successo? Andiamo un attimo a ritroso, e partiamo dalla prima modifica. In questo caso, infatti, abbiamo provato ad accedere alla variabile globale lista_voti , definita nel corpo \"principale\" dello script, dall'interno della funzione calcola_voto_accesso_laurea . Ci\u00f2 \u00e8 evidentemente possibile, in quanto possiamo accedere ad una variabile globale da un ambito locale. Il contrario, tuttavia, non \u00e8 possibile: infatti, nella seconda modifica, proviamo ad accedere ad una variabile locale alla funzione calcola_voto_accesso_laurea dall'esterno della funzione stessa. Questo non pu\u00f2 avvenire, perch\u00e9 le variabili locali \"scompaiono\" al termine della funzione in cui sono definite, per cui l'interprete ci dar\u00e0 un errore.","title":"C - Ambito di una variabile"},{"location":"material/appendix/03_scope/lecture/#appendice-c-ambito-di-una-variabile","text":"All'interno di un programma ogni variabile ha una sorta di \"ciclo di vita\", che ne prevede la creazione, utilizzo e, infine, distruzione. L'intero script ha un ambito definito come globale : ci\u00f2 significa che tutte le variabili specificate nel corpo \"principale\" dello script hanno validit\u00e0 in tutto il nostro codice. Le singole funzioni, invece, definiscono un ambito locale , creato alla chiamata della funzione, e distrutto al termine della stessa. Facciamo un esempio. Definiamo una funzione calcolo_voto_accesso_laurea che accetta in ingresso un argomento, ovvero la lista con i voti degli esami. def calcolo_voto_accesso_laurea ( voti_esami ): somma_voti = 0 for voto in voti_esami : somma_voti += voto voto_medio = somma_voti / len ( voti_esami ) voto_accesso = voto_medio / 3 * 11 return voto_accesso Proviamo a chiamarla. lista_voti = [ 18 , 20 , 19 , 30 , 24 , 30 ] print ( 'Il voto di accesso \u00e8: ' , calcolo_voto_accesso_laurea ( lista_voti )) A schermo vedremo: Il voto di accesso \u00e8 : 86.16666666666666","title":"Appendice C - Ambito di una variabile"},{"location":"material/appendix/03_scope/lecture/#c1-prima-modifica","text":"Facciamo una prima modifica: lista_voti = [ 18 , 20 , 19 , 30 , 24 , 30 ] def calcolo_voto_accesso_laurea ( voti_esami ): print ( f 'La lista dei voti \u00e8: { lista_voti } ' ) somma_voti = 0 for voto in voti_esami : somma_voti += voto voto_medio = somma_voti / len ( voti_esami ) voto_accesso = voto_medio / 3 * 11 return voto_accesso print ( 'Il voto di accesso \u00e8: ' , calcolo_voto_accesso_laurea ( lista_voti )) Adesso vedremo a schermo due valori: La lista dei voti \u00e8 : [ 18 , 20 , 19 , 30 , 24 , 30 ] Il voto di accesso \u00e8 : 86.16666666666666","title":"C1 - Prima modifica"},{"location":"material/appendix/03_scope/lecture/#c2-seconda-modifica","text":"Proviamo a modificare ancora il codice: lista_voti = [ 18 , 20 , 19 , 30 , 24 , 30 ] def calcolo_voto_accesso_laurea ( voti_esami ): somma_voti = 0 for voto in voti_esami : somma_voti += voto voto_medio = somma_voti / len ( voti_esami ) voto_accesso = voto_medio / 3 * 11 return voto_accesso print ( 'Il voto medio \u00e8: ' , voto_medio ) print ( 'Il voto di accesso \u00e8: ' , calcolo_voto_accesso_laurea ( lista_voti )) Adesso vedremo a schermo il seguente risultato: Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > NameError : name 'voto_medio' is not defined Il voto di accesso \u00e8 : 86.16666666666666 Cosa \u00e8 successo? Andiamo un attimo a ritroso, e partiamo dalla prima modifica. In questo caso, infatti, abbiamo provato ad accedere alla variabile globale lista_voti , definita nel corpo \"principale\" dello script, dall'interno della funzione calcola_voto_accesso_laurea . Ci\u00f2 \u00e8 evidentemente possibile, in quanto possiamo accedere ad una variabile globale da un ambito locale. Il contrario, tuttavia, non \u00e8 possibile: infatti, nella seconda modifica, proviamo ad accedere ad una variabile locale alla funzione calcola_voto_accesso_laurea dall'esterno della funzione stessa. Questo non pu\u00f2 avvenire, perch\u00e9 le variabili locali \"scompaiono\" al termine della funzione in cui sono definite, per cui l'interprete ci dar\u00e0 un errore.","title":"C2 - Seconda modifica"},{"location":"material/appendix/04_oop/lecture/","text":"Appendice D - Principi di Programmazione Orientata agli Oggetti \u00b6 La programmazione orientata agli oggetti (in inglese object-oriented programming , OOP ) \u00e8 un paradigma di programmazione che sposta il focus dalle funzioni ai dati . In particolare, la OOP prevede che tutto sia un oggetto : una qualsiasi variabile \u00e8 interpretata come un oggetto, cos\u00ec come anche le funzioni stesse (in alcuni linguaggi). Ci\u00f2 si estende ovviamente anche ai tipi definiti dall'utente, che assumono il nome di classi . Facciamo un esempio. La classe Persona \u00b6 Immaginiamo di voler definire una struttura dati che contenga al suo interno le informazioni necessarie a definire una persona, come nome, cognome, genere ed et\u00e0. Per farlo, ovviamente, dovremo \"unire\" tra di loro diversi dati primitivi: potremo usare una stringa per il nome, una per il cognome, una per il genere e, infine, un intero per l'et\u00e0. In tal senso, possiamo creare quindi la classe Persona , che avr\u00e0 quattro attributi , come mostrato in figura. Sottolineamo come una classe rappresenti tutte le possibili persone : infatti, si cerca di creare delle strutture dati generiche , che abbiano degli attributi comuni a tutte le possibili istanze . Nel nostro caso, sappiamo che ogni persona ha un nome, un cognome, un genere ed un'et\u00e0, quindi usiamo questi quattro valori come attributi di classe. Differenza tra classe ed istanza Abbiamo detto che una classe rappresenta tutte le possibili istanze della stessa. Ci\u00f2 si traduce, nel nostro esempio, nel fatto che la classe Persona \u00e8 in grado di rappresentare tutte le persone, e un'istanza della classe Persona \u00e8 una singola variabile, o oggetto, che rappresenta una certa persona. Per capirci: un'istanza di Persona \u00e8 \"Angelo, Cardellicchio, Uomo, 37\", mentre un'altra istanza \u00e8 data da \"Frank, Hood, Uomo, 42\", un'altra ancora da \"Camilla, Lilla, Donna, 55\", e cos\u00ec via. Ovviamente, potremo in qualche modo agire con degli opportuni metodi su questi attributi. Ad esempio, se avessimo a disposizione anche il luogo e la data di nascita, potremmo creare un metodo calcola_cf che, per l'appunto, permette di generare il codice fiscale di una singola istanza. Oltre al concetto di classe, tuttavia, la OOP definisce altri tre concetti base. Vediamoli di seguito. Concetto 1: Ereditariet\u00e0 \u00b6 Per ereditariet\u00e0 si intende la capacit\u00e0 di una classe di \"discendere\" da un'altra. Non dobbiamo, per\u00f2, pensare al nostro albero genealogico: infatti, noi abbiamo parte delle caratteristiche di ciascuno dei nostri genitori, mentre una classe figlia eredita in toto le caratteristiche di una classe madre. Ad esempio, potremmo definire la classe Studente come figlia della classe Persona , cui aggiunger\u00e0 i seguenti attributi: Possiamo visualizzare questa relazione in ordine gerarchico come segue: Da notare che la classe Studente pu\u00f2 aggiungere anche dei metodi, oltre che degli attributi a quelli offerti da Persona , come ad esempio genera_media_voto . In ultimo, notiamo come ogni istanza di Studente \u00e8 un'istanza di Persona , ma non \u00e8 vero il contrario , e quindi non tutte le persone sono degli studenti. Per aiutarci a comprendere questo concetto, possiamo visualizzare gli insiemi delle istanze di Persona e di Studente : Generalizzazione e specializzazione La relazione di ereditariet\u00e0 pu\u00f2 anche essere vista in termini di generalizzazione e specializzazione . In questo contesto, la classe Studente \u00e8 una specializzazione di Persona , in quanto sottende ad un insieme pi\u00f9 specifico; al contrario, le persone sono viste come una generalizzazione degli studenti. Ereditariet\u00e0 multipla e multilivello Alcuni linguaggi, compreso Python, offrono la possibilit\u00e0 di ereditare da pi\u00f9 classi; tale concetto \u00e8 chiamato ereditariet\u00e0 multipla . Se invece stabiliamo una vera e propria gerarchia di classi, con una classe \"nonna\", una \"madre\" ed una \"figlia\", avremo una struttura multilivello . Concetto 2: Incapsulamento \u00b6 Il concetto di incapsulamento prevede che sia possibile accedere ad un metodo (o anche ad un attributo) di una classe esclusivamente mediante la sua interfaccia verso il mondo esterno . Vediamo cosa significa. Immaginiamo di voler calcolare il codice fiscale di una persona: dovremo seguire una procedura ben precisa e moderatamente complessa, che potremo tranquillamente \"nascondere\" al codice che usa la classe Persona , il quale dovr\u00e0 semplicemente invocare il metodo calcola_cf . Tuttavia, se volessimo seguire il principio di modularit\u00e0, che ci suggerisce di \"suddividere\" funzioni complesse in maniera tale da renderle pi\u00f9 semplici, dovremmo creare altre funzioni ausiliarie, che potrebbero calcolare la rappresentazione di nome e cognome ( calcola_nc ) e i dati alfanumerici derivanti da luogo e data di nascita ( calcola_ld ). Ovviamente, non vi \u00e8 il bisogno di accedere dall'esterno della classe a questi metodi, in quanto hanno valenza esclusiva nell'ambito del calcolo del codice fiscale: per questo motivo, li si potr\u00e0 dichiarare come privati , e potranno essere acceduti soltanto dall'interno della classe . In questo modo, la classe mantiene un'interfaccia stabile ed essenziale verso l'esterno: il codice che usa la classe avr\u00e0 sempre un punto di accesso ben definito e, nel caso si debbano modificare dei comportamenti interni alla classe, non sar\u00e0 influenzato da dette modifiche. Ad esempio, infatti, se per qualche motivo si decidesse di cambiare l'ordine con cui si mostrano nel codice fiscale la rappresentazione del cognome e del nome, basterebbe modificare il metodo calcola_nome_cognome_codice_fiscale , ed il resto dell'implementazione (sia della classe, sia del codice chiamante) non ne sarebbe influenzata. Concetto 3: Polimorfismo \u00b6 Il concetto di polimorfismo prevede che sia possibile modificare il comportamento associato ad un metodo a seconda della classe che lo utilizza. Immaginiamo ad esempio di specializzare la classe Studente in due ulteriori rappresentazioni, ovvero StudenteUniversitario e StudenteScolastico . Ovviamente, il metodo genera_media_voto sar\u00e0 ereditato da entrambe le classi; tuttavia, l'implementazione dovr\u00e0 essere necessariamente differente, in quanto la media di laurea \u00e8 pesata in modo diverso rispetto alla classica media aritmetica usata nelle scuole fino alla secondaria. Il polimorfismo ci permette di raggiungere questo obiettivo: potremo effettuare una procedura di override del metodo genera_media_voto che, pur conservando la stessa firma, avr\u00e0 differenti implementazioni nelle classi StudenteUniversitario e StudenteScolastico . Ovviamente, il fatto che il metodo conservi la stessa firma rappresenta un vantaggio paragonabile a quello ottenuto mediante il polimorfismo: infatti, un programmatore potr\u00e0 usare il metodo genera_media_voto alla stessa maniera per uno studente universitario ed uno di scuola media secondaria, senza per questo dover tenere a mente due diverse interfacce.","title":"D - Programmazione orientata agli oggetti"},{"location":"material/appendix/04_oop/lecture/#appendice-d-principi-di-programmazione-orientata-agli-oggetti","text":"La programmazione orientata agli oggetti (in inglese object-oriented programming , OOP ) \u00e8 un paradigma di programmazione che sposta il focus dalle funzioni ai dati . In particolare, la OOP prevede che tutto sia un oggetto : una qualsiasi variabile \u00e8 interpretata come un oggetto, cos\u00ec come anche le funzioni stesse (in alcuni linguaggi). Ci\u00f2 si estende ovviamente anche ai tipi definiti dall'utente, che assumono il nome di classi . Facciamo un esempio.","title":"Appendice D - Principi di Programmazione Orientata agli Oggetti"},{"location":"material/appendix/04_oop/lecture/#la-classe-persona","text":"Immaginiamo di voler definire una struttura dati che contenga al suo interno le informazioni necessarie a definire una persona, come nome, cognome, genere ed et\u00e0. Per farlo, ovviamente, dovremo \"unire\" tra di loro diversi dati primitivi: potremo usare una stringa per il nome, una per il cognome, una per il genere e, infine, un intero per l'et\u00e0. In tal senso, possiamo creare quindi la classe Persona , che avr\u00e0 quattro attributi , come mostrato in figura. Sottolineamo come una classe rappresenti tutte le possibili persone : infatti, si cerca di creare delle strutture dati generiche , che abbiano degli attributi comuni a tutte le possibili istanze . Nel nostro caso, sappiamo che ogni persona ha un nome, un cognome, un genere ed un'et\u00e0, quindi usiamo questi quattro valori come attributi di classe. Differenza tra classe ed istanza Abbiamo detto che una classe rappresenta tutte le possibili istanze della stessa. Ci\u00f2 si traduce, nel nostro esempio, nel fatto che la classe Persona \u00e8 in grado di rappresentare tutte le persone, e un'istanza della classe Persona \u00e8 una singola variabile, o oggetto, che rappresenta una certa persona. Per capirci: un'istanza di Persona \u00e8 \"Angelo, Cardellicchio, Uomo, 37\", mentre un'altra istanza \u00e8 data da \"Frank, Hood, Uomo, 42\", un'altra ancora da \"Camilla, Lilla, Donna, 55\", e cos\u00ec via. Ovviamente, potremo in qualche modo agire con degli opportuni metodi su questi attributi. Ad esempio, se avessimo a disposizione anche il luogo e la data di nascita, potremmo creare un metodo calcola_cf che, per l'appunto, permette di generare il codice fiscale di una singola istanza. Oltre al concetto di classe, tuttavia, la OOP definisce altri tre concetti base. Vediamoli di seguito.","title":"La classe Persona"},{"location":"material/appendix/04_oop/lecture/#concetto-1-ereditarieta","text":"Per ereditariet\u00e0 si intende la capacit\u00e0 di una classe di \"discendere\" da un'altra. Non dobbiamo, per\u00f2, pensare al nostro albero genealogico: infatti, noi abbiamo parte delle caratteristiche di ciascuno dei nostri genitori, mentre una classe figlia eredita in toto le caratteristiche di una classe madre. Ad esempio, potremmo definire la classe Studente come figlia della classe Persona , cui aggiunger\u00e0 i seguenti attributi: Possiamo visualizzare questa relazione in ordine gerarchico come segue: Da notare che la classe Studente pu\u00f2 aggiungere anche dei metodi, oltre che degli attributi a quelli offerti da Persona , come ad esempio genera_media_voto . In ultimo, notiamo come ogni istanza di Studente \u00e8 un'istanza di Persona , ma non \u00e8 vero il contrario , e quindi non tutte le persone sono degli studenti. Per aiutarci a comprendere questo concetto, possiamo visualizzare gli insiemi delle istanze di Persona e di Studente : Generalizzazione e specializzazione La relazione di ereditariet\u00e0 pu\u00f2 anche essere vista in termini di generalizzazione e specializzazione . In questo contesto, la classe Studente \u00e8 una specializzazione di Persona , in quanto sottende ad un insieme pi\u00f9 specifico; al contrario, le persone sono viste come una generalizzazione degli studenti. Ereditariet\u00e0 multipla e multilivello Alcuni linguaggi, compreso Python, offrono la possibilit\u00e0 di ereditare da pi\u00f9 classi; tale concetto \u00e8 chiamato ereditariet\u00e0 multipla . Se invece stabiliamo una vera e propria gerarchia di classi, con una classe \"nonna\", una \"madre\" ed una \"figlia\", avremo una struttura multilivello .","title":"Concetto 1: Ereditariet\u00e0"},{"location":"material/appendix/04_oop/lecture/#concetto-2-incapsulamento","text":"Il concetto di incapsulamento prevede che sia possibile accedere ad un metodo (o anche ad un attributo) di una classe esclusivamente mediante la sua interfaccia verso il mondo esterno . Vediamo cosa significa. Immaginiamo di voler calcolare il codice fiscale di una persona: dovremo seguire una procedura ben precisa e moderatamente complessa, che potremo tranquillamente \"nascondere\" al codice che usa la classe Persona , il quale dovr\u00e0 semplicemente invocare il metodo calcola_cf . Tuttavia, se volessimo seguire il principio di modularit\u00e0, che ci suggerisce di \"suddividere\" funzioni complesse in maniera tale da renderle pi\u00f9 semplici, dovremmo creare altre funzioni ausiliarie, che potrebbero calcolare la rappresentazione di nome e cognome ( calcola_nc ) e i dati alfanumerici derivanti da luogo e data di nascita ( calcola_ld ). Ovviamente, non vi \u00e8 il bisogno di accedere dall'esterno della classe a questi metodi, in quanto hanno valenza esclusiva nell'ambito del calcolo del codice fiscale: per questo motivo, li si potr\u00e0 dichiarare come privati , e potranno essere acceduti soltanto dall'interno della classe . In questo modo, la classe mantiene un'interfaccia stabile ed essenziale verso l'esterno: il codice che usa la classe avr\u00e0 sempre un punto di accesso ben definito e, nel caso si debbano modificare dei comportamenti interni alla classe, non sar\u00e0 influenzato da dette modifiche. Ad esempio, infatti, se per qualche motivo si decidesse di cambiare l'ordine con cui si mostrano nel codice fiscale la rappresentazione del cognome e del nome, basterebbe modificare il metodo calcola_nome_cognome_codice_fiscale , ed il resto dell'implementazione (sia della classe, sia del codice chiamante) non ne sarebbe influenzata.","title":"Concetto 2: Incapsulamento"},{"location":"material/appendix/04_oop/lecture/#concetto-3-polimorfismo","text":"Il concetto di polimorfismo prevede che sia possibile modificare il comportamento associato ad un metodo a seconda della classe che lo utilizza. Immaginiamo ad esempio di specializzare la classe Studente in due ulteriori rappresentazioni, ovvero StudenteUniversitario e StudenteScolastico . Ovviamente, il metodo genera_media_voto sar\u00e0 ereditato da entrambe le classi; tuttavia, l'implementazione dovr\u00e0 essere necessariamente differente, in quanto la media di laurea \u00e8 pesata in modo diverso rispetto alla classica media aritmetica usata nelle scuole fino alla secondaria. Il polimorfismo ci permette di raggiungere questo obiettivo: potremo effettuare una procedura di override del metodo genera_media_voto che, pur conservando la stessa firma, avr\u00e0 differenti implementazioni nelle classi StudenteUniversitario e StudenteScolastico . Ovviamente, il fatto che il metodo conservi la stessa firma rappresenta un vantaggio paragonabile a quello ottenuto mediante il polimorfismo: infatti, un programmatore potr\u00e0 usare il metodo genera_media_voto alla stessa maniera per uno studente universitario ed uno di scuola media secondaria, senza per questo dover tenere a mente due diverse interfacce.","title":"Concetto 3: Polimorfismo"},{"location":"material/appendix/05_tips/lecture/","text":"Appendice E - Python \u00b6 Tabella degli operatori booleani \u00b6 Operatore Operazione logica Esempio Risultato and AND 1 and 2 True or OR True or False True not NOT True is not False True Gestione delle Eccezioni \u00b6 I decorator \u00b6 Prima di continuare a parlare dei metodi che \u00e8 possibile definire all'interno di una classe Python, \u00e8 necessario introdurre il concetto di decorator , ovvero una particolare notazione che viene usata in Python (ed in altri linguaggi di programmazione) per indicare una funzione che \"decora\" un'altra funzione. Funzioni come oggetti \u00b6 Python tratta le funzioni come degli oggetti . E' quindi possiible che una funzione restituisca una funzione : def main_character ( series ): def supernatural (): return \"Sam Winchester\" def breaking_bad (): return \"Walter White\" if series == \"Supernatural\" : return supernatural elif series == \"Breaking Bad\" : return breaking_bad Il valore di ritorno \u00e8 quindi un oggetto. Possiamo provare a chiamarlo dal nostro script: >>> mc = main_character ( \"Supernatural\" ) Se provassimo a mandarlo a schermo trattandolo come una variabile, avremmo in uscita una reference a funzione: >>> print ( \"Function reference: {} \" . format ( mc )) Function reference : < function main_character .< locals >. supernatural at 0x00000170C448BA60 > Per visualizzare il risultato, trattiamolo come se fosse una chiamata a funzione: >>> print ( \"Function outcoming value: {} \" . format ( mc ())) Function outcoming value : Sam Winchester Funzioni come argomenti di altre funzioni \u00b6 Possiamo passare una fuzione come argomento ad un'altra funzione: def favorite_series ( func ): def internal_check (): print ( \"Checking my favorite series...\" ) func () print ( \"Got it!\" ) return internal_check def check (): print ( 'Sons of Anarchy' ) Dal nostro script: >>> print_fav_series = favorite_series ( check ) >>> print_fav_series () Checking my favorite series ... Sons of Anarchy Got it ! Vediamo quindi come la funzione passata come argomento sar\u00e0 correttamente chiamata internamente al metodo favorite_series . Definizione ed uso di decorator \u00b6 La sintassi che abbiamo usato \u00e8, per dirla con Manzoni, ampollosa . Python ci offre quindi una sintassi equivalente, ma molto pi\u00f9 accessibile, per usare una funzione come argomento di un'altra funzione, ovvero i decorator. Infatti: @favorite_series def print_fav_series_decorated (): print ( 'Breaking Bad' ) >>> print_fav_series_decorated () Checking my favorite series ... Breaking Bad Got it !","title":"E - Tips"},{"location":"material/appendix/05_tips/lecture/#appendice-e-python","text":"","title":"Appendice E - Python"},{"location":"material/appendix/05_tips/lecture/#tabella-degli-operatori-booleani","text":"Operatore Operazione logica Esempio Risultato and AND 1 and 2 True or OR True or False True not NOT True is not False True","title":"Tabella degli operatori booleani"},{"location":"material/appendix/05_tips/lecture/#gestione-delle-eccezioni","text":"","title":"Gestione delle Eccezioni"},{"location":"material/appendix/05_tips/lecture/#i-decorator","text":"Prima di continuare a parlare dei metodi che \u00e8 possibile definire all'interno di una classe Python, \u00e8 necessario introdurre il concetto di decorator , ovvero una particolare notazione che viene usata in Python (ed in altri linguaggi di programmazione) per indicare una funzione che \"decora\" un'altra funzione.","title":"I decorator"},{"location":"material/appendix/05_tips/lecture/#funzioni-come-oggetti","text":"Python tratta le funzioni come degli oggetti . E' quindi possiible che una funzione restituisca una funzione : def main_character ( series ): def supernatural (): return \"Sam Winchester\" def breaking_bad (): return \"Walter White\" if series == \"Supernatural\" : return supernatural elif series == \"Breaking Bad\" : return breaking_bad Il valore di ritorno \u00e8 quindi un oggetto. Possiamo provare a chiamarlo dal nostro script: >>> mc = main_character ( \"Supernatural\" ) Se provassimo a mandarlo a schermo trattandolo come una variabile, avremmo in uscita una reference a funzione: >>> print ( \"Function reference: {} \" . format ( mc )) Function reference : < function main_character .< locals >. supernatural at 0x00000170C448BA60 > Per visualizzare il risultato, trattiamolo come se fosse una chiamata a funzione: >>> print ( \"Function outcoming value: {} \" . format ( mc ())) Function outcoming value : Sam Winchester","title":"Funzioni come oggetti"},{"location":"material/appendix/05_tips/lecture/#funzioni-come-argomenti-di-altre-funzioni","text":"Possiamo passare una fuzione come argomento ad un'altra funzione: def favorite_series ( func ): def internal_check (): print ( \"Checking my favorite series...\" ) func () print ( \"Got it!\" ) return internal_check def check (): print ( 'Sons of Anarchy' ) Dal nostro script: >>> print_fav_series = favorite_series ( check ) >>> print_fav_series () Checking my favorite series ... Sons of Anarchy Got it ! Vediamo quindi come la funzione passata come argomento sar\u00e0 correttamente chiamata internamente al metodo favorite_series .","title":"Funzioni come argomenti di altre funzioni"},{"location":"material/appendix/05_tips/lecture/#definizione-ed-uso-di-decorator","text":"La sintassi che abbiamo usato \u00e8, per dirla con Manzoni, ampollosa . Python ci offre quindi una sintassi equivalente, ma molto pi\u00f9 accessibile, per usare una funzione come argomento di un'altra funzione, ovvero i decorator. Infatti: @favorite_series def print_fav_series_decorated (): print ( 'Breaking Bad' ) >>> print_fav_series_decorated () Checking my favorite series ... Breaking Bad Got it !","title":"Definizione ed uso di decorator"},{"location":"material/appendix/06_argparse/lecture/","text":"Appendice F - Il modulo argparse \u00b6 Il modulo argparse ci permette di inserire degli argomenti da passare al nostro script Python mediante riga di comando. Per farlo, dobbiamo seguire un processo articolato in quattro step: creiamo un oggetto di classe ArgumentParser ; aggiungiamo gli argomenti di cui intendiamo fare il parsing; effettuiamo il parsing di questi argomenti; usiamo gli argomenti per chiamare il metodo opportuno Vediamo un esempio. Supponiamo di avere una classe Persona , e di voler scrivere uno script per creare un oggetto di questa classe mediante riga di comando. Potremo scrivere: from argparse import ArgumentParser class Persona (): def __init__ ( self , nome , cognome ): self . nome = nome self . cognome = cognome def __str__ ( self ): return f ' { self . nome } { self . cognome } ' def run ( args ): \"\"\" Definiamo il metodo `run` che sar\u00e0 invocato ad ogni esecuzione dello script. Il metodo accetta un parametro args che rappresenta gli argomenti di cui \u00e8 stato effettuato il parsing. \"\"\" p = Persona ( args . nome , args . cognome ) print ( p ) if __name__ == '__main__' : # Step 1: creiamo un oggetto di classe `ArgumentParser` parser = ArgumentParser () # Step 2: aggiungiamo due argomenti al parser parser . add_argument ( '-n' , # abbreviazione con cui invocare l'argomento '--nome' , # nome completo dell'argomento help = 'Nome della persona' , # messaggio di aiuto per descrivere l'argomento default = 'Pippo' , # valore di default dell'argomento ) parser . add_argument ( '-c' , '--cognome' , help = 'Cognome della persona' , required = True # indica che l'argomento non pu\u00f2 essere omesso ) # Step 3: facciamo il parsing degli argomenti args = parser . parse_args () # gli argomenti saranno salvati in args # Step 4: passiamo gli argomenti al metodo run run ( args ) Proviamo a salvare questo script in un file run.py , ed eseguiamolo usando la notazione abbreviata: python run.py -n Nome -c Cognome A schermo vedremo: Nome Cognome Possiamo anche omettere il nome, ma non il cognome, in quanto \u00e8 un parametro richiesto: python run.py -c Cognome Pippo Cognome Possiamo poi invocare l'help scrivendo: python run.py -h Proviamo infine ad utilizzare la notazione completa: python run.py --nome Nome --cognome Cognome Nome Cognome Proviamo adesso a modificare la classe Persona inserendovi l'et\u00e0. In tal senso, specifichiamo che l'et\u00e0 deve essere un valore intero; qualora questo non avvenga, sar\u00e0 lanciata un'eccezione. class Persona (): def __init__ ( self , nome , cognome , eta ): self . nome = nome self . cognome = cognome self . eta = eta @property def eta ( self ): return self . _eta @eta . setter def eta ( self , value ): if not isinstance ( eta , int ): raise ValueError ( \"Fornire un intero per l'et\u00e0.\" ) self . _eta = value def __str__ ( self ): return f ' { self . nome } { self . cognome } ' Modifichiamo il resto dello script per adattarci alle nuove esigenze. Partiamo dal metodo run : def run ( args ): \"\"\" Definiamo il metodo `run` che sar\u00e0 invocato ad ogni esecuzione dello script. Il metodo accetta un parametro args che rappresenta gli argomenti di cui \u00e8 stato effettuato il parsing. \"\"\" p = Persona ( args . nome , args . cognome , args . eta ) print ( p ) Aggiungiamo poi un altro argomento al parser : ``py hl_lines \"16 17 18 19 20\" if __name__ == '__main__': # Step 1: creiamo un oggetto di classe ArgumentParser` parser = ArgumentParser() # Step 2: aggiungiamo due argomenti al parser parser.add argument( '-n', # abbreviazione con cui invocare l'argomento '--nome', # nome completo dell'argomento help='Nome della persona', # messaggio di aiuto per descrivere l'argomento default='Pippo', # valore di default dell'argomento ) parser.add argument( '-c', '--cognome', help='Cognome della persona', required=True # indica che l'argomento non pu\u00f2 essere omesso ) parser.add argument( '-e', '--eta', help='Et\u00e0 della persona' ) # Step 3: facciamo il parsing degli argomenti args = parser.parse args() # gli argomenti saranno salvati in args # Step 4: passiamo gli argomenti al metodo run run(args) Proviamo ad eseguire di nuovo lo script: ```sh python run.py -n Nome -c Cognome -e 18 Vedremo che viene lanciato un errore, in quanto gli argomenti passati mediante argparse sono normalmente interpretati come delle stringhe. Per risolvere questo problema dovremo specificare il parametro type , ponendolo ad int : py hl_lines \"5\" parser.add_argument( '-e', '--eta', help='Et\u00e0 della persona', type=int ) Se proviamo ad eseguire nuovamente lo script non riscontreremo alcun errore.","title":"F - Il modulo argparse"},{"location":"material/appendix/06_argparse/lecture/#appendice-f-il-modulo-argparse","text":"Il modulo argparse ci permette di inserire degli argomenti da passare al nostro script Python mediante riga di comando. Per farlo, dobbiamo seguire un processo articolato in quattro step: creiamo un oggetto di classe ArgumentParser ; aggiungiamo gli argomenti di cui intendiamo fare il parsing; effettuiamo il parsing di questi argomenti; usiamo gli argomenti per chiamare il metodo opportuno Vediamo un esempio. Supponiamo di avere una classe Persona , e di voler scrivere uno script per creare un oggetto di questa classe mediante riga di comando. Potremo scrivere: from argparse import ArgumentParser class Persona (): def __init__ ( self , nome , cognome ): self . nome = nome self . cognome = cognome def __str__ ( self ): return f ' { self . nome } { self . cognome } ' def run ( args ): \"\"\" Definiamo il metodo `run` che sar\u00e0 invocato ad ogni esecuzione dello script. Il metodo accetta un parametro args che rappresenta gli argomenti di cui \u00e8 stato effettuato il parsing. \"\"\" p = Persona ( args . nome , args . cognome ) print ( p ) if __name__ == '__main__' : # Step 1: creiamo un oggetto di classe `ArgumentParser` parser = ArgumentParser () # Step 2: aggiungiamo due argomenti al parser parser . add_argument ( '-n' , # abbreviazione con cui invocare l'argomento '--nome' , # nome completo dell'argomento help = 'Nome della persona' , # messaggio di aiuto per descrivere l'argomento default = 'Pippo' , # valore di default dell'argomento ) parser . add_argument ( '-c' , '--cognome' , help = 'Cognome della persona' , required = True # indica che l'argomento non pu\u00f2 essere omesso ) # Step 3: facciamo il parsing degli argomenti args = parser . parse_args () # gli argomenti saranno salvati in args # Step 4: passiamo gli argomenti al metodo run run ( args ) Proviamo a salvare questo script in un file run.py , ed eseguiamolo usando la notazione abbreviata: python run.py -n Nome -c Cognome A schermo vedremo: Nome Cognome Possiamo anche omettere il nome, ma non il cognome, in quanto \u00e8 un parametro richiesto: python run.py -c Cognome Pippo Cognome Possiamo poi invocare l'help scrivendo: python run.py -h Proviamo infine ad utilizzare la notazione completa: python run.py --nome Nome --cognome Cognome Nome Cognome Proviamo adesso a modificare la classe Persona inserendovi l'et\u00e0. In tal senso, specifichiamo che l'et\u00e0 deve essere un valore intero; qualora questo non avvenga, sar\u00e0 lanciata un'eccezione. class Persona (): def __init__ ( self , nome , cognome , eta ): self . nome = nome self . cognome = cognome self . eta = eta @property def eta ( self ): return self . _eta @eta . setter def eta ( self , value ): if not isinstance ( eta , int ): raise ValueError ( \"Fornire un intero per l'et\u00e0.\" ) self . _eta = value def __str__ ( self ): return f ' { self . nome } { self . cognome } ' Modifichiamo il resto dello script per adattarci alle nuove esigenze. Partiamo dal metodo run : def run ( args ): \"\"\" Definiamo il metodo `run` che sar\u00e0 invocato ad ogni esecuzione dello script. Il metodo accetta un parametro args che rappresenta gli argomenti di cui \u00e8 stato effettuato il parsing. \"\"\" p = Persona ( args . nome , args . cognome , args . eta ) print ( p ) Aggiungiamo poi un altro argomento al parser : ``py hl_lines \"16 17 18 19 20\" if __name__ == '__main__': # Step 1: creiamo un oggetto di classe ArgumentParser` parser = ArgumentParser() # Step 2: aggiungiamo due argomenti al parser parser.add argument( '-n', # abbreviazione con cui invocare l'argomento '--nome', # nome completo dell'argomento help='Nome della persona', # messaggio di aiuto per descrivere l'argomento default='Pippo', # valore di default dell'argomento ) parser.add argument( '-c', '--cognome', help='Cognome della persona', required=True # indica che l'argomento non pu\u00f2 essere omesso ) parser.add argument( '-e', '--eta', help='Et\u00e0 della persona' ) # Step 3: facciamo il parsing degli argomenti args = parser.parse args() # gli argomenti saranno salvati in args # Step 4: passiamo gli argomenti al metodo run run(args) Proviamo ad eseguire di nuovo lo script: ```sh python run.py -n Nome -c Cognome -e 18 Vedremo che viene lanciato un errore, in quanto gli argomenti passati mediante argparse sono normalmente interpretati come delle stringhe. Per risolvere questo problema dovremo specificare il parametro type , ponendolo ad int : py hl_lines \"5\" parser.add_argument( '-e', '--eta', help='Et\u00e0 della persona', type=int ) Se proviamo ad eseguire nuovamente lo script non riscontreremo alcun errore.","title":"Appendice F - Il modulo argparse"},{"location":"material/appendix/07_algorithms/01_svd/lecture/","text":"Appendice G.1 - Decomposizione ai valori singolari \u00b6 La decomposizione ai valori singolari (SVD) \u00e8 una tecnica di decomposizione matriciale , che permette quindi di trovare un prodotto di matrice che equivalga alla matrice iniziale. La decomposizione ai valori singolari prevede che una data matrice \\(M\\) sia decomposta in tre matrici: \\[ M = U \\Sigma V^* \\] dove: \\(M\\) \u00e8 una generica matrice ad \\(m\\) righe ed \\(n\\) colonne; \\(U\\) \u00e8 la matrice \\(m \\times m\\) dei vettori singolari sinistri; \\(\\Sigma\\) \u00e8 la matrice \\(m \\times n\\) dei valori singolari; \\(V^*\\) \u00e8 la matrice \\(n \\times n\\) dei vettori singolari destri. Inoltre: \\(U\\) e \\(V\\) sono matrici ortogonali, per cui \\(U^T = U^{-1}\\) e \\(V^T = V^{-1}\\) ; \\(\\Sigma\\) \u00e8 una matrice diagonale non necessariamente quadrata . Prima di dare una definizione intuitiva di SVD, per\u00f2, \u00e8 opportuno introdurre il concetto di trasformazione lineare. Trasformazione 1: rescaling di matrice \u00b6 Applichiamo al vettore \\(v\\) una trasformazione lineare definita da una matrice diagonale \\(d\\) : \\[ v = \\left[ \\begin{array}{c} x \\\\ y \\end{array} \\right] \\\\ d = \\left[ \\begin{array}{cc} 3 & 0 \\\\ 0 & 3 \\end{array} \\right] \\] Se moltiplichiamo \\(d\\) per \\(v\\) , abbiamo: \\[ \\left[ \\begin{array}{c} x^{'} \\\\ y^{'} \\end{array} \\right] = \\\\ \\left[ \\begin{array}{cc} 3 & 0 \\\\ 0 & 3 \\end{array} \\right] \\times \\left[ \\begin{array}{c} x \\\\ y \\end{array} \\right] = \\\\ = \\left[ \\begin{array}{c} 3x + 0 \\\\ 0 + 3y \\end{array} \\right] = \\\\ \\left[ \\begin{array}{c} 3x \\\\ 3y \\end{array} \\right] \\] In pratica, abbiamo triplicato il nostro vettore! Trasformazione 2: rotazione di matrice \u00b6 Laddove le matrici diagonali sono normalmente utilizzate per effettuare un rescaling, le matrici non diagonali possono essere usate per indurre una rotazione su un vettore. Ad esempio, possiamo pensare ad una matrice \\(R\\) di questo tipo: \\[ R = \\left[ \\begin{array}{cc} cos(\\theta) & sin(\\theta) \\\\ sin(\\theta) & cos(\\theta) \\end{array} \\right] \\] Se applicata ad un vettore, questa matrice applicher\u00e0 una certa rotazione a \\(\\theta\\) : \\[ \\left[ \\begin{array}{c} x^{'} \\\\ y^{'} \\end{array} \\right] = \\\\ \\left[ \\begin{array}{c} x cos(\\theta) + y sin(\\theta) \\\\ x sin(\\theta) + y cos(\\theta) \\end{array} \\right] \\] Se consideriamo un vettore a coordinate \\(x = 1, y = 0\\) , ed un valore di \\(\\theta\\) pari a \\(45\u00b0\\) , allora: \\[ x^{'} = 1 \\cdot cos(45) + 0 \\cdot sin(45) = cos(45) \\] e: \\[ y^{'} = 1 \\cdot sin(45) + 0 \\cdot cos(45) = sin(45) \\] In questo caso specifico, i nuovi valori di \\((x^{'}, y^{'})\\) saranno pari a \\(\\sqrt{2}/2\\) , per cui avremo \"ruotato\" il nostro punto originario di 45 gradi. Interpretazione della SVD \u00b6 Possiamo quindi adesso applicare le nozioni viste in precedenza per \"comprendere\" la SVD, che pu\u00f2 essere vista come una serie di trasformazioni lineari . In particolare, se partissimo dal cerchio di raggio unitario, ed applicassimo le trasformazioni lineari imposte da una generica matrice \\(M\\) , avremmo una certa trasformazione delle coordinate definite sul cerchio. Mediante le tre diverse matrici definite dalla SVD scomponiamo linearmente in tre passi questa trasformazione, imponendo prima una rotazione, poi uno scaling, e poi un'ultima rotazione. Ci\u00f2 implica che il prodotto delle tre matrici in uscita dalla SVD applica una trasformazione lineare equivalente a quella applicata dalla sola matrice \\(M\\) . Questo concetto \u00e8 riassunto nella seguente figura. Il ruolo principale della SVD \u00e8 quindi quello di definire una serie di valori singolari (la matrice \\(\\Sigma\\) ) di una matrice \\(M\\) , i quali possono essere ricondotti ai valori originari mediante trasformazioni lineari. In tal senso, se la matrice originaria rappresenta un insieme di feature per un dato dataset, i valori singolari rappresentano un nuovo insieme di queste feature, corrispondenti ad una combinazione lineare di quelle originarie, ordinati secondo il grado di variabilit\u00e0 dei dati originari che riescono a \"rappresentare\", o \"spiegare\". Ne consegue che se un numero ridotto di feature individuate dalla SVD riesce a rappresentare un buon grado di variabilit\u00e0 dei dati iniziali, \u00e8 possibile scartare le feature rimanenti, effettuando quindi un'operazione di riduzione della dimensionalit\u00e0 .","title":"SVD"},{"location":"material/appendix/07_algorithms/01_svd/lecture/#appendice-g1-decomposizione-ai-valori-singolari","text":"La decomposizione ai valori singolari (SVD) \u00e8 una tecnica di decomposizione matriciale , che permette quindi di trovare un prodotto di matrice che equivalga alla matrice iniziale. La decomposizione ai valori singolari prevede che una data matrice \\(M\\) sia decomposta in tre matrici: \\[ M = U \\Sigma V^* \\] dove: \\(M\\) \u00e8 una generica matrice ad \\(m\\) righe ed \\(n\\) colonne; \\(U\\) \u00e8 la matrice \\(m \\times m\\) dei vettori singolari sinistri; \\(\\Sigma\\) \u00e8 la matrice \\(m \\times n\\) dei valori singolari; \\(V^*\\) \u00e8 la matrice \\(n \\times n\\) dei vettori singolari destri. Inoltre: \\(U\\) e \\(V\\) sono matrici ortogonali, per cui \\(U^T = U^{-1}\\) e \\(V^T = V^{-1}\\) ; \\(\\Sigma\\) \u00e8 una matrice diagonale non necessariamente quadrata . Prima di dare una definizione intuitiva di SVD, per\u00f2, \u00e8 opportuno introdurre il concetto di trasformazione lineare.","title":"Appendice G.1 - Decomposizione ai valori singolari"},{"location":"material/appendix/07_algorithms/01_svd/lecture/#trasformazione-1-rescaling-di-matrice","text":"Applichiamo al vettore \\(v\\) una trasformazione lineare definita da una matrice diagonale \\(d\\) : \\[ v = \\left[ \\begin{array}{c} x \\\\ y \\end{array} \\right] \\\\ d = \\left[ \\begin{array}{cc} 3 & 0 \\\\ 0 & 3 \\end{array} \\right] \\] Se moltiplichiamo \\(d\\) per \\(v\\) , abbiamo: \\[ \\left[ \\begin{array}{c} x^{'} \\\\ y^{'} \\end{array} \\right] = \\\\ \\left[ \\begin{array}{cc} 3 & 0 \\\\ 0 & 3 \\end{array} \\right] \\times \\left[ \\begin{array}{c} x \\\\ y \\end{array} \\right] = \\\\ = \\left[ \\begin{array}{c} 3x + 0 \\\\ 0 + 3y \\end{array} \\right] = \\\\ \\left[ \\begin{array}{c} 3x \\\\ 3y \\end{array} \\right] \\] In pratica, abbiamo triplicato il nostro vettore!","title":"Trasformazione 1: rescaling di matrice"},{"location":"material/appendix/07_algorithms/01_svd/lecture/#trasformazione-2-rotazione-di-matrice","text":"Laddove le matrici diagonali sono normalmente utilizzate per effettuare un rescaling, le matrici non diagonali possono essere usate per indurre una rotazione su un vettore. Ad esempio, possiamo pensare ad una matrice \\(R\\) di questo tipo: \\[ R = \\left[ \\begin{array}{cc} cos(\\theta) & sin(\\theta) \\\\ sin(\\theta) & cos(\\theta) \\end{array} \\right] \\] Se applicata ad un vettore, questa matrice applicher\u00e0 una certa rotazione a \\(\\theta\\) : \\[ \\left[ \\begin{array}{c} x^{'} \\\\ y^{'} \\end{array} \\right] = \\\\ \\left[ \\begin{array}{c} x cos(\\theta) + y sin(\\theta) \\\\ x sin(\\theta) + y cos(\\theta) \\end{array} \\right] \\] Se consideriamo un vettore a coordinate \\(x = 1, y = 0\\) , ed un valore di \\(\\theta\\) pari a \\(45\u00b0\\) , allora: \\[ x^{'} = 1 \\cdot cos(45) + 0 \\cdot sin(45) = cos(45) \\] e: \\[ y^{'} = 1 \\cdot sin(45) + 0 \\cdot cos(45) = sin(45) \\] In questo caso specifico, i nuovi valori di \\((x^{'}, y^{'})\\) saranno pari a \\(\\sqrt{2}/2\\) , per cui avremo \"ruotato\" il nostro punto originario di 45 gradi.","title":"Trasformazione 2: rotazione di matrice"},{"location":"material/appendix/07_algorithms/01_svd/lecture/#interpretazione-della-svd","text":"Possiamo quindi adesso applicare le nozioni viste in precedenza per \"comprendere\" la SVD, che pu\u00f2 essere vista come una serie di trasformazioni lineari . In particolare, se partissimo dal cerchio di raggio unitario, ed applicassimo le trasformazioni lineari imposte da una generica matrice \\(M\\) , avremmo una certa trasformazione delle coordinate definite sul cerchio. Mediante le tre diverse matrici definite dalla SVD scomponiamo linearmente in tre passi questa trasformazione, imponendo prima una rotazione, poi uno scaling, e poi un'ultima rotazione. Ci\u00f2 implica che il prodotto delle tre matrici in uscita dalla SVD applica una trasformazione lineare equivalente a quella applicata dalla sola matrice \\(M\\) . Questo concetto \u00e8 riassunto nella seguente figura. Il ruolo principale della SVD \u00e8 quindi quello di definire una serie di valori singolari (la matrice \\(\\Sigma\\) ) di una matrice \\(M\\) , i quali possono essere ricondotti ai valori originari mediante trasformazioni lineari. In tal senso, se la matrice originaria rappresenta un insieme di feature per un dato dataset, i valori singolari rappresentano un nuovo insieme di queste feature, corrispondenti ad una combinazione lineare di quelle originarie, ordinati secondo il grado di variabilit\u00e0 dei dati originari che riescono a \"rappresentare\", o \"spiegare\". Ne consegue che se un numero ridotto di feature individuate dalla SVD riesce a rappresentare un buon grado di variabilit\u00e0 dei dati iniziali, \u00e8 possibile scartare le feature rimanenti, effettuando quindi un'operazione di riduzione della dimensionalit\u00e0 .","title":"Interpretazione della SVD"},{"location":"material/appendix/07_algorithms/cnn/resnet/","text":"1. il problema delle plain network \u00b6 le normali reti di deep learning hanno i conv layer quindi i layer fully connected per i task di classificazione come AlexNet, ZFNet e VGGNet. Quando le plain networ sono abbastanza profonde, pu\u00f2 avvenire il problema dei vainshing gradient. Durante la backpropagation, uando le derivate parziali della funzione dell'errore rispetto all'attuale peso in gi iterazione di training, questo ha l'effetto di moltiplicare \\(n\\) di di questi piccoli o grandi numeri per calcolare i gradienti dei layer frontali in una rete ad \\(n\\) livelli. quando la rete \u00e8 profonda, e si moltiplicano \\(n\\) di questi piccoli numeri, il gradiente diventer\u00e0 zero (vanishing). Quando la rete \u00e8 prfonoda, e si moltiplicano \\(n\\) di questi grossi numeri, il gradiente diventer\u00e0 troppo grande (exploded). Ci aspettiamo che le reti pi\u00f9 profonde abbiano predizioni pi\u00f9 accurate. Tuttavia, quando ci\u00f2 non avviene, \u00e8 probabile che ci sia un degradation problem legato ai vanishing gradient. https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8 https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803","title":"1. il problema delle plain network"},{"location":"material/appendix/07_algorithms/cnn/resnet/#1-il-problema-delle-plain-network","text":"le normali reti di deep learning hanno i conv layer quindi i layer fully connected per i task di classificazione come AlexNet, ZFNet e VGGNet. Quando le plain networ sono abbastanza profonde, pu\u00f2 avvenire il problema dei vainshing gradient. Durante la backpropagation, uando le derivate parziali della funzione dell'errore rispetto all'attuale peso in gi iterazione di training, questo ha l'effetto di moltiplicare \\(n\\) di di questi piccoli o grandi numeri per calcolare i gradienti dei layer frontali in una rete ad \\(n\\) livelli. quando la rete \u00e8 profonda, e si moltiplicano \\(n\\) di questi piccoli numeri, il gradiente diventer\u00e0 zero (vanishing). Quando la rete \u00e8 prfonoda, e si moltiplicano \\(n\\) di questi grossi numeri, il gradiente diventer\u00e0 troppo grande (exploded). Ci aspettiamo che le reti pi\u00f9 profonde abbiano predizioni pi\u00f9 accurate. Tuttavia, quando ci\u00f2 non avviene, \u00e8 probabile che ci sia un degradation problem legato ai vanishing gradient. https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8 https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803","title":"1. il problema delle plain network"},{"location":"material/appendix/07_algorithms/xai/shap/lecture/","text":"XAI con gli Shapley values \u00b6 Gli Shapley values sono un approccio molto utiizzato per la teoria dei giochi coperativa che ha delle propriet\u00e0 affascinati. Spiegare un modello di regresisone lineare \u00b6 Prima di usare i valori di Shapley per spiegare dei modelli complessi, \u00e8 utile comprendere come funzioanno per dei modelli semplici. Uno dei tipi pi\u00f9 semplice di modello \u00e8 quello di regressione lineare. Usiamo il California housing dataset. Questo \u00e8 fatto di 20.640 isolati di case in California nel 1990, con il nostro obiettivo che \u00e8 quello di predire il logaritmo naturale del prezzo emediano di una casa a partire da otto diverse feature: MedInc: income medio in un isolato HouseAge et\u00e0 media delle case in un isolato AveRooms: numero medio di camere per ciascuna propriet\u00e0 AveBedrms: numero medio di camere da letto per propriet\u00e0 Population: popolazione dell'isolato AveOccup: numero medio di membri delle propriet\u00e0 Latitude: latitudine dell'isolato Longitude: longitudine dell'isolato import pandas as pd import shap import sklearn X , y = shap . datasets . california ( n_points = 1000 ) X100 = shap . utils . sample ( X , 100 ) model = sklearn . linear_model . LinearRegression () model . fit ( X , y ) Esame dei coefficienti del modello \u00b6 Il modo pi\u00f9 comune di comprendere un modello lineare \u00e8 quello di esaminare i coefficienti appresi per ciascuna feature. Questi coefficienti ci indicano quanto dell'output del modello cambia quando cambiamo ciascuna delle feature dell'input. print ( 'Coefficienti del modello: \\n ' ) for i in range ( X . shape [ 1 ]): print ( X . columns [ i ], '=' , model . coef_ [ i ] . round ( 5 )) I coefficienti ci indicano cosa succede quando cambiamo il valore di uan feature di input, tuttavia da soli non sono un buon modo per misurare l'importanza complessiva di una feature. Questo avviene pech\u00e9 il valore di ogni coefficiente dipende dalla scala delle feature di input. Se per esempio dovessimo misurare l'et\u00e0 di una casa in minuti piuttosto che anni, il coefficiente per lo HouseAge dioventerebe 0.0115 / (365 * 24 * 60). Chiaramente, il numero di anni passatp per una caasa non \u00e8 diverso dal numero di minuti, ma il valore del suo coefficiente \u00e8 molto pi\u00f9 ampio. Questo significa che la magnitudine di un coefficiente non \u00e8 necessariamente una buona misura dell'importanza della feature in un modello lineare. Da qui: https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html#A-more-complete-picture-using-partial-dependence-plots","title":"XAI con gli Shapley values"},{"location":"material/appendix/07_algorithms/xai/shap/lecture/#xai-con-gli-shapley-values","text":"Gli Shapley values sono un approccio molto utiizzato per la teoria dei giochi coperativa che ha delle propriet\u00e0 affascinati.","title":"XAI con gli Shapley values"},{"location":"material/appendix/07_algorithms/xai/shap/lecture/#spiegare-un-modello-di-regresisone-lineare","text":"Prima di usare i valori di Shapley per spiegare dei modelli complessi, \u00e8 utile comprendere come funzioanno per dei modelli semplici. Uno dei tipi pi\u00f9 semplice di modello \u00e8 quello di regressione lineare. Usiamo il California housing dataset. Questo \u00e8 fatto di 20.640 isolati di case in California nel 1990, con il nostro obiettivo che \u00e8 quello di predire il logaritmo naturale del prezzo emediano di una casa a partire da otto diverse feature: MedInc: income medio in un isolato HouseAge et\u00e0 media delle case in un isolato AveRooms: numero medio di camere per ciascuna propriet\u00e0 AveBedrms: numero medio di camere da letto per propriet\u00e0 Population: popolazione dell'isolato AveOccup: numero medio di membri delle propriet\u00e0 Latitude: latitudine dell'isolato Longitude: longitudine dell'isolato import pandas as pd import shap import sklearn X , y = shap . datasets . california ( n_points = 1000 ) X100 = shap . utils . sample ( X , 100 ) model = sklearn . linear_model . LinearRegression () model . fit ( X , y )","title":"Spiegare un modello di regresisone lineare"},{"location":"material/appendix/07_algorithms/xai/shap/lecture/#esame-dei-coefficienti-del-modello","text":"Il modo pi\u00f9 comune di comprendere un modello lineare \u00e8 quello di esaminare i coefficienti appresi per ciascuna feature. Questi coefficienti ci indicano quanto dell'output del modello cambia quando cambiamo ciascuna delle feature dell'input. print ( 'Coefficienti del modello: \\n ' ) for i in range ( X . shape [ 1 ]): print ( X . columns [ i ], '=' , model . coef_ [ i ] . round ( 5 )) I coefficienti ci indicano cosa succede quando cambiamo il valore di uan feature di input, tuttavia da soli non sono un buon modo per misurare l'importanza complessiva di una feature. Questo avviene pech\u00e9 il valore di ogni coefficiente dipende dalla scala delle feature di input. Se per esempio dovessimo misurare l'et\u00e0 di una casa in minuti piuttosto che anni, il coefficiente per lo HouseAge dioventerebe 0.0115 / (365 * 24 * 60). Chiaramente, il numero di anni passatp per una caasa non \u00e8 diverso dal numero di minuti, ma il valore del suo coefficiente \u00e8 molto pi\u00f9 ampio. Questo significa che la magnitudine di un coefficiente non \u00e8 necessariamente una buona misura dell'importanza della feature in un modello lineare. Da qui: https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html#A-more-complete-picture-using-partial-dependence-plots","title":"Esame dei coefficienti del modello"},{"location":"material/notebooks/","text":"Indice dei notebook \u00b6 In questa pagina \u00e8 presente la raccolta dei notebook presentati durante le lezioni. Notebook Link Lezione 7.4 Lezione 7.5 Lezione 7.6 Lezione 8 - Matplotlib Lezione 9 - Pandas Lezione 10 - Seaborn Lezione 11 - SciPy Lezione 16 - Regressione lineare Lezione 17 - Regressione logistica Lezione 25 - Tensorflow Lezione 26 - Overfitting e regolarizzazione Lezione 27 - Convolutional Neural Networks in TensorFlow e Keras Lezione 28 - TensorFlow Tips&Tricks Lezione 29 - Introduzione alle serie temporali","title":"Notebook"},{"location":"material/notebooks/#indice-dei-notebook","text":"In questa pagina \u00e8 presente la raccolta dei notebook presentati durante le lezioni. Notebook Link Lezione 7.4 Lezione 7.5 Lezione 7.6 Lezione 8 - Matplotlib Lezione 9 - Pandas Lezione 10 - Seaborn Lezione 11 - SciPy Lezione 16 - Regressione lineare Lezione 17 - Regressione logistica Lezione 25 - Tensorflow Lezione 26 - Overfitting e regolarizzazione Lezione 27 - Convolutional Neural Networks in TensorFlow e Keras Lezione 28 - TensorFlow Tips&Tricks Lezione 29 - Introduzione alle serie temporali","title":"Indice dei notebook"},{"location":"material/projects/","text":"Elenco delle tematiche di interesse per il tema d'anno \u00b6 Nella seguente tabella sono elencate alcune tematiche di interesse per effettuare un tema d'anno di tipo compilativo . Questa modalit\u00e0, prevalentemente indirizzata a studenti dei corsi triennali e magistrali , prevede un approfondimento sul tema proposto, composto da: presentazione; relazione; codice di esempio testato e funzionante. Non \u00e8 possibile assegnare lo stesso tema a pi\u00f9 di un gruppo ; in tal senso, l'assegnazione sar\u00e0 data in base alla priorit\u00e0 acquisita, ovvero all'ordine temporale in cui saranno ricevute le richieste. Per prenotare un tema d'anno, \u00e8 necessario compilare il form a questo indirizzo . Il form sar\u00e0 attivo fino al 5 luglio; oltre quella data,ed in base alle necessit\u00e0, potr\u00e0 essere pubblicato un nuovo form con temi aggiornati. Suggerimento La data da segnalare sul modulo \u00e8, ovviamente, indicativa. Tuttavia, anche per permettere una migliore organizzazione del lavoro a tutti, si consiglia di \"bloccare\" un tema di interesse soltanto se si \u00e8 effettivamente sicuri di sostenere l'esame in questa modalit\u00e0. Ambito Tema d'anno Disponibile Assegnatari Scikit Learn Support Vector Machines Di Cosmo Scikit Learn Algoritmi multiclasse-multioutput Dipalma Scikit Learn Novelty/Outlier detection Leserri Scikit Learn Reti neurali non supervisionate Fazio - Ambrosio Scipy Clustering Giorgio - Tota - Susca Scipy Problemi di ottimizzazione Cagnazzi - Rago Scipy Fattorizzazione di matrici De Rosa - Leone TensorFlow/Keras Tuning degli iperparametri Fiume TensorFlow/Keras AutoML Capodiferro - Nenna - Racanati TensorFlow/Keras Image segmentation Bellino - Gentile TensorFlow/Keras Classificazione del testo con RNN Maldera TensorFlow/Keras Classificazione del testo con BERT TensorFlow/Keras Predizione di serie temporali Disanto - Ginestra - Pisani TensorFlow/Keras Autoencoders Loizzo Proposte approvate \u00b6 Di seguito l'elenco delle proposte per il tema d'anno approvate. Tema d'anno Assegnatari Decomposizione SVD Gioiosa - Rubino - Nicoletti - Erione Color detection De Giglio - Percoco","title":"Temi d'anno"},{"location":"material/projects/#elenco-delle-tematiche-di-interesse-per-il-tema-danno","text":"Nella seguente tabella sono elencate alcune tematiche di interesse per effettuare un tema d'anno di tipo compilativo . Questa modalit\u00e0, prevalentemente indirizzata a studenti dei corsi triennali e magistrali , prevede un approfondimento sul tema proposto, composto da: presentazione; relazione; codice di esempio testato e funzionante. Non \u00e8 possibile assegnare lo stesso tema a pi\u00f9 di un gruppo ; in tal senso, l'assegnazione sar\u00e0 data in base alla priorit\u00e0 acquisita, ovvero all'ordine temporale in cui saranno ricevute le richieste. Per prenotare un tema d'anno, \u00e8 necessario compilare il form a questo indirizzo . Il form sar\u00e0 attivo fino al 5 luglio; oltre quella data,ed in base alle necessit\u00e0, potr\u00e0 essere pubblicato un nuovo form con temi aggiornati. Suggerimento La data da segnalare sul modulo \u00e8, ovviamente, indicativa. Tuttavia, anche per permettere una migliore organizzazione del lavoro a tutti, si consiglia di \"bloccare\" un tema di interesse soltanto se si \u00e8 effettivamente sicuri di sostenere l'esame in questa modalit\u00e0. Ambito Tema d'anno Disponibile Assegnatari Scikit Learn Support Vector Machines Di Cosmo Scikit Learn Algoritmi multiclasse-multioutput Dipalma Scikit Learn Novelty/Outlier detection Leserri Scikit Learn Reti neurali non supervisionate Fazio - Ambrosio Scipy Clustering Giorgio - Tota - Susca Scipy Problemi di ottimizzazione Cagnazzi - Rago Scipy Fattorizzazione di matrici De Rosa - Leone TensorFlow/Keras Tuning degli iperparametri Fiume TensorFlow/Keras AutoML Capodiferro - Nenna - Racanati TensorFlow/Keras Image segmentation Bellino - Gentile TensorFlow/Keras Classificazione del testo con RNN Maldera TensorFlow/Keras Classificazione del testo con BERT TensorFlow/Keras Predizione di serie temporali Disanto - Ginestra - Pisani TensorFlow/Keras Autoencoders Loizzo","title":"Elenco delle tematiche di interesse per il tema d'anno"},{"location":"material/projects/#proposte-approvate","text":"Di seguito l'elenco delle proposte per il tema d'anno approvate. Tema d'anno Assegnatari Decomposizione SVD Gioiosa - Rubino - Nicoletti - Erione Color detection De Giglio - Percoco","title":"Proposte approvate"},{"location":"material/slides/","text":"Slide proiettate a lezione \u00b6 Argomento Slides 00 - Programma del corso 01 - Introduzione a Python 02 - Concetti sintattici fondamentali 03 - Strutture dati in Python 04 - OOP in Python 05 - Introduzione a NumPy 06 - Operazioni in NumPy 07 - Matplotlib 08 - Pandas 09 - Seaborn 10 - SciPy 11 - Introduzione al machine learning 12 - Definizione del problema 13 - Preparazione dei dati 14 - Introduzione a Scikit Learn 15 - Regressione lineare 16 - Regressione logistica 17 - Metriche 18 - Classificatori e regressori 19 - Clustering 21 - Reti neurali 22 - Overfitting e regolarizzazione 23 - Image classification 24 - Serie temporali Appendice D - Principi di OOP","title":"Slides proiettate a lezione"},{"location":"material/slides/#slide-proiettate-a-lezione","text":"Argomento Slides 00 - Programma del corso 01 - Introduzione a Python 02 - Concetti sintattici fondamentali 03 - Strutture dati in Python 04 - OOP in Python 05 - Introduzione a NumPy 06 - Operazioni in NumPy 07 - Matplotlib 08 - Pandas 09 - Seaborn 10 - SciPy 11 - Introduzione al machine learning 12 - Definizione del problema 13 - Preparazione dei dati 14 - Introduzione a Scikit Learn 15 - Regressione lineare 16 - Regressione logistica 17 - Metriche 18 - Classificatori e regressori 19 - Clustering 21 - Reti neurali 22 - Overfitting e regolarizzazione 23 - Image classification 24 - Serie temporali Appendice D - Principi di OOP","title":"Slide proiettate a lezione"}]}
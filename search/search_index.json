{"config":{"lang":["it"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Python 4 Data Science","text":"<p>Benvenuti!</p> <p>Questo sito \u00e8 stato originariamente progettato come supporto al corso di Python per il Calcolo Scientifico, che ho tenuto dal 2021 al 2023 presso il Dipartimento di Matematica dell'Universit\u00e0 degli Studi di Bari.</p> <p>Nonostante il corso non esista pi\u00f9, tuttavia, ho deciso di mantenere attivo l'accesso al sito, aggiornandolo quando possibile, in quanto lo reputo uno strumento utile soprattutto per me, in quanto mi permette di approfondire e mettere \"nero su bianco\" alcuni concetti legati alla programmazione ed all'intelligenza artificiale che, per vari motivi, ho la possibilit\u00e0 di approfondire nel mio lavoro.</p> <p>Vi auguro quindi una buona lettura, magari a partire dai concetti introduttivi, oppure navigando sull'argomento di interesse dal menu qui a sinistra. Ovviamente, vi ricordo anche che suggerimenti e critiche sono benvenuti e considerati se posti in maniera civile. In caso contrario, qualora sentiate l'esigenza di esprimervi, vi consiglierei un giro sui tanti social network di cui Internet ormai abbonda.</p>"},{"location":"contributors/","title":"Crediti","text":"<p>Un particolare ringraziamento a tutti coloro che hanno contribuito alla costruzione e manutenzione del materiale del corso.</p> Nome Contributo Mirko Cal\u00f2 Registrazione delle lezioni. Simone Fidanza Revisione ed update della repository. Vito Ren\u00f2 Materiale sulla PCA"},{"location":"projects/","title":"Elenco delle tematiche di interesse per il tema d'anno","text":"<p>Nella seguente tabella sono elencate alcune tematiche di interesse per effettuare un tema d'anno di tipo compilativo. Questa modalit\u00e0, prevalentemente indirizzata a studenti dei corsi triennali e magistrali, prevede un approfondimento sul tema proposto, composto da:</p> <ol> <li>presentazione;</li> <li>relazione;</li> <li>codice di esempio testato e funzionante.</li> </ol> <p>Non \u00e8 possibile assegnare lo stesso tema a pi\u00f9 di un gruppo; in tal senso, l'assegnazione sar\u00e0 data in base alla priorit\u00e0 acquisita, ovvero all'ordine temporale in cui saranno ricevute le richieste.</p> <p>Per prenotare un tema d'anno, \u00e8 necessario compilare il form a questo indirizzo. Il form sar\u00e0 attivo fino al 5 luglio; oltre quella data,ed in base alle necessit\u00e0, potr\u00e0 essere pubblicato un nuovo form con temi aggiornati.</p> <p>Suggerimento</p> <p>La data da segnalare sul modulo \u00e8, ovviamente, indicativa. Tuttavia, anche per permettere una migliore organizzazione del lavoro a tutti, si consiglia di \"bloccare\" un tema di interesse soltanto se si \u00e8 effettivamente sicuri di sostenere l'esame in questa modalit\u00e0.</p> Ambito Tema d'anno Disponibile Assegnatari Scikit-Learn Support Vector Machines Di Cosmo Scikit-Learn Algoritmi multiclasse-multioutput Dipalma Scikit-Learn Novelty/Outlier detection Leserri Scikit-Learn Reti neurali non supervisionate Fazio - Ambrosio Scipy Clustering Giorgio - Tota - Susca Scipy Problemi di ottimizzazione Cagnazzi - Rago Scipy Fattorizzazione di matrici De Rosa - Leone TensorFlow/Keras Tuning degli iperparametri Fiume TensorFlow/Keras AutoML Capodiferro - Nenna - Racanati TensorFlow/Keras Image segmentation Bellino - Gentile TensorFlow/Keras Classificazione del testo con RNN Maldera TensorFlow/Keras Classificazione del testo con BERT Cotimbo TensorFlow/Keras Predizione di serie temporali Disanto - Ginestra - Pisani TensorFlow/Keras Autoencoders Loizzo"},{"location":"projects/#proposte-approvate","title":"Proposte approvate","text":"<p>Di seguito l'elenco delle proposte per il tema d'anno approvate.</p> Tema d'anno Assegnatari Decomposizione SVD Gioiosa - Rubino - Nicoletti - Erione Color detection De Giglio - Percoco Big transfer Antonacci - Pacucci"},{"location":"material/01_python/01_intro/01_intro.en/","title":"1.1.1 - Introduction to Python","text":"<p>Before we start talking about the Python language, it's important to make sure that the interpreter is installed on our system. To do this, we open a terminal (Shell or Command Prompt, depending on our system) and type:</p> <pre><code>python\n</code></pre> <p>If a screen similar to the one shown in figure 1 appears, Python is already correctly installed on our system.</p> <p> </p> Figure 1 - Python Interpreter <p>Alternatively, we will need to install it following the procedure indicated on the official website and add it to the system path.</p>"},{"location":"material/01_python/01_intro/01_intro.en/#python-and-typing","title":"Python and Typing","text":""},{"location":"material/01_python/01_intro/01_intro.en/#dynamic-typing","title":"Dynamic Typing","text":"<p>Python is an interpreted and dynamically typed language. In short, this means that the interpreter evaluates the type of each variable at runtime, and that this can change during the execution of the program.</p> <p>But, in the end, what does this mean for the programmer? Well, very simple.</p> <p>Let's say we have to define and initialize an integer variable in a statically typed language like C++. To do this, we will write something like:</p> <pre><code>int var = 0;\n</code></pre> <p>In Python, we can omit the type, which will be inferred directly from the value assigned to the variable:</p> <pre><code>var = 0\n</code></pre> <p>Now let's imagine that our variable needs to become a decimal. In C++, we'll need to do casting:</p> <pre><code>float fVar = float(var);\nfVar + 1.1;\n</code></pre> <p>In Python, this won't be necessary, and we can directly perform the desired operations:</p> <pre><code>var + 1.1           # The result will be 1.1\n</code></pre> <p>This can apparently simplify life a lot, as there is no longer any need to worry about the type of the variable. However, not everything that glitters is gold: to understand this, it's time to talk about the (Pilate-like) principle of duck typing.</p>"},{"location":"material/01_python/01_intro/01_intro.en/#duck-typing","title":"Duck Typing","text":"<p>Duck typing can be summarized in the following maxim:</p> <p>Duck Typing</p> <p>If it walks like a duck and it quacks like a duck, then it must be a duck.</p> <p>which in Italian sounds more or less like Se cammina come un papero, e starnazza come un papero, deve essere un papero (if it walks like a duck and quacks like a duck, it must be a duck). Let's briefly translate it into \"computerese.\"</p> <p>Let's instruct our Python interpreter to assign our variable <code>var</code> the value of <code>1</code>. The interpreter notes that the variable \"behaves\" like an integer, and therefore \"establishes\" that it is indeed an integer.</p> <p>Now let's try to add a value equal to <code>1.1</code> to <code>var</code>. The result, as expected, will be a decimal number, and therefore the interpreter will \"change its mind,\" since the behaviors assumed by <code>var</code> are now assimilable to a variable of type <code>float</code>.</p> <p>The usefulness of duck typing is evident: it allows the developer to \"save\" numerous casting operations, making the code simpler to write and maintain. However, it must be taken into account when using classes and objects, as the interpreter will try to infer and automatically use a type based on the context in which the variable is used, with the conveniences (and potential disasters) that this entails.</p>"},{"location":"material/01_python/01_intro/01_intro.en/#built-in-types-in-python","title":"Built-in Types in Python","text":"<p>Python provides a series of *built</p>"},{"location":"material/01_python/01_intro/01_intro.en/#112-basic-data-types-in-python","title":"1.1.2 - Basic Data Types in Python","text":"<p>In Python, there are several built-in data types that allow us to store different types of data. Here is a brief overview:</p>"},{"location":"material/01_python/01_intro/01_intro.en/#numbers","title":"Numbers","text":"<p>Python supports integers, floating-point numbers, and complex numbers. Integers and floating-point numbers are widely used in most programming languages, and complex numbers are used less frequently.</p> <pre><code># Integers\nx = 10\ny = -5\nz = 0\n\n# Floating-point numbers\na = 3.14\nb = -0.27\n\n# Complex numbers\nc = 2 + 3j\nd = -1j\n</code></pre>"},{"location":"material/01_python/01_intro/01_intro.en/#strings","title":"Strings","text":"<p>Strings are used to store text. In Python, we can create a string by enclosing a sequence of characters in either single quotes or double quotes.</p> <pre><code># Single-line string\nname = \"Alice\"\ngreeting = 'Hello, ' + name + '!'\nprint(greeting)\n\n# Multi-line string\nlyrics = \"\"\"\nAnd I said, what about Breakfast at Tiffany's?\nShe said, I think I remember the film\nAnd as I recall, I think, we both kind of liked it\n\"\"\"\nprint(lyrics)\n</code></pre>"},{"location":"material/01_python/01_intro/01_intro.en/#booleans","title":"Booleans","text":"<p>A Boolean value is a value that can either be <code>True</code> or <code>False</code>. In Python, Booleans are very important for making decisions based on whether something is <code>True</code> or <code>False</code>.</p> <pre><code># Boolean values\nx = True\ny = False\n\n# Logical operators\nprint(x and y)  # False\nprint(x or y)   # True\nprint(not x)    # False\n</code></pre>"},{"location":"material/01_python/01_intro/01_intro.en/#lists","title":"Lists","text":"<p>Lists are used to store a collection of items. In Python, lists are mutable, which means we can add, remove, or modify items after the list is created.</p> <pre><code># List of integers\nnumbers = [1, 2, 3, 4, 5]\n\n# List of strings\nfruits = ['apple', 'banana', 'orange']\n\n# Mixed list\nmixed = [10, 'hello', True, 3.14]\n\n# Adding items\nfruits.append('kiwi')\nprint(fruits)\n\n# Removing items\ndel mixed[1]\nprint(mixed)\n\n# Accessing items\nprint(numbers[2])\nprint(fruits[-1])\n</code></pre>"},{"location":"material/01_python/01_intro/01_intro.en/#tuples","title":"Tuples","text":"<p>Tuples are similar to lists in that they can store a collection of items. However, tuples are immutable, which means we cannot add, remove, or modify items after the tuple is created.</p> <pre><code># Tuple of integers\nnumbers = (1, 2, 3, 4, 5)\n\n# Tuple of strings\nfruits = ('apple', 'banana', 'orange')\n\n# Mixed tuple\nmixed = (10, 'hello', True, 3.14)\n\n# Accessing items\nprint(numbers[2])\nprint(fruits[-1])\n</code></pre>"},{"location":"material/01_python/01_intro/01_intro.en/#dictionaries","title":"Dictionaries","text":"<p>Dictionaries are used to store key-value pairs. In Python, dictionaries are very useful for storing data in a structured format.</p> <pre><code># Dictionary of prices\nprices = {\n    'apple': 1.50,\n    'banana': 0.99,\n    'orange': 1.25\n}\n\n# Accessing values\nprint(prices['banana'])\n\n# Adding or modifying values\nprices['kiwi'] = 2.00\nprices['banana'] = 1.25\nprint(prices)\n\n# Removing values\ndel prices['orange']\nprint(prices)\n</code></pre> <p>These are just some of the basic data types in Python. There are other more advanced data types such as sets, arrays, and classes that we can learn about</p>"},{"location":"material/01_python/01_intro/01_intro/","title":"1.1.1 - Introduzione al Python","text":"<p>Prima di iniziare a parlare del linguaggio Python, \u00e8 opportuno verificare che l'interprete sia installato nel nostro sistema. Per farlo, apriamo un terminale (Shell o Command Prompt, a seconda del nostro sistema), e scriviamo:</p> <pre><code>python\n</code></pre> <p>Se apparir\u00e0 una schermata simile a quella mostrata in figura 1, Python sar\u00e0 gi\u00e0 correttamente installato sul nostro sistema.</p> <p> </p> Figura 1 - Interprete Python <p>In alternativa, dovremo provvedere ad installarlo seguendo la procedura indicata sul sito ufficiale, ed aggiungerlo al path di sistema.</p>"},{"location":"material/01_python/01_intro/01_intro/#python-e-tipizzazione","title":"Python e tipizzazione","text":""},{"location":"material/01_python/01_intro/01_intro/#tipizzazione-dinamica","title":"Tipizzazione dinamica","text":"<p>Python \u00e8 un linguaggio interpretato ed a tipizzazione dinamica. In breve, questo significa che l'interprete valuta il tipo di ciascuna variabile a runtime, e che questo pu\u00f2 cambiare durante l'esecuzione del programma.</p> <p>Ma, a conti fatti, in cosa si traduce per il programmatore? Beh, molto semplice.</p> <p>Immaginiamo di dover definire ed inizializzare una variabile di tipo intero in un linguaggio a tipizzazione statica, come ad esempio il C++. Per farlo, scriveremo qualcosa simile a:</p> <pre><code>int var = 0;\n</code></pre> <p>In Python, potremo omettere il tipo, che sar\u00e0 inferito direttamente dal valore assegnato alla variabile:</p> <pre><code>var = 0\n</code></pre> <p>Immaginiamo ora che la nostra variabile debba diventare un decimale. In C++, dovremo effettuare il casting:</p> <pre><code>float fVar = float(var);\nfVar + 1.1;\n</code></pre> <p>In Python questo non sar\u00e0 necessario, e potremo effettuare direttamente le operazioni desiderate:</p> <pre><code>var + 1.1           # Il risultato sar\u00e0 1.1\n</code></pre> <p>Questo pu\u00f2 apparentemente semplificare di molto la vita, in quanto non \u00e8 pi\u00f9 necessario preoccuparsi del tipo della variabile. Non \u00e8 per\u00f2 tutto oro ci\u00f2 che luccica: per comprenderlo, infatti, \u00e8 il momento di parlare del (pilatesco) principio del duck typing.</p>"},{"location":"material/01_python/01_intro/01_intro/#duck-typing","title":"Duck Typing","text":"<p>Il duck typing \u00e8 riassumibile nella seguente massima:</p> <p>Duck Typing</p> <p>If it walks like a duck and it quacks like a duck, then it must be a duck.</p> <p>che in italiano suona pi\u00f9 o meno Se cammina come un papero, e starnazza come un papero, deve essere un papero. Traduciamola brevemente in \"informatichese\". </p> <p>Immaginiamo di istruire il nostro interprete Python ad assegnare alla nostra variabile <code>var</code> il valore di <code>1</code>. L'interprete nota che la variabile si \"comporta\" come un numero intero, e quindi \"stabilir\u00e0\" che si tratti proprio di questo.</p> <p>Proviamo ora a sommare a <code>var</code> un valore pari ad <code>1.1</code>. Il risultato, come ovvio, sar\u00e0 un numero decimale, e quindi l'interprete \"cambier\u00e0 idea\", in quanto i comportamenti assunti da <code>var</code> sono adesso assimilabili ad una variabile di tipo <code>float</code>.</p> <p>L'utilit\u00e0 del duck typing \u00e8 evidente: permette allo sviluppatore di \"risparmiare\" numerose operazioni di cast, rendendo il codice pi\u00f9 semplice da scrivere e manutenere. Tuttavia, occorre tenerne conto nel momento in cui si usano classi ed oggetti, in quanto l'interprete prover\u00e0 ad inferire ed usare automaticamente un tipo in base al contesto in cui viene usata la variabile, con le comodit\u00e0 (ed i potenziali disastri) che questo comporta.</p>"},{"location":"material/01_python/01_intro/01_intro/#tipi-built-in-in-python","title":"Tipi built-in in Python","text":"<p>Python prevede una serie di tipi built-in, ovvero nativamente disponibili nel linguaggio. Ne esiste un gran numero; tuttavia, quelli che ci troveremo pi\u00f9 frequentemente ad utilizzare sono riassunti in tabella 1.</p> Tipo Descrizione Esempio <code>int</code> Numeri interi <code>1</code> <code>float</code> Numeri decimali <code>1.0</code> <code>complex</code> Numeri complessi <code>1 + 1j</code> <code>list</code> Liste di oggetti <code>[1, 'pippo', [1, 2, 3]]</code> <code>tuple</code> Tuple di oggetti <code>(1, 'pippo', [1, 2, 3])</code> <code>str</code> Stringhe <code>'pippo'</code> <code>set</code> Insiemi <code>{1, 2, 3}</code> <code>dict</code> Dizionari <code>{'a': 1, 2: 'b'}</code> <p>Nella prossima lezione, vedremo alcuni tra gli operatori pi\u00f9 comunemente utilizzati sui dati di tipo numerico.</p>"},{"location":"material/01_python/01_intro/02_operators.en/","title":"1.1.2 - Arithmetic and logical operators","text":"<p>As briefly mentioned in the previous lesson, let's now see what the arithmetic and logical operators are, and how they can be used on basic numeric types.</p>"},{"location":"material/01_python/01_intro/02_operators.en/#arithmetic-operators","title":"Arithmetic operators","text":""},{"location":"material/01_python/01_intro/02_operators.en/#addition-multiplication-and-subtraction","title":"Addition, multiplication and subtraction","text":"<p>Let's try to use the interpreter as a simple calculator. To do this, we write directly after the <code>&gt;&gt;&gt;</code> symbol the operations we want to perform, and press the <code>Enter</code> key. For example:</p> <pre><code>&gt;&gt;&gt; 2 + 2\n4\n&gt;&gt;&gt; 3 * 5\n15\n&gt;&gt;&gt; 10 - 2 * 4\n2\n</code></pre>"},{"location":"material/01_python/01_intro/02_operators.en/#division","title":"Division","text":"<p>Divisions always return a floating-point number. For example:</p> <pre><code>&gt;&gt;&gt; 16 / 3\n5.333333333333333\n&gt;&gt;&gt; 2 / 2\n1.0\n</code></pre> <p>Evaluating the type of a variable</p> <p>To evaluate the type of a variable <code>a</code>, we can use the <code>type()</code> function, passing the variable as an argument. For example:</p> <pre><code>&gt;&gt;&gt; a = 10\n&gt;&gt;&gt; type(a)\nint\n&gt;&gt;&gt; b = 3.3\n&gt;&gt;&gt; type(b)\nfloat\n</code></pre>"},{"location":"material/01_python/01_intro/02_operators.en/#quotient-and-remainder-of-a-division","title":"Quotient and remainder of a division","text":"<p>There are two contextual but distinct operations with respect to classical division. In particular, we can use the <code>//</code> operator to obtain the quotient of the division. For example, if we try to divide <code>16</code> by <code>3</code>, we will get a quotient of <code>5</code>:</p> <pre><code>&gt;&gt;&gt; 16 // 3\n5\n</code></pre> <p>The <code>%</code> operator, on the other hand, returns the remainder of the division. In the previous case, it will therefore return <code>1</code>:</p> <pre><code>&gt;&gt;&gt; 16 % 3\n1\n</code></pre> <p>Modular arithmetic</p> <p>It is obvious that the <code>%</code> operator can be used for applications of modular arithmetic.</p> <p>It is important to emphasize that the <code>//</code> and <code>%</code> operators return integer values.</p>"},{"location":"material/01_python/01_intro/02_operators.en/#exponentiation","title":"Exponentiation","text":"<p>To raise a number to a power, it is necessary to use the <code>**</code> operator, where the left operand is the base, while the right one is the exponent:</p> <pre><code>&gt;&gt;&gt; 3 ** 2\n9\n&gt;&gt;&gt; 2 ** 8\n256\n</code></pre>"},{"location":"material/01_python/01_intro/02_operators.en/#summary","title":"Summary","text":"<p>We end this section with a brief summary of the different types of arithmetic operator, and their effects on numerical variables.</p> Operator Description Example Result <code>+</code> Addition <code>1 + 1</code> <code>2</code> <code>-</code> Subtraction <code>1 - 1</code> <code>0</code> <code>*</code> Multiplication <code>2 * 1</code> <code>2</code> <code>**</code> Exponentiation <code>2 ** 3</code> <code>8</code> <code>/</code> Division <code>2 / 1</code> <code>2</code> <code>//</code> Quotient <code>4 // 3</code> <code>1</code> <code>%</code> Modulo <code>5 % 3</code> <code>2</code>"},{"location":"material/01_python/01_intro/02_operators.en/#logical-operators","title":"Logical operators","text":"<p>Logical operators allow implementing the basic operations of Boolean algebra.</p> <p>In this sense, it is appropriate to briefly summarize the principles and operations underlying this type of algebra.</p> <ol> <li>In Boolean algebra, variables can only take two possible values: true, or \\(1\\), and false, or \\(0\\).</li> <li>By combining the values of multiple variables, it is possible to evaluate more or less complex conditions</li> </ol>"},{"location":"material/01_python/01_intro/02_operators.en/#113-comparison-operators","title":"1.1.3 - Comparison operators","text":"<p>Another important category of operators are the comparison operators, which allow us to compare two values and determine whether they are equal, unequal, greater, less, etc. They are frequently used in control structures such as if-else statements and loops.</p> <p>Here are the comparison operators available in Python:</p> Operator Description <code>==</code> Equal to <code>!=</code> Not equal to <code>&gt;</code> Greater than <code>&lt;</code> Less than <code>&gt;=</code> Greater than or equal to <code>&lt;=</code> Less than or equal to <p>These operators compare two operands and return a boolean value (True or False) based on the result of the comparison.</p> <p>Let's see some examples:</p> <pre><code>&gt;&gt;&gt; x = 10\n&gt;&gt;&gt; y = 5\n&gt;&gt;&gt; x == y\nFalse\n&gt;&gt;&gt; x != y\nTrue\n&gt;&gt;&gt; x &gt; y\nTrue\n&gt;&gt;&gt; x &lt; y\nFalse\n&gt;&gt;&gt; x &gt;= y\nTrue\n&gt;&gt;&gt; x &lt;= y\nFalse\n</code></pre> <p>Note that the comparison operator <code>==</code> (equal to) is different from the assignment operator <code>=</code>. The assignment operator is used to assign a value to a variable, while the comparison operator is used to compare two values.</p> <p>It is also worth mentioning that comparison operators can be chained together using logical operators, as we will see in the next section.</p>"},{"location":"material/01_python/01_intro/02_operators/","title":"1.1.2 - Operatori aritmetici e logici","text":"<p>Come brevemente accennato nella scorsa lezione, vediamo di seguito quelli che sono gli operatori aritmetici e logici, e come possono essere utilizzati sui tipi numerici di base.</p>"},{"location":"material/01_python/01_intro/02_operators/#operatori-aritmetici","title":"Operatori aritmetici","text":""},{"location":"material/01_python/01_intro/02_operators/#addizione-moltiplicazione-e-sottrazione","title":"Addizione, moltiplicazione e sottrazione","text":"<p>Proviamo ad usare l'interprete come una semplice calcolatrice; per farlo, scriviamo direttamente dopo il simbolo <code>&gt;&gt;&gt;</code> le operazioni che vogliamo eseguire, e premiamo il tasto <code>Invio</code>. Ad esempio:</p> <pre><code>&gt;&gt;&gt; 2 + 2\n4\n&gt;&gt;&gt; 3 * 5\n15\n&gt;&gt;&gt; 10 - 2 * 4\n2\n</code></pre>"},{"location":"material/01_python/01_intro/02_operators/#divisione","title":"Divisione","text":"<p>Le divisioni restituiscono sempre un numero in virgola mobile. Ad esempio:</p> <pre><code>&gt;&gt;&gt; 16 / 3\n5.333333333333333\n&gt;&gt;&gt; 2 / 2\n1.0\n</code></pre> <p>Valutare il tipo di una variabile</p> <p>Per valutare il tipo di una variabile <code>a</code> possiamo usare la funzione <code>type()</code> cui passeremo la variabile come argomento. Ad esempio:</p> <pre><code>&gt;&gt;&gt; a = 10\n&gt;&gt;&gt; type(a)\nint\n&gt;&gt;&gt; b = 3.3\n&gt;&gt;&gt; type(b)\nfloat\n</code></pre>"},{"location":"material/01_python/01_intro/02_operators/#quoziente-e-resto-di-una-divisione","title":"Quoziente e resto di una divisione","text":"<p>Esistono due operazioni contestuali, ma distinte, rispetto alla classica divisione. In particolare, possiamo utilizzare l'operatore <code>//</code> per ottenere il quoziente della divisione. Ad esempio, provando a dividere <code>16</code> per <code>3</code> avremo un quoziente di <code>5</code>:</p> <pre><code>&gt;&gt;&gt; 16 // 3\n5\n</code></pre> <p>L'operatore <code>%</code>, invece, restituisce il resto della divisione. Nel caso precedente, restituir\u00e0 quindi <code>1</code>:</p> <pre><code>&gt;&gt;&gt; 16 % 3\n1\n</code></pre> <p>Aritmetica modulare</p> <p>Va da s\u00e8 che l'operatore <code>%</code> pu\u00f2 essere usato per applicazioni di aritmetica modulare.</p> <p>E' importante sottolineare come gli operatori <code>//</code> e <code>%</code> restituiscano dei valori interi.</p>"},{"location":"material/01_python/01_intro/02_operators/#elevazione-a-potenza","title":"Elevazione a potenza","text":"<p>Per elevare un numero a potenza, \u00e8 necessario usare l'operatore <code>**</code>, in cui l'operando sinistro \u00e8 la base, mentre quello destro l'esponente:</p> <pre><code>&gt;&gt;&gt; 3 ** 2\n9\n&gt;&gt;&gt; 2 ** 8\n256\n</code></pre>"},{"location":"material/01_python/01_intro/02_operators/#sommario","title":"Sommario","text":"<p>Terminiamo questa sezione con un breve sommario dei diversi tipi di operatore aritmetico, e degli effetti che hanno sulle variabili di tipo numerico.</p> Operatore Descrizione Esempio Risultato <code>+</code> Somma <code>1 + 1</code> <code>2</code> <code>-</code> Sottrazione <code>1 - 1</code> <code>0</code> <code>*</code> Moltiplicazione <code>2 * 1</code> <code>2</code> <code>**</code> Elevazione a potenza <code>2 ** 3</code> <code>8</code> <code>/</code> Divisione <code>2 / 1</code> <code>2</code> <code>//</code> Quoziente <code>4 // 3</code> <code>1</code> <code>%</code> Modulo <code>5 % 3</code> <code>2</code>"},{"location":"material/01_python/01_intro/02_operators/#operatori-logici","title":"Operatori logici","text":"<p>Gli operatori logici permettono di implementare le operazioni base dell'algebra booleana.</p> <p>In tal senso, \u00e8 opportuno riassumere brevemente i principi e le operazioni alla base di questo tipo di algebra.</p> <ol> <li>Nell'algebra booleana, le variabili possono assumere solo due possibili valori: vero, o \\(1\\), e falso, o \\(0\\).</li> <li>Combinando i valori di pi\u00f9 variabili, \u00e8 possibile valutare condizioni pi\u00f9 o meno complesse partendo dalle operazioni \\(AND\\), \\(OR\\) e \\(NOT\\).</li> <li>I valori assunti da una condizione sono esprimibili mediante le tabelle di verit\u00e0.</li> </ol> <p>L'operazione XOR</p> <p>Esiste una quarta operazione fondamentale, la XOR, che per\u00f2 non tratteremo in questa sede.</p>"},{"location":"material/01_python/01_intro/02_operators/#tabelle-di-verita-delle-operazioni-logiche-fondamentali","title":"Tabelle di verit\u00e0 delle operazioni logiche fondamentali","text":"<p>Vediamo brevemente le tabelle di verit\u00e0 per le tre operazioni logiche fondamentali descritte al punto \\(2\\) del paragrafo precedente. Per ciascuna di queste tabelle, considereremo due variabili booleane \\(A\\) e \\(B\\), le quali potranno assumere valore \\(0\\) o \\(1\\). Nelle colonne \\(AND\\) ed \\(OR\\) saranno quindi riportati i risultati della rispettiva operazione logica.</p> \\(A\\) \\(B\\) \\(AND\\) \\(OR\\) \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(0\\) \\(1\\) \\(0\\) \\(1\\) \\(1\\) \\(0\\) \\(0\\) \\(1\\) \\(1\\) \\(1\\) \\(1\\) \\(1\\) <p>In particolare, notiamo che il risultato dell'\\(AND\\) \u00e8 pari ad \\(1\\) quando entrambe le variabili sono \\(1\\); altrimenti, il risultato \u00e8 \\(0\\). Per quello che riguarda invece la \\(OR\\), il risultato \u00e8 \\(1\\) quando almeno una delle variabili \u00e8 pari ad \\(1\\).</p> <p>Trasponendo questo concetto nella verifica di una o pi\u00f9 condizioni:</p> <ul> <li>un AND \u00e8 vero se e solo se entrambe le condizioni sono vere;</li> <li>un OR \u00e8 vero se e solo se almeno una delle condizioni \u00e8 vera.</li> </ul> <p>In ultimo, l'operazione \\(NOT\\) agisce su un'unica variabile, negandola. Di conseguenza, la tabella di verit\u00e0 \u00e8 riassumibile come segue:</p> \\(A\\) \\(NOT\\) \\(0\\) \\(1\\) \\(1\\) \\(0\\) <p>Di conseguenza, la \\(NOT\\) di una condizione \u00e8 vera se la condizione \u00e8 falsa, e viceversa.</p> <p>Facciamo un esempio pratico. Immaginiamo di voler verificare che un numero intero \\(x\\) sia compreso tra \\(0\\) e \\(10\\). Ragionando mediante l'algebra booleana, potremo scrivere:</p> \\[ \\begin{aligned}     &amp;cond_{min} = x &lt; 10 \\\\     &amp;cond_{max} = x &gt; 0 \\\\     &amp;res = cond_{min} \\text{ AND } cond_{max} \\end{aligned} \\] <p>In altri termini, il risultato finale (\\(res\\)) sar\u00e0 vero se e solo se sia \\(cond_{min}\\) e \\(cond_{max}\\) sono vere.</p>"},{"location":"material/01_python/01_intro/02_operators/#operazioni-logiche-in-python","title":"Operazioni logiche in Python","text":"<p>Per realizzare le operazioni logiche, Python ci mette a disposizione tre parole chiave, ovvero <code>and</code>, <code>or</code> e <code>not</code>. L'associazione tra parola chiave ed operazione logica \u00e8 lasciata al lettore come esercizio.</p>"},{"location":"material/01_python/01_intro/03_strings.en/","title":"1.1.3 - Strings","text":"<p>In Python, strings can be enclosed in either single or double quotes.</p> <pre><code>&gt;&gt;&gt; \"a string\"\n'a string'\n&gt;&gt;&gt; 'another string'\n'another string'\n</code></pre> <p>In the second statement, we use the escape character (<code>\\</code>) before the apostrophe. If we omit it, the interpreter would raise a syntax error (<code>SyntaxError</code>):</p> <pre><code>&gt;&gt;&gt; 'another string'\n  File \"&lt;stdin&gt;\", line 1\n    'another string\n                   ^\nSyntaxError: invalid syntax\n</code></pre> <p>Note</p> <p>All characters preceded by the backslash symbol (<code>\\</code>) will be interpreted as escape characters unless we add the <code>r</code> symbol before the start of the string:</p> <pre><code>&gt;&gt;&gt; print('C:\\new_folder')\nC:\new_folder\n&gt;&gt;&gt; print(r'C:\\new_folder')\nC:\\new_folder\n</code></pre>"},{"location":"material/01_python/01_intro/03_strings.en/#multi-line-strings","title":"Multi-line Strings","text":"<p>Strings and Lists</p> <p>Most of the concepts we will see next are also applicable to lists. In fact, to be precise, they are derived from lists, as Python considers a string as a particular type of list.</p> <p>Strings can span multiple lines. To do this, we can use triple quotes (three consecutive quotation marks) to indicate the beginning and end of the string:</p> <pre><code>&gt;&gt;&gt; print(\"\"\"This is an example\\\n         of\n    a multi-line string\\\n    \"\"\")\nThis is an example\nof\na multi-line string\n</code></pre> <p>Note</p> <p>In the above snippet, we use the <code>\\</code> character to prevent the interpreter from automatically adding a newline character (<code>\\n</code>) at the end of each line. As a result, you can see that the newline character is not added in the highlighted lines, but it is present in line 2.</p>"},{"location":"material/01_python/01_intro/03_strings.en/#string-concatenation","title":"String Concatenation","text":"<p>Concatenating two strings in Python is straightforward; you can use the <code>+</code> operator:</p> <pre><code>&gt;&gt;&gt; string_a = \"First string\"\n&gt;&gt;&gt; string_b = \"Second string\"\n&gt;&gt;&gt; print(string_a + \" - \" + string_b)\nFirst string - Second string\n</code></pre> <p>Note</p> <p>If you use the <code>*</code> operator, you can concatenate the same string multiple times:</p> <pre><code>&gt;&gt;&gt; 3 * 'co.'\n'co.co.co.'\n</code></pre> <p>You can also simply place the two strings next to each other:</p> <pre><code>&gt;&gt;&gt; \"Py\" \"thon\"\n'Python'\n</code></pre> <p>Caution</p> <p>Be careful not to concatenate a string literal (a string enclosed in quotes) with a string variable. If you try to do so, the interpreter will raise an error:</p> <pre><code>&gt;&gt;&gt; py=\"Py\"\n&gt;&gt;&gt; py \"thon\"\n  File \"&lt;stdin&gt;\", line 1\n    py \"thon\"\n        ^\nSyntaxError: invalid syntax\n</code></pre> <p>The same error would occur if, instead of the variable <code>py</code>, you used the result of a concatenation operation:</p> <p><pre><code>&gt;&gt;&gt; ('p' + 'y') 'thon'\n  File \"&lt;stdin&gt;\", line 1\n    ('p' + 'y') 'thon'\n                ^\nSyntaxError: invalid syntax\n</code></pre> In such \"hybrid\" cases, it is advisable to use the standard concatenation operator, which is <code>+</code>.</p> <p>Note</p> <p>There are more efficient ways to concatenate strings, especially when dealing with numerous concatenation operations in large loops. However, the exploration of such methods is beyond the scope of this introduction.</p>"},{"location":"material/01_python/01_intro/03_strings.en/#indexing-strings","title":"Indexing Strings","text":"<p>Python defines strings as *arrays</p> <p>of characters*, so you can index them. For example:</p> <pre><code>&gt;&gt;&gt; string = 'Python'\n&gt;&gt;&gt; string[0]\n'P'\n</code></pre> <p>Individual characters are also considered as strings, each with a length of one:</p> <pre><code>&gt;&gt;&gt; letter = 'P'\n&gt;&gt;&gt; letter[0]\n'P'\n</code></pre> <p>Python also allows accessing elements using negative indices, considering the elements from right to left. In this case, the index of the first element from the right is denoted as <code>-1</code>:</p> <pre><code>&gt;&gt;&gt; string[-1]\n'n'\n</code></pre>"},{"location":"material/01_python/01_intro/03_strings.en/#string-slicing","title":"String Slicing","text":"<p>The slicing operation allows you to extract a specific part of a string. In general, it takes the following form:</p> <pre><code>&gt;&gt;&gt; string[i:j:s]\n</code></pre> <p>where <code>i</code> is the starting index, <code>j</code> is the ending index, and <code>s</code> is the step used. It is important to note that the element at the starting index will be included, while the element at the ending index will be excluded.</p> <p>For example:</p> <pre><code>&gt;&gt;&gt; string[0:2]\n'Py'\n&gt;&gt;&gt; string[2:5]\n'tho'\n</code></pre> <p>If you want to consider all characters up to <code>j</code> (exclusive), you can use the following notation:</p> <pre><code>&gt;&gt;&gt; string[:j]\n</code></pre> <p>If you want to consider all characters starting from <code>i</code> (inclusive), you can use the following notation:</p> <pre><code>&gt;&gt;&gt; string[i:]\n</code></pre> <p>For example:</p> <pre><code>&gt;&gt;&gt; string[1:]\n'ython'\n&gt;&gt;&gt; string[:5]\n'Pytho'\n</code></pre> <p>In this case as well, negative indices can be used. For example, if you want to take all characters from the third-to-last letter to the end, you can write:</p> <pre><code>&gt;&gt;&gt; string[-3:]\n'hon'\n</code></pre> <p>And if you want to take all characters up to the third-to-last letter (exclusive):</p> <pre><code>&gt;&gt;&gt; string[:-3]\n'Pyt'\n</code></pre> <p>Tip</p> <p>You can obtain the entire string using the slicing operation in the following way:</p> <pre><code>&gt;&gt;&gt; string[:]\n'Python'\n</code></pre>"},{"location":"material/01_python/01_intro/03_strings.en/#string-length","title":"String Length","text":"<p>The <code>len()</code> function returns the length of a string:</p> <pre><code>&gt;&gt;&gt; len(string)\n6\n</code></pre>"},{"location":"material/01_python/01_intro/03_strings.en/#immutability-of-strings","title":"Immutability of Strings","text":"<p>Strings in Python are immutable. As the term implies, this means that they cannot be modified. For example, if you try to redefine one or more elements of a string accessed through indexing or slicing, you will encounter an error.</p> <pre><code>&gt;&gt;&gt; string[0] = 'C'  # Error!\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: 'str' object does not support item assignment\n</code></pre> <p>Tip</p> <p>However, you can assign the name <code>string</code> to a new variable if needed.</p>"},{"location":"material/01_python/01_intro/03_strings/","title":"1.1.3 - Stringhe","text":"<p>In Python le stringhe possono indifferentemente essere racchiuse tra virgolette singole e doppie.</p> <pre><code>&gt;&gt;&gt; \"una stringa\"\n'una stringa'\n&gt;&gt;&gt; 'un\\'altra stringa'\n\"un'altra stringa\"\n</code></pre> <p>Notiamo nella seconda istruzione l'uso del carattere di escape (<code>\\</code>) che precede l'apostrofo; se lo omettessimo, l'interprete ci restituirebbe un errore sintattico (<code>SyntaxError</code>):</p> <pre><code>&gt;&gt;&gt; 'un'altra stringa'\n  File \"&lt;stdin&gt;\", line 1\n    'un'altra stringa\n            ^\nSyntaxError: invalid syntax\n</code></pre> <p>Nota</p> <p>Tutti i caratteri preceduti dal simbolo <code>\\</code> saranno interpretati come escape character, a meno di aggiungere il simbolo <code>r</code> prima dell'inizio della stringa:</p> <pre><code>&gt;&gt;&gt; print('C:\\nuova_cartella')\nC:\nuova_cartella\n&gt;&gt;&gt; print(r'C:\\nuova_cartella')\nC:\\nuova_cartella\n</code></pre>"},{"location":"material/01_python/01_intro/03_strings/#stringhe-su-righe-multiple","title":"Stringhe su righe multiple","text":"<p>Stringhe e liste</p> <p>La maggior parte dei concetti che vedremo nel seguito sono applicabili anche alle liste. Anzi, per essere precisi, derivano proprio dalle liste, in quanto Python considera una stringa un particolare tipo di lista.</p> <p>Le stringhe possono articolarsi su pi\u00f9 righe. Per farlo, possiamo usare le triple-quotes, ovvero tre virgolette di seguito, per indicare l'inizio e la fine della stringa:</p> <pre><code>&gt;&gt;&gt; print(\"\"\"Questo \u00e8 un esempio\\\n         di\n        riga multipla\\\n        \"\"\")\n    Questo \u00e8 un esempio di\n    riga multipla\n</code></pre> <p>Nota</p> <p>Notiamo nel precedente snippet il carattere <code>\\</code>, usato per evitare che venga automaticamente inserito dall'interprete il carattere newline (<code>\\n</code>) al termine di ogni riga. Infatti, si vede come il newline non sia stato aggiunto nelle righe evidenziate, mentre sia presente nella riga 2.</p>"},{"location":"material/01_python/01_intro/03_strings/#concatenazione-di-stringhe","title":"Concatenazione di stringhe","text":"<p>Concatenare due stringhe in Python \u00e8 estremamente semplice, e basta usare l'operatore <code>+</code>:</p> <pre><code>&gt;&gt;&gt; stringa_a = \"Prima stringa\"\n&gt;&gt;&gt; stringa_b = \"Seconda stringa\"\n&gt;&gt;&gt; print(stringa_a + \" - \" + stringa_b)\nPrima stringa - Seconda stringa\n</code></pre> <p>Nota</p> <p>Se usiamo l'operatore <code>*</code> possiamo concatenare pi\u00f9 volte la stessa stringa:</p> <pre><code>&gt;&gt;&gt; 3 * 'co.'\n'co.co.co.'\n</code></pre> <p>Possiamo anche semplicemente porre le due stringhe l'una di seguito all'altra:</p> <pre><code>&gt;&gt;&gt; \"Py\" \"thon\"\n'Python'\n</code></pre> <p>Attenzione</p> <p>Bisogna fare particolare attenzione a non concatenare un literal (ovvero una stringa racchiusa tra virgolette) ad una variabile di tipo stringa. Se proviamo a farlo, l'interprete ci restituir\u00e0 questo errore:</p> <pre><code>&gt;&gt;&gt; py=\"Py\"\n&gt;&gt;&gt; py \"thon\"\nFile \"&lt;stdin&gt;\", line 1\npy \"thon\"\n        ^\nSyntaxError: invalid syntax\n</code></pre> <p>Lo stesso errore si presenterebbe se al posto della variabile <code>py</code> usassimo il risultato di una operazione di concatenazione:</p> <p><pre><code>&gt;&gt;&gt; ('p' + 'y') 'thon'\nFile \"&lt;stdin&gt;\", line 1\n    ('p' + 'y') 'thon'\n                  ^\nSyntaxError: invalid syntax\n</code></pre> Il consiglio, in questi casi \"ibridi\", \u00e8 quello di usare l'operatore standard di concatenazione, ovvero il <code>+</code>.</p> <p>Nota</p> <p>Esistono modi pi\u00f9 efficienti di concatenare delle stringhe, specialmente quando si ha a che fare con numerose operazioni di concatenazione in grossi cicli; l'approfondimento di tali metodi \u00e8 demandato al lettore.</p>"},{"location":"material/01_python/01_intro/03_strings/#indicizzazione-di-stringhe","title":"Indicizzazione di stringhe","text":"<p>Python definisce le stringhe come degli array di caratteri; \u00e8 quindi possibile indicizzarli. Ad esempio:</p> <pre><code>&gt;&gt;&gt; stringa = 'Python'\n&gt;&gt;&gt; stringa[0]\n'P'\n</code></pre> <p>Anche i singoli caratteri sono considerati come delle stringhe, ovviamente di lunghezza unitaria:</p> <pre><code>&gt;&gt;&gt; lettera = 'P'\n&gt;&gt;&gt; lettera[0]\n'P'\n</code></pre> <p>Python permette di accedere anche usando degli indici negativi, considerando quindi gli elementi che vanno da destra verso sinistra. In questo caso, l'indice del primo elemento da destra sar\u00e0 indicato con <code>-1</code>:</p> <pre><code>&gt;&gt;&gt; stringa[-1]\n'n'\n</code></pre>"},{"location":"material/01_python/01_intro/03_strings/#slicing-su-stringhe","title":"Slicing su stringhe","text":"<p>L'operazione di slicing permette di estrarre una certa parte di una stringa. In generale, assume la seguente forma:</p> <pre><code>&gt;&gt;&gt; stringa[i:j:s]\n</code></pre> <p>dove <code>i</code> \u00e8 l'indice iniziale, <code>j</code> quello finale ed <code>s</code> lo step utilizzato. E' importante sottolineare come l'elemento all'indice iniziale sar\u00e0 incluso, mentre quello all'indice finale sar\u00e0 escluso.</p> <p>Ad esempio:</p> <pre><code>&gt;&gt;&gt; stringa[0:2]\n'Py'\n&gt;&gt;&gt; stringa[2:5]\n'tho'\n</code></pre> <p>Se volessimo considerare tutti i caratteri fino a <code>j</code> (escluso), dovremmo usare la seguente notazione:</p> <pre><code>&gt;&gt;&gt; stringa [:j]\n</code></pre> <p>Se invece volessimo considerare tutti i caratteri a partire da <code>i</code> (incluso), dovremmo usare la seguente notazione:</p> <pre><code>&gt;&gt;&gt; stringa [i:]\n</code></pre> <p>Ad esempio:</p> <pre><code>&gt;&gt;&gt; stringa[1:]\n'ython'\n&gt;&gt;&gt; stringa[:5]\n'Pytho'\n</code></pre> <p>Anche in questo caso, \u00e8 possibile usare degli indici negativi. Ad esempio, se volessimo prendere tutti i caratteri dalla terzultima lettera fino alla fine, potremmo scrivere:</p> <pre><code>&gt;&gt;&gt; stringa[-3:]\n'hon'\n</code></pre> <p>mentre se volessimo prendere tutti i caratteri fino alla terzultima lettera (esclusa):</p> <pre><code>&gt;&gt;&gt; stringa[:-3]\n'Pyt'\n</code></pre> <p>Suggerimento</p> <p>E' possibile ottenere un'intera stringa mediante l'operazione di slicing in questo modo:</p> <pre><code>&gt;&gt;&gt; stringa[:]\n'Python'\n</code></pre>"},{"location":"material/01_python/01_intro/03_strings/#lunghezza-di-una-stringa","title":"Lunghezza di una stringa","text":"<p>La funzione <code>len()</code> ci restituisce la lunghezza di una stringa:</p> <pre><code>&gt;&gt;&gt; len(stringa)\n6\n</code></pre>"},{"location":"material/01_python/01_intro/03_strings/#immutabilita-di-una-stringa","title":"Immutabilit\u00e0 di una stringa","text":"<p>Le stringhe in Python sono immutabili. Come indica la parola stessa, questo significa che non possono essere modificate: se, ad esempio, provassimo a ridefinirne uno o pi\u00f9 elementi, acceduti magari mediante indexing o slicing, avremmo un errore.</p> <pre><code>&gt;&gt;&gt; stringa[0] = 'C'# Errore!\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: 'str' object does not support item assignment\n</code></pre> <p>Suggerimento</p> <p>Possiamo comunque assegnare il nome <code>stringa</code> ad una nuova variabile.</p>"},{"location":"material/01_python/01_intro/04_lists.en/","title":"1.1.4 - Lists","text":"<p>We have already mentioned that a string is nothing but a special case of a list. The natural question that arises is: what is a list?</p> <p>Lists are one of the four built-in data structures that Python offers for storing sequences of data. From a purely \"conceptual\" point of view, we can consider them similar to arrays found in other programming languages, albeit with some significant differences.</p> <p>We can create a list in the following way:</p> <pre><code>&gt;&gt;&gt; lista = [1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n</code></pre>"},{"location":"material/01_python/01_intro/04_lists.en/#concatenation-indexing-and-slicing-on-lists","title":"Concatenation, Indexing, and Slicing on Lists","text":"<p>Just like with strings, we can perform indexing, slicing, and concatenation operations on lists:</p> <pre><code>&gt;&gt;&gt; lista[0]\n1\n&gt;&gt;&gt; lista[2:]\n[3, 4, 5]\n&gt;&gt;&gt; lista_two = [6, 7]\n&gt;&gt;&gt; lista + lista_two\n[1, 2, 3, 4, 5, 6, 7]\n&gt;&gt;&gt; lista + [6]\n[1, 2, 3, 4, 5, 6]\n</code></pre> <p>Let's look at some examples. Consider the following list:</p> <pre><code>&gt;&gt;&gt; l = [1, 2, 3, 4, 5, 6]\n</code></pre> <p>Let's take the elements at even indices (i.e., 0, 2, and 4):</p> <pre><code>&gt;&gt;&gt; l[0::2]\n[1, 3, 5]\n</code></pre> <p>Let's take all elements starting from the third-to-last and with even indices:</p> <pre><code>&gt;&gt;&gt; l[(-3 + 1)::2]\n[5]\n</code></pre> <p>Note</p> <p>In the previous example, a small \"trick\" was used to account for the fact that indexing starts from 0 and not 1.</p> <p>Let's start from the third-to-last element and go backward towards the origin:</p> <pre><code>&gt;&gt;&gt; l[-3::-1]\n[4, 3, 2, 1]\n</code></pre> <p>Let's start from the last element and go up to the third-to-last from the origin:</p> <pre><code>&gt;&gt;&gt; l[:3:-1]\n[6, 5]\n</code></pre> <p>Let's take the last three elements in reverse order:</p> <pre><code>&gt;&gt;&gt; l[len(l)-1:len(l)-4:-1]\n[6, 5, 4]\n</code></pre> <p>Let's take elements at even indices in reverse order:</p> <pre><code>&gt;&gt;&gt; l[::-2]\n[6, 4, 2]\n</code></pre>"},{"location":"material/01_python/01_intro/04_lists.en/#mutability-of-a-list","title":"Mutability of a List","text":"<p>Unlike strings, lists are mutable objects. Therefore, we can modify their contents:</p> <pre><code>&gt;&gt;&gt; lista[0] = 99\n&gt;&gt;&gt; lista\n[99, 2, 3, 4, 5]\n</code></pre>"},{"location":"material/01_python/01_intro/04_lists.en/#list-operations","title":"List Operations","text":"<p>We can also remove elements from a list using the <code>[]</code> operator combined with the slicing operation:</p> <pre><code>&gt;&gt;&gt; lista[4:] = []\n&gt;&gt;&gt; lista\n[99, 2, 3, 4]\n</code></pre> <p>Note</p> <p>The observant ones may have noticed that the <code>[]</code> operator simply indicates an empty list.</p> <p>Tip</p> <p>We can remove all elements from a list using slicing and the <code>[]</code> operator:</p> <pre><code>&gt;&gt;&gt; lista[:] = []\n&gt;&gt;&gt; lista\n[]\n</code></pre> <p>A list can contain heterogeneous elements. It is even allowed to contain iterables, including other lists:</p> <pre><code>&gt;&gt;&gt; lista.append([1, 2, 3])\n&gt;&gt;&gt; lista\n[99, 2, 3, 4, [1, 2, 3]]\n</code></pre> <p>In the above example, we used the <code>append()</code> function to insert an element at the end of the list. It is interesting to note that the inserted element is itself a list and \"coexists\" peacefully with the other numerical elements.</p> <p>Let's further extend the list by changing the first element to a string:</p> <pre><code>&gt;&gt;&gt; lista[0] = string\n&gt;&gt;&gt; lista\n['Python', 2, 3, 4, [1, 2, 3]]\n</code></pre>"},{"location":"material/01_python/01_intro/04_lists/","title":"1.1.4 - Liste","text":"<p>Abbiamo gi\u00e0 detto che una stringa altro non \u00e8 se non un caso particolare di lista. La domanda che sorge spontanea \u00e8 quindi: cosa \u00e8 una lista?</p> <p>Le liste sono uno dei quattro tipi di strutture built-in che Python offre per memorizzare sequenze di dati. Da un punto di vista puramente \"concettuale\", potremmo considerarle alla stregua degli array presenti in altri linguaggi di programmazione, seppur con alcune, significative differenze.</p> <p>Possiamo creare una lista in questo modo:</p> <pre><code>&gt;&gt;&gt; lista = [1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n</code></pre>"},{"location":"material/01_python/01_intro/04_lists/#concatenazione-indicizzazione-e-slicing-su-liste","title":"Concatenazione, indicizzazione e slicing su liste","text":"<p>Come sulle stringhe, sulle liste \u00e8 possibile effettuare operazioni di indicizzazione, slicing e concatenazione:</p> <pre><code>&gt;&gt;&gt; lista[0]\n1\n&gt;&gt;&gt; lista[2:]\n[3, 4, 5]\n&gt;&gt;&gt; lista_due = [6,7]\n&gt;&gt;&gt; lista + lista_due\n[1, 2, 3, 4, 5, 6, 7]\n&gt;&gt;&gt; lista + [6]\n[1, 2, 3, 4, 5, 6]\n</code></pre> <p>Facciamo alcuni esempi. Consideriamo la seguente stringa:</p> <pre><code>&gt;&gt;&gt; l = [1, 2, 3, 4, 5, 6]\n</code></pre> <p>Prendiamo gli elementi sugli indice pari (ovvero 0, 2 e 4):</p> <pre><code>&gt;&gt;&gt; l[0::2]\n[1, 3, 5]\n</code></pre> <p>Prendiamo tutti gli elementi a partire dal terzultimo e con indice pari:</p> <pre><code>&gt;&gt;&gt; l[(-3 + 1)::2]\n[5]\n</code></pre> <p>Nota</p> <p>Nell'esempio precedente, usato un piccolo \"trucco\" per tenere in conto il fatto che l'indicizzazione parte da 0 e non da 1.</p> <p>Partiamo dal terzultimo elemento, e proseguiamo all'indietro verso l'origine:</p> <pre><code>&gt;&gt;&gt; l[-3::-1]\n[4, 3, 2, 1]\n</code></pre> <p>Partiamo dall'ultimo elemento e proseguiamo sino al terz'ultimo dall'origine:</p> <pre><code>&gt;&gt;&gt; l[:3:-1]\n[6, 5]\n</code></pre> <p>Prendiamo gli ultimi tre elementi in ordine inverso:</p> <pre><code>&gt;&gt;&gt; l[len(l)-1:len(l)-4:-1]\n[6, 5, 4]\n</code></pre> <p>Prendiamo gli elementi agli indici pari in ordine inverso:</p> <pre><code>&gt;&gt;&gt; l[::-2]\n[6, 4, 2]\n</code></pre>"},{"location":"material/01_python/01_intro/04_lists/#mutabilita-di-una-lista","title":"Mutabilit\u00e0 di una lista","text":"<p>A differenza delle stringhe, le liste sono oggetti mutabili. Di conseguenza, possiamo modificarne il contenuto:</p> <pre><code>&gt;&gt;&gt; lista[0] = 99\n&gt;&gt;&gt; lista\n[99, 2, 3, 4, 5]\n</code></pre>"},{"location":"material/01_python/01_intro/04_lists/#operazioni-sulle-liste","title":"Operazioni sulle liste","text":"<p>Possiamo anche eliminare elementi da una lista usando l'operatore <code>[]</code> combinato all'operazione di slicing:</p> <pre><code>&gt;&gt;&gt; lista[4:] = []\n&gt;&gt;&gt; lista\n[99, 2, 3, 4]\n</code></pre> <p>Nota</p> <p>I pi\u00f9 attenti avranno notato che l'operatore <code>[]</code> non fa altro che indicare una lista vuota.</p> <p>Suggerimento</p> <p>Possiamo eliminare tutti gli elementi contenuti in una lista mediante lo slicing e l'operatore <code>[]</code>:</p> <pre><code>&gt;&gt;&gt; lista[:] = []\n&gt;&gt;&gt; lista\n[]\n</code></pre> <p>Una lista pu\u00f2 contenere elementi tra loro eterogenei. E' addirittura consentito contenere degli iterabili, tra cui altre liste:</p> <pre><code>&gt;&gt;&gt; lista.append([1,2,3])\n&gt;&gt;&gt; lista\n[99, 2, 3, 4, [1, 2, 3]]\n</code></pre> <p>Nell'esempio precedente, abbiamo usato la funzione <code>append()</code> per inserire un elemento in coda alla lista. E' interessante notare l'elemento inserito in coda sia esso stesso una lista, e \"conviva\" tranquillamente con gli altri elementi di tipo numerico.</p> <p>Proviamo ad estendere ulteriormente la lista cambiando il primo elemento con una stringa:</p> <pre><code>&gt;&gt;&gt; lista [0] = stringa\n&gt;&gt;&gt; lista\n['Python', 2, 3, 4, [1, 2, 3]]\n</code></pre>"},{"location":"material/01_python/01_intro/exercises.en/","title":"Exercise 1.1","text":"<p>Prompt: Let's create a string that has the value PCS using the integrated interpreter.</p> <p>Solution</p> <p>Open the Python interpreter by typing <code>python</code> from the command line. Then, enter the following statement and press <code>Enter</code>:</p> <pre><code>&gt;&gt;&gt; s = 'PCS'\nPCS\n</code></pre>"},{"location":"material/01_python/01_intro/exercises.en/#exercise-12","title":"Exercise 1.2","text":"<p>Prompt: Evaluate the length of the string created in the previous exercise and verify that it is equal to 3.</p> <p>Solution</p> <p>Firstly, we can use the <code>len()</code> function, which, as we have seen in the lesson, accepts a sequence (i.e., an iterable object) and returns an integer representing the length of the iterable.</p> <p>Since the string is a sequence, we can invoke the <code>len()</code> function and pass <code>s</code> as an argument:</p> <pre><code>&gt;&gt;&gt; len(s)\n3\n</code></pre> <p>By calling the <code>len(s)</code> function, we notice that the returned value is the expected one, which is <code>3</code>.</p> <p>We can also assign the returned value to a variable <code>l</code>:</p> <pre><code>&gt;&gt;&gt; l = len(s)\n</code></pre> <p>At this point, we can verify that <code>l</code> is equal to <code>3</code> using the equality operator:</p> <pre><code>&gt;&gt;&gt; l == 3\nTrue\n</code></pre>"},{"location":"material/01_python/01_intro/exercises.en/#exercise-13","title":"Exercise 1.3","text":"<p>Prompt: Verify that the number <code>x</code> is between <code>0</code> and <code>10</code>.</p> <p>Solution</p> <p>First, let's assign an arbitrary value to <code>x</code>:</p> <pre><code>&gt;&gt;&gt; x = 1\n</code></pre> <p>Now, we can check if <code>x</code> is between <code>0</code> and <code>10</code> using the <code>and</code> boolean operator:</p> <pre><code>&gt;&gt;&gt; x &lt; 10 and x &gt; 0\nTrue\n</code></pre>"},{"location":"material/01_python/01_intro/exercises.en/#exercise-14","title":"Exercise 1.4","text":"<p>Prompt: Create a list from the string defined in the previous exercises.</p> <p>Solution</p> <p>One possibility is to use the class constructor <code>list()</code>, which accepts a sequence and returns a list based on it. We can write:</p> <pre><code>&gt;&gt;&gt; l_1 = list(s)\n</code></pre> <p>If we try to display <code>l_1</code>, we will get the following result:</p> <pre><code>&gt;&gt;&gt; l_1\n['p', 'c', 's']\n</code></pre> <p>Another way is to use the <code>[]</code> operator, which will create a list with a single element, namely the string <code>s</code>. In practice:</p> <pre><code>&gt;&gt;&gt; l_2 = [s]\n&gt;&gt;&gt; l_2\n['pcs']\n</code></pre>"},{"location":"material/01_python/01_intro/exercises/","title":"Esercitazione 1.1","text":""},{"location":"material/01_python/01_intro/exercises/#esercizio-11","title":"Esercizio 1.1","text":"<p>Traccia: Creiamo una stringa che assuma valore PCS usando l'interprete integrato.</p> <p>Soluzione</p> <p>Apriamo l'interprete Python digitando <code>python</code> da riga di comando. A quel punto, inseriamo la seguente istruzione, e premiamo <code>Invio</code>:</p> <pre><code>&gt;&gt;&gt; s = 'PCS'\nPCS\n</code></pre>"},{"location":"material/01_python/01_intro/exercises/#esercizio-12","title":"Esercizio 1.2","text":"<p>Traccia: Valutiamo la lunghezza della stringa creata nell'esercizio precedente, e verifichiamo che sia uguale a 3.</p> <p>Soluzione</p> <p>Innanzitutto, possiamo usare la funzione <code>len()</code> che, come abbiamo visto nella lezione, accetta una sequenza (ovvero un oggetto su cui si possa iterare), e restituisce un intero rappresentativo della lunghezza dell'iterabile.</p> <p>Dato che la stringa \u00e8 una sequenza, possiamo invocare la funzione <code>len()</code> passandogli come argomento <code>s</code>:</p> <pre><code>&gt;&gt;&gt; len(s)\n3\n</code></pre> <p>Chiamando la funzione <code>len(s)</code> notiamo come il valore restituito sia quello atteso, ovvero <code>3</code>.</p> <p>Possiamo anche provare ad assegnare il valore restituito ad una variabile <code>l</code>:</p> <pre><code>&gt;&gt;&gt; l = len(s)\n</code></pre> <p>A questo punto, possiamo verificare che <code>l</code> sia pari a <code>3</code> utilizzando l'operatore di uguaglianza:</p> <pre><code>&gt;&gt;&gt; l == 3\nTrue\n</code></pre>"},{"location":"material/01_python/01_intro/exercises/#esercizio-13","title":"Esercizio 1.3","text":"<p>Traccia: Verifichiamo che il numero <code>x</code> sia compreso tra <code>0</code> e <code>10</code>.</p> <p>Soluzione</p> <p>Per prima cosa, dichiariamo un valore qualsiasi per <code>x</code>:</p> <pre><code>&gt;&gt;&gt; x = 1\n</code></pre> <p>A questo punto, verifichiamo che <code>x</code> sia compreso tra <code>0</code> e <code>10</code> mediante l'operatore booleano <code>and</code>:</p> <pre><code>&gt;&gt;&gt; x &lt; 10 and x &gt; 0\nTrue\n</code></pre>"},{"location":"material/01_python/01_intro/exercises/#esercizio-14","title":"Esercizio 1.4","text":"<p>Traccia: Creare una lista a partire dalla stringa definita negli esercizi precedenti.</p> <p>Soluzione</p> <p>Una prima possibilit\u00e0 \u00e8 quella di utilizzare il costruttore di classe <code>list()</code> che accetta una sequenza e restituisce una lista a partire da questa. Possiamo quindi scrivere:</p> <pre><code>&gt;&gt;&gt; l_1 = list(s)\n</code></pre> <p>Se proviamo a visualizzare <code>l_1</code>, avremo il seguente risultato:</p> <pre><code>&gt;&gt;&gt; l_1\n['p', 'c', 's']\n</code></pre> <p>Un altro modo \u00e8 quello di usare l'operatore <code>[]</code>, che per\u00f2 avr\u00e0 risultati leggermente differenti, in quanto creer\u00e0 una lista con all'interno un unico elemento, ovvero la stringa <code>s</code>. In pratica:</p> <pre><code>&gt;&gt;&gt; l_2 = [s]\n&gt;&gt;&gt; l_2\n['pcs']\n</code></pre>"},{"location":"material/01_python/02_syntax/01_syntax.en/","title":"1.2.1 - Basic Syntax","text":"<p>Let's explore some of the fundamental concepts that characterize the syntax of the Python language.</p>"},{"location":"material/01_python/02_syntax/01_syntax.en/#comments","title":"Comments","text":"<p>Python supports two types of comments. The first, and simplest, is the single-line comment, which is preceded by the <code>#</code> (hash) character:</p> <pre><code>&gt;&gt;&gt; # This is a comment!\n</code></pre> <p>The other type of comment is a multiline comment, and it is defined in the same way as a multiline string:</p> <pre><code>\"\"\" This is an example\nof a multiline comment!\n\"\"\"\n</code></pre> <p>Single and Double Quotes</p> <p>Similar to strings, you can use either single or double quotes interchangeably (but consistently!) for comments.</p>"},{"location":"material/01_python/02_syntax/01_syntax.en/#use-of-parentheses","title":"Use of Parentheses","text":"<p>The use of parentheses in Python differs from what is done in many C-like languages. In particular, let's see how round parentheses, square brackets, and curly braces are used (and what they are used for).</p>"},{"location":"material/01_python/02_syntax/01_syntax.en/#round-parentheses","title":"Round Parentheses","text":"<p>Round parentheses are used in three cases:</p> <ul> <li>for function calls;</li> <li>for creating a tuple;</li> <li>to express precedence in a mathematical operation.</li> </ul> <p>In other cases, they are optional and can be omitted.</p> <p>Let's see a brief example of expressing precedence in a mathematical operation:</p> <pre><code>&gt;&gt;&gt; a = 2\n&gt;&gt;&gt; b = 3\n&gt;&gt;&gt; c = 4\n&gt;&gt;&gt; r_1 = a + b * c\n14\n&gt;&gt;&gt; r_2 = (a + b) * c\n20\n</code></pre> <p>In the upcoming lessons, we will see how to use round parentheses in tuples and function calls.</p>"},{"location":"material/01_python/02_syntax/01_syntax.en/#square-brackets","title":"Square Brackets","text":"<p>Square brackets are used for creating and accessing elements of a data structure.</p> <pre><code># Create a list\nmy_list = [1, 2, 3]\n# Access the second element of the list\nmy_list[1]          # The accessed value is 2\n</code></pre> <p>We will discuss this more extensively in the next lesson.</p>"},{"location":"material/01_python/02_syntax/01_syntax.en/#curly-braces","title":"Curly Braces","text":"<p>Curly braces are used to create sets and dictionaries.</p> <pre><code>my_dict = {'a': 1, 'b': 2}\nmy_set = {1, 2, 3}\n</code></pre> <p>Again, we will discuss this more extensively in the next lesson.</p>"},{"location":"material/01_python/02_syntax/01_syntax.en/#end-of-an-instruction","title":"End of an Instruction","text":"<p>Unlike C-like languages that require a <code>;</code> (semicolon) to terminate a single instruction, Python considers a newline <code>\\n</code> as the termination character of an instruction. In other words, the interpreter associates the end of an instruction with the newline character.</p>"},{"location":"material/01_python/02_syntax/01_syntax.en/#code-blocks","title":"Code Blocks","text":"<p>To delimit a code block in Python, the colon <code>:</code> character and indentation are used. In particular, all the code that follows a colon <code>:</code> and is indented at the same level belongs to the same block. For example:</p> <pre><code>&gt;&gt;&gt; a = 1               # Code not in a block\n&gt;&gt;&gt; if a &lt; 10:          # Start of the outer if block\n&gt;&gt;&gt;     b = 5\n&gt;&gt;&gt;     print('outer block')\n&gt;&gt;&gt;     if b &gt; 1:       # Start of the nested if block\n&gt;&gt;&gt;         print('inner block')\n</code></pre> <p>Indentation and Curly Braces</p> <p>Keen observers may have noticed that indentations play a role similar to curly braces in C-like languages.</p>"},{"location":"material/01_python/02_syntax/01_syntax/","title":"1.2.1 - Sintassi fondamentale","text":"<p>Vediamo alcuni dei concetti fondamentali che caratterizzano la sintassi del linguaggio Python.</p>"},{"location":"material/01_python/02_syntax/01_syntax/#commenti","title":"Commenti","text":"<p>Python prevede due tipi di commento. Il primo, e pi\u00f9 semplice, \u00e8 quello a singola riga, anteceduto da un carattere <code>#</code> (asterisco):</p> <pre><code>&gt;&gt;&gt; # Questo \u00e8 un commento!\n</code></pre> <p>L'altro tipo di commento \u00e8 un commento multilinea, ed \u00e8 definito esattamente allo stesso modo in cui si definisce una stringa multilinea:</p> <pre><code>\"\"\" Questo \u00e8 un esempio\ndi commento multilinea!\n\"\"\"\n</code></pre> <p>Singole e doppie apici</p> <p>Cos\u00ec come per le stringhe, \u00e8 possibile usare indifferentemente (ma coerentemente!) singole o doppie apici.</p>"},{"location":"material/01_python/02_syntax/01_syntax/#uso-delle-parentesi","title":"Uso delle parentesi","text":"<p>L'utilizzo delle parentesi in Python differisce da quello che ne si fa in molti linguaggi C-like. In particolare, vediamo come sono utilizzate (ed a cosa servono) le parentesi tonde, quadre e graffe.</p>"},{"location":"material/01_python/02_syntax/01_syntax/#parentesi-tonde","title":"Parentesi tonde","text":"<p>Le parentesi tonde sono usate in tre casi:</p> <ul> <li>per le chiamate a funzione;</li> <li>per la creazione di una tupla;</li> <li>per esprimere la precedenza in un'operazione matematica.</li> </ul> <p>Negli altri casi, sono opzionali, e possono essere tranquillamente omesse.</p> <p>Vediamo un breve esempio di come esprimere la precedenza in un'operazione matematica:</p> <pre><code>&gt;&gt;&gt; a = 2\n&gt;&gt;&gt; b = 3\n&gt;&gt;&gt; c = 4\n&gt;&gt;&gt; r_1 = a + b * c\n14\n&gt;&gt;&gt; r_2 = (a + b) * c\n20\n</code></pre> <p>Nelle prossime lezioni vedremo come utilizzare le parentesi tonde nelle tuple e nelle chiamate a funzione.</p>"},{"location":"material/01_python/02_syntax/01_syntax/#parentesi-quadre","title":"Parentesi quadre","text":"<p>Le parentesi quadre sono usate per la creazione e l'accesso agli elementi di una struttura dati.</p> <pre><code># Creo una lista\nlista = [1, 2, 3]\n# Accedo al secondo elemento della lista\nlista[1]            # Il valore acceduto \u00e8 2\n</code></pre> <p>Ne parleremo pi\u00f9 diffusamente nella prossima lezione.</p>"},{"location":"material/01_python/02_syntax/01_syntax/#parentesi-graffe","title":"Parentesi graffe","text":"<p>Le parentesi graffe sono usate per creare set e dizionari.</p> <pre><code>dizionario = {'a': 1, 'b': 2}\ninsieme = {1, 2, 3}\n</code></pre> <p>Anche in questo caso, ne parleremo pi\u00f9 diffusamente nella prossima lezione.</p>"},{"location":"material/01_python/02_syntax/01_syntax/#termine-di-unistruzione","title":"Termine di un'istruzione","text":"<p>A differenza dei linguaggi C-like, che prevedono che la singola istruzione termini con un <code>;</code> (un punto e virgola), Python fa in modo che il carattere di terminazione di un'istruzione sia dato dal newline <code>\\n</code>. In altri termini, l'interprete associer\u00e0 il termine dell'istruzione all'andata a capo.</p>"},{"location":"material/01_python/02_syntax/01_syntax/#blocchi-di-codice","title":"Blocchi di codice","text":"<p>Per delimitare un blocco di codice Python ricorre al carattere <code>:</code> (due punti) ed all'indentazione. In particolare, tutto il codice che segue un carattere <code>:</code> ed \u00e8 ad uno stesso livello di indentazione risulta far parte di uno stesso blocco. Ad esempio:</p> <pre><code>&gt;&gt;&gt; a = 1               # Codice non in un blocco\n&gt;&gt;&gt; if a &lt; 10:          # Inizia il blocco if esterno\n&gt;&gt;&gt;     b = 5\n&gt;&gt;&gt;     print('blocco esterno')\n&gt;&gt;&gt;     if b &gt; 1:       # Inizia il blocco if annidato\n&gt;&gt;&gt;         print('blocco interno')\n</code></pre> <p>Indentazioni e parentesi graffe</p> <p>I pi\u00f9 attenti avranno notato che le indentazioni svolgono, in buona sostanza, un ruolo analogo a quello delle parentesi graffe nei linguaggi C-like.</p>"},{"location":"material/01_python/02_syntax/02_structured.en/","title":"1.2.2 - Structured Programming","text":"<p>Like all C-derived languages, Python also supports the main constructs of structured programming. Let's briefly go over them.</p>"},{"location":"material/01_python/02_syntax/02_structured.en/#conditional-statement","title":"Conditional Statement","text":"<p>Let's start with the conditional statement, which, like in any programming language, is expressed using the <code>if</code> and <code>else</code> keywords.</p> <p>The basic syntax is as follows:</p> <pre><code>if condition:\n    statements\nelse:\n    other_statements\n</code></pre> <p>In practice, if the <code>condition</code> is true, the <code>statements</code> will be executed. Alternatively, if the <code>condition</code> is false, the <code>other_statements</code> will be executed. For example:</p> <pre><code>if a &lt; 5:\n    print('a is less than 5')\nelse:\n    print('a is greater than or equal to 5')\n</code></pre> <p>To use the <code>else if</code> notation, we need to use the <code>elif</code> keyword. For example:</p> <pre><code>if a &lt; 5:\n    print('a is less than 5')\nelif a == 5:\n    print('a is equal to 5')\nelse:\n    print('a is greater than 5')\n</code></pre> <p>Switch/Case in Python</p> <p>Until version 3.10, Python did not provide an implementation for the <code>switch/case</code> construct. However, starting from version 3.10, the so-called pattern matching has been implemented using the <code>match/case</code> syntax:</p> <p><pre><code>match command:\n    case \"case 1\":\n        statements()\n    case \"other case\":\n        print(\"Unknown command\")\n</code></pre> We will explore this construct in the appropriate lesson.</p>"},{"location":"material/01_python/02_syntax/02_structured.en/#loops","title":"Loops","text":""},{"location":"material/01_python/02_syntax/02_structured.en/#for-loop","title":"<code>for</code> Loop","text":"<p>The <code>for</code> loop iterates over a sequence, such as a list or a string, and has the following syntax:</p> <pre><code>for element in sequence:\n    statements()\n</code></pre> <p>For example, in the following code block, we iterate over a sequence and print the numbers from 0 to 5 (excluding 5) iteratively:</p> <pre><code>vals = [0, 1, 2, 3, 4]\nfor i in vals:\n    print(i)\n</code></pre> <p>The output will be:</p> <pre><code>0\n1\n2\n3\n4\n</code></pre> <p>Compared to \"classic\" languages, the <code>for</code> loop in Python only operates on iterables, so in some cases, it may require a code redesign. However, this feature of Python also results in simpler code. For example, iterating over a string is straightforward:</p> <pre><code>string = \"Python\"\nfor char in string:\n    print(char)\n</code></pre> <p>In both cases, the output will be:</p> <pre><code>P\ny\nt\nh\no\nn\n</code></pre> <p>No free lunches!</p> <p>As the no free lunches theorem reminds us, there are no free lunches! In fact, the syntactic simplicity offered by Python comes at a cost. A Python script, no matter how optimized, will almost never offer performance comparable to optimized C or C++ code, unless you use specific (and advanced) techniques.</p>"},{"location":"material/01_python/02_syntax/02_structured.en/#while-loop","title":"<code>while</code> Loop","text":"<p>Unlike the <code>for</code> loop, the <code>while</code> loop operates similarly to its counterparts in other programming languages. The generic syntax is:</p> <pre><code>while condition:\n    statements()\n</code></pre> <p>For example:</p> <pre><code>import random\ni = True\nwhile i:\n    if random.randint(-5, 5) &gt; 0:\n        print(\"Continuing!\")\n    else:\n        print(\"Exiting!\")\n        i = False\n</code></pre> <p>The code in the above block generates a random integer value in the range $</p> <p>[-5, 5]$ using the <code>randint</code> function. If the generated value is greater than 0, the loop continues; otherwise, it exits.</p> <p>The output could be, for example:</p> <pre><code>Continuing!\nContinuing!\nExiting!\n</code></pre> <p>Boolean Values in Python</p> <p>Keen observers may notice that boolean values in Python are written as <code>True</code> and <code>False</code>. This is not a typo: the first letter is capitalized.</p>"},{"location":"material/01_python/02_syntax/02_structured.en/#the-range-function","title":"The <code>range()</code> Function","text":"<p>Let's revisit the <code>for</code> loop we saw earlier:</p> <pre><code>vals = [0, 1, 2, 3, 4]\nfor i in vals:\n    print(i)\n</code></pre> <p>Although the code is already concise, manually writing the sequence to iterate over can become quite complex. Python comes to our rescue with the <code>range(i, j, s)</code> function, which generates a sequence with all the numbers between <code>i</code> (inclusive) and <code>j</code> (exclusive) with a step of <code>s</code>. For example, to generate numbers from 0 to 4, we can write:</p> <pre><code>&gt;&gt;&gt; r = range(0, 5, 1)\n&gt;&gt;&gt; print(list(r))\n[0, 1, 2, 3, 4]\n</code></pre> <p>Note</p> <p>Notice that we need to convert <code>r</code> to a list (<code>list(r)</code>) to print its values.</p> <p>If omitted, <code>i</code> and <code>s</code> assume default values of 0 and 1, respectively:</p> <pre><code>&gt;&gt;&gt; r = range(5)\n&gt;&gt;&gt; print(list(r))\n[0, 1, 2, 3, 4]\n</code></pre> <p>You can also specify a decremental sequence by setting <code>i &gt; j</code> and <code>s &lt; 0</code>:</p> <pre><code>&gt;&gt;&gt; r = range(5, 1, -1)\n&gt;&gt;&gt; print(list(r))\n[5, 4, 3, 2]\n</code></pre>"},{"location":"material/01_python/02_syntax/02_structured.en/#break-and-continue-statements","title":"<code>break</code> and <code>continue</code> Statements","text":"<p>The <code>break</code> and <code>continue</code> statements allow you to exit the loop or skip to the next iteration, respectively. For example:</p> <pre><code>while True:\n    if randint(-5, 5) &gt; 0:\n        print(\"Continuing!\")\n        continue\n    else:\n        print(\"Exiting!\")\n        break\nprint(\"Exited!\")\n</code></pre> <p>The preceding statements will exit the loop when a negative number is randomly generated and continue iterating when a positive number is randomly generated.</p>"},{"location":"material/01_python/02_syntax/02_structured/","title":"1.2.2 - Programmazione strutturata","text":"<p>Come tutti i linguaggi derivati da C, anche Python offre il supporto ai principali costrutti della programmazione strutturata. Vediamoli brevemente.</p>"},{"location":"material/01_python/02_syntax/02_structured/#istruzione-condizionale","title":"Istruzione condizionale","text":"<p>Partiamo dall'istruzione condizionale che, come in tutti i linguaggi di programmazione, \u00e8 espressa dalle parole chiave <code>if</code> ed <code>else</code>.</p> <p>La sintassi base \u00e8 di questo tipo:</p> <pre><code>if condizione_verificata:\n    istruzioni\nelse:\n    altre_istruzioni\n</code></pre> <p>In pratica, se <code>condizione_verificata</code> \u00e8 vero, allora saranno eseguite le <code>istruzioni</code>. In alternativa, se <code>condizione_verificata</code> \u00e8 falso, allora saranno eseguite le <code>altre_istruzioni</code>. Ad esempio:</p> <pre><code>&gt;&gt;&gt; if a &lt; 5:\n&gt;&gt;&gt;     print('a \u00e8 minore di 5')\n&gt;&gt;&gt; else:\n&gt;&gt;&gt;     print('a \u00e8 maggiore di 5')\n</code></pre> <p>Per utilizzare la notazione <code>else if</code> dovremo ricorrere alla parola chiave <code>elif</code>. Ad esempio:</p> <pre><code>&gt;&gt;&gt; if a &lt; 5:\n&gt;&gt;&gt;     print('a \u00e8 minore di 5')\n&gt;&gt;&gt; elif a == 5:\n&gt;&gt;&gt;     print('a \u00e8 uguale a 5')\n&gt;&gt;&gt; else:\n&gt;&gt;&gt;     print('a \u00e8 maggiore di 5')\n</code></pre> <p>Switch/case in Python</p> <p>Fino alla versione 3.10, Python non offriva un'implementazione per il costrutto <code>switch/case</code>. A partire da quest'ultima, per\u00f2, \u00e8 stato implementato il cosiddetto pattern matching mediante la sintassi <code>match/case</code>:</p> <p><pre><code>match command:\n    case \"caso 1\":\n        istruzioni()\n    case \"altro caso\":\n        print(\"Comando sconosciuto\")\n</code></pre> Approfondiremo questo costrutto nell'apposita lezione.</p>"},{"location":"material/01_python/02_syntax/02_structured/#cicli","title":"Cicli","text":""},{"location":"material/01_python/02_syntax/02_structured/#ciclo-for","title":"Ciclo <code>for</code>","text":"<p>Il ciclo <code>for</code> itera su una sequenza, come una lista o una stringa, ed ha una sintassi del tipo:</p> <pre><code>for elemento in sequenza:\n    istruzioni()\n</code></pre> <p>Per fare un esempio, nel seguente blocco di codice vediamo come mostrare a schermo in maniera iterativa i numeri che vanno da 0 a 5 (escluso):</p> <pre><code>vals = [0,1,2,3,4]\nfor i in vals:\n    print(i)\n</code></pre> <p>Il risultato che sar\u00e0 stampato a schermo \u00e8:</p> <pre><code>0\n1\n2\n3\n4\n</code></pre> <p>Rispetto ai linguaggi \"classici\", quindi, il ciclo <code>for</code> opera esclusivamente sugli iterabili, per cui potrebbe in qualche caso occorrere una riprogettazione del codice. Tuttavia, questa caratteristica di Python comporta anche una maggiore semplicit\u00e0 del codice; ad esempio, vediamo come \u00e8 molto semplice iterare su una stringa:</p> <pre><code>string = \"Python\"\nfor char in string:\n    print(char)\n</code></pre> <p>A schermo vedremo in entrambi i casi il seguente risultato:</p> <pre><code>P\ny\nt\nh\no\nn\n</code></pre> <p>No free lunches!</p> <p>Come ci ricorda il no free lunches theorem, non esistono pasti gratuiti! Infatti, la maggiore semplicit\u00e0 sintattica offerta da Python non \u00e8 indolore, ma ha un costo. Uno script Python, infatti, per quanto ottimizzato, non potr\u00e0 quasi mai offrire performance paragonabili ad un codice ottimizzato in C o C++, a meno di non usare particolari (ed avanzati) accorgimenti.</p>"},{"location":"material/01_python/02_syntax/02_structured/#ciclo-while","title":"Ciclo <code>while</code>","text":"<p>A differenza del ciclo <code>for</code>, il funzionamento del <code>while</code> \u00e8 analogo a quello delle controparti negli altri linguaggi di programmazione. La sintassi generica \u00e8:</p> <pre><code>while(condizione):\n    istruzioni()\n</code></pre> <p>Ad esempio:</p> <pre><code>import random\ni = True\nwhile (i):\n    if random.randint(-5, 5) &gt; 0:\n        print(\"Continuo!\")\n    else:\n        print(\"Esco!\")\n        i = False\n</code></pre> <p>Il codice nel blocco precedente non fa altro che generare un valore numerico intero casuale nell'intervallo \\([-5, 5]\\) mediante la funzione <code>randint</code>. Se tale valore \u00e8 superiore a \\(0\\), il ciclo continua, altrimenti si esce dallo stesso.</p> <p>A schermo vedremo, ad esempio:</p> <pre><code>Continuo!\nContinuo!\nEsco!\n</code></pre> <p>I valori booleani in Python</p> <p>I pi\u00f9 attenti avranno notato come i valori booleani in Python siano stati scritti come <code>True</code> e <code>False</code>. Questo non \u00e8 un refuso: la prima lettera \u00e8 proprio una maiuscola.</p>"},{"location":"material/01_python/02_syntax/02_structured/#la-funzione-range","title":"La funzione <code>range()</code>","text":"<p>Riprendiamo adesso il ciclo <code>for</code> visto in precedenza.</p> <pre><code>vals = [0, 1, 2, 3, 4]\nfor i in vals:\n    print(i)\n</code></pre> <p>Nonostante il codice sia gi\u00e0 compatto, scrivere manualmente la sequenza da iterare pu\u00f2 facilmente diventare un'operazione abbastanza complessa. Python ci viene quindi in aiuto tramite la funzione <code>range(i, j, s)</code>, che genera una sequenza avente tutti i numeri compresi tra <code>i</code> (incluso) e <code>j</code> (escluso) a passo <code>s</code>. Ad esempio, per generare i numeri compresi tra 0 e 4 scriveremo:</p> <pre><code>&gt;&gt;&gt; r = range(0, 5, 1)\n&gt;&gt;&gt; print(list(r))\n[0, 1, 2, 3, 4]\n</code></pre> <p>Nota</p> <p>Notiamo che per mandare in output i valori di <code>r</code> dovremo convertirlo in lista (<code>list(r)</code>).</p> <p>Qualora omessi, <code>i</code> ed <code>s</code> assumono valori di default rispettivamente 0 ed 1:</p> <pre><code>&gt;&gt;&gt; r = range(5)\n&gt;&gt;&gt; print(list(r))\n[0, 1, 2, 3, 4]\n</code></pre> <p>E' anche possibile specificare una sequenza decrementale ponendo <code>i &gt; j</code> ed <code>s &lt; 0</code>:</p> <pre><code>&gt;&gt;&gt; r = range(5, 1, -1)\n&gt;&gt;&gt; print(list(r))\n[5, 4, 3, 2]\n</code></pre>"},{"location":"material/01_python/02_syntax/02_structured/#istruzioni-break-e-continue","title":"Istruzioni <code>break</code> e <code>continue</code>","text":"<p>Le istruzioni <code>break</code> e <code>continue</code> permettono rispettivamente di uscire dal ciclo o di saltare all'iterazione successiva. Ad esempio:</p> <pre><code>while (True):\n    if randint(-5, 5) &gt; 0:\n        print(\"Continuo!\")\n        continue\n    else:\n        print(\"Esco!\")\n        break\nprint(\"Sono uscito!\")\n</code></pre> <p>Le istruzioni precedenti usciranno dal ciclo quando viene generato casualmente un numero negativo, mentre continueranno ad iterare quando viene generato casualmente un numero positivo.</p>"},{"location":"material/01_python/02_syntax/03_functions.en/","title":"1.2.3 - Functions","text":""},{"location":"material/01_python/02_syntax/03_functions.en/#function-definition","title":"Function Definition","text":"<p>Python allows you to define a function using the <code>def</code> keyword. The general syntax is as follows:</p> <pre><code>def function_name(parameters):\n    # statements\n    return return_value\n</code></pre> <p>It's important to note that:</p> <ul> <li>It is not necessary to define a type, only a return value. If the function doesn't return any value, the <code>return</code> statement can be omitted.</li> <li>It is not strictly necessary to define the type of each parameter passed.</li> <li>Optional parameters with default values can be included.</li> </ul> <p>For example, the following function adds <code>1</code> to the input value and returns the new value:</p> <pre><code>def add_one(i):\n    val = i + 1\n    return val\n</code></pre> <p>Input Parameter Types</p> <p>Duck typing means that no checks are performed on the input parameters. However, this doesn't mean that you can't try calling the <code>add_one()</code> function with a string parameter, for example; this will result in a (predictable) error.</p>"},{"location":"material/01_python/02_syntax/03_functions.en/#passing-parameters-to-functions","title":"Passing Parameters to Functions","text":"<p>Python expects parameters to be passed to a function using a hybrid mode called call-by-assignment. In practice, the pass is done exclusively by value, but with different effects on mutable and immutable types.</p> <p>For example, when passing a primitive value (such as an integer), Python behaves as if it were performing a pass by value:</p> <pre><code>def double(integer):\n    integer = integer * 2\n    print(f'Value inside the function: {integer}')\n</code></pre> <p>The result will be:</p> <pre><code>&gt;&gt;&gt; integer = 1\n&gt;&gt;&gt; double(integer)\n\"Value inside the function: 2\"\n&gt;&gt;&gt; print(f'Value outside the function: {integer}')\n\"Value outside the function: 1\"\n</code></pre> <p>This is because the pass is done by value, so the <code>double()</code> function acts on a copy of the variable passed as an argument, not the original variable. If we use a function that modifies a list instead:</p> <pre><code>def add_to_list(lst, element):\n    lst.append(element)\n    print(f'Value inside the function: {lst}')\n</code></pre> <p>The result will be:</p> <pre><code>&gt;&gt;&gt; lst = [1, 2]\n&gt;&gt;&gt; add_to_list(lst, 3)\n\"Value inside the function: [1, 2, 3]\"\n&gt;&gt;&gt; print(f'Value outside the function: {lst}')\n\"Value outside the function: [1, 2, 3]\"\n</code></pre> <p>In this case, since the list is mutable, the pass is done by reference: this means that operations performed inside the <code>add_to_list()</code> function will affect the original list.</p> <p>Shallow and Deep Copy</p> <p>By default, Python copies variables using a shallow copy: this means that an assignment operation like <code>a = b</code> makes <code>a</code> point to the same memory address as <code>b</code>, and as a result, any modifications to <code>b</code> will be reflected in <code>a</code>. To avoid this, you can use a deep copy using the <code>deepcopy()</code> function from the <code>copy</code> library.</p>"},{"location":"material/01_python/02_syntax/03_functions.en/#the-pass-statement","title":"The <code>pass</code> Statement","text":"<p>Finally, let's briefly discuss the <code>pass</code> statement. This statement does nothing; it is useful, for example, when you want to define an empty function (or class) that you plan to implement later:</p> <pre><code>&gt;&gt;&gt; def empty_function():\n...     pass\n...\n&gt;&gt;&gt; empty_function()\n</code></pre> <p>Note</p> <p>Although it may not be apparent at first, there are several situations where the <code>pass</code> statement proves to be extremely useful.</p>"},{"location":"material/01_python/02_syntax/03_functions/","title":"1.2.3 - Funzioni","text":""},{"location":"material/01_python/02_syntax/03_functions/#definizione-di-funzione","title":"Definizione di funzione","text":"<p>Python permette di definire una funzione utilizzando la parola chiave <code>def</code>. La sintassi generica \u00e8 la seguente:</p> <pre><code>def nome_funzione(parametri):\n    # istruzioni\n    return valore_ritorno\n</code></pre> <p>E' importante notare che:</p> <ul> <li>non \u00e8 necessario definire un tipo, ma soltanto un valore di ritorno. Qualora la funzione non restituisca alcun valore, potr\u00e0 essere omessa l'istruzione <code>return</code>;</li> <li>non \u00e8 (strettamente) necessario definire il tipo di ciascuno dei parametri passati;</li> <li>\u00e8 consentito inserire dei parametri opzionali, con valori di default.</li> </ul> <p>Ad esempio, la seguente funzione somma <code>1</code> al valore in ingresso e restituisce il nuovo valore:</p> <pre><code>def aggiungi_uno(i):\n    val = i + 1\n    return val\n</code></pre> <p>Tipo dei parametri di ingresso</p> <p>Il duck typing fa s\u00ec che non venga effettuato alcun controllo sui parametri in ingresso. Ci\u00f2 per\u00f2 non significa che non si possa provare a chiamare (ad esempio) la funzione <code>aggiungi_uno()</code> passando come parametro una stringa; ci\u00f2 tuttavia causer\u00e0 un (prevedibile) errore.</p>"},{"location":"material/01_python/02_syntax/03_functions/#passaggio-di-parametri-a-funzione","title":"Passaggio di parametri a funzione","text":"<p>Python prevede che i parametri siano passati ad una funzione secondo una modalit\u00e0 ibrida chiamata call-by-assignment. In pratica, il passaggio avviene esclusivamente per valore, ma con effetti differenti su tipi mutabili ed immutabili.</p> <p>Ad esempio, provando a passare un valore primitivo (come un intero), Python si comporter\u00e0 come se si stesse effettuando un passaggio per valore:</p> <pre><code>def raddoppia(intero):\n    intero = intero * 2\n    print(f'Valore all\\'interno della funzione: {intero}')\n</code></pre> <p>Il risultato sar\u00e0:</p> <pre><code>&gt;&gt;&gt; intero = 1\n&gt;&gt;&gt; raddoppia(intero)\n\"Valore all'interno della funzione: 2\"\n&gt;&gt;&gt; print(f'Valore all\\'esterno della funzione: {intero}')\n\"Valore all'interno della funzione: 1\"\n</code></pre> <p>Ci\u00f2 \u00e8 legato al fatto che il passaggio viene effettuato per valore, per cui la funzione <code>raddoppia()</code> agir\u00e0 su una copia della variabile passata come argomento, e non sulla variabile originaria. Se invece usassimo una funzione che modifica una lista:</p> <pre><code>def aggiungi_a_lista(lista, elemento):\n    lista.append(elemento)\n    print(f'Valore all\\'interno della funzione: {lista}')\n</code></pre> <p>Il risultato sar\u00e0:</p> <pre><code>&gt;&gt;&gt; lista = [1, 2]\n&gt;&gt;&gt; aggiungi_a_lista(lista, 3)\n\"Valore all'interno della funzione: [1, 2, 3]\"\n&gt;&gt;&gt; print(f'Valore all\\'esterno della funzione: {lista}')\n\"Valore all'interno della funzione: [1, 2, 3]\"\n</code></pre> <p>In questo caso, essendo la lista mutabile, il passaggio viene effettuato nei fatti per reference: ci\u00f2 significa che le operazioni compiute all'interno della funzione <code>aggiungi_a_lista()</code> agiranno sulla lista originaria.</p> <p>Shallow e deep copy</p> <p>Di default, Python copia le variabili per mezzo di una shallow copy: ci\u00f2 significa che un'operazione di assignment del tipo <code>a = b</code> fa in modo che <code>a</code> punti allo stesso indirizzo di memoria di <code>b</code> e, di conseguenza, ogni modifica a <code>b</code> si rifletta su <code>a</code>. Per evitare un fenomeno di questo tipo occorre usare una deep copy grazie alla funzione <code>deepcopy()</code> della libreria <code>copy</code>.</p>"},{"location":"material/01_python/02_syntax/03_functions/#listruzione-pass","title":"L'istruzione <code>pass</code>","text":"<p>Chiudiamo accennando all'istruzione <code>pass</code>. Questa non fa assolutamente nulla; \u00e8 utile, ad esempio, quando vogliamo inserire una funzione (o una classe) vuota, che definiremo per qualche motivo in seguito:</p> <pre><code>&gt;&gt;&gt; def funzione_vuota():\n...     pass\n...\n&gt;&gt;&gt; funzione_vuota()\n</code></pre> <p>Nota</p> <p>Anche se di primo acchitto potrebbe non essere evidente, esistono diverse situazioni in cui l'istruzione <code>pass</code> risulta essere estremamente utile.</p>"},{"location":"material/01_python/02_syntax/04_data_structures.en/","title":"1.2.4 - Data Structures","text":"<p>In this lesson, we will delve into some techniques for manipulating lists and other fundamental data structures.</p>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#list-comprehension","title":"List Comprehension","text":"<p>One of the most commonly used techniques for creating a list from another sequence is called list comprehension, which replaces the syntax defined by traditional loops with more Pythonic constructs.</p> <p>In its basic form, a list comprehension has the following syntax:</p> <pre><code>output_list = [f(element) for element in input_list]\n</code></pre> <p>In other words, it generates an output list (<code>output_list</code>) by applying the function <code>f()</code> to each <code>element</code> of the original list (<code>input_list</code>).</p> <p>For example, given a list of integers, we can create a list where the \\(i\\)-th element is twice the value of the \\(i\\)-th element in the input list.</p> <pre><code>&gt;&gt;&gt; input_list = [1, 2, 3]\n&gt;&gt;&gt; output_list = [x * 2 for x in input_list]\n[2, 4, 6]\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#extended-form-with-if-else","title":"Extended Form with if-else","text":"<p>List comprehensions can also include conditional statements. Here's an example of the extended form:</p> <pre><code>output_list_if = [f(element) for element in input_list if condition]\n</code></pre> <p>In this case, the function <code>f()</code> will be called only on the elements that satisfy the given <code>condition</code>. For example, we can use a list comprehension to return all the even values in a list:</p> <pre><code>&gt;&gt;&gt; output_even = [x for x in input_list if x % 2 == 0]\n[2]\n</code></pre> <p>If we want to include an <code>else</code> clause:</p> <pre><code>output_list_if_else = [f(element) if condition else g(element) for element in input_list]\n</code></pre> <p>The function <code>f()</code> will be invoked on the elements that satisfy the <code>condition</code>, while the function <code>g()</code> will be invoked on the elements that don't satisfy it. For instance, we can return all the even elements and double the odd elements simultaneously:</p> <pre><code>&gt;&gt;&gt; complex_output = [x if x % 2 == 0 else x * 2 for x in input_list]\n[2, 2, 6]\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#tuples","title":"Tuples","text":"<p>Tuples allow you to represent a collection of heterogeneous values separated by commas. For example:</p> <pre><code>tuple = ('hello', 'world', 12)\n</code></pre> <p>Similar to lists, one of the values in a tuple can also be another tuple. For example:</p> <pre><code>tuple = ('hello', 'world', (1, 2))\n</code></pre> <p>However, unlike a list, tuples are immutable. This doesn't mean they can't contain mutable objects within them. Let's look at the following example:</p> <pre><code>tuple = ('hello', 'world', [1, 2, 3])\n</code></pre> <p>The tuple contains two strings (immutable) and a list (mutable). Let's try to modify the list:</p> <pre><code>tuple[2] = [2, 2, 3]\n</code></pre> <p>It will result in an error similar to this:</p> <pre><code>Traceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: 'tuple' object does not support item assignment\n</code></pre> <p>As expected, we encountered an assignment error due to the immutability of the tuple. However, let's now try to modify the list directly:</p> <pre><code>tuple[2][0] = 2         # The tuple will be ('hello', 'world', [2, 2, 3])\n</code></pre> <p>This operation is permissible,</p> <p>and the result is as expected.</p> <p>Tuples and Lists</p> <p>An observant reader will notice that tuples and lists are similar in terms of syntax but differ mainly in mutability. From this, it follows that tuples are extremely effective when we only need to access the elements contained within, while lists should be used when it's also necessary to modify the elements as needed.</p>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#sets","title":"Sets","text":"<p>Sets are also similar to lists in terms of syntax but offer a significant difference: they cannot contain duplicate elements.</p> <p>Note</p> <p>We can draw a clear analogy with the mathematical concept of a set.</p> <p>The syntax for creating a set is as follows:</p> <pre><code>set = {1, \"string\", 2}\n</code></pre> <p>A set can contain heterogeneous data, but it cannot include lists or dictionaries. This is because sets (as well as dictionaries themselves) are hash tables and utilize the concept of hashing to represent data compactly and efficiently. The inability to represent lists and dictionaries in this way automatically excludes them from being included within a set.</p> <p>Another consideration is that sets are unordered, which makes it impossible to access (and modify) an element of the set using its index, as was the case with lists and tuples.</p> <p>Tip</p> <p>Sets can be used to isolate unique elements present in a list. To do this, simply convert the list to a set:</p> <pre><code>l = [1, 2, 2, 3]      # The list is [1, 2, 2, 3]\ns = set(l)            # The set is [1, 2, 3]\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#dictionaries","title":"Dictionaries","text":"<p>The fourth and final type of data container is the dictionary, also known as an associative array or hash map in other programming languages.</p> <p>The basic element of a dictionary is a key-value pair, where a certain value (of any type) is associated with a specific key (of immutable type).</p> <p>Dictionaries share several characteristics with sets, such as the inability to use lists as keys and the prevention of duplicate keys. Additionally, key-value pairs are accessed based on the key itself, not the order of the pairs.</p> <p>Note</p> <p>One difference between sets and dictionaries is that dictionaries are ordered starting from Python 3.7.</p> <p>To create a dictionary, we can use a similar syntax to that used for sets. For example, to create an empty dictionary:</p> <pre><code>&gt;&gt;&gt; dictionary = {}\n</code></pre> <p>We can then add key-value pairs to the dictionary as follows:</p> <pre><code>&gt;&gt;&gt; dictionary['k'] = 'v'\n&gt;&gt;&gt; dictionary[1] = 'n'\n{'k': 'v', 1: 'n'}\n</code></pre> <p>To access the value associated with a specific key:</p> <pre><code>&gt;&gt;&gt; dictionary[1]\n'n'\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#keys-and-values","title":"Keys and Values","text":"<p>We can retrieve the list of all keys present in a dictionary using the <code>keys()</code> method, which returns an object of type <code>dict_keys</code>, which can be converted to a list:</p> <pre><code>&gt;&gt;&gt; keys = dictionary.keys()\ndict_keys(['k', 1])\n&gt;&gt;&gt; list(keys)\n['k', 1]\n</code></pre> <p>Similarly, we can access all the values in the dictionary using the <code>values()</code> method, which returns an object of type <code>dict_values</code>, which can also be converted to a list:</p> <pre><code>&gt;&gt;&gt; values = dictionary.values()\ndict_values(['v', 'n'])\n&gt;&gt;&gt; list(values)\n['v', 'n']\n</code></pre> <p>We can also access all key-value pairs using the <code>items()</code> method, which returns an object of type <code>dict_items</code>, which can</p> <p>be converted into a list of tuples:</p> <pre><code>&gt;&gt;&gt; pairs = dictionary.items()\ndict_items([('k', 'v'), (1, 'n')])\n&gt;&gt;&gt; list(pairs)\n[('k', 'v'), (1, 'n')]\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#creating-a-non-empty-dictionary","title":"Creating a Non-Empty Dictionary","text":"<p>There are several ways to create a non-empty dictionary.</p>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#using-the-operator","title":"Using the <code>{}</code> operator","text":"<p>The simplest and most common way is to declare the initial key-value pairs within the <code>{}</code> operator:</p> <pre><code>&gt;&gt;&gt; dictionary = {'k1': 1, 'k2': 2}\n&gt;&gt;&gt; dictionary\n{'k1': 1, 'k2': 2}\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#using-the-dict-constructor","title":"Using the <code>dict()</code> constructor","text":"<p>Another way is to use the <code>dict()</code> constructor:</p> <pre><code>&gt;&gt;&gt; dictionary = dict(k1=1, k2=2)\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#using-the-zip-function","title":"Using the <code>zip</code> function","text":"<p>We can also use the <code>zip</code> function to create a dictionary from two lists:</p> <pre><code>&gt;&gt;&gt; keys = ['k1', 'k2']\n&gt;&gt;&gt; values = [1, 2]\n&gt;&gt;&gt; dictionary = dict(zip(keys, values))\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures.en/#dict-comprehension","title":"Dict Comprehension","text":"<p>A way to obtain a dictionary from another iterable object is through dict comprehension, which has the following form:</p> <pre><code>&gt;&gt;&gt; output = {key: value for value in iterable}\n</code></pre> <p>For example, we can create a dictionary where the keys are the numbers from 1 to 9, and the corresponding values are their squares:</p> <pre><code>&gt;&gt;&gt; squares = {str(i): i ** 2 for i in range(1, 10)}\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures/","title":"1.2.4 - Strutture dati","text":"<p>In questa lezione approfondiremo alcune tecniche per manipolare le liste, oltre ad altre strutture dati fondamentali.</p>"},{"location":"material/01_python/02_syntax/04_data_structures/#list-comprehension","title":"List comprehension","text":"<p>Una delle tecniche pi\u00f9 usate per creare una lista a partire da un'altra sequenza \u00e8 la cosiddetta list comprehension, che sostituisce la sintassi definita dai classici cicli con costrutti decisamente pi\u00f9 pythonic.</p> <p>Nella forma base, una list comprehension ha una sintassi di questo tipo:</p> <pre><code>lista_output = [f(elemento) for elemento in lista_input]\n</code></pre> <p>In altre parole, otterremo in output una lista (<code>lista_output</code>) applicando ad ogni <code>elemento</code> della lista originaria (<code>lista_input</code>) la funzione <code>f()</code>.</p> <p>Ad esempio, prendendo una lista di interi, possiamo creare una lista che abbia all'\\(i\\)-mo elemento il doppio del valore \\(i\\)-mo della lista di ingresso.</p> <pre><code>&gt;&gt;&gt; lista_input = [1, 2, 3]\n&gt;&gt;&gt; lista_output = [x * 2 for x in lista_input]\n[2, 4, 6]\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures/#forma-estesa-con-if-else","title":"Forma estesa con if-else","text":"<p>La list comprehension pu\u00f2 anche includere delle istruzioni condizionali. Un primo esempio \u00e8 la seguente forma:</p> <pre><code>lista_output_if = [f(elemento) for elemento in lista_input if condizione]\n</code></pre> <p>In questo caso, la funzione <code>f()</code> sar\u00e0 chiamata esclusivamente sugli elementi che soddisfano la <code>condizione</code> indicata. Ad esempio, possiamo utilizzare una list comprehension per restituire tutti i valori pari in una lista:</p> <pre><code>&gt;&gt;&gt; lista_output_pari = [x for x in lista_input if x % 2 == 0]\n[2]\n</code></pre> <p>Se invece volessimo integrare l'<code>else</code>:</p> <pre><code>lista_output_if_else = [f(elemento) if condizione else g(elemento) for elemento in lista_input]\n</code></pre> <p>la funzione <code>f()</code> sarebbe invocata su tutti gli elementi che soddisfano la <code>condizione</code>, mentre la funzione <code>g()</code> su tutti quelli che non la soddisfano. Potremmo ad esempio restituire tutti gli elementi pari, raddoppiando contestualmente i dispari:</p> <pre><code>&gt;&gt;&gt; lista_output_complessa = [x if x % 2 == 0 else x * 2 for elemento in lista_input]\n[2, 2, 6]\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures/#tuple","title":"Tuple","text":"<p>Le tuple permettono di rappresentano un insieme di valori eterogenei separadoli da una virgola. Ad esempio:</p> <pre><code>tupla = ('hello', 'world', 12)\n</code></pre> <p>Un po' come avviene per le liste, uno dei valori della tupla pu\u00f2 a sua volta essere un'altra tupla. Ad esempio:</p> <pre><code>tupla = ('hello', 'world', (1, 2))\n</code></pre> <p>A differenza di una lista, per\u00f2, le tuple sono immutabili. Ci\u00f2 non implica per\u00f2 che non possano contenere al loro interno oggetti mutabili. Guardiamo il seguente esempio:</p> <pre><code>tupla = ('hello', 'world', [1, 2, 3])\n</code></pre> <p>La tupla avr\u00e0 al suo interno due stringhe (immutabili) ed una lista (mutabile). Proviamo a modificare la lista:</p> <pre><code>tupla[2] = [2, 2, 3]\n</code></pre> <p>Apparir\u00e0 un errore simile a questo:</p> <pre><code>Traceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: 'tuple' object does not support item assignment\n</code></pre> <p>Come prevedibile, abbiamo avuto un errore di assegnazione legato all'immutabilit\u00e0 della tupla. Proviamo adesso per\u00f2 a modificare direttamente la lista:</p> <pre><code>tupla[2][0] = 2         # La tupla sar\u00e0 ('hello', 'world', [2, 2, 3])\n</code></pre> <p>L'operazione \u00e8 evidentemente ammissibile, ed il risultato \u00e8 stato proprio quello atteso.</p> <p>Tuple e liste</p> <p>Ad un attento osservatore non sfuggir\u00e0 come tuple e liste siano simili dal punto di vista sintattico, e differiscano in buona sostanza per la mutabilit\u00e0. Da qui discende che le tuple sono estremamente efficaci nel caso si debba esclusivamente accedere agli elementi contenuti, mentre le liste devono essere usate quando \u00e8 anche necessario modificare all'occorrenza detti elementi.</p>"},{"location":"material/01_python/02_syntax/04_data_structures/#set","title":"Set","text":"<p>Anche i set sono molto simili alle liste dal punto di vista sintattico, ma offrono una significativa differenza: infatti, in un set non possono esserci elementi ripetuti.</p> <p>Nota</p> <p>Notiamo un'evidente analogia con il concetto matematico di insieme.</p> <p>La sintassi da usare per creare un set \u00e8 la seguente.</p> <pre><code>insieme = {1, \"stringa\", 2}\n</code></pre> <p>Il set ammette al suo interno dati eterogenei, tuttavia non pu\u00f2 contenere al suo interno delle liste o dei dizionari. Questo \u00e8 legato al fatto che i set (cos\u00ec come gli stessi dizionari) sono delle hash table, e quindi sfruttano il concetto di hash per rappresentare i dati contenuti in maniera compatta ed efficiente. Il fatto che le liste ed i dizionari non possano essere rappresentati in questo modo li esclude in automatico dall'includibilit\u00e0 all'interno di un set.</p> <p>Un'altra considerazione da fare \u00e8 che il set non \u00e8 ordinato: ci\u00f2 rende impossibile accedere ad (e modificare) un elemento del set mediante il suo indice, come succedeva per liste e tuple.</p> <p>Suggerimento</p> <p>I set possono essere usati per isolare gli elementi univoci presenti in una lista. Per farlo, basta convertire la lista in set:</p> <pre><code>l = [1, 2, 2, 3]      # La lista sar\u00e0 [1, 2, 2, 3]\ns = set(l)            # Il set sar\u00e0 [1, 2, 3]\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures/#dizionari","title":"Dizionari","text":"<p>Il quarto ed ultimo tipo di contenitore per sequenze di dati \u00e8 il dizionario, presente anche in altri linguaggi di programmazione con il nome di array associativo o hash map.</p> <p>L'elemento base di un dizionario \u00e8 la coppia chiave - valore, nella quale un certo valore (di qualsiasi tipo) \u00e8 associato ad una determinata chiave (di tipo immutabile).</p> <p>I dizionari hanno diverse caratteristiche comuni ai set, dall'inutilizzabilit\u00e0 delle liste come chiavi al fatto di non permettere chiavi ripetute. Inoltre, le coppie chiave - valore sono accedute, per l'appunto, per chiave, e non in base all'ordine delle coppie.</p> <p>Nota</p> <p>Una differenza tra set e dizionari sta nel fatto che questi ultimi sono ordinati a partire da Python 3.7.</p> <p>Per creare un dizionario, possiamo usare una sintassi simile a quella usata per i set. Ad esempio, per creare un dizionario vuoto:</p> <pre><code>&gt;&gt;&gt; dizionario = {}\n</code></pre> <p>Possiamo quindi inserire delle coppie chiave - valore in questo modo:</p> <pre><code>&gt;&gt;&gt; dizionario['k'] = 'v'\n&gt;&gt;&gt; dizionario[1] = 'n'\n{'k': 'v', 1: 'n'}\n</code></pre> <p>Per accedere al valore associato ad una determinata chiave:</p> <pre><code>&gt;&gt;&gt; dizionario[1]\n'n'\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures/#chiavi-e-valori","title":"Chiavi e valori","text":"<p>E' possibile recuperare la lista di tutte le chiavi presenti in un dizionario usando il metodo <code>keys()</code>, che restituisce un oggetto di tipo <code>dict_keys</code>, a sua volta convertibile in lista:</p> <pre><code>&gt;&gt;&gt; chiavi = dizionario.keys()\ndict_keys(['k', 1])\n&gt;&gt;&gt; list(chiavi)\n['k', 1]\n</code></pre> <p>In modo analogo, si pu\u00f2 accedere a tutti i valori presenti nel dizionario mediante il metodo <code>values()</code>, che restituir\u00e0 un oggetto di tipo <code>dict_values</code>, da convertire anch'esso in lista:</p> <pre><code>&gt;&gt;&gt; valori = dizionario.values()\ndict_values(['v', 'n'])\n&gt;&gt;&gt; list(valori)\n['v', 'n']\n</code></pre> <p>Possiamo accedere anche a tutte le coppie chiave - valore mediante il metodo <code>items()</code>, che ci restituisce un oggetto di tipo <code>dict_items</code>, il quale pu\u00f2 essere convertito in una lista di tuple:</p> <pre><code>&gt;&gt;&gt; coppie = dizionario.items()\ndict_items([('k', 'v'), (1, 'n')])\n&gt;&gt;&gt; list(coppie)\n[('k', 'v'), (1, 'n')]\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures/#creazione-di-un-dizionario-non-vuoto","title":"Creazione di un dizionario (non vuoto)","text":"<p>Abbiamo diversi modi per creare un dizionario non vuoto.</p>"},{"location":"material/01_python/02_syntax/04_data_structures/#uso-delloperatore","title":"Uso dell'operatore <code>{}</code>","text":"<p>Il pi\u00f9 semplice, che \u00e8 quello che useremo pi\u00f9 spesso, \u00e8 quello di dichiarare nell'operatore <code>{}</code> le coppie chiave - valore iniziali:</p> <pre><code>&gt;&gt;&gt; dizionario = {'k1': 1, 'k2': 2}\n&gt;&gt;&gt; dizionario\n{'k1': 1, 'k2': 2}\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures/#uso-del-costruttore-dict","title":"Uso del costruttore <code>dict()</code>","text":"<p>Un altro modo \u00e8 usare il metodo costruttore <code>dict()</code>:</p> <pre><code>&gt;&gt;&gt; dizionario = dict(k1=1, k2=2)\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures/#uso-della-funzione-zip","title":"Uso della funzione <code>zip</code>","text":"<p>Possiamo poi usare la funzione <code>zip</code> per creare un dizionario a partire da due liste:</p> <pre><code>&gt;&gt;&gt; chiavi = ['k1', 'k2']\n&gt;&gt;&gt; valori = [1, 2]\n&gt;&gt;&gt; dizionario = dict(zip(chiavi, valori))\n</code></pre>"},{"location":"material/01_python/02_syntax/04_data_structures/#dict-comprehension","title":"Dict comprehension","text":"<p>Un modo per ottenere un dizionario a partire da un altro oggetto iterabile \u00e8 la dict comprehension, che ha una forma del tipo:</p> <pre><code>&gt;&gt;&gt; output = {chiave: valore for valore in iterabile}\n</code></pre> <p>Possiamo ad esempio creare un dizionario contenente come chiave i numeri da 1 a 9, e come valori corrispondenti i quadrati degli stessi:</p> <pre><code>&gt;&gt;&gt; quadrati = {str(i): i ** 2 for i in range(1, 10)}\n</code></pre>"},{"location":"material/01_python/02_syntax/05_classes.en/","title":"1.2.5 - Object Oriented Programming (OOP) in Python","text":"<p>Python offers extensive support for object-oriented programming (OOP). Before proceeding, however, it is useful to briefly introduce this concept.</p>"},{"location":"material/01_python/02_syntax/05_classes.en/#object-oriented-programming","title":"Object-Oriented Programming","text":"<p>Object-oriented programming (OOP) is a programming paradigm that allows you to create new user-defined types, which should be understood as complementary to the types defined by the programming language. In this sense, OOP shifts the focus from functions, which are central to languages like C and the procedural paradigm, to data.</p> <p>In this sense, it is said that in OOP everything is an object.</p>"},{"location":"material/01_python/02_syntax/05_classes.en/#classes","title":"Classes","text":"<p>A class is a prototype for a particular user-defined data type. For example:</p> <ul> <li>the <code>Student</code> class represents all the properties and actions associated with a student;</li> <li>the <code>Car</code> class represents all the properties and actions associated with a car;</li> <li>the <code>Engine</code> class defines the behavior of engines;</li> </ul> <p>and so on.</p> <p>In general, there can be a class for every type of object in the world, whether real or digital.</p> <p>It is important not to confuse the class with the single object, called an instance. For example:</p> <ul> <li>the student Angelo Cardellicchio is an instance of the <code>Student</code> class;</li> <li>the Opel Corsa car with license plate AB 123 CD is an instance of the <code>Car</code> class;</li> <li>the Hyundai Tucson car with license plate CD 321 AB is an instance of the <code>Car</code> class;</li> <li>the Opel Corsa car with license plate AA 123 CC is another instance of the <code>Car</code> class.</li> </ul>"},{"location":"material/01_python/02_syntax/05_classes.en/#methods-and-attributes","title":"Methods and Attributes","text":"<p>Each class has methods, which define the actions that can be performed on each instance of the class, and attributes, which are characteristics of the instance.</p> <p>In particular, each new type, called a class, will have appropriate attributes and methods, each of which is accessible from the outside through appropriate modifiers.</p> <p>For example, the Opel Corsa car with license plate AB 123 CD has a manufacturer (Opel), a model (Corsa), a license plate (AB 123 CD), an engine size, and so on.</p> <p>Further reading</p> <p>We can delve into the concepts underlying OOP in this appendix.</p>"},{"location":"material/01_python/02_syntax/05_classes.en/#classes-in-python","title":"Classes in Python","text":"<p>To define a class, we will use the <code>class</code> keyword:</p> <pre><code>class ClassName(BaseClass):\n    # Class attributes and methods...\n</code></pre> <p>With the above syntax, we have created a class called <code>ClassName</code> that descends from a base class (<code>BaseClass</code>).</p>"},{"location":"material/01_python/02_syntax/05_classes.en/#the-__init__-method","title":"The <code>__init__</code> Method","text":"<p>Most programming languages use the concept of a constructor to create an instance of a class. Python, however, does not use a true constructor, but rather a method for initializing the individual attributes of the instance. Hence the name of the method, <code>__init__</code>:</p> <pre><code>class ClassName(BaseClass):\n\n    def __init__(self, *args, **kwargs):\n        # ...\n        self.arg_1 = arg_1\n        # ...\n</code></pre> <p>Unpacking</p> <p>With the syntax <code>*args</code> and <code>**kwargs</code>, we want to represent the action of unpacking a list and a dictionary, respectively, through which we are passing all the values contained within the sequence.</p> <p>Particular attention must be paid to the use of the keyword <code>self</code>, which allows you to refer to the specific instance of a class (for those familiar with languages like C++, it is conceptually similar to the <code>this</code> keyword). For example:</p> <pre><code>class Person(object):\n\n    def __init__(self, name, surname, age=18):\n        self.name = name\n        self._surname = surname\n        self.__age = age\n</code></pre> <p>This snippet allows us to highlight four points:</p> <ol> <li>the generic <code>object</code> class, from which all Python classes inherit (although its declaration can be omitted);</li> <li>the functioning of the <code>self</code> keyword, which allows associating a specific value to the attributes of the individual instance;</li> <li>the ability to include optional and default parameter values (in this case, <code>age</code>, which defaults to <code>18</code>);</li> <li>the presence of one or two underscores <code>_</code> in front of some attributes.</li> </ol> <p>Let's briefly explore point 4.</p>"},{"location":"material/01_python/02_syntax/05_classes.en/#access-modifiers","title":"Access Modifiers","text":"<p>Python provides access modifiers for data; specifically, we have the classic <code>public</code>, <code>protected</code>, and <code>private</code>. However, unlike other languages, one or two underscores are used as a suffix to the attribute name to distinguish between the three access modifiers; specifically, a single underscore indicates a protected attribute, while a double underscore indicates a <code>private</code> attribute. In our case:</p> <pre><code>class Person(object):\n\n    def __init__(self, name, surname, age=18):\n        self.name = name                # \"public\" member\n        self._surname = surname         # \"protected\" member\n        self.__age = age                # \"private\" member\n</code></pre> <p>Attention</p> <p>Despite the access modifier, it is still possible to access protected members from outside the class. In fact:</p> <pre><code>&gt;&gt;&gt; p = Person('Jax', 'Teller')\n&gt;&gt;&gt; print(p.name)\n'Jax'\n&gt;&gt;&gt; print(p._surname)\n'Teller'\n</code></pre> <p>This does not apply to private attributes:</p> <pre><code>&gt;&gt;&gt; try:\n&gt;&gt;&gt;     print(p.__age)\n&gt;&gt;&gt; except AttributeError:\n&gt;&gt;&gt;     print('Age is private!')\nAge is private!\n</code></pre> <p>This syntax can also be used to define protected or private methods.</p> <p>Tip</p> <p>The syntax shown in the previous snippet is related to exception handling.</p>"},{"location":"material/01_python/02_syntax/05_classes.en/#methods","title":"Methods","text":"<p>The syntax for defining a class method is similar to that used for defining a function.</p> <pre><code>def method(self, *args, **kwargs):\n    pass\n</code></pre> <p>However, there is a fundamental difference: the first attribute of a method belonging to a class is always a reference to the instance through the <code>self</code> keyword. This reference does not need to be specified when the method is called from outside:</p> <pre><code># ...\np = Person()            # p is an instance of Person\np.method(parameter)     # calling the method from the instance\n# ...\n</code></pre> <p>In the above code, we used the dot operator <code>.</code> to access <code>method()</code> defined inside the <code>Person</code> class.</p> <p>Now let's explore some specific types of methods that can be obtained using certain decorators (see Appendix B).</p>"},{"location":"material/01_python/02_syntax/05_classes.en/#class-methods","title":"Class Methods","text":"<p>The <code>@classmethod</code> decorator allows us to define the so-called class methods:</p> <pre><code>@classmethod\ndef build_person(cls, name: str, surname: str, age: int):\n    return cls(name, surname, age)\n</code></pre> <p>Unlike standard methods, class methods have a reference to the class (<code>cls</code>) rather than the instance (<code>self</code>). This means they are methods that apply to the entire class, not to a single instance. A typical example of using a class method is shown in the previous snippet, where we create a <code>Person</code> class object from a string.</p> <p>Fun Fact</p> <p>The previous method is, in fact, an implementation of the Builder design pattern.</p> <p>To call a class method, you need to refer to the name of the class itself, not to a single instance:</p> <pre><code>&gt;&gt;&gt; person = Person.build_person('Bobby Munson', 58)\n&gt;&gt;&gt; print(\"{} {}\".format(person.name, person._surname))\nBobby Munson\n</code></pre>"},{"location":"material/01_python/02_syntax/05_classes.en/#static-methods","title":"Static Methods","text":"<p>Using the <code>@staticmethod</code> decorator, we can define a static method. In Python, the behavior of such a method can be summarized as a function defined within the class, which can be called on instances of the same class. For example:</p> <pre><code>@staticmethod\ndef is_valid_name(name):\n    if len(name) &lt; 2:\n        return False\n    else:\n        return True\n</code></pre> <p>This method can be freely called using the dot operator <code>.</code> on a single instance:</p> <pre><code>&gt;&gt;&gt; print(Person.is_valid_name('Li'))\nTrue\n</code></pre> <p>Another possibility is to call it on the class itself:</p> <pre><code>&gt;&gt;&gt; print(Person.is_valid_name('X'))\nFalse\n</code></pre>"},{"location":"material/01_python/02_syntax/05_classes.en/#abstract-methods","title":"Abstract Methods","text":"<p>We can define abstract methods (see Appendix C) using the <code>@abstractmethod</code> decorator. To do this, our class must inherit from the <code>ABC</code> class (which stands for Abstract Base Class) in the <code>abc</code> package:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseClass(ABC):\n\n    # ...\n\n    @abstractmethod\n    def method_to_override(self):\n        pass\n</code></pre> <p>Methods marked with the <code>@abstractmethod</code> decorator must be implemented in the derived classes (in other words, we need to override them):</p> <pre><code>class DerivedClass(BaseClass):\n\n    # ...\n\n    def method_to_override(self):\n        # ...\n</code></pre>"},{"location":"material/01_python/02_syntax/05_classes.en/#properties","title":"Properties","text":"<p>In many programming languages, traditional accessor (getter) and mutator (setter) methods are used to access instance attributes of a class. Python doesn't prohibit this: for example, we can write a <code>get_name(self)</code> method to access a person's name and a <code>set_name(self, name)</code> method to set this property.</p> <p>However, it's possible to use a more concise syntax (and ultimately more Pythonic) using the <code>@property</code> decorator, which represents a four-parameter function:</p> <pre><code>property(fget=None, fset=None, fdel=None, doc=None)\n</code></pre> <p>In particular:</p> <ul> <li><code>fget</code> is the function used to retrieve the attribute's value;</li> <li><code>fset</code> is the function used to set the attribute's value;</li> <li><code>fdel</code> is the function to delete the attribute;</li> <li><code>doc</code> is the function to document and describe the attribute.</li> </ul> <p>Thanks to <code>property</code>, we can follow the OOP best practices, making the class attributes private and accessing them through appropriate methods.</p> <pre><code>class Person():\n\n    def __init__(self, name, surname, age):\n        self.name = name\n        self.surname = surname\n        self.age = age\n\n    @property\n    def name(self):\n        return self.__name\n\n    @name.setter\n    def name(self, value):\n        if len(value) &lt; 2:\n            raise ValueError(\"The name must be at least two characters long.\")\n        else:\n            self.__name = value\n\n    @property\n    def surname(self):\n        return self.__surname\n\n    @surname.setter\n    def surname(self, value):\n        if len(value) &lt; 2:\n            raise ValueError(\"The surname must be at least two characters long.\")\n        else:\n            self.__surname = value\n\n    @property\n    def age(self):\n        return self.__age\n\n    @age.setter\n    def age(self, value\n\n):\n        if value &lt; 0:\n            raise ValueError(\"Age cannot be negative.\")\n        else:\n            self.__age = value\n</code></pre> <p>Some notes:</p> <ul> <li>We have rewritten the <code>Person</code> class to turn all attributes into properties.</li> <li>For each property, we have specified a getter that returns the value of the property.</li> <li>In addition to the getter, we have specified a setter that includes a form of validation for the value passed as input.</li> </ul> <p>Let's see how to use our new class:</p> <pre><code>&gt;&gt;&gt; draco = Person('Draco', 'Malfoy', 12)\n&gt;&gt;&gt; print(draco.name)\n'Draco'\n&gt;&gt;&gt; print(draco.age)\n12\n&gt;&gt;&gt; hermione = Person('', 'Granger', 18)\nTraceback (most recent call last):\n    File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n    File \"&lt;stdin&gt;\", line 3, in __init__\n    File \"&lt;stdin&gt;\", line 14, in name\nValueError: The name must be at least two characters long.\n</code></pre> <p>Note that from the perspective of the script calling the class, there are no differences. However, the validation logic allows us to avoid errors and inconsistent situations, and we can also use properties to access the class's private attributes.</p>"},{"location":"material/01_python/02_syntax/05_classes/","title":"1.2.5 - Classi ed OOP","text":"<p>Python offre un esteso supporto alla programmazione orientata agli oggetti. Prima di proseguire, per\u00f2, \u00e8 opportuno introdurre brevemente questo concetto.</p>"},{"location":"material/01_python/02_syntax/05_classes/#la-programmazione-orientata-agli-oggetti","title":"La programmazione orientata agli oggetti","text":"<p>Quello della programmazione orientata agli oggetti (OOP) \u00e8 un paradigma di programmazione che permette di creare nuovi tipi definiti dall'utente, da intendersi come complementari ai tipi definiti dal linguaggio di programmazione. In tal senso, la OOP sposta il focus dalle funzioni, centrali nei linguaggi come il C e nel paradigma procedurale, ai dati.</p> <p>In tal senso, si arriva a dire che nella OOP tutto \u00e8 un oggetto.</p>"},{"location":"material/01_python/02_syntax/05_classes/#classi","title":"Classi","text":"<p>Una classe \u00e8 un prototipo per un determinato tipo di dati definito dall'utente. Ad esempio:</p> <ul> <li>la classe <code>Studente</code> rappresenta tutte le propriet\u00e0 e le azioni associate ad uno studente;</li> <li>la classe <code>Auto</code> rappresenta tutte le propriet\u00e0 e le azioni associate ad un'auto;</li> <li>la classe <code>Motore</code> definisce i comportamenti dei motori;</li> </ul> <p>e via discorrendo.</p> <p>In generale, quindi, pu\u00f2 esistere una classe per ogni tipologia di oggetti presenti nel mondo, sia esso reale o informatico.</p> <p>Importante \u00e8 non confondere la classe con il singolo oggetto, chiamato istanza. Ad esempio:</p> <ul> <li>lo studente Angelo Cardellicchio \u00e8 un'istanza della classe <code>Studente</code>;</li> <li>l'auto Opel Corsa targata AB 123 CD \u00e8 un'istanza della classe <code>Auto</code>;</li> <li>l'auto Hyundai Tucson CD 321 AB \u00e8 un'istanza della classe <code>Auto</code>;</li> <li>l'auto Opel Corsa targata AA 123 CC \u00e8 un'altra istanza della classe <code>Auto</code>.</li> </ul>"},{"location":"material/01_python/02_syntax/05_classes/#metodi-ed-attributi","title":"Metodi ed attributi","text":"<p>Ogni classe ha dei metodi, che caratterizzano delle azioni che \u00e8 possibile effettuare su ogni istanza della classe, e degli attributi, ovvero delle caratteristiche dell'istanza.</p> <p>In particolare, ogni nuovo tipo, chiamato classe, avr\u00e0 opportuni attributi e metodi, ognuno dei quali accessibile dall'esterno mediante opportuni modificatori.</p> <p>Ad esempio, l'auto Opel Corsa targata AB 123 CD ha una casa costruttrice (Opel), un modello (Corsa), una targa (AB 123 CD), una cilindrata, e via dicendo.</p> <p>Approfondimento</p> <p>Possiamo approfondire i concetti alla base della OOP in questa appendice.</p>"},{"location":"material/01_python/02_syntax/05_classes/#classi-in-python","title":"Classi in Python","text":"<p>Per definire una classe, dovremo usare la parola chiave <code>class</code>:</p> <pre><code>class NomeClasse(ClasseBase):\n    # Attributi e metodi di classe...\n</code></pre> <p>Con la sintassi precedente, abbiamo creato una classe chiamata <code>NomeClasse</code> discendente da una classe base (<code>ClasseBase</code>).</p>"},{"location":"material/01_python/02_syntax/05_classes/#il-metodo-__init__","title":"Il metodo <code>__init__</code>","text":"<p>La maggior parte dei linguaggi di programmazione utilizza il concetto di costruttore per creare un'istanza di una classe. Il Python, tuttavia, non prevede l'utilizzo di un costruttore vero e proprio, quanto piuttosto di un metodo di inizializzazione dei singoli attributi dell'istanza. Da qui deriva il nome del metodo, ovvero <code>__init__</code>:</p> <pre><code>class NomeClasse(ClasseBase):\n\n    def __init__(self, *args, **kwargs):\n        # ...\n        self.arg_1 = arg_1\n        # ...\n</code></pre> <p>Unpacking</p> <p>Con la sintassi <code>*args</code> e <code>**kwargs</code> vogliamo rappresentare l'azione di unpacking di (rispettivamente) una lista ed un dizionario, mediante la quale stiamo passando tutti i valori contenuti all'interno della sequenza.</p> <p>Occorre prestare particolare attenzione all'uso della keyword <code>self</code>, che permette di riferirsi alla specifica istanza di una classe (per chi ha familiarit\u00e0 con i linguaggi come il C++, \u00e8 concettualmente simile alla parola chiave <code>this</code>). Ad esempio:</p> <pre><code>class Persona(object):\n\n    def __init__(self, nome, cognome, eta=18):\n        self.nome = nome\n        self._cognome = cognome\n        self.__eta = eta\n</code></pre> <p>Questo snippet ci permette di evidenziare quattro punti:</p> <ol> <li>la classe generica <code>object</code>, da cui derivano tutte le classi Python (ma la cui dichiarazione pu\u00f2 comunque essere omessa);</li> <li>il funzionamento della parola chiave <code>self</code>, che permette di associare agli attributi della singola istanza un determinato valore;</li> <li>la possibilit\u00e0 di inserire tra i parametri dei valori opzionali e di default (in questo caso <code>eta</code>, che di default vale <code>18</code>);</li> <li>la presenza di uno o due simboli <code>_</code> (underscore) davanti ad alcuni attributi.</li> </ol> <p>Approfondiamo brevemente il punto 4.</p>"},{"location":"material/01_python/02_syntax/05_classes/#modificatori-di-accesso","title":"Modificatori di accesso","text":"<p>Python prevede l'uso di modificatori di accesso ai dati; nello specifico, troviamo i classici <code>public</code>, <code>protected</code> e <code>private</code>. Tuttavia, a differenza di altri linguaggi, per distinguere tra i tre modificatori di accesso si utilizzano uno o due underscore come suffisso al nome dell'attributo; in particolare, usare un underscore singolo indica un attributo protected, mentre un underscore doppio indica un attributo <code>private</code>. Nel nostro caso:</p> <pre><code>class Persona(object):\n\n    def __init__(self, nome, cognome, eta=18):\n        self.nome = nome                # Membro \"public\"\n        self._cognome = cognome         # Membro \"protected\"\n        self.__eta = eta                # Membro \"private\"\n</code></pre> <p>Attenzione</p> <p>Nonostante il modificatore di accesso, \u00e8 possibile accedere ai membri protetti dall'esterno della classe. Infatti:</p> <pre><code>&gt;&gt;&gt; p = Persona('Jax', 'Teller')\n&gt;&gt;&gt; print(p.nome)\n'Jax'\n&gt;&gt;&gt; print(p._cognome)\n'Teller'\n</code></pre> <p>Questo non vale per gli attributi privati:</p> <pre><code>&gt;&gt;&gt; try:\n&gt;&gt;&gt;     print(p.__eta)\n&gt;&gt;&gt; except AttributeError:\n&gt;&gt;&gt;     print('Et\u00e0 \u00e8 privato!')\nEt\u00e0 \u00e8 privato!\n</code></pre> <p>Questa sintassi pu\u00f2 ovviamente essere utilizzata per definire dei metodi protetti o privati.</p> <p>Suggerimento</p> <p>La sintassi che abbiamo mostrato nello snippet precedente \u00e8 relativa alla gestione delle eccezioni.</p>"},{"location":"material/01_python/02_syntax/05_classes/#metodi","title":"Metodi","text":"<p>La sintassi per definire il metodo di una classe \u00e8 analoga a quella usata per definire una funzione.</p> <pre><code>def metodo(self, *args, **kwargs):\n    pass\n</code></pre> <p>Esiste tuttavia una differenza fondamentale: infatti, il primo attributo di un metodo appartenente ad una classe \u00e8 sempre un riferimento all'istanza tramite la parola chiave <code>self</code>. Tale riferimento non va specificato quando il metodo viene chiamato dall'esterno:</p> <pre><code># ...\np = Persona()           # p \u00e8 un'istanza di Persona\np.metodo(parametro)     # richiamo il metodo dall'istanza\n# ...\n</code></pre> <p>Nel codice precedente, abbiamo usato l'operatore <code>.</code> per accedere a <code>metodo()</code> definito all'interno della classe <code>Persona</code>.</p> <p>Approfondiamo adesso alcune particolari tipologie di metodi, ottenibili usando determinati decorator (cfr. appendice B).</p>"},{"location":"material/01_python/02_syntax/05_classes/#metodi-di-classe","title":"Metodi di classe","text":"<p>Il decorator <code>@classmethod</code> ci permette di definire i cosiddetti metodi di classe:</p> <pre><code>@classmethod\ndef builder_stringa(cls, stringa: str):\n    nome, cognome, eta = stringa.split(' ')\n    return Persona(nome, cognome, eta)\n</code></pre> <p>A differenza dei metodi standard, i metodi di classe hanno un riferimento alla classe (<code>cls</code>) e non all'istanza (<code>self</code>). Questo significa che sono dei metodi che si applicano all'intera classe, e non alla singola istanza. Un tipico esempio di utilizzo di un metodo di classe \u00e8 mostrato nello snippet precedente, nel quale stiamo creando un oggetto di classe <code>Persona</code> a partire da una stringa.</p> <p>Curiosit\u00e0</p> <p>Il metodo precedente \u00e8, di fatto, un'implementazione del design pattern Builder.</p> <p>Per richiamare un metodo di classe occorre riferirsi al nome della classe stessa, e non ad una singola istanza:</p> <pre><code>&gt;&gt;&gt; persona = Persona.builder_stringa('Bobby Munson 58')\n&gt;&gt;&gt; print(\"{} {}\".format(persona.nome, persona._cognome))\nBobby Munson\n</code></pre>"},{"location":"material/01_python/02_syntax/05_classes/#metodi-statici","title":"Metodi statici","text":"<p>Mediante il decoratore <code>@staticmethod</code> possiamo definire un metodo statico. In Python il funzionamento di un metodo di questo tipo \u00e8 riassumibile in un comportamento assimilabile ad una funzione \"semplice\", definita per\u00f2 all'interno della classe, e richiamabile su istanze della stessa. Ad esempio:</p> <pre><code>@staticmethod\ndef nome_valido(nome):\n    if len(nome) &lt; 2:\n        return False\n    else:\n        return True\n</code></pre> <p>Questo metodo \u00e8 quindi liberamente richiamabile mediante l'operatore <code>.</code> da una singola istanza:</p> <pre><code>&gt;&gt;&gt; print(Persona.nome_valido('Li'))\nTrue\n</code></pre> <p>Un'altra possibilit\u00e0 \u00e8 richiamarlo sulla classe stessa:</p> <pre><code>&gt;&gt;&gt; print(Persona.nome_valido('X'))\nFalse\n</code></pre>"},{"location":"material/01_python/02_syntax/05_classes/#metodi-astratti","title":"Metodi astratti","text":"<p>Possiamo definire dei metodi astratti (cfr. Appendice C) mediante il decorator <code>@abstractmethod</code>. Per farlo, la nostra classe deve discendere dalla classe <code>ABC</code> (acronimo che sta per Abstract Base Class), contenuta nel package <code>abc</code>:</p> <pre><code>from abc import ABC\n\nclass ClasseBase(ABC):\n\n    # ...\n\n    @abstractmethod\n    def metodo_da_sovrascrivere(self):\n        pass\n</code></pre> <p>I metodi contrassegnati con il decorator <code>@abstractmethod</code> dovranno essere implementati nelle classi derivate (in altre parole, dovremo farne l'override):</p> <pre><code>class ClasseDerivata(ClasseBase):\n\n    # ...\n\n    def metodo_da_sovrascrivere(self):\n        # ...\n</code></pre>"},{"location":"material/01_python/02_syntax/05_classes/#le-proprieta","title":"Le propriet\u00e0","text":"<p>In molti linguaggi di programmazione si usano tradizionalmente i metodi accessori (getter) e modificatori (setter) per accedere agli attributi delle istanze di una classe. Python non vieta di farlo: ad esempio, possiamo scrivere un metodo <code>get_nome(self)</code> per accedere al nome di una persona, ed un metodo <code>set_nome(self, nome)</code> per impostare detta propriet\u00e0.</p> <p>Tuttavia, \u00e8 possibile usare una sintassi pi\u00f9 compatta (e, in definitiva, maggiormente pythonic) mediante il decorator <code>@property</code>, che rappresenta una funzione a quattro parametri:</p> <pre><code>property(fget=None, fset=None, fdel=None, doc=None)\n</code></pre> <p>In particolare:</p> <ul> <li><code>fget</code> \u00e8 la funzione usata per recuperare il valore dell'attributo;</li> <li><code>fset</code> \u00e8 la funzione usata per impostare il valore dell'attributo;</li> <li><code>fdel</code> \u00e8 la funzione per rimuovere l'attributo;</li> <li><code>doc</code> \u00e8 la funzione per documentare e descrivere l'attributo.</li> </ul> <p>Grazie a <code>property</code>, potremo seguire le \"best practice\" della OOP, rendendo privati gli attributi della classe ed accedendovi mediante opportuni metodi.</p> <pre><code>class Persona():\n\n    def __init__(self, nome, cognome, eta):\n        self.nome = nome\n        self.cognome = cognome\n        self.eta = eta\n\n    @property\n    def nome(self):\n        return self.__nome\n\n    @nome.setter\n    def nome(self, value):\n        if len(value) &lt; 2:\n            raise ValueError('La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri.')\n        else:\n            self.__nome = value\n\n    @property\n    def cognome(self):\n        return self.__cognome\n\n    @cognome.setter\n    def cognome(self, value):\n        if len(value) &lt; 2:\n            raise ValueError('La lunghezza del cognome non pu\u00f2 essere inferiore a due caratteri.')\n        else:\n            self.__cognome = value\n\n    @property\n    def eta(self):\n        return self.__eta\n\n    @eta.setter\n    def eta(self, value):\n        if value &lt; 0:\n            raise ValueError(\"L'et\u00e0 non pu\u00f2 essere negativa.\")\n        else:\n            self.__eta = value\n</code></pre> <p>Alcune note:</p> <ul> <li>abbiamo riscritto la classe <code>Persona</code> in modo da trasformare tutti gli attributi in propriet\u00e0;</li> <li>per ogni propriet\u00e0, abbiamo specificato un getter, che restituisce il valore della stessa;</li> <li>oltre al getter, \u00e8 stato specificato un setter, nel quale vi \u00e8 anche una forma di validazione del valore passato in input.</li> </ul> <p>Vediamo come usare la nostra nuova classe:</p> <pre><code>&gt;&gt;&gt; draco = Persona('Draco', 'Malfoy', 12)\n&gt;&gt;&gt; print(draco.nome)\n'Draco'\n&gt;&gt;&gt; print(draco.eta)\n12\n&gt;&gt;&gt; hermione = Persona('', 'Granger', 18)\nTraceback (most recent call last):\n    File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n    File \"&lt;stdin&gt;\", line 3, in __init__\n    File \"&lt;stdin&gt;\", line 12, in nome\nValueError: La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri.\n</code></pre> <p>Notiamo che, dal punto di vista dello script che richiama la classe, non ci sono differenze di sorta; tuttavia, la logica di validazione ci permette di evitare errori e situazioni incoerenti, ed \u00e8 inoltre possibile sfruttare le propriet\u00e0 per accedere agli attributi privati della classe.</p>"},{"location":"material/01_python/02_syntax/06_modules.en/","title":"1.2.6 - Scripts, Modules, and Packages","text":"<p>When using Python, the temptation is to interact directly with the interpreter, launching it from the terminal and executing the necessary instructions each time. However, this approach, although immediate, has several disadvantages, such as:</p> <ul> <li>We won't have access to the syntax highlighting offered by a regular IDE.</li> <li>We won't be able to retrieve the code once the interpreter is closed.</li> <li>We won't be able to easily modify or verify the code's functionality.</li> </ul> <p>It is evident that using the interpreter directly is not an optimal way to develop Python code. Therefore, it is necessary to define actual scripts using our preferred IDE. These scripts will be saved as files with the <code>.py</code> extension, each containing a series of instructions required for the execution of our program.</p>"},{"location":"material/01_python/02_syntax/06_modules.en/#the-first-script","title":"The First Script","text":"<p>Let's create our first Python script. To do this, open our preferred IDE, such as Visual Studio Code, and create a file named <code>main.py</code>. Inside this file, we will write the following code:</p> <pre><code># main.py\ndef hello_world():\n    print('Hello, world')\n\nhello_world()\n</code></pre> <p>Now open a terminal, navigate to the folder where we saved this script, and execute it using the following command:</p> <pre><code>cd path_to_script_folder\npython main.py\n</code></pre> <p>The two previous instructions:</p> <ul> <li>Change the directory (<code>cd</code> command) to the folder where the script is located.</li> <li>Tell the Python interpreter to run the <code>main.py</code> script.</li> </ul> <p>If everything goes well, we should see the output <code>Hello, world</code> on the screen:</p> <pre><code>Hello, world\n</code></pre>"},{"location":"material/01_python/02_syntax/06_modules.en/#modules","title":"Modules","text":"<p>When the size of our code base (the amount of code we write for our program) starts to become particularly \"bulky,\" it is advisable to adopt a modular approach by separating different parts of the code into separate files, each containing functions related to different tasks. Let's consider an example.</p> <p>Imagine we want to write a program that defines functions to calculate the area of various geometric shapes. Let's modify our <code>main.py</code> file as follows:</p> <pre><code># main.py\ndef calculate_square_area(side):\n    return side * side\n\n\ndef calculate_rectangle_area(length, width):\n    return length * width\n\n\ndef calculate_triangle_area(base, height):\n    return (base * height) / 2\n\n\nsquare_area = calculate_square_area(4)\nrectangle_area = calculate_rectangle_area(2, 3)\ntriangle_area = calculate_triangle_area(2, 3)\n</code></pre> <p>Now, let's say we want to add a trigonometric calculation function:</p> <pre><code># main.py\nimport math\n\ndef calculate_tangent(angle):\n    return math.sin(angle) / math.cos(angle)\n\n\ntangent_pi = calculate_tangent(math.pi)\n</code></pre> <p>Our <code>main.py</code> file now includes both geometric and trigonometric functions.</p> <p>What would happen if we wanted to integrate integral calculation functions or other types of functions? In such cases, the codebase would increase in size, and there would be a \"mix\" of functions related to different areas (although similar to each other). A good idea would be to separate different parts of the program, grouping geometric functions in a file called <code>geometry.py</code>, trigonometric functions in a file called <code>trigonometry.py</code>, and so on.</p> <p>These files, which primarily contain functions (but not limited to), are called modules.</p> <p>Note</p> <p>The line between scripts and modules is very thin, and in practice, it is easy to confuse and use them interchangeably. However, it is important to note that, ideally, scripts should only contain code that will be executed, while modules should contain.</p>"},{"location":"material/01_python/02_syntax/06_modules.en/#i-moduli-geometria-e-trigonometria","title":"I moduli <code>geometria</code> e <code>trigonometria</code>","text":"<p>Now let's create the <code>geometria.py</code> file, where we will \"move\" the previously defined functions for geometric calculations.</p> <pre><code># geometria.py\ndef calculate_square_area(side):\n    return side * side\n\n\ndef calculate_rectangle_area(base, height):\n    return base * height\n\n\ndef calculate_triangle_area(base, height):\n    return (base * height) / 2\n</code></pre> <p>Similarly, in the <code>trigonometria.py</code> file, we will define the function for calculating the tangent.</p> <pre><code># trigonometria.py\nimport math\n\ndef calculate_tangent(angle):\n    return math.sin(angle) / math.cos(angle)\n</code></pre> <p>Now let's rewrite the <code>main.py</code> file:</p> <pre><code># main.py\nimport geometria\nimport trigonometria\n\nif __name__ == \"__main__\":\n    print(geometria.calculate_square_area(4))\n    print(trigonometria.calculate_tangent(math.pi))\n</code></pre> <p>We can notice two things:</p> <ol> <li>Firstly, we are calling the <code>calculate_square_area()</code> and <code>calculate_tangent()</code> functions defined in the <code>geometria</code> and <code>trigonometria</code> modules, respectively. These modules are imported into our script using the <code>import</code> directive.</li> <li>On line 5, the \"strange\" syntax is used to declare what is known as the <code>main</code> entry point, which is the starting point of our program's code. The <code>main</code> entry point is typically present in all programming languages, sometimes in slightly different forms than shown here. However, in the case of particularly simple scripts, the <code>main</code> entry point can be omitted as the interpreter will be able to execute it autonomously.</li> </ol> <p>Let's try running the script by executing the command <code>python main.py</code> in the terminal. If everything went well, we should see the values of the area of a square and the tangent of \u03c0 on the screen.</p>"},{"location":"material/01_python/02_syntax/06_modules.en/#using-imports","title":"Using Imports","text":"<p>Regarding the <code>geometria</code> module, we have only used the <code>calculate_square_area()</code> function, \"neglecting\" the other two functions still present in the module. In such cases, we can use a modified version of the <code>import</code> directive, which takes the following form:</p> <pre><code>from module import function_or_class\n</code></pre> <p>which, in our specific case, becomes:</p> <pre><code>from geometria import calculate_square_area\n</code></pre> <p>This way, we can import only what we need, which is particularly useful for improving the efficiency of our code; the reason will become clear shortly.</p>"},{"location":"material/01_python/02_syntax/06_modules.en/#aliases","title":"Aliases","text":"<p>The <code>import</code> directive also allows us to define aliases, which are particularly useful when using complex package names. For example:</p> <pre><code>import trigonometria as tr\n\nprint(tr.calculate_tangent(math.pi))\n</code></pre>"},{"location":"material/01_python/02_syntax/06_modules.en/#the-dir-function","title":"The <code>dir()</code> Function","text":"<p>The <code>dir()</code> function returns a list of all names (both functions and classes) defined in a module. For example:</p> <pre><code>&gt;&gt;&gt; dir(geometria)\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'calculate_square_area', 'calculate_rectangle_area', 'calculate_triangle_area']\n</code></pre> <p>It's interesting to note that, in addition to the functions, classes, and variables we defined, the <code>geometria</code> module automatically defines other variables, which are imported using <code>import</code>:</p> <pre><code>import geometria\n\nif __name__ == \"__main__\":\n    print(geometria.__file__)\n    print(geometria.calculate_square_area(4))\n</code></pre> <p>We can access the <code>__file__</code> variable of</p> <p>the <code>geometria</code> module, which indicates its relative path within the file system. Obviously, this variable is rarely useful, but it adds additional overhead to the code, highlighting the importance of using the <code>from</code> directive appropriately.</p>"},{"location":"material/01_python/02_syntax/06_modules.en/#standard-library-modules","title":"Standard Library Modules","text":"<p>Python has several modules that belong to the standard library, which are automatically available once the interpreter is installed. Some of the most commonly used ones are:</p> <ul> <li><code>sys</code>: the module integrated into the interpreter, providing various utilities necessary for its functioning.</li> <li><code>os</code>: the module delegated to interacting with the operating system on which the interpreter runs.</li> <li><code>time</code>: the module used for all time-related utility functions, such as timing the execution of a function.</li> <li><code>datetime</code>: the module used for date and time functionalities.</li> <li><code>copy</code>: the module used for managing, among other things, deep copying of an object.</li> </ul> <p>For a comprehensive list, please refer to the Python Library Reference.</p>"},{"location":"material/01_python/02_syntax/06_modules.en/#packages","title":"Packages","text":"<p>We conclude by mentioning packages, which are collections that group together coherent modules, making it easier to access them later. In practice, packages are nothing more than folders containing multiple modules (i.e., files with the <code>.py</code> extension), along with a file called <code>__init__.py</code>, which allows the interpreter to recognize that folder as a package and, occasionally, contains package initialization instructions.</p> <p>To access a module contained within a package, we can use the <code>import</code> directive, modifying it as follows:</p> <pre><code>import package_name.module_name\n# or...\nfrom package_name.module_name import function_name\n</code></pre>"},{"location":"material/01_python/02_syntax/06_modules/","title":"1.2.6 - Script, moduli e package","text":"<p>Quando si usa Python la tentazione \u00e8 quella di interagire direttamente con l'interprete, lanciandolo da terminale ed eseguendo di volta in volta le istruzioni necessarie. Ovviamente questo approccio, seppur immediato, presenta diversi svantaggi. Ad esempio:</p> <ul> <li>non avremo a disposizione il syntax highlighting offerto da una normale IDE;</li> <li>non potremo recuperare il codice una volta chiuso l'interprete;</li> <li>non potremo n\u00e9 modificare, n\u00e9 verificare facilmente il funzionamento del codice.</li> </ul> <p>Appare quindi evidente come usare l'interprete non sia un modo ottimale di sviluppare codice Python. Di conseguenza, sar\u00e0 necessario definire, mediante la nostra IDE di riferimento, dei veri e propri script che saranno salvati sotto forma di file con estensione <code>.py</code>, ognuno dei quali contenenti una serie di istruzioni necessarie all'esecuzione del nostro programma.</p>"},{"location":"material/01_python/02_syntax/06_modules/#il-primo-script","title":"Il primo script","text":"<p>Proviamo quindi a creare il nostro primo script Python. Per farlo, apriamo la nostra IDE di riferimento, come Visual Studio Code, e creiamo un file chiamato <code>main.py</code>, all'interno del quale inseriremo il seguente codice:</p> <pre><code># main.py\ndef hello_world():\n    print('Hello, world')\n\nhello_world()\n</code></pre> <p>Adesso apriamo un terminale, spostiamoci nella cartella nel quale abbiamo salvato questo script, ed eseguiamolo:</p> <pre><code>cd cartella_dove_risiede_lo_script\npython main.py\n</code></pre> <p>Le due istruzioni precedenti:</p> <ul> <li>servono a cambiare cartella (change directory, <code>cd</code>), spostandoci nella cartella dove risiede lo script;</li> <li>dicono all'interprete Python di lanciare lo script <code>main.py</code>.</li> </ul> <p>A schermo, se tutto \u00e8 andato per il verso giusto, apparir\u00e0 la scritta <code>Hello, world</code>:</p> <pre><code>Hello, world\n</code></pre>"},{"location":"material/01_python/02_syntax/06_modules/#i-moduli","title":"I moduli","text":"<p>Quando le dimensioni della nostra code base (ovvero la quantit\u00e0 di codice che scriviamo nel nostro programma) iniziano ad essere particolarmente \"ingombranti\", \u00e8 opportuno adottare un approccio modulare, separando in file differenti parti di codice delegate a funzioni eterogenee. Facciamo un esempio.</p> <p>Immaginiamo di voler scrivere un programma che definisca delle funzioni per calcolare l'area delle principali figure geometriche. Modifichiamo quindi il nostro file <code>main.py</code> come segue:</p> <pre><code># main.py\ndef calcola_area_quadrato(lato):\n    return lato * lato\n\n\ndef calcola_area_rettangolo(base, altezza):\n    return base * altezza\n\n\ndef calcola_area_triangolo(base, altezza):\n    return (base * altezza) / 2\n\n\narea_quadrato = calcola_area_quadrato(4)\narea_rettangolo = calcola_area_rettangolo(2, 3)\narea_triangolo = calcola_area_triangolo(2, 3)\n</code></pre> <p>Immaginiamo di voler quindi aggiungere una funzione di calcolo trigonometrico:</p> <pre><code># main.py\nimport math\n\ndef calcola_tangente(angolo):\n    return math.sin(angolo) / math.cos(angolo)\n\n\ntangente_pi = calcola_tangente(math.pi)\n</code></pre> <p>Il codice del nostro file <code>main.py</code> comprender\u00e0 adesso funzioni di tipo geometrico e trigonometrico.</p> <p>Cosa succederebbe se volessimo integrare delle funzioni di calcolo integrale, o di altro tipo? Ovviamente, ci sarebbe da un lato un aumento delle dimensioni della code base, dall'altro un \"mix\" tra funzioni che afferiscono ad ambiti differenti (seppur simili tra loro). Una buona idea sarebbe quindi quella di separare le diverse parti del programma, magari raggruppando le funzioni geometriche nel file <code>geometria.py</code>, le funzioni trigonometriche nel file <code>trigonometria.py</code>, e via discorrendo.</p> <p>Questi file, che conterranno al loro interno prevalentemente funzioni (ma non solo), sono chiamati moduli.</p> <p>Nota</p> <p>La linea che distingue gli script dai moduli \u00e8 molto sottile, e nei fatti \u00e8 facile fare confusione ed utilizzarli in maniera \"intercambiabile\". Sottolineamo per\u00f2 che, idealmente, gli script devono contenere al loro interno soltanto del codice che sar\u00e0 eseguito, mentre i moduli solo del codice che sar\u00e0 invocato da uno o pi\u00f9 script.</p> <p>Interprete e nome di un modulo</p> <p>L'interprete \u00e8 in grado di risalire al nome di un modulo dal nome del file in cui \u00e8 contenuto. Se, ad esempio, definiamo un modulo nel file <code>geometria.py</code>, l'interprete associer\u00e0 a quel modulo il nome <code>geometria</code>. Detto nome \u00e8 inoltre accessibile globalmente e dall'interno del modulo richiamando la variabile globale <code>__name__</code>.</p>"},{"location":"material/01_python/02_syntax/06_modules/#i-moduli-geometria-e-trigonometria","title":"I moduli <code>geometria</code> e <code>trigonometria</code>","text":"<p>Creiamo adesso il file <code>geometria.py</code>, all'interno del quale \"sposteremo\" le funzioni definite in precedenza per il calcolo geometrico.</p> <pre><code># geometria.py\ndef calcola_area_quadrato(lato):\n    return lato * lato\n\n\ndef calcola_area_rettangolo(base, altezza):\n    return base * altezza\n\n\ndef calcola_area_triangolo(base, altezza):\n    return (base * altezza) / 2\n</code></pre> <p>Analogamente, nel file <code>trigonometria.py</code> andremo a definire la funzione per il calcolo della tangente.</p> <pre><code># trigonometria.py\nimport math\n\ndef calcola_tangente(angolo):\n    return math.sin(angolo) / math.cos(angolo)\n</code></pre> <p>Riscriviamo ora il file <code>main.py</code>:</p> <pre><code># main.py\nimport geometria\nimport trigonometria\n\nif __name__ == \"__main__\":\n    print(geometria.calcola_area_quadrato(4))\n    print(trigonometria.calcola_tangente(math.pi))\n</code></pre> <p>Possiamo notare due cose.</p> <ol> <li>In primis, stiamo richiamando le funzioni <code>calcola_area_quadrato()</code> e <code>calcola_tangente()</code> definite nei moduli <code>geometria</code> e <code>trigonometria</code>, rispettivamente. Questi moduli sono importati all'interno del nostro script mediante la direttiva <code>import</code>.</li> <li>Al rigo 5, la \"strana\" sintassi mostrata serve a dichiarare quello che \u00e8 il <code>main</code>, ovvero il punto di \"accesso\" al codice del nostro programma. Il <code>main</code> \u00e8 normalmente presente in tutti i linguaggi di programmazione, alle volte sotto forme un po' differenti da quella qui mostrata; tuttavia, nel caso di script particolarmente semplici, il <code>main</code> pu\u00f2 essere tranquillamente omesso, in quanto l'interprete riuscir\u00e0 ad eseguirlo in maniera autonoma.</li> </ol> <p>Proviamo a lanciare lo script; per farlo, digitiamo l'istruzione <code>python main.py</code> da terminale. A schermo, se tutto \u00e8 andato per il verso giusto, vedremo i valori dell'area di un quadrato e della tangente di \\(\\pi\\).</p>"},{"location":"material/01_python/02_syntax/06_modules/#usare-gli-import","title":"Usare gli import","text":"<p>Relativamente al modulo <code>geometria</code>, abbiamo usato esclusivamente la funzione <code>calcola_area_quadrato()</code>, \"trascurando\" le altre due funzioni comunque presenti nel modulo. In queste circostanze, possiamo usare una versione modificata della direttiva <code>import</code>, che assume la seguente forma:</p> <pre><code>from modulo import funzione_o_classe\n</code></pre> <p>il che, nel nostro caso specifico, diventa:</p> <pre><code>from geometria import calcola_area_quadrato\n</code></pre> <p>In questo modo, possiamo importare solamente quello che ci serve, il che risulta particolarmente utile a migliorare l'efficienza del nostro codice; il perch\u00e9 sar\u00e0 chiaro a breve.</p>"},{"location":"material/01_python/02_syntax/06_modules/#alias","title":"Alias","text":"<p>La direttiva <code>import</code> ci permette di definire anche degli alias, particolarmente utili nel caso si usino dei nomi di package complessi. Ad esempio:</p> <pre><code>import trigonometria as tr\n\nprint(tr.calcola_tangente(math.pi))\n</code></pre>"},{"location":"material/01_python/02_syntax/06_modules/#la-funzione-dir","title":"La funzione <code>dir()</code>","text":"<p>La funzione <code>dir()</code> restituisce una lista con tutti i nomi (sia di funzione, sia di classe) definiti da un modulo. Ad esempio:</p> <pre><code>&gt;&gt;&gt; dir(geometria)\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'calcola_area_quadrato', 'calcola_area_rettangolo', 'calcola_area_triangolo']\n</code></pre> <p>E' interessante notare come, oltre a funzioni, classi e variabili da noi definite, nel modulo <code>geometria</code> siano automaticamente definite altre variabili, che saranno importate usando import:</p> <pre><code>import geometria\n\nif __name__ == \"__main__\":\n    print(geometria.__file__)\n    print(geometria.calcola_area_quadrato(4))\n</code></pre> <p>Notiamo che saremo in grado di accedere alla variabile <code>__file__</code> del modulo <code>geometria</code>, che indica il percorso relativo dello stesso all'interno del file system. Ovviamente, questa variabile non \u00e8 quasi mai utile, ma comporta un ulteriore carico sul codice, da cui diventa evidente l'importanza dell'opportuno uso della direttiva <code>from</code>.</p>"},{"location":"material/01_python/02_syntax/06_modules/#moduli-della-libreria-standard","title":"Moduli della libreria standard","text":"<p>Python ha diversi moduli appartenenti ad una libreria standard, i quali sono automaticamente disponibili a valle dell'installazione dell'interprete. Alcuni tra i pi\u00f9 utilizzati sono:</p> <ul> <li><code>sys</code>: \u00e8 il modulo integrato nell'interprete, ed offre diverse utility necessarie al suo funzionamento;</li> <li><code>os</code>: modulo delegato all'interazione con il sistema operativo su cui gira l'interprete;</li> <li><code>time</code>: modulo usato per tutte le utility riguardanti il \"cronometraggio\" del tempo di esecuzione di una funzione;</li> <li><code>datetime</code>: modulo usato per le funzionalit\u00e0 di data ed ora;</li> <li><code>copy</code>: modulo usato per gestire, tra le altre cose, la deep copy di un oggetto.</li> </ul> <p>Per una lista esaustiva, si rimanda alla Python Library Reference.</p>"},{"location":"material/01_python/02_syntax/06_modules/#package","title":"Package","text":"<p>Chiudiamo la trattazione con un accenno ai package, ovvero a delle vere e proprie \"collezioni\" che raggruppano moduli tra loro coerenti, in modo da facilitarne il successivo accesso. In pratica, i package non sono altro se non delle cartelle contenenti pi\u00f9 moduli (quindi, file con estensione <code>nome_modulo.py</code>), oltre ad un file, chiamato <code>__init__.py</code>, che permette all'interprete di riconoscere quella cartella come package e, occasionalmente, contiene delle istruzioni di inizializzazione del package.</p> <p>Per poter accedere ad un modulo contenuto all'interno di un package, possiamo usare la direttiva <code>import</code>, modificandola come segue:</p> <pre><code>import nome_package.nome_modulo\n# oppure...\nfrom nome_package.nome_modulo import nome_funzione\n</code></pre>"},{"location":"material/01_python/02_syntax/exercises/exercises/","title":"Esercitazione 1.2","text":""},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizi-sulla-programmazione-strutturata","title":"Esercizi sulla programmazione strutturata","text":""},{"location":"material/01_python/02_syntax/exercises/exercises/#esecizio-21","title":"Esecizio 2.1","text":"<p>Scriviamo un ciclo che iteri fino a che il valore associato ad un contatore intero risulta essere minore di 10. Usiamo sia un ciclo <code>while</code>, sia un ciclo <code>for</code>.</p> <p>Soluzione</p> <p>Per prima cosa, \u00e8 opportuno tracciare un breve diagramma di flusso che mostri l'andamento delle informazioni all'interno del nostro codice.</p> <pre><code>flowchart TD\n    A(START) --&gt; B[i = 1]\n    B --&gt; C{i &lt;= 10}\n    C --&gt;|No| E[/\"print(i)\"/]\n    C --&gt; |S\u00ec| D[i = i + 1]\n    D --&gt; C\n    E --&gt; F(END)</code></pre> <p>Implementiamo il codice in primis utilizzando un ciclo <code>while</code>. Per farlo, inizializziamo a zero un contatore <code>i</code> e come condizione verifichiamo che <code>i</code> sia minore di <code>10</code>:</p> <pre><code>&gt;&gt;&gt; i = 1\n&gt;&gt;&gt; while i &lt;= 10:\n...     print(f'Il valore di i \u00e8 {i}')\n...     i = i + 1\n</code></pre> <p>Se proviamo ad eseguire questo codice, otterremo il seguente risultato:</p> <pre><code>Il valore di i \u00e8 1\nIl valore di i \u00e8 2\nIl valore di i \u00e8 3\nIl valore di i \u00e8 4\nIl valore di i \u00e8 5\nIl valore di i \u00e8 6\nIl valore di i \u00e8 7\nIl valore di i \u00e8 8\nIl valore di i \u00e8 9\nIl valore di i \u00e8 10\n</code></pre> <p>Proviamo adesso ad usare un <code>for</code>. In questo caso, potremo limitarci ad usare in maniera opportuna la funzione <code>range()</code>:</p> <pre><code>&gt;&gt;&gt; for i in range(1, 11):\n...     print(f'Il valore di i \u00e8 {i}')\n</code></pre> <p>Eseguendo questa istruzione, otterremo un risultato analogo al precedente.</p>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-22","title":"Esercizio 2.2","text":"<p>Data una lista di numeri, scrivere una funzione che iteri fino a che non si trova un numero divisibile per \\(7\\). Utilizzare un ciclo <code>for</code>.</p> <p>Soluzione</p> <p>La soluzione a questo esercizio prevede l'utilizzo dell'istruzione <code>break</code>, che dovr\u00e0 essere lanciata quando troveremo un multiplo intero di \\(7\\).</p> <pre><code>def multiplo_sette(lista):\n    for el in lista:\n        if el % 7 == 0:\n            print(f'{el} \u00e8 multiplo di 7. Uscita in corso.')\n            break\n        else:\n            print(f'{el} non \u00e8 multiplo di 7.')\n</code></pre> <p>In pratica:</p> <ul> <li>alla riga 2 iteriamo su ogni elemento della lista;</li> <li>alla riga 3, se l'elemento \u00e8 perfettamente divisibile per \\(7\\) (e, quindi, il modulo della divisione \u00e8 \\(0\\)), stampiamo a schermo un messaggio ed usciamo dalla funzione;</li> <li>se la precedente non \u00e8 verficata, continuiamo l'iterazione sulla lista.</li> </ul> <p>Ad esempio:</p> <pre><code>&gt;&gt;&gt; l = [1, 2, 5, 13, 21, 5]\n&gt;&gt;&gt; multiplo_sette(l)\n1 non \u00e8 multiplo di 7.\n2 non \u00e8 multiplo di 7.\n5 non \u00e8 multiplo di 7.\n13 non \u00e8 multiplo di 7.\n21 \u00e8 multiplo di 7. Uscita in corso.\n</code></pre>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizi-sulle-funzioni","title":"Esercizi sulle funzioni","text":""},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-23","title":"Esercizio 2.3","text":"<p>Estraiamo tutti gli indici pari di una lista arbitraria di dieci elementi in ordine inverso. Per farlo, usiamo sia la funzione <code>range</code> sia lo slicing.</p> <p>Soluzione</p> <p>Una possibile implementazione delle funzioni \u00e8 la seguente:</p> <pre><code>def estrai_con_slice(l):\n    return l[-1::-2]\n\ndef estrai_con_range(l):\n    l_out = []\n    for i in range(len(l) - 1, -2, -2):\n        l_out.append(l[i])\n    return l_out\n</code></pre> <p>Partiamo dalla funzione <code>estrai_con_slice(l)</code>. Qui sfruttiamo il meccanismo di slicing su lista, che ricordiamo essere dato dall'espressione:</p> <pre><code>l[i:f:s]\n</code></pre> <p>dove <code>l</code> \u00e8 la lista sotto analisi, <code>i</code> \u00e8 l'indice iniziale, <code>f</code> \u00e8 l'indice di terminazione, ed <code>s</code> \u00e8 lo step da utilizzare. In questo caso, stiamo specificando <code>i</code> pari a <code>-1</code>, per cui lo slicing partir\u00e0 dall'ultimo elemento della lista, ed andr\u00e0 verso l'indice finale (che omettiamo). Tuttavia, se omettessimo anche <code>s</code>, lo slicing ci restituirebbe esclusivamente l'ultimo elemento della lista; di conseguenza, specifichiamo un passo negativo, che ci assicurer\u00e0 che <code>l</code> venga completamente attraversata considerando soltanto gli elementi pari. Un altro modo sarebbe stato il seguente:</p> <pre><code>&gt;&gt;&gt; l_out = l[1::2]\n&gt;&gt;&gt; l_out.reverse()\n&gt;&gt;&gt; l_out\n[10, 8, 6, 4, 2]\n</code></pre> <p>In questo caso, aabbiamo potuto utilizzare uno slicing \"classico\", da sinistra verso destra, ma abbiamo dovuto utilizzare il metodo <code>reverse()</code> per invertire l'ordine della lista.</p> <p>Applichiamo la funzione alla lista <code>l</code>:</p> <pre><code>&gt;&gt;&gt; l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n&gt;&gt;&gt; estrai_con_slice(l)\n[10, 8, 6, 4, 2]\n</code></pre> <p>Nella funzione <code>estrai_con_range()</code>, invece, definiamo un ciclo <code>for</code> su una sequenza generata dalla funzione <code>range(i, f, s)</code>, specificando <code>i = len(l) - 1</code>, ovvero <code>9</code>, ed <code>f = 0</code>. Ci\u00f2 \u00e8 legato al fatto che <code>l[9]</code> \u00e8, a causa dello 0-indexing di Python, l'ultimo elemento della lista. Come step manteniamo, come prevedibile, <code>-2</code>.</p> <pre><code>&gt;&gt;&gt; estrai_con_range(l)\n[10, 8, 6, 4, 2]\n</code></pre> <p>Suggerimento</p> <p>Nel caso della funzione <code>estrai_con_range()</code>, anche <code>f = -1</code> \u00e8 valido. Tuttavia, se impostassimo <code>f = -2</code>, il risultato sarebbe <code>[10, 8, 6, 4, 2, 10]</code>. Il motivo \u00e8 legato al fatto che, all'ultima iterazione, staremmo considerando <code>l[-1]</code>, che \u00e8 proprio <code>10</code>.</p>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-24","title":"Esercizio 2.4","text":"<p>Duplicare una lista passata come argomento in ingresso. Provare ad utilizzare un ciclo <code>for</code>.</p> <p>Soluzione</p> <p>Potremmo essere tentati di scrivere una funzione come la seguente:</p> <pre><code>def raddoppia_lista(lista):\n    for elemento in lista:\n        lista.append(elemento)\n        print(f\"Lista all'iterazione attuale: {lista}\")\n</code></pre> <p>Se provassimo ad utilizzare questa funzione, faremmo finire l'interprete in un ciclo infinito. Infatti, la funzione tenta di agire sulla lista originaria che, ad ogni iterazione, aumenta le sue dimensioni di un elemento, il che, ovviamente, rende impossibile raggiungere il termine della stessa!</p> <p>Per ottenere il risultato previsto, dobbiamo usare una deep copy, ovvero fare una copia esatta della lista. Per chi ha familiarit\u00e0 con il C, si tratta, in poche parole, di una copia per valore. Usiamo il metodo <code>deepcopy</code>:</p> <pre><code>from copy import deepcopy\n\ndef raddoppia_lista_deep(lista):\n    lista_appoggio = deepcopy(lista)\n    for elemento in lista_appoggio:\n        lista.append(elemento)\n        print(f\"Lista di appoggio: {lista_appoggio}\")\n        print(f\"Lista attuale: {lista}\")\n</code></pre> <p>Stiamo creando una nuova lista, chiamata <code>lista_appoggio</code>, che sar\u00e0 utilizzata come base per duplicare la lista originaria. Provando a chiamare questa funzione, otterremo il risultato desiderato:</p> <pre><code>&gt;&gt;&gt; raddoppia_lista_deep([1, 2])\nLista di appoggio: [1, 2]\nLista attuale: [1, 2, 1]\nLista di appoggio: [1, 2]\nLista attuale: [1, 2, 1, 2]\n</code></pre>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-25","title":"Esercizio 2.5","text":"<p>Generare una lista di elementi casuali compresi tra \\(0\\) e \\(10\\). Usare sia una funzione con un unico parametro opzionale.</p> <p>Soluzione</p> <p>Scriviamo la seguente funzione:</p> <pre><code>import random\n\ndef genera_lista_casuale(lunghezza=5):\n    l = []\n    for i in range(lunghezza):\n        l.append(random.randint(0, 10))\n    return l\n</code></pre> <p>In particolare, <code>genera_lista_casuale()</code> accetta come parametro opzionale <code>lunghezza</code>, il cui valore \u00e8 5, ed usa la funzione <code>append()</code> per aggiungere un valore generato casualmente.</p> <p>Volendo, \u00e8 possibile usare anche una list comprehension:</p> <pre><code>def genera_lista_casuale_con_l_c(lunghezza=5):\n    return [random.randint(0, 10) for i in range(lunghezza)]\n</code></pre> <p>Verifichiamo entrambe le funzioni:</p> <pre><code>&gt;&gt;&gt; genera_lista_casuale()\n[0, 2, 1, 0, 2]\n&gt;&gt;&gt; genera_lista_casuale(lunghezza=10) \n[2, 2, 2, 9, 5, 1, 5, 10, 9, 6]\n&gt;&gt;&gt; genera_lista_casuale_con_l_c()\n[10, 7, 2, 5, 1]\n&gt;&gt;&gt; genera_lista_casuale_con_l_c(lunghezza=10) \n[0, 9, 3, 7, 1, 3, 9, 3, 8, 7]\n</code></pre>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizi-su-liste-e-strutture-dati","title":"Esercizi su liste e strutture dati","text":""},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-26","title":"Esercizio 2.6","text":"<p>Selezionare tutti i nomi che iniziano con la lettera <code>B</code> dalla seguente lista. Utilizzare sia un ciclo sia una list comprehension.</p> <pre><code>lista_nomi = [\n    \"Jax Teller\",\n    \"Walter White\",\n    \"Billy Butcher\",\n    \"Luke Skywalker\",\n    \"Bobby Singer\",\n    \"Johnny Lawrence\"\n]\n</code></pre> <p>Soluzione</p> <p>Proviamo innanzitutto ad usare un ciclo. In particolare, useremo un <code>for</code> che itera su tutti le stringhe nella lista, verificando se il primo carattere \u00e8 <code>B</code>.</p> <pre><code>l_out = []\nfor nome in lista_nomi:\n    if nome[0] == \"B\":\n        l_out.append(nome)\n</code></pre> <p>La formulazione mediante list comprehension \u00e8 molto pi\u00f9 compatta, ma utilizza lo stesso principio:</p> <pre><code>output = [nome for nome in lista_nomi if nome[0] == \"B\"]\n</code></pre>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-27","title":"Esercizio 2.7","text":"<p>Ottenere la lista di tutti i quadrati dei numeri da 1 a 10. Utilizzare una list comprehension ed un'apposita funzione per il calcolo del quadrato.</p> <p>Soluzione</p> <p>Per prima cosa, definiamo la funzione <code>quadrato()</code> che accetta un <code>numero</code> e restituisce il suo quadrato:</p> <pre><code>def quadrato(numero):\n    return numero ** 2\n</code></pre> <p>A questo punto, possiamo invocare la funzione direttamente da una list comprehension:</p> <pre><code>l_out = [quadrato(i) for i in range(1, 11)]\n</code></pre>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-28","title":"Esercizio 2.8","text":"<p>Data una lista di interi <code>l_int</code>, ottenere una lista <code>l_out</code> il cui \\(i\\)-mo elemento sia la stringa <code>'pari'</code> se l'\\(i\\)-mo elemento di <code>l_int</code> \u00e8 pari, e <code>'dispari'</code> altrimenti.</p> <p>Soluzione</p> <p>Possiamo risolvere l'esercizio utilizzando la list comprehension nella forma completa (ovvero con l'<code>if/else</code>). Ad esempio:</p> <pre><code>&gt;&gt;&gt; l_int = [1, 5, 2, 4]\n&gt;&gt;&gt; l_out = ['pari' if el % 2 == 0 else 'dispari' for el in l_int] \n&gt;&gt;&gt; l_out\n['dispari', 'dispari', 'pari', 'pari']\n</code></pre>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-29","title":"Esercizio 2.9","text":"<p>Scrivere una dict comprehension che permetta di ottenere il dizionario <code>vecchio_o_giovane</code> dato il seguente dizionario:</p> <pre><code>dizionario = {\n\u00a0 \u00a0 'Jax Teller': 27,\n\u00a0 \u00a0 'Walter White': 52,\n\u00a0 \u00a0 'Billy Butcher': 41,\n\u00a0 \u00a0 'Luke Skywalker': 79,\n\u00a0 \u00a0 'Bobby Singer': 68,\n\u00a0 \u00a0 'Johnny Lawrence': 49\n}\n</code></pre> <p>In particolare, il dizionario <code>vecchio_o_giovane</code> avr\u00e0 le stesse chiavi del dizionario di partenza, a cui sar\u00e0 associato il valore <code>'giovane'</code> soltanto se il valore della chiave del dizionario di partenza \u00e8 inferiore a 65.</p> <p>Soluzione</p> <p>Ricordando che la sintassi della dict comprehension \u00e8 sostanzialmente analoga a quella della list comprehension, possiamo scrivere:</p> <pre><code>vecchio_o_giovane = {k: 'vecchio' if v &gt; 65 else 'giovane' for (k, v) in dizionario.items()}\n</code></pre> <p>In pratica, stiamo associando il valore <code>'vecchio'</code> alla chiave <code>k</code> se il valore <code>v</code> ad essa associato in <code>dizionario</code> \u00e8 superiore a <code>65</code>. Da notare inoltre come l'iterazione avvenga su tutte le coppie chiave - valore presenti in <code>dizionario</code>, ottenibili mediante il metodo <code>items()</code>.</p>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizi-sulla-programmazione-orientata-agli-oggetti","title":"Esercizi sulla programmazione orientata agli oggetti","text":""},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-210","title":"Esercizio 2.10","text":"<p>Scrivere una classe <code>Persona</code> applicando i concetti visti durante la lezione.</p> <p>Soluzione</p> <p>Supponiamo che la classe <code>Persona</code> abbia tre attributi:</p> <ul> <li>un attributo <code>nome</code>, stringa rappresentativa del nome della persona;</li> <li>un attributo <code>cognome</code>, stringa rappresentativa del cognome della persona;</li> <li>un attributo <code>eta</code>, intero rappresentativo dell'et\u00e0 della persona.</li> </ul> <p>Per prima cosa, scriviamo il metodo <code>__init__</code>:</p> <pre><code>def __init__(self, nome, cognome, eta):\n    self.nome = nome\n    self.cognome = cognome\n    self.eta = eta\n</code></pre> <p>Passeremo al metodo <code>__init__</code> tre parametri che, per semplicit\u00e0, chiameremo proprio <code>nome</code>, <code>cognome</code> ed <code>eta</code>. Questi parametri andranno ad inizializzare gli omonimi attributi di classe.</p> <p>Fatto questo, scriviamo tre propriet\u00e0, una per ciascun attributo:</p> <pre><code>@property\ndef nome(self):\n    return self.__nome\n\n@nome.setter\ndef nome(self, value):\n    if len(value) &lt; 2:\n        raise ValueError('La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri.')\n    else:\n        self.__nome = value\n\n@property\ndef cognome(self):\n    return self.__cognome\n\n@cognome.setter\ndef cognome(self, value):\n    if len(value) &lt; 2:\n        raise ValueError('La lunghezza del cognome non pu\u00f2 essere inferiore a due caratteri.')\n    else:\n        self.__cognome = value\n\n@property\ndef eta(self):\n    return self.__eta\n\n@eta.setter\ndef eta(self, value):\n    if value &lt; 0:\n        raise ValueError(\"L'et\u00e0 non pu\u00f2 essere negativa.\")\n    else:\n        self.__eta = value\n</code></pre> <p>Notiamo come le propriet\u00e0 <code>nome</code> e <code>cognome</code> siano fatte in modo che se la lunghezza della stringa passata risulta essere inferiore a due caratteri venga lanciato un errore di tipo <code>ValueError</code>. Analogamente, il valore della propriet\u00e0 <code>eta</code> non potr\u00e0 essere inferiore a zero.</p> <p>Facciamo un esempio di uso della nostra classe mediante l'interprete Python:</p> <pre><code>&gt;&gt;&gt; draco = Persona('Draco', 'Malfoy', 12)\n&gt;&gt;&gt; print(draco.nome)\n'Draco'\n&gt;&gt;&gt; print(draco.eta)\n12\n&gt;&gt;&gt; hermione = Persona('', 'Granger', 18)\nTraceback (most recent call last):\n    File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n    File \"&lt;stdin&gt;\", line 3, in __init__\n    File \"&lt;stdin&gt;\", line 12, in nome\nValueError: La lunghezza del nome non pu\u00f2 essere inferiore a due caratteri.\n</code></pre> <p>Codice</p> <p>Il codice completo per questo esercizio \u00e8 disponibile qui.</p>"},{"location":"material/01_python/02_syntax/exercises/exercises/#esercizio-211","title":"Esercizio 2.11","text":"<p>Creare le classi <code>Quadrato</code> e <code>Cerchio</code> che modellano (rispettivamente) i quadrati ed i cerchi. Entrambe devono essere progettate in modo da discendere da una classe base chiamata <code>Figura</code>.</p> <p>Soluzione</p> <p>Per prima cosa, definiamo la classe <code>Figura</code> come classe astratta. In particolare, definiremo due propriet\u00e0 comuni a tutte le figure, ovvero il <code>perimetro</code> e l'<code>area</code>, e due metodi astratti <code>calcola_perimetro</code> e <code>calcola_area</code> che andremo ad implementare nelle diverse sottoclassi.</p> <pre><code>from abc import ABC, abstractmethod\n\nclass Figura(ABC):\n\n    def __init__(self, perimetro=0, area=0):\n        self.perimetro = perimetro\n        self.area = area\n\n    @property\n    def perimetro(self):\n        return self.__perimetro\n\n    @area.setter\n    def perimetro(self, value):\n        if value &lt;= 0:\n            raise ValueError(\"L'area non pu\u00f2 essere negativa.\")\n        self.__perimetro = value\n\n    @property\n    def area(self):\n        return self.__area\n\n    @area.setter\n    def area(self, value):\n        if value &lt;= 0:\n            raise ValueError(\"L'area non pu\u00f2 essere negativa.\")\n        self.__area = value\n\n    @abstractmethod\n    def calcola_perimetro(self):\n        pass\n\n    @abstractmethod\n    def calcola_area(self):\n        pass\n</code></pre> <p>Notiamo come implementiamo un metodo <code>__init__</code> di base che inizializza i valori di perimetro ed area; di default, inseriremo dei valori pari a <code>0</code> per entrambe le propriet\u00e0.</p> <p>Fatto questo, potremo implementare le singole sottoclassi. In particolare, il <code>Quadrato</code> sar\u00e0 inizializzato mediante una specifica propriet\u00e0 <code>lato</code>, ed imposter\u00e0 il perimetro e l'area secondo le leggi del quadrato; il <code>Cerchio</code>, invece, avr\u00e0 la propriet\u00e0 <code>raggio</code>, mentre perimetro ed area seguiranno le specifiche convenzioni. </p> <pre><code>from math import pi\n\nclass Quadrato(Figura):\n\n    def __init__(self, lato, area=0, perimetro=0):\n        super().__init__(perimetro, area)\n        self.lato = lato\n\n    @property\n    def lato(self):\n        return self.__lato\n\n    @lato.setter\n    def lato(self, value):\n        self.__lato = value\n\n    def calcola_perimetro(self):\n        self.perimetro = self.lato * 4\n\n    def calcola_area(self):\n        self.area = self.lato ** 2\n\n\nclass Cerchio(Figura):\n\n    def __init__(self, raggio, area=0, perimetro=0):\n        super().__init__(perimetro, area)\n        self.raggio = raggio\n\n    @property\n    def raggio(self):\n        return self.__raggio\n\n    @raggio.setter\n    def raggio(self, value):\n        self.__raggio = value\n\n    def calcola_perimetro(self):\n        self.perimetro = 2 * pi * self.raggio\n\n    def calcola_area(self):\n        self.area = pi * (self.raggio ** 2)\n</code></pre> <p>Notiamo che entrambi i metodi <code>__init__</code> richiamano l'omologo della classe <code>Figura</code> mediante il metodo <code>super()</code>.</p> <p>Proviamo ad utilizzare il codice precedente:</p> <pre><code>q = Quadrato(5)\nq.calcola_area()\nq.calcola_perimetro()\nprint(f'Lato: {q.lato} - Perimetro: {q.perimetro} - Area: {q.area}')\n\nc = Cerchio(5)\nc.calcola_area()\nc.calcola_perimetro()\nprint(f'Raggio: {c.raggio} - Perimetro: {c.perimetro} - Area: {c.area}')\n</code></pre> <p>Codice</p> <p>Il codice completo per questo esercizio \u00e8 disponibile qui.</p>"},{"location":"material/01_python/03_advanced/01_argparse.en/","title":"1.3.1 - The <code>argparse</code> Module","text":"<p>The <code>argparse</code> module allows us to pass arguments to a Python script using the command line.</p> <p>To do this, we need to follow a process articulated in four steps:</p> <ol> <li>Create an instance of the <code>ArgumentParser</code> class.</li> <li>Add the arguments that we want to parse.</li> <li>Parse these arguments.</li> <li>Use the passed arguments.</li> </ol> <p>Like many things in Python, describing this series of steps is more complex than implementing it. Therefore, we will use the usual learn by doing approach, using a simple example.</p> <p>Suppose we have a <code>Person</code> class defined as follows:</p> <pre><code>class Person():\n\n    def __init__(self, name, surname):\n        self.name = name\n        self.surname = surname\n\n    def __str__(self):\n        return f'{self.name} {self.surname}'\n</code></pre> <p>Now let's write a script to create an object of this class, specifying its parameters through the command line. First, let's import the <code>argparse</code> library:</p> <pre><code>import argparse\n</code></pre> <p>Next, let's define a method that accepts a generic set of arguments (which we'll call <code>args</code>) as a parameter and creates an instance of <code>Person</code> from these arguments:</p> <pre><code>def run(args):\n    \"\"\" Define the `run` method that will be invoked\n    every time the script is executed.\n    The method accepts a parameter `args` that represents\n    the parsed arguments.\n    \"\"\"\n    p = Person(args.name, args.surname)\n    print(p)\n</code></pre> <p>Now we can define the entry point to our script as follows:</p> <pre><code>if __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '-n',\n        '--name',\n        help='Name of the person',\n        default='Pippo')\n    parser.add_argument(\n        '-s',\n        '--surname',\n        help='Surname of the person',\n        required=True)\n    args = parser.parse_args()\n    run(args)\n</code></pre> <p>In particular:</p> <ul> <li>on line 2, we create an instance of the <code>ArgumentParser</code> type, which we call <code>parser</code>.</li> <li>on line 3, we add the first argument (the name) to the <code>parser</code> using the <code>add_argument()</code> method.</li> <li>on line 4, we add a flag that marks the argument.</li> <li>on line 5, we add a name to indicate the argument.</li> <li>on line 6, we define a help message using the <code>help</code> parameter that describes the purpose of the argument.</li> <li>on line 7, we assign a default value to the argument related to the person's name.</li> <li>on line 8, we create a second argument, which is the surname, and add it to the parser.</li> <li>on line 12, we specify that the <code>surname</code> argument is required by setting the <code>required</code> parameter to <code>True</code>.</li> <li>on line 13, we parse the passed arguments, saving them in a variable called <code>args</code>.</li> <li>finally, on line 14, we pass the <code>args</code> variable to the previously defined <code>run()</code> method.</li> </ul> <p>The <code>args</code> variable</p> <p>The <code>args</code> variable defines an object of type <code>Namespace</code> in which all the arguments passed to the script are saved</p> <p>, each of which can be invoked using the <code>args.argument_name</code> notation.</p> <p>Using <code>help</code></p> <p>Defining the <code>help</code> parameter allows us to use the <code>python script_name -h</code> command, which displays the specified help messages during parsing. Therefore, it is advisable to avoid using flags like <code>-h</code> to prevent parser collisions and errors.</p> <p>Save our code in a script called <code>run.py</code>. To execute it, we'll use the <code>-n</code> and <code>-s</code> flags to specify the name and surname, respectively:</p> <pre><code>python run.py -n Name -s Surname\n</code></pre> <p>The output will be:</p> <pre><code>Name Surname\n</code></pre> <p>We can omit the name, but not the surname, as it is a required parameter:</p> <pre><code>python run.py -s Surname\nPippo Surname\n</code></pre> <p>We can also invoke the help by running:</p> <pre><code>python run.py -h\n</code></pre> <p>Finally, let's try using the full notation:</p> <pre><code>python run.py --name Name --surname Surname\nName Surname\n</code></pre> <p>Now let's modify the <code>Person</code> class to include age. In this case, let's specify that the age must be an integer; otherwise, an exception will be raised.</p> <pre><code>class Person():\n\n    def __init__(self, name, surname, age):\n        self.name = name\n        self.surname = surname\n        self.age = age\n\n    @property\n    def age(self):\n        return self._age\n\n    @age.setter\n    def age(self, value):\n        if not isinstance(age, int):\n            raise ValueError(\"Please provide an integer for the age.\")\n        self._age = value\n\n    def __str__(self):\n        return f'{self.name} {self.surname}'\n</code></pre> <p>Let's modify the rest of the script to adapt to the new requirements. We'll start with the <code>run</code> method:</p> <pre><code>def run(args):\n    \"\"\" Define the `run` method that will be invoked\n    every time the script is executed.\n    The method accepts a parameter `args` that represents\n    the parsed arguments.\n    \"\"\"\n    p = Person(args.name, args.surname, args.age)\n    print(p)\n</code></pre> <p>Then, let's add another argument to the <code>parser</code>:</p> <pre><code>if __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument(\n        '-n',\n        '--name',\n        help='Name of the person',\n        default='Pippo')\n    parser.add_argument(\n        '-s',\n        '--surname',\n        help='Surname of the person',\n        required=True)\n    parser.add_argument(\n        '-a',\n        '--age',\n        help='Age of the person')\n    args = parser.parse_args()\n    run(args)\n</code></pre> <p>Let's try running the script again:</p> <pre><code>python run.py -n Name -s Surname -a 18\n</code></pre> <p>You will see an error because the arguments passed through <code>argparse</code> are normally interpreted as strings.</p> <p>To resolve this issue, we need to specify the <code>type</code> parameter as <code>int</code>:</p> <pre><code>parser.add_argument(\n    '-a',\n    '--age',\n    help='Age of the person',\n    type=int)\n</code></pre> <p>If we run the script again, we won't encounter any errors.</p>"},{"location":"material/01_python/03_advanced/01_argparse/","title":"1.3.1 - Il modulo <code>argparse</code>","text":"<p>Il modulo <code>argparse</code> ci permette di passare ad uno script Python degli argomenti utilizzando la riga di comando.</p> <p>Per farlo, dobbiamo seguire un processo articolato in quattro step:</p> <ol> <li>creiamo un oggetto di classe <code>ArgumentParser</code>;</li> <li>aggiungiamo gli argomenti di cui intendiamo fare il parsing;</li> <li>effettuiamo il parsing di questi argomenti;</li> <li>usiamo gli argomenti passati.</li> </ol> <p>Come molte cose in Python, \u00e8 pi\u00f9 complesso descrivere questa serie di passaggi che implementarla. Di conseguenza, usiamo il solito approccio learn by doing, usando un semplice esempio.</p> <p>Supponiamo di avere una classe <code>Persona</code>, definita come segue:</p> <pre><code>class Persona():\n\n    def __init__(self, nome, cognome):\n        self.nome = nome\n        self.cognome = cognome\n\n    def __str__(self):\n        return f'{self.nome} {self.cognome}'\n</code></pre> <p>Scriviamo adesso uno script per creare un oggetto di questa classe, specificandone i parametri mediante riga di comando. Per prima cosa, importiamo la libreria <code>argparse</code>:</p> <pre><code>import argparse\n</code></pre> <p>Definiamo quindi un metodo che accetti come parametro un generico insieme di argomenti (che chiameremo <code>args</code>), e che crei al suo interno un'istanza di <code>Persona</code> a partire da questi argomenti.</p> <pre><code>def run(args):\n    \"\"\" Definiamo il metodo `run` che sar\u00e0 invocato\n    ad ogni esecuzione dello script.\n    Il metodo accetta un parametro args che rappresenta\n    gli argomenti di cui \u00e8 stato effettuato il parsing.\n    \"\"\"\n    p = Persona(args.nome, args.cognome)\n    print(p)\n</code></pre> <p>Possiamo adesso definire il punto di accesso al nostro script come segue:</p> <pre><code>if __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '-n',\n        '--nome',\n        help='Nome della persona',\n        default='Pippo')\n    parser.add_argument(\n        '-c',\n        '--cognome',\n        help='Cognome della persona',\n        required=True)\n    args = parser.parse_args()\n    run(args)\n</code></pre> <p>In particolare:</p> <ul> <li>alla riga 2, creeremo un oggetto di tipo <code>ArgumentParser</code>, che chiameremo <code>parser</code>;</li> <li>alla riga 3, aggiungeremo un primo argomento (il nome) al <code>parser</code> mediante il metodo <code>add_argument()</code>;</li> <li>alla riga 4, aggiungeremo un flag che contrassegner\u00e0 l'argomento;</li> <li>alla riga 5, aggiungeremo un nome per indicare l'argomento;</li> <li>alla riga 6, andremo a definire un messaggio di aiuto mediante il parametro <code>help</code> che descriver\u00e0 a cosa serve l'argomento;</li> <li>alla riga 7, assegneremo un valore di default all'argomento relativo al nome della persona;</li> <li>alla riga 8, creeremo un secondo argomento, ovvero il cognome, aggiungendolo al parser;</li> <li>alla riga 12, specificheremo che l'argomento <code>cognome</code> \u00e8 richiesto settando il parametro <code>required</code> a <code>True</code>;</li> <li>alla riga 13, andremo ad effettuare il parsing degli argomenti passati, salvandoli in una variabile chiamata <code>args</code>;</li> <li>infine, alla riga 14, passeremo la variabile <code>args</code> al metodo <code>run()</code> definito in precedenza.</li> </ul> <p>La variabile <code>args</code></p> <p>La variabile <code>args</code> definisce un oggetto di tipo <code>Namespace</code> all'interno del quale sono salvati tutti gli argomenti passati allo script, ciascuno dei quali invocabile con la notazione <code>args.nome_argomento</code>.</p> <p>Utilizzare l'<code>help</code></p> <p>Definire il parametro <code>help</code> ci permette di usare il comando <code>python nome_script -h</code>, che ci restituisce le righe di aiuto specificate durante il parsing. Di conseguenza, \u00e8 opportuno evitare di inserire dei flag del tipo <code>-h</code>, onde evitare collisioni ed errori del parser.</p> <p>Salviamo il nostro codice in uno script chiamato <code>run.py</code>. Per eseguirlo, sfrutteremo i flag <code>-n</code> e <code>-c</code> per specificare rispettivamente nome e cognome:</p> <pre><code>python run.py -n Nome -c Cognome\n</code></pre> <p>A schermo vedremo:</p> <pre><code>Nome Cognome\n</code></pre> <p>Possiamo anche omettere il nome, ma non il cognome, in quanto \u00e8 un parametro richiesto:</p> <pre><code>python run.py -c Cognome\nPippo Cognome\n</code></pre> <p>Possiamo poi invocare l'help scrivendo:</p> <pre><code>python run.py -h\n</code></pre> <p>Proviamo infine ad utilizzare la notazione completa:</p> <pre><code>python run.py --nome Nome --cognome Cognome\nNome Cognome\n</code></pre> <p>Proviamo adesso a modificare la classe <code>Persona</code> inserendovi l'et\u00e0. In tal senso, specifichiamo che l'et\u00e0 deve essere un valore intero; qualora questo non avvenga, sar\u00e0 lanciata un'eccezione.</p> <pre><code>class Persona():\n\n    def __init__(self, nome, cognome, eta):\n        self.nome = nome\n        self.cognome = cognome\n        self.eta = eta\n\n    @property\n    def eta(self):\n        return self._eta\n\n    @eta.setter\n    def eta(self, value):\n        if not isinstance(eta, int):\n            raise ValueError(\"Fornire un intero per l'et\u00e0.\")\n        self._eta = value\n\n    def __str__(self):\n        return f'{self.nome} {self.cognome}'\n</code></pre> <p>Modifichiamo il resto dello script per adattarci alle nuove esigenze. Partiamo dal metodo <code>run</code>:</p> <pre><code>def run(args):\n    \"\"\" Definiamo il metodo `run` che sar\u00e0 invocato\n    ad ogni esecuzione dello script.\n    Il metodo accetta un parametro args che rappresenta\n    gli argomenti di cui \u00e8 stato effettuato il parsing.\n    \"\"\"\n    p = Persona(args.nome, args.cognome, args.eta)\n    print(p)\n</code></pre> <p>Aggiungiamo poi un altro argomento al <code>parser</code>:</p> <pre><code>if __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument(\n        '-n',                       \n        '--nome',                   \n        help='Nome della persona',  \n        default='Pippo')            \n    parser.add_argument(\n        '-c',\n        '--cognome',\n        help='Cognome della persona',\n        required=True)              \n    parser.add_argument(\n        '-e',\n        '--eta',\n        help='Et\u00e0 della persona')\n    args = parser.parse_args()\n    run(args)\n</code></pre> <p>Proviamo ad eseguire di nuovo lo script:</p> <pre><code>python run.py -n Nome -c Cognome -e 18\n</code></pre> <p>Vedremo che viene lanciato un errore, in quanto gli argomenti passati mediante argparse sono normalmente interpretati come delle stringhe.</p> <p>Per risolvere questo problema dovremo specificare il parametro <code>type</code>, ponendolo ad <code>int</code>:</p> <pre><code>parser.add_argument(\n    '-e',\n    '--eta',\n    help='Et\u00e0 della persona',\n    type=int)\n</code></pre> <p>Se proviamo ad eseguire nuovamente lo script non riscontreremo alcun errore.</p>"},{"location":"material/01_python/03_advanced/enumerate/","title":"Enumerate","text":"<p>In Python, un ciclo for \u00e8 normalmente scritto come un ciclo su un oggetto iterabile. Questo singifica che non abbiamo bisogno di un contatore per accedere agli oggetti nell'iterabile. Alle volte, tuttavia, vogliamo avere una variabile che cambia ad ogni iterazione del ciclo. Piuttosto che creare ed incrementare una varaibile, possiamo usare il metodo enumerate() per ottenere un contatore ed un valore dall'iterabile allo stesso tempo.</p>"},{"location":"material/01_python/03_advanced/enumerate/#iterare-con-i-cicli-for-in-python","title":"iterare con i cicli for in Python","text":"<p>Un ciclo for in Python usa delle collection-based iteration. In altri termini, ci\u00f2 singifica che Python assegna l'oggetto successivo in un iterabile alla variabile di loop ad ogni iterazione, come in questo esempio:</p> <pre><code>&gt;&gt;&gt; values = [\"a\", \"b\", \"c\"]\n\n&gt;&gt;&gt; for value in values:\n...     print(value)\n...\na\nb\nc\n</code></pre> <p>In questo esempio, <code>values</code> \u00e8 una lista con tre stringhe. In Python, le liste sono un tipo di oggetto iterabile. Nel ciclo for, la variabile su cui si cicla \u00e8 il valore. Ad ogni iterazione del ciclo, il valore \u00e8 impostato all'oggetto successivo sui valori.</p> <p>Adesso, stampiamo a schermo <code>value</code>. Il vantaggio delle iterazioni collection-based \u00e8 che ci aiutano ad evitare l'errore <code>off-by-one</code> che \u00e8 comune in altri lingauggi di programmazione.</p> <p>Adesso immaginiamo che, oltre al valore stesso, vogliamo stampare l'indice dell'oggetto nella lista sullo schermo ad ogni iterazione. Un modo di approcciare questo task \u00e8 creare una variabile per memorizzare l'indice ed aggiornarla ad ogni iterazione:</p> <pre><code>&gt;&gt;&gt; index = 0\n\n&gt;&gt;&gt; for value in values:\n...     print(index, value)\n...     index += 1\n...\n0 a\n1 b\n2 c\n</code></pre> <p>In questo esempio, <code>index</code> \u00e8 un intero che tiene traccia di dove siamo nella lista. Ad ogni iterazione del loop, stampiamo <code>index</code> cos\u00ec come <code>value</code>. L'ultimo step nel ciclo \u00e8 aggiornare il numero memorizzato nell'indice di uno. Un bug comune avviene quando dimentichiamo di aggiornare l'indice ad ogni iterazione:</p> <pre><code>&gt;&gt;&gt; index = 0\n\n&gt;&gt;&gt; for value in values:\n...     print(index, value)\n...\n0 a\n0 b\n0 c\n</code></pre> <p>In questo esempio, l'indice rimane a 0 ad ogni iterazione percH\u00e9 non c'\u00e8 del codice che ne aggiorna il valore al termine del ciclo. Specie epr cicli lunghi o complicati, questo tipo di bug \u00e8 notoriamente difficile da individuare.</p> <p>Un altro modo comune per approcciare questo problema \u00e8 usare range() combinato con len() per creare automaticamente un indice. In questo modo, non dobbiamo ricordarci di aggiornare l'indiice:</p> <pre><code>&gt;&gt;&gt; for index in range(len(values)):\n...     value = values[index]\n...     print(index, value)\n...\n0 a\n1 b\n2 c\n</code></pre> <p>In questo esempio, <code>len(values)</code> restituisce la lunghezza di <code>values</code>, che \u00e8 \\(3\\). Quindi, <code>range()</code> crea un iteratore che va dal valore di default di partenza di <code>0</code> fino a quando non arriva a <code>len(values)</code> meno uno. In questo caso, <code>index</code> diventa la variabile su cui si cicla. Nel loop, impostiamo il valore uguale all'oggetto nei valori al valore attuale dell'indice. Successivamente, stampiamo l'indice ed il valore.</p> <p>Con questo esempio, un bug comune che pu\u00f2 avvenire \u00e8 quando ci dimentichiamo di aggiornare il valore all'inizio di ogni iterazione. Questo \u00e8 simile al bug precedente in cui ci dimentichiamo di aggiornare l'indice. Questa \u00e8 una delle ragioni per le quali questo loop non viene considerato Pythonic.</p> <p>Questo esempio \u00e8 anche in qualche modo ristretto perch\u00e9 values deve permettere l'accesso ai suoi uggetti usando indici interi. Gli iterabili che permettono questo tipo di accessi sono chiamati sequenze in Python.</p> <p>Dettaglio tecnico</p> <p>Secondo la documentazione Python, un iterabile \u00e8 un qualsiasi oggetto che pu\u00f2 restituire i suoi membri uno alla volta. Per definizione, gli iterabili supportano il protocollo iterator, che specifica come i membri dell'oggetto sono restituiti quando un oggetto \u00e8 usato in un iteratore. Python ha due tipi comuni di iteraotri: sequenze e generatori.</p> <p>Un qualsiasi iterabile pu\u00f2 essere usato in un ciclo for, ma solo le sequenze possono essere accedute da indici interi. Provare ad accedere agli oggetti per indice da un generatore o un iteratore lancer\u00e0 un TypeError:</p> <pre><code>&gt;&gt;&gt; enum = enumerate(values)\n&gt;&gt;&gt; enum[0]\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: 'enumerate' object is not subscriptable\n</code></pre> <p>In questo esempio, assegnamo il valore di ritorno di <code>enumerate()</code> ad <code>enum</code>. <code>enumerate()</code> \u00e8 un iteratore, per cui provare ad accedere ai suoi valori per indice lancia un <code>TypeError</code>.</p> <p>Fortunatamente, il metodo <code>enumerate()</code> di Python ci permette di evitare tutti questi problemi. E' una funzione integrata, il che significa che \u00e8 stato disponibile in ogni versione di Python dal momento in cui \u00e8 stato aggiunto in Python 2.3, nel 2003.</p>"},{"location":"material/01_python/03_advanced/enumerate/#usare-il-metodo-enumerate","title":"Usare il metodo enumerate()","text":"<p>Possiamo usare enumerate() in un loop allo stesso modo in cui si usa l'oggetto iterabile originario. Invece di inserire l'iterabile direttamente dopo <code>in</code> nel loop, lo mettiamo tra le parentesi di <code>enumerate()</code>. Dobbiamo anche cambiare la variabile di iterazione:</p> <pre><code>&gt;&gt;&gt; for count, value in enumerate(values):\n...     print(count, value)\n...\n0 a\n1 b\n2 c\n</code></pre> <p>QUando usiamo <code>enumerate()</code>, la funzione ci restituisce due variabili su cui iterare:</p> <ul> <li>il conteggio dell'iterazione attuale</li> <li>il valore ell'oggetto all'attuale iterazione</li> </ul> <p>Proprio come nel caso di un normale ciclo for, le variabili di loop possono essere chiamate come vogliamo. Possiamo usare <code>count</code> e <code>value</code> dell'esempio precedente, ma potremmo ad esempio usare <code>i</code> e <code>v</code>, o ogni altro nome Python valido.</p> <p>Con <code>enumerate()</code>, non dobbiamo ricordare l'accesso all'oggetto dall'iterabile, e non dobbiamo ricordare di aumentare l'indice alla fine del ciclo. Tutto viene gestito automaticamente.</p> <p>Dettaglio tecnico</p> <p>L'uso di di due variabili di loop, <code>count</code> e <code>value</code>, separate da una virgola \u00e8 un esempio di argument unpacking. Questa feature Python sar\u00e0 discussa in seguito.</p> <p>La funzione <code>enumerate()</code> ha un ulteriore argomento che possiamo usare per controllare il valore di partenza del conteggio. Di default, il valore di partenza \u00e8 <code>0</code> perch\u00e9 le sequenze Python sono indicizzati partendo da zero. In altre parole, quando vogliamo recuperare il primo elemento di una lista, usiamo l'indice <code>0</code>:</p> <pre><code>&gt;&gt;&gt; print(values[0])\na\n</code></pre> <p>Possiamo vedere in questo esempio che accedere a <code>values</code> con indice <code>0</code> restituisce il primo elemento, <code>a</code>. Tuttavia, ci sono molte volte quando non vogliamo che il conteggio di <code>enumerate()</code> parta a <code>0</code>. Ad esempio, potremmo voler stampare a schermo un contatore come output per l'utente. IN questo caso, possiamo usare l'argomento <code>start</code> passato ad <code>ennumerate()</code> per cambiare il contatore di partenza:</p> <pre><code>&gt;&gt;&gt; for count, value in enumerate(values, start=1):\n...     print(count, value)\n...\n1 a\n2 b\n3 c\n</code></pre> <p>In questo esempio, passiamo <code>start=1</code>, che inizia il conteggio con il valore <code>1</code> sulla prima iterazione del loop. Se compariamo questo con gli esempi precedenti, nei quali <code>start</code> aveva il valore di default di <code>0</code>, e possiamo vedere la diffferenza.</p>"},{"location":"material/01_python/03_advanced/enumerate/#pratica","title":"Pratica","text":"<p>Dovremmo usare <code>enumerate()</code> ogni volta che dobbiamo usare il contatore ed un oggetto in un ciclo. Teniamo a mente che <code>enumerate()</code> aumenta il conteggio di uno ad ogni iterazione. Tuttavia, questo limita soltanto leggermente la nostra flessibilit\u00e0. Dato che <code>count</code> \u00e8 un intero standard, possiamo usarlo in molti modi. Vediamo alcuni esempi.</p>"},{"location":"material/01_python/03_advanced/enumerate/#conteggio-naturale-di-oggetti-iterabili","title":"Conteggio naturale di oggetti iterabili","text":"<p>Nella sezione precedente, abbiamo visto come usare <code>enumerate()</code> per iniziare a creare un contatore naturale da stampare a schermo per l'utente. <code>enumerate()</code> \u00e8 usato anche in questo modo all'interno della codebase Python. Possiamo vedere un esempio in uno script che legge i file reST e dice all'utente se ci sono problemi di formattazione.</p> <p>Nota</p> <p>reST, detto anche reStructured Text, \u00e8 un formato standard per i file di testo che Python usa per la documentazione. Vedremo spesso delle stringhe formattate secondo reST incluse come docstring nelle classi e funzioni Python. Gli script che leggono i file del codice sorgente e dicono all'utente dei problemi di formattazione sono chiamati linter perch\u00e9 cercano i metaphorical lint (TODO: TRADURRE) nel codice.</p> <p>Questo esempio \u00e8 leggermente modificato da rstlint.py. Non ci preoccupiamo come questa funzione controlla i problemi: il punto \u00e8 quello di mostrare un uso vero di enumerate().</p> <pre><code>def check_whitespace(lines):\n    \"\"\"Check for whitespace and line length issues.\"\"\"\n    for lno, line in enumerate(lines):\n        if \"\\r\" in line:\n            yield lno+1, \"\\\\r in line\"\n        if \"\\t\" in line:\n            yield lno+1, \"OMG TABS!!!1\"\n        if line[:-1].rstrip(\" \\t\") != line[:-1]:\n            yield lno+1, \"trailing whitespace\"\n</code></pre> <p><code>check_whitespae()</code> prende un argomento, <code>lines</code>, che sono le linee del file che deve essere valutato. Alla terza riga di <code>check_whitespace()</code>, enumerate() \u00e8 usato in un loop su <code>lines</code>. Questo restituisce il numero di linea, abbreviato in <code>lno</code>, e la linea stessa. Dal momento che <code>start</code> non \u00e8usato, <code>lno</code> \u00e8 un contatore che parte da zero delle righe nel file. <code>check_whitespaces()</code> fa diversi controlli per individuare i caratteri fuori psoto:</p> <ul> <li>il carriage return \\r</li> <li>il carattere tab \\t</li> <li>un qualsiasi spazio o tab alla fine della riga</li> </ul> <p>Quando uno di questi oggetti \u00e8 presente, <code>check_whitespace()</code> restituisce il numero di riga attuale ed un messaggio utile all'utente. La variabile contatore <code>lno</code> vi aggiunge uno in modo che restituisca il numero di contatore di linea piuttosto che un idnice preso a partire da zero. QUando un utente di <code>rslint.py</code> legge il messaggio, sapr\u00e0 a quale riga andare e quello da controllare.</p>"},{"location":"material/01_python/03_advanced/enumerate/#istruzioni-condizionali-per-saltare-oggetti","title":"Istruzioni condizionali per saltare oggetti","text":"<p>Usare le istruzioni condizionali per elaborare gli oggetti pu\u00f2 essere una tecnica molto potetne. Alle volte, dobbiamo effettuare un'azione soltanto alla prima iterazione di un loop, come in questo esempio:</p> <pre><code>&gt;&gt;&gt; users = [\"Test User\", \"Real User 1\", \"Real User 2\"]\n&gt;&gt;&gt; for index, user in enumerate(users):\n...     if index == 0:\n...         print(\"Extra verbose output for:\", user)\n...     print(user)\n...\nExtra verbose output for: Test User\nReal User 1\nReal User 2\n</code></pre> <p>In questo esempio, usiamo una lista come mock di un database di utenti. Il primo utente \u00e8 il nostro utente di test, per cui vogliamo creare delle informazioni di diagnostica su quell'utente. Dal momento che abbiamo impostato il sistema in modo che l'utente di test sia il primo, possiamo usare il primo valore dell'indice del loop per stampare dell'output verboso extra.</p> <p>Possiamo anche combinare delle operazionimatematiche con delle condizioni per il conteggio o l'indice. PEr esempio, possiamo restituire degli oggetti da un iterabile, ma soltanto se hanno un indice pari. Possiamo far questo usando enumerate().</p> <pre><code>&gt;&gt;&gt; def even_items(iterable):\n...     \"\"\"Return items from ``iterable`` when their index is even.\"\"\"\n...     values = []\n...     for index, value in enumerate(iterable, start=1):\n...         if not index % 2:\n...             values.append(value)\n...     return values\n...\n</code></pre> <p>La funzione <code>even_items()</code> prende un argomento, chiamato <code>iterable</code>, che dovrebbe essere un qualche tipo di oggetto su cui Python pu\u00f2 ciclare. Per prima cosa, <code>values</code> viene inizializzato ad una lista vuota. Quindi possiamo creare un ciclo for sull'iterabile con <code>enumerate()</code> ed impostare <code>start=1</code>.</p> <p>All'interno del ciclo for, controlliamo se il resto della divisione per due \u00e8 zero. Se questo \u00e8 vero, aggiungiamo l'oggetto a values. Infine, restituiamo values.</p> <p>Possiamo rendere il codice pi\u00f9 Pythonic usando una list comprehension per fare la stessa cosa in una riga senza inizializzare una lista vuota:</p> <pre><code>&gt;&gt;&gt; def even_items(iterable):\n...     return [v for i, v in enumerate(iterable, start=1) if not i % 2]\n...\n</code></pre> <p>In questo codice di esempio, <code>even_items()</code> usa una list comprehension piuttosto che un ciclo for per estrarre ogni oggetto dalla ista il cui indice \u00e8 un numero pari.</p> <p>Possiamo verificare che <code>even_items()</code> funziona come atteso ottenendo gli oggetti con indice pari da un range di interi da 1 a 10. Il risultato sar\u00e0:</p> <pre><code>&gt;&gt;&gt; seq = list(range(1, 11))\n\n&gt;&gt;&gt; print(seq)\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n&gt;&gt;&gt; even_items(seq)\n[2, 4, 6, 8, 10]\n</code></pre> <p>Come atteso, <code>even_items()</code> restituisce gli oggetti ad indice pari da <code>seq</code>. Questo non \u00e8 il modo pi\u00f9 efficienti per ottenere i numeri pari quando stiamo lavorando con gli interi. Tuttavia, ora che abbiamo verificato che <code>even_items()</code> lavora propriamente, possiamo ottenere le lettere ad indice pari dell'alfabeto ASCII:</p> <pre><code>&gt;&gt;&gt; alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n\n&gt;&gt;&gt; even_items(alphabet)\n['b', 'd', 'f', 'h', 'j', 'l', 'n', 'p', 'r', 't', 'v', 'x', 'z']\n</code></pre> <p>In questo caso, <code>alphabet</code> \u00e8 una stringa che ha tutte le ventisei lettere minuscole dell'alfabeto ASCII. Chiamare <code>even_items()</code> e passare <code>alphabet</code> restituisce una lista di lettere alternate dall'alfabeto.</p> <p>Le stringhe Python sono sequenze, che possono essere usate in cicli cos\u00ec come nell'indicizzazione intera e nello slicing. In caso di stringhe, possiamo usare delle parentesi quadre per ottenere la stessa funzionalit\u00e0 di <code>evne_items()</code> in modo pi\u00f9 efficiente:</p> <pre><code>&gt;&gt;&gt; list(alphabet[1::2])\n['b', 'd', 'f', 'h', 'j', 'l', 'n', 'p', 'r', 't', 'v', 'x', 'z']\n</code></pre> <p>Usando lo string slicing qui, diamo all'indice di partenza <code>1</code>, che corrisponde al secondo elemento. Non vi \u00e8 un indice finale dopo la prima virgola, per cui Python va fino alla fine della stringa. Quindi aggiungiamo la seconda virgola seguita da un 2, in modo che Python prenda ogni elemento pari.</p> <p>Tuttavia, come visto in precedenza, i generator e gli iterator non possono essere indicizzati o con slicing, per cui troveremo sempre utile <code>enumerate()</code>. Per continuare l'esempio precedente, possiamo creare una funzione generator che restituisce le lettere dell'alfabeto su richiesta:</p> <pre><code>&gt;&gt;&gt; def alphabet():\n...     alpha = \"abcdefghijklmnopqrstuvwxyz\"\n...     for a in alpha:\n...         yield a\n\n&gt;&gt;&gt; alphabet[1::2]\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: 'function' object is not subscriptable\n\n&gt;&gt;&gt; even_items(alphabet())\n['b', 'd', 'f', 'h', 'j', 'l', 'n', 'p', 'r', 't', 'v', 'x', 'z']\n</code></pre> <p>In questo esempio, definiamo <code>alphabet()</code>, una funzione generator che restituisce le lettere dell'alfabeto una ad una quando la funzione \u00e8 usata in un loop. Le funzioni Python, siano esse generator o funzioni regolari, non possono essere accedute mediante l'indicizzazione con parentesi quadre. Questo viene provato sulla seconda riga e lancia un TypeError.</p> <p>Possiamo usare le funzioni generator in dei loop, tuttavia, e lo facciamo sull'ultima riga passando <code>alphabet()</code> ad <code>even_items()</code>. Possiamo vedere che i risultati sono gli stessi dei due esempi precedenti.</p>"},{"location":"material/01_python/03_advanced/enumerate/#comprendere-la-funzione-enumerate","title":"Comprendere la funzione enumerate()","text":"<p>Nell'ultima parte abbiamo visto esempi di quando e come usare enumerate() a nostro vantaggio. Ora che abbiamo visto un assaggio degli aspetti pratici di enumerate(), possiamo capire di pi\u00f9 su come la funzione lavora internamente.</p> <p>Per ottenere un migliore comprensione di come funziona enumerate(), possiamo implementare la nostra versione con Python. La nostra versione di enumerate() ha due requisiti:</p> <ul> <li>accettare un iterabile ed un valore di conteggio iniziale come argomenti</li> <li>mandare indietro una tupla con l'attuale valore di conteggio e l'oggetto associato dall'iterabile</li> </ul> <p>Un modo di scrivere una funzione che rispetti queste specifiche \u00e8 data nella documentazione Python:</p> <pre><code>&gt;&gt;&gt; def my_enumerate(sequence, start=0):\n...     n = start\n...     for elem in sequence:\n...         yield n, elem\n...         n += 1\n...\n</code></pre> <p>La funzione <code>my_enumerate()</code> prende due argomenti, <code>sequence</code> e <code>start</code>. Il valore di default di start \u00e8 0. All'interno della definizione di funzione, si inizializza <code>n</code> per essere il valore di start ed eseguire un ciclo for nella sequenza.</p> <p>Per ogni elem nella sequenza, ci d\u00e0 il controlo alla posizione chiamante ed invia all'indietro il valore attuale di <code>n</code> ed <code>elem</code>. Infine, incrementiamo <code>n</code> per essere pronti all'iterazione successiva. Possiamo vedere <code>my_enumerate()</code> in azione:</p> <pre><code>&gt;&gt;&gt; seasons = [\"Spring\", \"Summer\", \"Fall\", \"Winter\"]\n\n&gt;&gt;&gt; my_enumerate(seasons)\n&lt;generator object my_enumerate at 0x7f48d7a9ca50&gt;\n\n&gt;&gt;&gt; list(my_enumerate(seasons))\n[(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]\n\n&gt;&gt;&gt; list(my_enumerate(seasons, start=1))\n[(1, 'Spring'), (2, 'Summer'), (3, 'Fall'), (4, 'Winter')]\n</code></pre> <p>Per prima cosa, creiamo una lista di quattro stagioni con cui lavorare. Inoltre, mostriamo che chiamare <code>my_enumerate()</code> con <code>seasons</code> in quanto la sequenza crea un generator. Questo \u00e8 perch\u00e9 usiamo la parola chaive <code>yield</code> per inviare i valori indietro al chiamante.</p> <p>Infine, creiamo due liste da <code>my_enumrate()</code>, uno nel quale il valore <code>start</code> \u00e8 lasciato di default (0) ed uno nel quale start \u00e8 cambiato ad 1. In entrambi i casi, finiamo con una lista di tuple nella quale il primo elemenot di ogni tupla \u00e8 il conteggio ed il secondo elemento \u00e8 il valore da seasons.</p> <p>Anche se possiamo implementare una funzione di equivalente per enumerate() in poche righe di codice Python, il codice vero e proprio di enumerate() \u00e8 scritto in C. Questo significa che \u00e8 molto veloce ed efficiente.</p>"},{"location":"material/01_python/03_advanced/enumerate/#spacchettare-gli-argomenti-con-enumerate","title":"Spacchettare gli argomenti con enumerate()","text":"<p>Quando usiamo enumeratre() in un ciclo for, diciamo a Python di usare due variabili, uno per il conteggio ed uno per il valore. Siamo abile di fare qeusto usando un concetto Python chiamato argument unpacking.</p> <p>TODO DA QUI</p> <p>Unpacking Arguments With enumerate() When you use enumerate() in a for loop, you tell Python to use two variables, one for the count and one for the value itself. You\u2019re able to do this by using a Python concept called argument unpacking.</p> <p>Argument unpacking is the idea that a tuple can be split into several variables depending on the length of the sequence. For instance, you can unpack a tuple of two elements into two variables:</p> <p>tuple2 = (10, \"a\") firstelem, secondelem = tuple2 firstelem 10 secondelem 'a' First, you create a tuple with two elements, 10 and \"a\". Then you unpack that tuple into firstelem and secondelem, which are each assigned one of the values from the tuple.</p> <p>When you call enumerate() and pass a sequence of values, Python returns an iterator. When you ask the iterator for its next value, it yields a tuple with two elements. The first element of the tuple is the count, and the second element is the value from the sequence that you passed:</p> <p>values = [\"a\", \"b\"] enuminstance = enumerate(values) enuminstance  next(enuminstance) (0, 'a') next(enuminstance) (1, 'b') next(enuminstance) Traceback (most recent call last):   File \"\", line 1, in  StopIteration In this example, you create a list called values with two elements, \"a\" and \"b\". Then you pass values to enumerate() and assign the return value to enuminstance. When you print enum_instance, you can see that it\u2019s an instance of enumerate() with a particular memory address. <p>Then you use Python\u2019s built-in next() to get the next value from enuminstance. The first value that enuminstance returns is a tuple with the count 0 and the first element from values, which is \"a\".</p> <p>Calling next() again on enuminstance yields another tuple, this time with the count 1 and the second element from values, \"b\". Finally, calling next() one more time raises StopIteration since there are no more values to be returned from enuminstance.</p> <p>When an iterable is used in a for loop, Python automatically calls next() at the start of every iteration until StopIteration is raised. Python assigns the value it retrieves from the iterable to the loop variable.</p> <p>If an iterable returns a tuple, then you can use argument unpacking to assign the elements of the tuple to multiple variables. This is what you did earlier in this tutorial by using two loop variables.</p> <p>Another time you might have seen argument unpacking with a for loop is with the built-in zip(), which allows you to iterate through two or more sequences at the same time. On each iteration, zip() returns a tuple that collects the elements from all the sequences that were passed:</p> <p>first = [\"a\", \"b\", \"c\"] second = [\"d\", \"e\", \"f\"] third = [\"g\", \"h\", \"i\"] for one, two, three in zip(first, second, third): ...     print(one, two, three) ... a d g b e h c f i By using zip(), you can iterate through first, second, and third at the same time. In the for loop, you assign the element from first to one, from second to two, and from third to three. Then you print the three values.</p> <p>You can combine zip() and enumerate() by using nested argument unpacking:</p> <p>for count, (one, two, three) in enumerate(zip(first, second, third)): ...     print(count, one, two, three) ... 0 a d g 1 b e h 2 c f i In the for loop in this example, you nest zip() inside enumerate(). This means that each time the for loop iterates, enumerate() yields a tuple with the first value as the count and the second value as another tuple containing the elements from the arguments to zip(). To unpack the nested structure, you need to add parentheses to capture the elements from the nested tuple of elements from zip().</p> <p>There are other ways to emulate the behavior of enumerate() combined with zip(). One method uses itertools.count(), which returns consecutive integers by default, starting at zero. You can change the previous example to use itertools.count():</p> <p>import itertools for count, one, two, three in zip(itertools.count(), first, second, third): ...     print(count, one, two, three) ... 0 a d g 1 b e h 2 c f i Using itertools.count() in this example allows you to use a single zip() call to generate the count as well as the loop variables without nested argument unpacking.</p> <p>Remove ads Conclusion Python\u2019s enumerate() lets you write Pythonic for loops when you need a count and the value from an iterable. The big advantage of enumerate() is that it returns a tuple with the counter and value, so you don\u2019t have to increment the counter yourself. It also gives you the option to change the starting value for the counter.</p> <p>In this tutorial, you learned how to:</p> <p>Use Python\u2019s enumerate() in your for loops Apply enumerate() in a few real-world examples Get values from enumerate() using argument unpacking Implement your own equivalent function to enumerate() You also saw enumerate() used in some real-world code, including within the CPython code repository. You now have the superpower of simplifying your loops and making your Python code stylish!</p>"},{"location":"material/01_python/03_advanced/exceptions/","title":"1.6 - La gestione degli errori","text":"<p>Nella maggior parte dei linguaggi interpretati, l'esecuzione di un programma termina non appena viene individuato un errore.</p> <p>In Python, esistono due tipi di errore: il primo \u00e8 quello sintattico, mentre il secondo \u00e8 chiamato eccezione.</p>"},{"location":"material/01_python/03_advanced/exceptions/#161-errori-sintattici-ed-eccezioni","title":"1.6.1 - Errori sintattici ed eccezioni","text":"<p>Gli errori di sintassi avvengono qunado il parser individua un'istruzione scritta in maniera non corretta. Ad esempio:</p> <pre><code>&gt;&gt;&gt; print(0/0))\n  File \"&lt;stdin&gt;\", line 1\n    print(0/0))\n              ^\nSyntaxError: unmatched ')'\n</code></pre> <p>In questo caso, notiamo la presenza di una parentesi di chiusura di troppo. Di conseguenza, Python lancia un <code>SyntaxError</code>, indicandoci anche dove \u00e8 occorso l'errore (in questo caso, mediante l'informazione <code>unmatched ')'</code>).</p> <p>Proviamo adesso a rimuovere la parentesi.</p> <pre><code>&gt;&gt;&gt; print(0/0)\n</code></pre> <p>Se proviamo ad eseguire questa istruzione, avremo l'altro tipo di errore, ovvero l'eccezione. In questo caso, infatti, il codice risulta essere (sintatticamente) corretto, ma \u00e8 comunque occorso un evento ritenuto \"impossibile\", che viene adeguatamente descritto nel <code>Traceback</code>:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nZeroDivisionError: division by zero\n</code></pre> <p>In particolare, notiamo come il traceback ci dia diverse informazioni:</p> <ul> <li>il file dove \u00e8 stata generata l'eccezione. In questo caso, viene riportato <code>&lt;stdin&gt;</code> perch\u00e9 il codice precedente \u00e8 stato inserito mediante interprete;</li> <li>la riga dove \u00e8 occorsa l'eccezione. In questo caso, notiamo che \u00e8 riportato <code>line 1</code>;</li> <li>il tipo di eccezione occorsa, con una breve descrizione dell'errore. In questo caso, notiamo come venga lanciata una <code>ZeroDivisionError</code>, che occorre quando il secondo argomento della divisione \u00e8 pari a 0.</li> </ul> <p>Eccezioni built-in</p> <p>Notiamo come l'eccezione <code>ZeroDivisionError</code> sia una built-in exception, ovvero un'eccezione gi\u00e0 integrata nel linguaggio Python, che comunque ci offre la possibilit\u00e0 di definire da noi nuovi tipi di eccezione. Per una panoramica completa, consultare la reference.</p>"},{"location":"material/01_python/03_advanced/exceptions/#162-lanciare-uneccezione","title":"1.6.2 - Lanciare un'eccezione","text":"<p>Possiamo lanciare una specifica eccezione all'occorrenza di una condizione non voluta all'interno del nosro codice utilizzando l'istruzione <code>raise</code>. Ad esempio, possiamo verificare che il valore di una variabile non sia superiore ad un dato numero:</p> <pre><code>x = 10\nif x &gt; 5:\n    raise Exception(f'x vale {x}. Non deve superare 5.')\n</code></pre> <p>Provando ad eseguire il codice precedente, avremo il seguente risultato:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 2, in &lt;module&gt;\nException: x vale 10. Non deve superare 5.\n</code></pre> <p>Dal traceback, notiamo come sia stata lanciata un'eccezione generica con il messaggio da noi elaborato.</p>"},{"location":"material/01_python/03_advanced/exceptions/#163-listruzione-assert","title":"1.6.3 - L'istruzione assert","text":"<p>Un altro modo di gestire situaizoni impreviste \u00e8 quello di utilizzare l'istruzione <code>assert</code>, che verifica la condizione passata come parametro. Se tale condizione risulta essere vera, il programma continuer\u00e0 la sua esecuzione; in alternativa, sar\u00e0 lanciata un'eccezione di tipo <code>AssertionError</code>.</p> <p>Possiamo, ad esempio, verificare che il nostro codice sia eseguito su un sistema Windows.</p> <pre><code>import sys\nassert ('win32' in sys.platform), 'Questo codice pu\u00f2 essere eseguito solo su una macchina Windows!'\n</code></pre> <p>Eseguendo questa istruzione su una macchina Windows, non sar\u00e0 lanciata alcuna eccezione, ed il programma continuer\u00e0 tranquillamente l'esecuzione. Eseguendo invece lo stesso codice su una macchina Unix, sar\u00e0 lanciata un'<code>AssertionError</code>, ed il programma terminer\u00e0:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nAssertionError: Questo codice pu\u00f2 essere eseguito solo su una macchina Windows!\n</code></pre>"},{"location":"material/01_python/03_advanced/exceptions/#164-gestione-delle-eccezioni-il-blocco-tryexcept","title":"1.6.4 - Gestione delle eccezioni: il blocco try/except","text":"<p>Abbiamo visto come le istruzioni <code>raise</code> ed <code>assert</code> permettano di verificare rispettivamente l'occorrenza di un errore o di una determinata condizione. Tuttavia, \u00e8 possibile gestire le situazioni in cui avviene un errore; per farlo, \u00e8 necessario utilizzare il blocco <code>try/except</code>.</p> <p>Nella pratica, durante l'esecuzione normale del programma, Python eseguir\u00e0 regolarmente le istruzioni contenute nella clausola <code>try</code>. Tuttavia, se viene riscontrata un'eccezione, il controllo passer\u00e0 direttamente alla clausola <code>except</code>, cui \u00e8 delegato il compito di gestirla. Questo funzionamento \u00e8 schematizzato nella figura successiva.</p> <pre><code>---\ntitle: Flusso in un blocco try/except\n---\nstateDiagram-v2\n    state if_state &lt;&lt;choice&gt;&gt;\n    state join_state &lt;&lt;join&gt;&gt;\n    \"Programma\" --&gt; \"Blocco try/except\"\n    \"Clausola try\" --&gt; if_state\n    if_state --&gt; \"Clausola except\": Eccezione\n    \"Clausola except\" --&gt; join_state\n    if_state --&gt; join_state\n    join_state --&gt; \"Programma\"</code></pre> <p>Notiamo come l'esecuzione del programma debba (idealmente) continuare, che la clausola <code>except</code> venga eseguita o meno. Proviamo a definire la funzione <code>controlla_sistema_operativo()</code>, che controlla se siamo su Windows:</p> <pre><code>def controlla_sistema_operativo():\n    assert ('win32' in sys.platform), 'Questo codice pu\u00f2 essere eseguito solo su una macchina Windows!'\n    print('Il sistema operativo \u00e8 Windows!')\n</code></pre> <p>Nella funzione <code>controlla_sistema_operativo()</code>, l'istruzione <code>assert</code> controlla che si sia su una macchina Windows; in caso contrario, sar\u00e0 lanciata un'eccezione di tipo <code>AssertionError</code>. Proviamo ad integrare la funzione all'interno di un blocco <code>try/except</code>:</p> <pre><code>try:\n    controlla_sistema_operativo()\nexcept:\n    pass\n</code></pre> <p>In questo caso, la gestione dell'errore sar\u00e0 delegata ad un'istruzione <code>pass</code> che, come sappiamo, non fa nulla. Eseguendo questo codice su una macchina Windows, sar\u00e0 mandato a schermo il messaggio <code>Il sistema operativo \u00e8 Windows!</code>, indice del fatto che l'<code>assert</code> non ha lanciato un'eccezione di tipo <code>AssertionError</code>. Se invece dovessimo eseguire questa funzione su una macchina Linux, non avremmo alcun risultato: infatti, la clausola <code>try</code> catturerebbe l'<code>AssertionError</code> e ne delegherebbe la gestione alla clausola <code>except</code>, che si limita ad eseguire un'istruzione <code>pass</code>.</p> <p>Proviamo a modificare il codice precedente come segue:</p> <pre><code>try:\n    controlla_sistema_operativo()\nexcept:\n    print('Sembra che non siamo su Windows!')\n</code></pre> <p>Eseguendo questo codice su macchina Linux, avremo il seguente messaggio:</p> <pre><code>Sembra che non siamo su Windows!\n</code></pre>"},{"location":"material/01_python/03_advanced/exceptions/#1641-gestione-di-eccezioni-specifiche","title":"1.6.4.1 - Gestione di eccezioni specifiche","text":"<p>Come \u00e8 possibile verificare dal codice precedente, qualora venga lanciata un'eccezione, non sar\u00e0 pi\u00f9 disponibile il Traceback visto in precedenza, bens\u00ec il messaggio <code>Sembra che non siamo su Windows!</code>. Ci\u00f2 comporta un problema: infatti, non avremo informazioni a riguardo di quale eccezione sia stata lanciata. Per recuperare queste informazioni, dovremo fare una modifica all'istruzione <code>except</code>:</p> <pre><code>try:\n    controlla_sistema_operativo()\nexcept AssertionError as error:\n    print(error)\n    print('Sembra che non siamo su Windows!')\n</code></pre> <p>Eseguendo questa funzione su macchina Linux, l'output sar\u00e0 stavolta il seguente:</p> <pre><code>Questo codice pu\u00f2 essere eseguito solo su una macchina Windows!\nSembra che non siamo su Windows!\n</code></pre> <p>Il primo messaggio, quindi, \u00e8 lanciato a valle della mancata verifica della condizione nella clausola <code>assert</code> specificata in <code>controlla_sistema_operativo()</code>, mentre il secondo \u00e8 direttamente specificato nella clausola <code>except</code>.</p> <p>Vediamo un altro esempio, nel quale proviamo ad aprire un file, gestendo l'eccezione lanciata quando questo non esiste:</p> <pre><code>try:\n    with open('file.log') as file:\n        read_data = file.read()\nexcept:\n    print('Impossibile aprire il file specificato.')\n</code></pre> <p>Eseguendo questo codice, una volta appurato che <code>file.log</code> non esiste, avremo il seguente messaggio:</p> <pre><code>Impossibile aprire il file specificato.\n</code></pre> <p>Nonostante il messaggio sia abbastanza informativo, stiamo comunque gestendo tutte le possibili eccezioni all'interno della clausola <code>except</code>. Cerchiamo quindi di trovarne una pi\u00f9 adatta al nostro scopo tra quelle specificate nella reference Python. In particolare, scegliamo l'eccezione FileNotFoundError, e modifichiamo il codice come segue:</p> <pre><code>try:\n    with open('file.log') as file:\n        read_data = file.read()\nexcept FileNotFoundError as error:\n    print(error)\n</code></pre> <p>In questo caso, se <code>file.log</code> non esiste, in output sar\u00e0 mandato il messaggio associato all'eccezione <code>FileNotFoundError</code>, ovvero:</p> <pre><code>[Errno 2] No such file or directory: 'file.log'\n</code></pre>"},{"location":"material/01_python/03_advanced/exceptions/#1642-eccezioni-multiple","title":"1.6.4.2 - Eccezioni multiple","text":"<p>A questo punto \u00e8 importante sottolineare come il blocco <code>try/except</code> sia in grado di gestire situazioni anche abbastanza complesse. Infatti, laddove \u00e8 evidente come la clausola <code>try</code> sia in grado di contenere diverse istruzioni, anche la clausola <code>except</code> pu\u00f2 gestire diverse tipologie di eccezione.</p> <p>In tal senso, proviamo a modificare il nostro codice, controllando dapprima il sistema operativo, e poi provando a leggere un file:</p> <pre><code>try:\n    controlla_sistema_operativo()\n    with open('file.log') as file:\n        read_data = file.read()\nexcept FileNotFoundError:\n    print('Il file specificato non esiste.')\nexcept AssertionError as error:\n    print(error)\n    print('Sembra che non siamo su Windows!')\n</code></pre> <p>Vediamo cosa succede in due diversi casi. Per prima cosa, eseguiamo il codice su macchina Linux, con <code>file.log</code> non esistente. In questo caso, il codice dar\u00e0 in output i seguenti messaggi:</p> <pre><code>Questo codice pu\u00f2 essere eseguito solo su una macchina Windows!\nSembra che non siamo su Windows!\n</code></pre> <p>In pratica, la prima istruzione del <code>try</code>, delegata al controllo del sistema operativo, riscontra un'eccezione, per cui viene chiamata la seconda clausola <code>except</code>. Nel caso invece si esegua il codice su macchina Windows, e supponendo sempre l'assenza di <code>file.log</code>, il risultato sar\u00e0 il seguente:</p> <pre><code>Siamo su Windows!\nIl file specificato non esiste.\n</code></pre> <p>In questo caso, la clausola <code>try</code> non ha riscontrato problemi nell'esecuzione della prima istruzione. Tuttavia, l'eccezione \u00e8 stata lanciata in corrispondenza della fase di lettura del file, lanciando un'eccezione gestita dalla prima clausola <code>except</code>.</p>"},{"location":"material/01_python/03_advanced/exceptions/#165-la-clausola-else","title":"1.6.5 - La clausola else","text":"<p>La clausola <code>else</code> pu\u00f2 essere usata in abbinamento al blocco <code>try/except</code> per eseguire un certo blocco di codice soltanto in assenza di eccezioni. Ad esempio:</p> <pre><code>try:\n    controlla_sistema_operativo()\nexcept AssertionError as error:\n    print(error)\nelse:\n    print('Il sistema operativo \u00e8 Windows!')\n</code></pre> <p>Eseguendo questo codice su Windows, il programma non riscontrer\u00e0 alcuna eccezione, e l'output sar\u00e0 quello esplicitato nella clausola <code>else</code>:</p> <pre><code>Siamo su Windows!\nIl sistema operativo \u00e8 Windows!\n</code></pre> <p>Potremmo usare una clausola <code>else</code> per gestire ulteriori eccezioni. Ad esempio, proviamo ad eseguire questo blocco di codice su una macchina Windows (e senza il file <code>file.log</code>):</p> <pre><code>try:\n    controlla_sistema_operativo()\nexcept AssertionError as error:\n    print(error)\nelse:\n    try:\n        with open('file.log') as file:\n            read_data = file.read()\n    except FileNotFoundError:\n        print('Il file specificato non esiste.')\n</code></pre> <p>Il risultato sar\u00e0:</p> <pre><code>Siamo su Windows!\nIl file specificato non esiste.\n</code></pre> <p>Suggerimento</p> <p>Ricordiamo sempre che la clausola <code>else</code> viene eseguita soltanto se non sono state trovate eccezioni. Se \u00e8 stata trovata un'eccezione, e gestita nell'<code>except</code>, il flusso del programma continuer\u00e0 comunque la sua regolare esecuzione, ma l'<code>else</code> non sar\u00e0 invocato.</p>"},{"location":"material/01_python/03_advanced/exceptions/#166-la-clausola-finally","title":"1.6.6 - La clausola finally","text":"<p>La clausola <code>finally</code> ci permette di eseguire delle istruzioni indipendentemente dal fatto che siano state riscontrate o meno eccezioni in un blocco <code>try/except</code> antecedente. Andiamo a modificare leggermente il codice relativo all'ultimo esempio:</p> <pre><code>try:\n    su_windows()\nexcept AssertionError as error:\n    print(error)\nelse:\n    try:\n        with open('file.log') as file:\n            read_data = file.read()\n    except FileNotFoundError:\n        print('Il file specificato non esiste.')\nfinally:\n    print('Istruzioni eseguite indipendentemente dal resto del programma.')\n</code></pre> <p>In questo caso, ci\u00f2 che viene inserito nella clausola <code>finally</code> sar\u00e0 sempre eseguito, indipendentemente da ci\u00f2 che \u00e8 accaduto nel codice precedente. Provando ad eseguire questo codice su macchina Windows in assenza di <code>file.log</code>, avremo il seguente output:</p> <pre><code>Siamo su Windows!\nIl file specificato non esiste.\nIstruzioni eseguite indipendentemente dal resto del programma.\n</code></pre> <p>L'utilit\u00e0 della clausola <code>finally</code></p> <p>La clausola <code>finally</code> trova utilit\u00e0 in tutte quelle situazioni nelle quali \u00e8 necessario effettuare delle operazioni a valle della gestione dell'eccezione. Queste operazioni sono spesso definite \"di pulizia\", perch\u00e9 prevedono l'eliminazione di stati o variabili che possono occupare memoria e che non sono pi\u00f9 utili ai fini dell'esecuzione del programma.</p>"},{"location":"material/01_python/03_advanced/exceptions/#166-per-ricapitolare","title":"1.6.6 - Per ricapitolare...","text":"<p>In questa lezione, abbiamo:</p> <ul> <li>visto la differenza tra errore sintattico ed eccezione;</li> <li>utilizzato l'istruzione <code>raise</code> per lanciare un'eccezione in qualsiasi momento;</li> <li>utilizzato l'istruzione <code>assert</code> per verificare il rispetto di una determinata condizione, lanciando in caso contrario un <code>AssertionError</code>;</li> <li>utilizzato la clausola <code>try</code> per verificare l'occorrenza di eccezioni all'interno di un blocco di istruzioni;</li> <li>utilizzato la clausola <code>except</code> per catturare e gestire una o pi\u00f9 delle eccezioni trovate nella clausola <code>try</code> corrispondente;</li> <li>utilizzato la clausola <code>else</code> per eseguire blocchi di codice esclusivamente nel caso non siano state individuate eccezioni nel <code>try</code> precedente;</li> <li>utilizzato la clausola <code>finally</code> per eseguire blocchi di codice indipendentemente dal fatto che siano state individuate o meno eccezioni.</li> </ul> <p>Nella prossima lezione andremo ad introdurre i principi alla base della programmazione funzionale.</p>"},{"location":"material/01_python/03_advanced/func_prog/","title":"1.7: La programmazione funzionale","text":"<p>La programmazione funzionale \u00e8 un paradigma di programmazione che definisce l'elaborazione del dato attraverso e concetti di funzione e stato immutabile.</p> <p>Per comprendere cosa questo comporti, \u00e8 utile pensare alla programmazione imperativa. In questo paradigma, infatti, l'elaborazione avviene mediante le singole istruzioni scritte nel codice sorgente, le quali consistono di una serie di comandi la cui esecuzione modifica il valore di una variabile e, conseguentemente, lo stato del programma. Ad esempio, un ciclo esegue ripetutamente una certa istruzione, cambiando di volta in volta il valore di una variabile:</p> <pre><code>contatore = 0\nfor i in range(10):\n    contatore += 1\n</code></pre> <p>Man mano che il valore di <code>contatore</code> aumenta, lo stato delle variabili varia di conseguenza. Di contro, la programmazione funzionale elimina il concetto di stato. Invece di modificare i valori delle variabili, questo tipo di programmazione lavora soltanto su tipi immutabili che, come sappiamo, non possono essere alterati. Di conseguenza, la programmazione funzionale lavora esclusivamente su copie della strutture dati originarie. Inoltre, mentre nella programmazione imperativa usiamo normalmente costrutti come sequenze, cicli ed iterazioni, nella programmazione funzionale usiamo esclusivamente funzioni per modificare i dati, il che risulta in soluzioni pi\u00f9 eleganti, maggiormente modulari, e potenzialmente pi\u00f9 efficienti.</p> <p>Prima di iniziare a parlare di programmazione funzionale in Python, tuttavia, \u00e8 necessario introdurre alcuni concetti fondamentali.</p>"},{"location":"material/01_python/03_advanced/func_prog/#171-le-funzioni-di-ordine-superiore","title":"1.7.1 - Le funzioni di ordine superiore","text":"<p>Le funzioni di ordine superiore sono ampiamente utilizzate nella programmazione funzionale, e non sono altro che funzioni che possono a loro volta accettare una funzione come argomento, o restituire una funzione in uscita. </p> <p>Le funzioni di ordine superiore sono lo strumento principale per definire l'elaborazione nella programmazione funzionale. Queste sono funzioni che possono accettare una funzione come argomento, o restituire una funzione in uscita. In Python, <code>reduce()</code>, <code>map()</code> e <code>filter()</code> sono alcune tra le pi\u00f9 importanti funzioni di ordine superiore. Quando combinate con funzioni pi\u00f9 semplici, possono essere usate epr eseguire operazioni complesse.</p> <p>Vediamo un esempio di funzione di ordine superiore. In particoalre, <code>stampa_saluto()</code> accetta una funzione <code>f</code> ed una stringa <code>n</code> come argomenti, e restituisce la chiamata alla funzione <code>f</code> con argomento <code>n</code>, ovvero <code>f(n)</code>.</p> <pre><code>&gt;&gt;&gt; def saluto(nome):\n...     return f'Buongiorno, {}!'\n...\n&gt;&gt;&gt; def stampa_saluto(f, n):\n...     print(f(n))\n...\n&gt;&gt;&gt; stampa_saluto(saluto, 'Mondo')\nBuongiorno, Mondo!\n</code></pre>"},{"location":"material/01_python/03_advanced/func_prog/#funzioni-anonime","title":"Funzioni anonime","text":"<p>Abbiamo visto un esempio di una funzione che accetta come argomento un'altra funzione. <code>map()</code>, <code>filter()</code> e <code>reduce()</code> funzionano allo stesso modo: ognuno di loro accetta una funzione ed una sequenza di elementi, restituendo il risutato dell'applicazione della funzi0one ricevuta ad ogni elemento della sequenza. Nei due esempi precedenti, abbiamo definito le nostre funzioni suando la parola chiave <code>def</code> di Python. Questo crea un oggetto Python chiamabile che esegue l'istruzione specificata all'interno della sua definizione ogni volta che lo chiamiamo.</p> <p>Tuttavia, nella programmazione funzionale, le funzioni anonime (anche chiamate funzioni lambda) sono preferite. Ne parleremo estesamente nella prossima lezione. Tuttavia, per ora ci basti sapere che sono chiamate anonime perch\u00e9 non sono \"collegate\" ad alcun nome, allineandosi quindi alla tendenza della programmaizone funzionale verso l'assenza di stato. In pratica, la differenza principale tra le funzioni lambda e le funzioni Python normali sta nel fatto che la funzione lambda valuta una singola espressione restituendo una funzione, mentre le funzioni normali non lo fanno (necessariamente). In altri termini, le funzioni lambda non possono usare delle istruzioni come le condizionali, o anche un semplice <code>return</code>. Ecco un esempio di funzione lambda:</p> <pre><code>&gt;&gt;&gt; (lambda x: x + 1)(5)\n6\n</code></pre> <p>Potremmo definire questa operazione usando la parola chiave <code>def</code> ed otterremmo lo stesso output:</p> <pre><code>&gt;&gt;&gt; def aggiungi_uno(x):\n...     return x + 1\n...\n&gt;&gt;&gt; aggiungi_uno(5)\n6\n</code></pre> <p>In pratica, nel primo snippet, abbiamo usato la keyword <code>lambda</code>per definire una funzione inline, chiamandola con argomento <code>x = 5</code>. La funzione \u00e8 stata immediatamente valutata, ed un output prodotto. A differenza del secondo snippet, la funzione lambda non era legata ad un nome, per cui, per utilizzarla nuovamente, dovremmo riscrivere l'istruzione.</p> <p>Le funzioni lambda sono importanti per <code>map()</code>, <code>filter()</code> e <code>reduce()</code> perch\u00e9 gli argomenti che passiamo a queste funzioni sono spesso funzioni brevi che devono essere usate soltanto una volta nei nostri programmi, per cui non vi \u00e8 alcun motivo per salvarle. A differenza delle funzioni regolari che devono essere definite e salvate in memoria, le funzioni anonime sono pi\u00f9 concise, e vengono eliminate dopo l'esecuzione.</p> <p>Ora che abbiamo visto i preliminari, diamo uno sguardo pi\u00f9 approfondito alle funzioni <code>map()</code>, <code>filter()</code> e <code>reduce()</code>.</p>"},{"location":"material/01_python/03_advanced/func_prog/#map","title":"map()","text":"<p>La funzione <code>map()</code> di Python ha la seguente sintassi:</p> <pre><code>map(function, *sequence)\n</code></pre> <p>In pratica, <code>map()</code> applica la funzione ricevuta al primo argomento ad ogni elemento nella sequenza <code>sequence</code>, restituendo la sequenza risultatnte. Nell'esempio successivo, creiamo una lista di interi ed usiamo <code>map()</code> e la funzione <code>str</code> di Python per convertire ogni intero in stringhe.</p> <pre><code>&gt;&gt;&gt; seq = list(range(10))\n&gt;&gt;&gt; list(map(str, seq))\n['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n</code></pre> <p>Notiamo come i risultati restituiti dalla funzione <code>map()</code> siano a loro volta racchiusi in una lista. Questo \u00e8 legato al fatto che, in realt\u00e0, la funzione <code>map()</code> restituisce un generator.</p> <pre><code>&gt;&gt;&gt; map(str, seq)\n&lt;map object at 0x000001BB7143F100&gt;\n</code></pre> <p>I generatori Python adottano una astrategia conosciutra come lazy evaluation, il che implica che Python non conosce il valore di un oggetto fino a che non lo usa. Anche se pu\u00f2 sembrare controintuitivo, valuter un oggetto in maniera \"lazy\" permette di aumentare l'efficienza del codice riducendo il numero di elabroazioni non necessarie. Questo \u00e8 il motivo per cui abbiamo racchiuso il generator creato da <code>map()</code> in una lista: farlo valuta l'espressione del generatore, e permette di inserire i risultati in un oggetto di tipo lista facilmente accessibile.</p> <p>Nota</p> <p>Le list comprehension hanno lo stesso scopo della funzione <code>map()</code>. Tuttavia, in questo caso, non abbiamo lazy evaluation.</p>"},{"location":"material/01_python/03_advanced/func_prog/#filter","title":"filter()","text":"<p>La funzione <code>filter()</code> di Python ha la seguente sintassi:</p> <pre><code>filter(function, *sequence)\n</code></pre> <p>In questo caso, filter() prende una funzione chiamata predicato che restituisce <code>True</code> se un elemento soddisfa la condizione definita nel predicato, e <code>Falso</code>altrimenti. L'output della funzione <code>filter()</code>consiste degli elementi della sequenza originaria che soddisfano il predicato.</p> <p>Nel codice seguente, usiamo una funzione <code>filter()</code> su una sequenza di nomi per individuare quelli che iniziano per <code>A</code>:</p> <pre><code>&gt;&gt;&gt; nomi = ['Angelo', 'Andrea', 'Giovanni', 'Paolo', 'Mario', 'Luca']\n&gt;&gt;&gt; list(filter(lambda x: x[0] == 'A', nomi))\n['Angelo', 'Andrea']\n</code></pre> <p>I nomi per i quali la funzione lambda restituisce <code>True</code> sono Angelo ed Andrea, come prevedibile. Il resto della sequenza viene quindi trascurato.</p> <p>Nota</p> <p>Anche in questo caso avremmo potuto utilizzare una list comprehension, ottenendo una soluzione sicuramente maggiormente pythonic.</p> <p>Potremmo anche concatenare le funzioni <code>map()</code> e <code>filter()</code> nella stessa chiamata. Se, ad esempio, volessimo ottenere i nomi che iniziano per <code>A</code> e convertirli in maiuscolo, potremmo usare la seguente istruzione:</p> <pre><code>&gt;&gt;&gt; list(map(lambda x: x.upper(), filter(lambda x: x[0] == 'A', nomi)))\n['ANGELO', 'ANDREA']\n</code></pre> <p>In pratica, stiamo mappando la funzione lambda <code>x: x.upper()</code>, che trasforma le stringhe in maiuscolo, sui risultati della <code>filter()</code> precedentemente considerata.</p>"},{"location":"material/01_python/03_advanced/func_prog/#reduce","title":"reduce()","text":"<p>La funzione <code>reduce()</code> non restituisce una nuova sequenza come <code>map()</code> o <code>filter()</code>. Invece, restituisce un singolo valreo. La sintassi \u00e8 simile a quella delle altre due funzioni:</p> <pre><code>reduce(function, *sequence)\n</code></pre> <p>In questo caso, <code>reduce()</code> applica la funzione agli elementi della sequenza, da sinsitra verso destra, iniziando con i primi due elementi della sequenza. Combiniamo quindi i risultati dell\u00ec'applciazione della funzione ai primi due elmeenti della sequezna con il terzo elemnto, passandolo ad un'altra chiamata alla stessa funzione. Questo processo si ripete fino a che non raggiungiamo la fine dell'iteratore, e questo viene ridotto ad un singolo valore.</p> <p>Vediamo un esempio dove <code>reduce()</code> prende come argomento una funzione lambda che moltiplica due numeri consecutivamente:</p> <pre><code>&gt;&gt;&gt; from functools import reduce\n&gt;&gt;&gt; numeri = list(range(1, 5))\n&gt;&gt;&gt; numeri\n[1, 2, 3, 4]\n&gt;&gt;&gt; reduce(lambda x, y: x*y, numeri)\n24\n</code></pre> <p>Nota</p> <p>Notiamo che, a differenza di <code>map()</code> e <code>filter()</code>, la funzione <code>reduce()</code> non \u00e8 integrata in Python, ma deve essere importata dalla libreria <code>functools</code>.</p> <p>Se volessimo scrivere questa stessa funzione ricorrendo ad un approccio classic, potremmo usare un iteratore, oppure una serie ricorsiva di chiamate a funzione. Ad esempio:</p> <pre><code>&gt;&gt;&gt; def moltiplica(x, y):\n...     return x*y\n...\n&gt;&gt;&gt; moltiplica(moltiplica(moltiplica(4, 3), 2), 1) \n24\n</code></pre>"},{"location":"material/01_python/03_advanced/lambda/","title":"1.8 - Il calcolo lambda e la programmazione funzionale","text":"<p>Le espressioni lambda, le quali saranno oggetto di questa lezione, affondano le loro origini nel calcolo lambda, modello computazionale inventato da Alonzo Church negli anni '30 del ventesimo secolo.</p> <p>Il calcolo lambda \u00e8 un linguaggio puramente basato sul concetto di astrazione, ed \u00e8 in grado di codificare ogni possibile algoritmo. Di conseguenza, risulta essere Turing-completo ma, contrariamente alla macchina di Turing, non mantiene al suo interno alcuna informazione sullo stato.</p> <p>Da quello che abbiamo visto nella lezione precedente, quindi, appare chiaro come la programmazione funzionale affondi le sue origini in questo tipo di calcolo. Contestualmente, possiamo dedurre come il modello computazionale basato sullo stato ed introdotto da Alan Turing sia alla base del paradigma imperativo.</p> <p>Tesi di Church-Turing</p> <p>I modelli basati sul calcolo lambda possono essere tradotti in quelli basati sulle macchine di Turing, e viceversa. Questa equivalenza \u00e8 anche conosciuta come ipotesi di Church-Turing.</p>"},{"location":"material/01_python/03_advanced/lambda/#181-le-funzioni-lambda","title":"1.8.1 - Le funzioni lambda","text":"<p>Le funzioni lambda, chiamate anche funzioni anonime, sono dei costrutti che permettono una sintassi pi\u00f9 concisa rispetto alle normali funzioni e, nonostante una serie di restrizioni, lavorano assieme ai concetti visti in precedenza per implementare meccanismi di programmazione funzionale.</p> <p>Le funzioni lambda e le best practice Python</p> <p>Alcuni degli esempi che faremo ignoreranno deliberatamente le best practice definite da Python. Tuttavia, ci saranno utili ad illustrare i concetti alla base delle funzioni lambda.</p>"},{"location":"material/01_python/03_advanced/lambda/#1811-le-prime-funzioni-lambda","title":"1.8.1.1 - Le prime funzioni lambda","text":"<p>Iniziamo con un banale esempio, definendo una funzione <code>identita()</code> che restituisce l'argomento passatogli.</p> <pre><code>def identita(x):\n    return x\n</code></pre> <p>Proviamo a definire la stessa funzione usando le lambda:</p> <pre><code>lambda x: x\n</code></pre> <p>Notiamo quindi che l'espressione \u00e8 composta da:</p> <ul> <li>la keyword <code>lambda</code>, che indica l'inizio della funzione lambda;</li> <li>la variabile <code>x</code>, che rappresenta l'argomento in ingresso alla funzione lambda;</li> <li>il corpo della funzione, che segue l'operatore <code>:</code>, ed in questo caso \u00e8 semplicemente <code>x</code>.</li> </ul> <p>Possiamo definire la variabile <code>x</code> come dipendente, che si differenzia da una variabile indipendente a causa del fatto che quest'ultima non \u00e8 vincolata, e pu\u00f2 essere referenziata nel corpo dell'espressione. Proviamo in tal senso a scrivere un esempio leggermente pi\u00f9 elaborato, definendo una funzione <code>incrementa()</code> che aggiunge <code>1</code> al suo argomento, assieme alla corrispondente funzione lambda:</p> Funzione classicaFunzione lambda <pre><code>def incrementa(x):\n    return x + 1\n</code></pre> <pre><code>lambda x: x + 1\n</code></pre>"},{"location":"material/01_python/03_advanced/lambda/#1812-argomenti-ed-identificativi","title":"1.8.1.2 - Argomenti ed identificativi","text":"<p>Proviamo adesso ad applicare la funzione lambda definita in precedenza. Per farlo, dovremo circondare sia la funzione, sia l'argomento che vogliamo passare, tra parentesi tonde:</p> <pre><code>&gt;&gt;&gt; (lambda x: x + 1)(2)\n3\n</code></pre> <p>Come funziona?</p> <p>I pi\u00f9 curiosi potranno interrogarsi sul funzionamento interno della funzione lambda. Uno dei possibili approcci in tal senso \u00e8 quello della riduzione: in pratica, andiamo a rimpiazzare la variabile dipendente, <code>x</code> con l'argomento, <code>2</code>, ottenendo il seguente risultato:</p> <pre><code>(lambda x: x + 1)(2) = lambda 2: 2 + 1\n                    = 2 + 1\n                    = 3\n</code></pre> <p>Possiamo anche associare un identificativo alla funzione lambda. Per farlo, possiamo usare una sintassi di questo tipo:</p> <pre><code>incrementa = lambda x: x + 1\n</code></pre> <p>Le funzioni che abbiamo definito finora accettano un unico argomento. Tuttavia, potremmo aver notato che, nella definizione delle funzioni lambda, gli argomenti non sono circondati da parentesi: per specificarne pi\u00f9 di uno, dovremo elencarli separandoli mediante una virgola. Ad esempio:</p> <pre><code>nome_cognome = lambda nome, cognome: f'{nome.title()} {cognome.title()}'\n</code></pre> <p>A questo punto, potremo invocare la funzione come segue:</p> <pre><code>&gt;&gt;&gt; nome_cognome('guido', 'van rossum')\nGuido Van Rossum\n</code></pre> <p>Ricapitolando, la funzione lambda con identificativo <code>nome_cognome</code> accetter\u00e0 due argomenti, rispettivamente <code>nome</code> e <code>cognome</code>, restituendo una stringa formattata. Come abbiamo gi\u00e0 sottolineato, nella definizione della funzione lambda gli argomenti sono elencati senza parentesi, mentre la chiamata alla funzione lambda \u00e8 fatta esattamente come se avessimo a che fare con una classica funzione Python, con delle parentesi che circondano gli argomenti utilizzati.</p>"},{"location":"material/01_python/03_advanced/lambda/#funzioni-anonime","title":"Funzioni anonime","text":"<p>I seguenti termini possono essere usati in maniera intercambiabile a seconda del tipo di linguaggio di programmazione: funzioni anonime, funzioni lambda, espressioni lambda, astrazioni lambda, e via dicendo.</p> <p>Prese letteralmente, una funzione anonima \u00e8 una funzione senza un nome. In Python, una funzione anonima viene creata mediante la parola chiave <code>lambda</code>. Blandamente, pu\u00f2 o meno avere un nome. Considerando una funzione anonima a due argomenti definita con la lambda ma non vincolata ad una variabile. La lambda non ha un nome:</p> <pre><code>&gt;&gt;&gt; lambda x, y: x + y\n</code></pre> <p>La funzione precedente definisce un'espressione lambda che rpende due argomenti e restituisce la loro somma.</p> <p>Oltre chje fornirci il feedback che Python \u00e8 perfettamente ok con questa forma, non ha alcun uso pratico. Possiamo invocare la funzione nell'interprete Python:</p> <pre><code>&gt;&gt;&gt; _(1, 2)\n3\n</code></pre> <p>L'esempio precedente sfrutta la feature interattiva dell'interprete mediante l'underscore. Tuttavia, non possiamo scrivere del codice simile in un modulo Python: qui, dovremmo assegnare un nome alla lambda, o passare la lambda ad una funzione. Vedremo in seguito come fare.</p> <p>Nota</p> <p>In una sessione interattiva con l'interprete Python, il singolo underscore \u00e8 collegato all'ultima espressione valutata. Nell'esempio precedente, quindi, il <code>_</code> putna alla funzione lambda.</p> <p>Un altro pattern usato in altri linguaggi come JavaScript \u00e8 quello di usare immediatamente una funzione lambda Python. Questo \u00e8 conosciuto come Immediately Invoked Function Expression (IIFE). Ecco un esempio</p> <pre><code>&gt;&gt;&gt; (lambda x, y: x + y)(2, 3)\n5\n</code></pre> <p>La funzione lambda precedente \u00e8 definita e quindi chiuamata immediatamente con due argomenti, ovvero 2 e 3. Restituisce il valore 5, che \u00e8 la somma degli argomenti.</p>"},{"location":"material/01_python/03_advanced/lambda/#lambda-e-higher-order-functions","title":"lambda e higher order functions","text":"<p>Le funzioni lambda sono di frequente usate con le higher-order functions, che prendono una o pi\u00f9 funzioni come argomenti e restituiscono una o pi\u00f9 funzioni.</p> <p>Una funzione lambda pu\u00f2 essere una higher-order function prendendo una funzione (normale o lambda) come argomento, come nell'esempio successivo.</p> <pre><code>&gt;&gt;&gt; high_ord_func = lambda x, func: x + func(x)\n&gt;&gt;&gt; high_ord_func(2, lambda x: x * x)\n6\n&gt;&gt;&gt; high_ord_func(2, lambda x: x + 3)\n7\n</code></pre> <p>Python espone le higher-order function come funzioni built-in, o nella libreria standard. Esempi includono map(), filter(), reduce(), cos\u00ec come funzioni chiave come sort(), sorted(), min(), e max(). </p>"},{"location":"material/01_python/03_advanced/lambda/#lambda-e-funzioni-regolari","title":"Lambda e funzioni regolari","text":"<p>A differenza delle forme lambda in altri linguaggi, dove aggiungono nuove funzionalit\u00e0, le lambda Python sono solo una notazione abbrievata se siamo troppo pigri per definire una funzione.</p> <p>Nonostante questo, non dobbiamo fare in modo che questa definizione ci faccia desistere dall'usare le lambda function in Python. Di primo acchitto, potremmo accettare che una funzione lambda sia una funzione che ci permetta semplicemente di ridurre il codice necessario a definire o invocare una funzione. Vedremo per\u00f2 a breve che ci sono delle sottili differenze tra le normali funzioni Python e le funzioni lambda.</p>"},{"location":"material/01_python/03_advanced/lambda/#funzioni","title":"Funzioni","text":"<p>A questo punto, potremmo chiederci cosa distingue fondamentalmente una funzione lambda vincolata ad una variabile da una regolare funzione con una singola riga di ritorno: sotto il cofano, praticamente niente. Verifichaimo \u00e8per\u00f2 come Python vede una funzione costruita con una singola istruzione di ritorno rispettoa d una funzione costrrita come un'espressione lambda.</p> <p>Il modulo dis espone delle funzioni per analizzare il bytecode Python generato dal compilatore.</p> <pre><code>&gt;&gt;&gt; import dis\n&gt;&gt;&gt; add = lambda x, y: x + y\n&gt;&gt;&gt; type(add)\n&lt;class 'function'&gt;\n&gt;&gt;&gt; dis.dis(add)\n  1           0 LOAD_FAST                0 (x)\n              2 LOAD_FAST                1 (y)\n              4 BINARY_ADD\n              6 RETURN_VALUE\n&gt;&gt;&gt; add\n&lt;function &lt;lambda&gt; at 0x7f30c6ce9ea0&gt;\n</code></pre> <p>Possiamo vedere che <code>dis()</code> mostra una versione leggibile del bytecode Python, permettendo l'ispezione delle istruzioni a basso livello che l'interprete Pythjon usa mentre esegue il programma.</p> <p>Ora vediamo una funzione classica:</p> <pre><code>&gt;&gt;&gt; import dis\n&gt;&gt;&gt; def add(x, y): return x + y\n&gt;&gt;&gt; type(add)\n&lt;class 'function'&gt;\n&gt;&gt;&gt; dis.dis(add)\n  1           0 LOAD_FAST                0 (x)\n              2 LOAD_FAST                1 (y)\n              4 BINARY_ADD\n              6 RETURN_VALUE\n&gt;&gt;&gt; add\n&lt;function add at 0x7f30c6ce9f28&gt;\n</code></pre> <p>Il bytecode interpretato da Python \u00e8 lo stesso per entrambe le funzioni. Tuttavia, possiamo notare che il nome \u00e8 differente: il nome della funzione \u00e8 add per la funzione definita mediante def, mentre la funzione lambda Python \u00e8 vista come lambda.</p>"},{"location":"material/01_python/03_advanced/lambda/#traceback","title":"traceback","text":"<p>ABbiamo visto che, nel contesto della funzione lambda, Python non ha fornito il nome della funzione, ma solo . Questa pu\u00f2 essere una limitazione da considerare quando vi \u00e8 un'eccezione, ed un traceback mostra soltanto :  <pre><code>&gt;&gt;&gt; div_zero = lambda x: x / 0\n&gt;&gt;&gt; div_zero(2)\nTraceback (most recent call last):\n    File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n    File \"&lt;stdin&gt;\", line 1, in &lt;lambda&gt;\nZeroDivisionError: division by zero\n</code></pre> <p>Ecco la stessa eccezione lanciata da una funzione normale:</p> <pre><code>&gt;&gt;&gt; def div_zero(x): return x / 0\n&gt;&gt;&gt; div_zero(2)\nTraceback (most recent call last):\n    File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n    File \"&lt;stdin&gt;\", line 1, in div_zero\nZeroDivisionError: division by zero\n</code></pre> <p>La funzione normale causa un errore simile, ma risulta in un traceback pi\u00f9 preceiso, perch\u00e9 d\u00e0 il nome della funzione (<code>div_zero</code>).</p>"},{"location":"material/01_python/03_advanced/lambda/#sintassi","title":"Sintassi","text":"<p>Come abbiamo visto nelle sezioni precedente, una funzione lambda presenta delle distinzioni sintattiche da una funzione normale. In particolare, una funzione lambda ha le seguenti caratteristiche:</p> <ul> <li>pu\u00f2 contenre soltanto espressioni e non include istruzioni nel suo corpo;</li> <li>\u00e8 scritta come una singola riga di esecuzione;</li> <li>non supporta il type hinting;</li> <li>pu\u00f2 essere invocata immedaitamente.</li> </ul>"},{"location":"material/01_python/03_advanced/lambda/#nessuna-istruzione","title":"Nessuna istruzione","text":"<p>Una funzione lambda non pu\u00f2 contenere alcuna istruzione. In una funzione lambda, le istruizoni come return, pass, assert o raise lanceranno un'eccezione SyntaxError. Ecco un esempio dell'aggiunta di assert al corpo di una lambda:</p> <pre><code>&gt;&gt;&gt; (lambda x: assert x == 2)(2)\n  File \"&lt;input&gt;\", line 1\n    (lambda x: assert x == 2)(2)\n                    ^\nSyntaxError: invalid syntax\n</code></pre> <p>Questo conciso esempio intendeva asserire che il parametro <code>x</code> ha un valore pari a 2. Tuttavia, l'interprete identifica un SyntaxError mentre effettua il parsing del codice che coinvolge l'assert nel corpo della lambda.</p>"},{"location":"material/01_python/03_advanced/lambda/#singola-espressione","title":"Singola espressione","text":"<p>Invece di una funzione normale, una funzione lambda Python \u00e8 una singola espressione. Tuttavia, nel corpo di una lambda, possiamo suddividere l'espressione in varie righe usando parentesi o stringhe multilinea; ci\u00f2nonostante, rimane una singola espressione:</p> <pre><code>&gt;&gt;&gt; (lambda x:\n... (x % 2 and 'odd' or 'even'))(3)\n'odd'\n</code></pre> <p>L'esempio precedente restituisce la stringa <code>odd</code> quando l'argomento della lambda \u00e8 dispari, e pari quando l'argomento \u00e8 pari. E' suddivisa tra due linee perch\u00e9 \u00e8 contenuta in un insieme di parentesi, ma rimane una singola espressione.</p>"},{"location":"material/01_python/03_advanced/lambda/#type-hinting","title":"Type hinting","text":"<p>Se abbiamo iniziato ad adottare il type hiunting, abbiamo un'altra ragione per preferire le funzioni normali alle lambda function Python. In una funzione lambda, infatti, questa espressione non ha equivalenti:</p> <pre><code>def full_name(first: str, last: str) -&gt; str:\n    return f'{first.title()} {last.title()}'\n</code></pre> <p>Un qualsiasi errore di tipo pu\u00f2 essere catturato da strumetni come mypy o pyre, ed un SyntaxError con l'equivalente funzione lamb da viuene lanciato a runtime:</p> <pre><code>&gt;&gt;&gt; lambda first: str, last: str: first.title() + \" \" + last.title() -&gt; str\n  File \"&lt;stdin&gt;\", line 1\n    lambda first: str, last: str: first.title() + \" \" + last.title() -&gt; str\n\nSyntaxError: invalid syntax\n</code></pre> <p>In pratica, provare ad inserire del type hinting risulta immedaitamente in un SyntaxError a runtime.</p>"},{"location":"material/01_python/03_advanced/lambda/#iife","title":"IIFE","text":"<p>Abbiamo gi\u00e0 visto degli esempi di IIFE:</p> <pre><code>&gt;&gt;&gt; (lambda x: x * x)(3)\n9\n</code></pre> <p>Al di furoi dell'interprete Pyuthon, questa feature non viene praticamente usata in pratica. E' infatti una conseguenza diretta del fatto che la funzione lambda \u00e8 chiamabile come viene definita. Per esempio, questo ci permette di apssare la definizione di una funzione lambda Python ad una higher-order function come map(), filter o reduec().</p>"},{"location":"material/01_python/03_advanced/lambda/#argomenti","title":"Argomenti","text":"<p>Come una normale funzione definita mediante def, le espressioni lambda in Python supportano tutti i diversi modi per passare degli argomenti, come argomenti posizionali, args e kwargs.</p> <p>Vediamo un esempio:</p> <pre><code>&gt;&gt;&gt; (lambda x, y, z: x + y + z)(1, 2, 3)\n6\n&gt;&gt;&gt; (lambda x, y, z=3: x + y + z)(1, 2)\n6\n&gt;&gt;&gt; (lambda x, y, z=3: x + y + z)(1, y=2)\n6\n&gt;&gt;&gt; (lambda *args: sum(args))(1,2,3)\n6\n&gt;&gt;&gt; (lambda **kwargs: sum(kwargs.values()))(one=1, two=2, three=3)\n6\n&gt;&gt;&gt; (lambda x, *, y=0, z=0: x + y + z)(1, y=2, z=3)\n6\n</code></pre>"},{"location":"material/01_python/03_advanced/lambda/#decorators","title":"Decorators","text":"<p>In Python, un decorator \u00e8 l'implemnentazione di un pattern che permette di aggiungere un dato comportamento ad una funzione o classe. Viene normalmente anteponendo una sintassi del tipo @nome_decorator alla funzione. Ecco un esempio:</p> <pre><code>def some_decorator(f):\n    def wraps(*args):\n        print(f\"Calling function '{f.__name__}'\")\n        return f(args)\n    return wraps\n\n@some_decorator\ndef decorated_function(x):\n    print(f\"With argument '{x}'\")\n</code></pre> <p>Nell'esempio precedente, somedecorator() \u00e8 una funzione che aggiunge un dato comporrtamento alla funzione decoratedfucntion(), in modo che chiamare decorated_fucntion('Python') restituisce il seguente output:</p> <pre><code>Calling function 'decorated_function'\nWith argument 'Python'\n</code></pre> <p>In pratica, decoratedfunction() manda a schermo soltanto \"With argument 'Python'\", ma il decorator aggiunge un ulteriorie comportamento che permette anche di mostrare a schermo \"Calling function 'decoratedfunction'\".</p> <p>Un decorator pu\u00f2 essere applicato ad una funzione lambda. Anche se non \u00e8 possibile decorare una funzione lambda con la sintassi @decorator, dato che il decorator \u00e8 semplicemente una funzione, questo pu\u00f2 chiamare  una funzione lambda:</p> <pre><code># Defining a decorator\ndef trace(f):\n    def wrap(*args, **kwargs):\n        print(f\"[TRACE] func: {f.__name__}, args: {args}, kwargs: {kwargs}\")\n        return f(*args, **kwargs)\n\n    return wrap\n\n# Applying decorator to a function\n@trace\ndef add_two(x):\n    return x + 2\n\n# Calling the decorated function\nadd_two(3)\n\n# Applying decorator to a lambda\nprint((trace(lambda x: x ** 2))(3))\n</code></pre> <p>La funzione <code>add_two()</code>, decorata con <code>@trace</code> alla riga 11, viene invocata con l'argomento 3 alla riga 15. Di contro, alla riga 18, una funzione lambda viene immediatamente integrata in una chiamata a trace(), il decorator. Eseguendo il codice precedente, otteniamo il seguente output:</p> <pre><code>[TRACE] func: add_two, args: (3,), kwargs: {}\n[TRACE] func: &lt;lambda&gt;, args: (3,), kwargs: {}\n9\n</code></pre> <p>Vediamo che il nome della funzione lambda appare come <code>&lt;lambda&gt;</code>, mentre <code>add_two</code> viene chiaramente identificato come una funzione normale.</p> <p>Decorare la funzione lambda in questo modo pu\u00f2 essere utile a scopo di debugging, specialmente per analizzare il comportamento di una funzione almbda usata nel contesto di una higher-order function. Vediamo un esempio con <code>map()</code>:</p> <pre><code>list(map(trace(lambda x: x*2), range(3)))\n</code></pre> <p>The first argument of map() is a lambda that multiplies its argument by 2. This lambda is decorated with trace(). When executed, the example above outputs the following:</p> <p>[TRACE] Calling  with args (0,) and kwargs {} [TRACE] Calling  with args (1,) and kwargs {} [TRACE] Calling  with args (2,) and kwargs {} [0, 2, 4] The result [0, 2, 4] is a list obtained from multiplying each element of range(3). For now, consider range(3) equivalent to the list [0, 1, 2]. <p>You will be exposed to map() in more details in Map.</p> <p>A lambda can also be a decorator, but it\u2019s not recommended. If you find yourself needing to do this, consult PEP 8, Programming Recommendations.</p> <p>For more on Python decorators, check out Primer on Python Decorators.</p> <p>Remove ads Closure A closure is a function where every free variable, everything except parameters, used in that function is bound to a specific value defined in the enclosing scope of that function. In effect, closures define the environment in which they run, and so can be called from anywhere.</p> <p>The concepts of lambdas and closures are not necessarily related, although lambda functions can be closures in the same way that normal functions can also be closures. Some languages have special constructs for closure or lambda (for example, Groovy with an anonymous block of code as Closure object), or a lambda expression (for example, Java Lambda expression with a limited option for closure).</p> <p>Here\u2019s a closure constructed with a normal Python function:</p> <p>def outerfunc(x):     y = 4     def innerfunc(z):         print(f\"x = {x}, y = {y}, z = {z}\")         return x + y + z     return inner_func</p> <p>for i in range(3):     closure = outerfunc(i)     print(f\"closure({i+5}) = {closure(i+5)}\") outerfunc() returns inner_func(), a nested function that computes the sum of three arguments:</p> <p>x is passed as an argument to outerfunc(). y is a variable local to outerfunc(). z is an argument passed to innerfunc(). To test the behavior of outerfunc() and innerfunc(), outerfunc() is invoked three times in a for loop that prints the following:</p> <p>x = 0, y = 4, z = 5 closure(5) = 9 x = 1, y = 4, z = 6 closure(6) = 11 x = 2, y = 4, z = 7 closure(7) = 13 On line 9 of the code, innerfunc() returned by the invocation of outerfunc() is bound to the name closure. On line 5, inner_func() captures x and y because it has access to its embedding environment, such that upon invocation of the closure, it is able to operate on the two free variables x and y.</p> <p>Similarly, a lambda can also be a closure. Here\u2019s the same example with a Python lambda function:</p> <p>def outer_func(x):     y = 4     return lambda z: x + y + z</p> <p>for i in range(3):     closure = outer_func(i)     print(f\"closure({i+5}) = {closure(i+5)}\") When you execute the code above, you obtain the following output:</p> <p>closure(5) = 9 closure(6) = 11 closure(7) = 13 On line 6, outerfunc() returns a lambda and assigns it to to the variable closure. On line 3, the body of the lambda function references x and y. The variable y is available at definition time, whereas x is defined at runtime when outerfunc() is invoked.</p> <p>In this situation, both the normal function and the lambda behave similarly. In the next section, you\u2019ll see a situation where the behavior of a lambda can be deceptive due to its evaluation time (definition time vs runtime).</p> <p>Evaluation Time In some situations involving loops, the behavior of a Python lambda function as a closure may be counterintuitive. It requires understanding when free variables are bound in the context of a lambda. The following examples demonstrate the difference when using a regular function vs using a Python lambda.</p> <p>Test the scenario first using a regular function:</p> <p>def wrap(n): ...     def f(): ...         print(n) ...     return f ... numbers = 'one', 'two', 'three' funcs = [] for n in numbers: ...     funcs.append(wrap(n)) ... for f in funcs: ...     f() ... one two three In a normal function, n is evaluated at definition time, on line 9, when the function is added to the list: funcs.append(wrap(n)).</p> <p>Now, with the implementation of the same logic with a lambda function, observe the unexpected behavior:</p> <p>numbers = 'one', 'two', 'three' funcs = [] for n in numbers: ...     funcs.append(lambda: print(n)) ... for f in funcs: ...     f() ... three three three The unexpected result occurs because the free variable n, as implemented, is bound at the execution time of the lambda expression. The Python lambda function on line 4 is a closure that captures n, a free variable bound at runtime. At runtime, while invoking the function f on line 7, the value of n is three.</p> <p>To overcome this issue, you can assign the free variable at definition time as follows:</p> <p>numbers = 'one', 'two', 'three' funcs = [] for n in numbers: ...     funcs.append(lambda n=n: print(n)) ... for f in funcs: ...     f() ... one two three A Python lambda function behaves like a normal function in regard to arguments. Therefore, a lambda parameter can be initialized with a default value: the parameter n takes the outer n as a default value. The Python lambda function could have been written as lambda x=n: print(x) and have the same result.</p> <p>The Python lambda function is invoked without any argument on line 7, and it uses the default value n set at definition time.</p> <p>Remove ads Testing Lambdas Python lambdas can be tested similarly to regular functions. It\u2019s possible to use both unittest and doctest.</p> <p>unittest</p> <p>The unittest module handles Python lambda functions similarly to regular functions:</p> <p>import unittest</p> <p>addtwo = lambda x: x + 2</p> <p>class LambdaTest(unittest.TestCase):     def testaddtwo(self):         self.assertEqual(addtwo(2), 4)</p> <pre><code>def test_add_two_point_two(self):\n    self.assertEqual(addtwo(2.2), 4.2)\n\ndef test_add_three(self):\n    # Should fail\n    self.assertEqual(addtwo(3), 6)\n</code></pre> <p>if name == 'main':     unittest.main(verbosity=2) LambdaTest defines a test case with three test methods, each of them exercising a test scenario for addtwo() implemented as a lambda function. The execution of the Python file lambda_unittest.py that contains LambdaTest produces the following:</p> <p>$ python lambdaunittest.py testaddthree (main.LambdaTest) ... FAIL testaddtwo (main.LambdaTest) ... ok testaddtwopointtwo (main_.LambdaTest) ... ok</p> <p>====================================================================== FAIL: testaddthree (main.LambdaTest)</p> <p>Traceback (most recent call last):   File \"lambdaunittest.py\", line 18, in testadd_three     self.assertEqual(addtwo(3), 6) AssertionError: 5 != 6</p> <p>Ran 3 tests in 0.001s</p> <p>FAILED (failures=1) As expected, we have two successful test cases and one failure for testaddthree: the result is 5, but the expected result was 6. This failure is due to an intentional mistake in the test case. Changing the expected result from 6 to 5 will satisfy all the tests for LambdaTest.</p> <p>doctest</p> <p>The doctest module extracts interactive Python code from docstring to execute tests. Although the syntax of Python lambda functions does not support a typical docstring, it is possible to assign a string to the doc element of a named lambda:</p> <p>addtwo = lambda x: x + 2 addtwo.doc = \"\"\"Add 2 to a number.     &gt;&gt;&gt; addtwo(2)     4     &gt;&gt;&gt; addtwo(2.2)     4.2     &gt;&gt;&gt; addtwo(3) # Should fail     6     \"\"\"</p> <p>if name == 'main':     import doctest     doctest.testmod(verbose=True) The doctest in the doc comment of lambda addtwo() describes the same test cases as in the previous section.</p> <p>When you execute the tests via doctest.testmod(), you get the following:</p> <p>$ python lambda_doctest.py Trying:     addtwo(2) Expecting:     4 ok Trying:     addtwo(2.2) Expecting:     4.2 ok Trying:     addtwo(3) # Should fail Expecting:     6</p> <p>File \"lambdadoctest.py\", line 16, in _main.addtwo Failed example:     addtwo(3) # Should fail Expected:     6 Got:     5 1 items had no tests:     __main</p> <p>1 items had failures:    1 of   3 in main.addtwo 3 tests in 2 items. 2 passed and 1 failed. Test Failed 1 failures. The failed test results from the same failure explained in the execution of the unit tests in the previous section.</p> <p>You can add a docstring to a Python lambda via an assignment to doc to document a lambda function. Although possible, the Python syntax better accommodates docstring for normal functions than lambda functions.</p> <p>For a comprehensive overview of unit testing in Python, you may want to refer to Getting Started With Testing in Python.</p> <p>Lambda Expression Abuses Several examples in this article, if written in the context of professional Python code, would qualify as abuses.</p> <p>If you find yourself trying to overcome something that a lambda expression does not support, this is probably a sign that a normal function would be better suited. The docstring for a lambda expression in the previous section is a good example. Attempting to overcome the fact that a Python lambda function does not support statements is another red flag.</p> <p>The next sections illustrate a few examples of lambda usages that should be avoided. Those examples might be situations where, in the context of Python lambda, the code exhibits the following pattern:</p> <p>It doesn\u2019t follow the Python style guide (PEP 8) It\u2019s cumbersome and difficult to read. It\u2019s unnecessarily clever at the cost of difficult readability.</p> <p>Remove ads Raising an Exception Trying to raise an exception in a Python lambda should make you think twice. There are some clever ways to do so, but even something like the following is better to avoid:</p> <p>def throw(ex): raise ex (lambda: throw(Exception('Something bad happened')))() Traceback (most recent call last):     File \"\", line 1, in      File \"\", line 1, in      File \"\", line 1, in throw Exception: Something bad happened Because a statement is not syntactically correct in a Python lambda body, the workaround in the example above consists of abstracting the statement call with a dedicated function throw(). Using this type of workaround should be avoided. If you encounter this type of code, you should consider refactoring the code to use a regular function. <p>Cryptic Style As in any programming languages, you will find Python code that can be difficult to read because of the style used. Lambda functions, due to their conciseness, can be conducive to writing code that is difficult to read.</p> <p>The following lambda example contains several bad style choices:</p> <p>(lambda : list(map(lambda _: _ // 2, _)))([1,2,3,4,5,6,7,8,9,10]) [0, 1, 1, 2, 2, 3, 3, 4, 4, 5] The underscore () refers to a variable that you don\u2019t need to refer to explicitly. But in this example, three _ refer to different variables. An initial upgrade to this lambda code could be to name the variables:</p> <p>(lambda somelist: list(map(lambda n: n // 2,                                 somelist)))([1,2,3,4,5,6,7,8,9,10]) [0, 1, 1, 2, 2, 3, 3, 4, 4, 5] Admittedly, it\u2019s still difficult to read. By still taking advantage of a lambda, a regular function would go a long way to render this code more readable, spreading the logic over a few lines and function calls:</p> <p>def divitems(somelist):       divbytwo = lambda n: n // 2       return map(divbytwo, somelist) list(divitems([1,2,3,4,5,6,7,8,9,10]))) [0, 1, 1, 2, 2, 3, 3, 4, 4, 5] This is still not optimal but shows you a possible path to make code, and Python lambda functions in particular, more readable. In Alternatives to Lambdas, you\u2019ll learn to replace map() and lambda with list comprehensions or generator expressions. This will drastically improve the readability of the code.</p> <p>Python Classes You can but should not write class methods as Python lambda functions. The following example is perfectly legal Python code but exhibits unconventional Python code relying on lambda. For example, instead of implementing str as a regular function, it uses a lambda. Similarly, brand and year are properties also implemented with lambda functions, instead of regular functions or decorators:</p> <p>class Car:     \"\"\"Car with methods as lambda functions.\"\"\"     def init(self, brand, year):         self.brand = brand         self.year = year</p> <pre><code>brand = property(lambda self: getattr(self, '_brand'),\n                 lambda self, value: setattr(self, '_brand', value))\n\nyear = property(lambda self: getattr(self, '_year'),\n                lambda self, value: setattr(self, '_year', value))\n\n__str__ = lambda self: f'{self.brand} {self.year}'  # 1: error E731\n\nhonk = lambda self: print('Honk!')     # 2: error E731\n</code></pre> <p>Running a tool like flake8, a style guide enforcement tool, will display the following errors for str and honk:</p> <p>E731 do not assign a lambda expression, use a def Although flake8 doesn\u2019t point out an issue for the usage of the Python lambda functions in the properties, they are difficult to read and prone to error because of the usage of multiple strings like 'brand' and 'year'.</p> <p>Proper implementation of str would be expected to be as follows:</p> <p>def str(self):     return f'{self.brand} {self.year}' brand would be written as follows:</p> <p>@property def brand(self):     return self._brand</p> <p>@brand.setter def brand(self, value):     self._brand = value As a general rule, in the context of code written in Python, prefer regular functions over lambda expressions. Nonetheless, there are cases that benefit from lambda syntax, as you will see in the next section.</p> <p>Remove ads Appropriate Uses of Lambda Expressions Lambdas in Python tend to be the subject of controversies. Some of the arguments against lambdas in Python are:</p> <p>Issues with readability The imposition of a functional way of thinking Heavy syntax with the lambda keyword Despite the heated debates questioning the mere existence of this feature in Python, lambda functions have properties that sometimes provide value to the Python language and to developers.</p> <p>The following examples illustrate scenarios where the use of lambda functions is not only suitable but encouraged in Python code.</p> <p>Classic Functional Constructs Lambda functions are regularly used with the built-in functions map() and filter(), as well as functools.reduce(), exposed in the module functools. The following three examples are respective illustrations of using those functions with lambda expressions as companions:</p> <p>list(map(lambda x: x.upper(), ['cat', 'dog', 'cow'])) ['CAT', 'DOG', 'COW'] list(filter(lambda x: 'o' in x, ['cat', 'dog', 'cow'])) ['dog', 'cow'] from functools import reduce reduce(lambda acc, x: f'{acc} | {x}', ['cat', 'dog', 'cow']) 'cat | dog | cow' You may have to read code resembling the examples above, albeit with more relevant data. For that reason, it\u2019s important to recognize those constructs. Nevertheless, those constructs have equivalent alternatives that are considered more Pythonic. In Alternatives to Lambdas, you\u2019ll learn how to convert higher-order functions and their accompanying lambdas into other more idiomatic forms.</p> <p>Key Functions Key functions in Python are higher-order functions that take a parameter key as a named argument. key receives a function that can be a lambda. This function directly influences the algorithm driven by the key function itself. Here are some key functions:</p> <p>sort(): list method sorted(), min(), max(): built-in functions nlargest() and nsmallest(): in the Heap queue algorithm module heapq Imagine that you want to sort a list of IDs represented as strings. Each ID is the concatenation of the string id and a number. Sorting this list with the built-in function sorted(), by default, uses a lexicographic order as the elements in the list are strings.</p> <p>To influence the sorting execution, you can assign a lambda to the named argument key, such that the sorting will use the number associated with the ID:</p> <p>ids = ['id1', 'id2', 'id30', 'id3', 'id22', 'id100'] print(sorted(ids)) # Lexicographic sort ['id1', 'id100', 'id2', 'id22', 'id3', 'id30'] sortedids = sorted(ids, key=lambda x: int(x[2:])) # Integer sort print(sortedids) ['id1', 'id2', 'id3', 'id22', 'id30', 'id100'] UI Frameworks UI frameworks like Tkinter, wxPython, or .NET Windows Forms with IronPython take advantage of lambda functions for mapping actions in response to UI events.</p> <p>The naive Tkinter program below demonstrates the usage of a lambda assigned to the command of the Reverse button:</p> <p>import tkinter as tk import sys</p> <p>window = tk.Tk() window.grid_columnconfigure(0, weight=1) window.title(\"Lambda\") window.geometry(\"300x100\") label = tk.Label(window, text=\"Lambda Calculus\") label.grid(column=0, row=0) button = tk.Button(     window,     text=\"Reverse\",     command=lambda: label.configure(text=label.cget(\"text\")[::-1]), ) button.grid(column=0, row=1) window.mainloop() Clicking the button Reverse fires an event that triggers the lambda function, changing the label from Lambda Calculus to suluclaC adbmaL*:</p> <p>Animated TkInter Windows demonstrating the action of the button to the text Both wxPython and IronPython on the .NET platform share a similar approach for handling events. Note that lambda is one way to handle firing events, but a function may be used for the same purpose. It ends up being self-contained and less verbose to use a lambda when the amount of code needed is very short.</p> <p>To explore wxPython, check out How to Build a Python GUI Application With wxPython.</p> <p>Remove ads Python Interpreter When you\u2019re playing with Python code in the interactive interpreter, Python lambda functions are often a blessing. It\u2019s easy to craft a quick one-liner function to explore some snippets of code that will never see the light of day outside of the interpreter. The lambdas written in the interpreter, for the sake of speedy discovery, are like scrap paper that you can throw away after use.</p> <p>timeit In the same spirit as the experimentation in the Python interpreter, the module timeit provides functions to time small code fragments. timeit.timeit() in particular can be called directly, passing some Python code in a string. Here\u2019s an example:</p> <p>from timeit import timeit timeit(\"factorial(999)\", \"from math import factorial\", number=10) 0.0013087529951008037 When the statement is passed as a string, timeit() needs the full context. In the example above, this is provided by the second argument that sets up the environment needed by the main function to be timed. Not doing so would raise a NameError exception.</p> <p>Another approach is to use a lambda:</p> <p>from math import factorial timeit(lambda: factorial(999), number=10) 0.0012704220062005334 This solution is cleaner, more readable, and quicker to type in the interpreter. Although the execution time was slightly less for the lambda version, executing the functions again may show a slight advantage for the string version. The execution time of the setup is excluded from the overall execution time and shouldn\u2019t have any impact on the result.</p> <p>Monkey Patching For testing, it\u2019s sometimes necessary to rely on repeatable results, even if during the normal execution of a given software, the corresponding results are expected to differ, or even be totally random.</p> <p>Let\u2019s say you want to test a function that, at runtime, handles random values. But, during the testing execution, you need to assert against predictable values in a repeatable manner. The following example shows how, with a lambda function, monkey patching can help you:</p> <p>from contextlib import contextmanager import secrets</p> <p>def gentoken():     \"\"\"Generate a random token.\"\"\"     return f'TOKEN'</p> <p>@contextmanager def mocktoken():     \"\"\"Context manager to monkey patch the secrets.tokenhex     function during testing.     \"\"\"     defaulttokenhex = secrets.tokenhex     secrets.tokenhex = lambda : 'feedfacecafebeef'     yield     secrets.tokenhex = defaulttokenhex</p> <p>def testgenkey():     \"\"\"Test the random token.\"\"\"     with mocktoken():         assert gentoken() == f\"TOKEN_{'feedfacecafebeef'}\"</p> <p>testgenkey() A context manager helps with insulating the operation of monkey patching a function from the standard library (secrets, in this example). The lambda function assigned to secrets.token_hex() substitutes the default behavior by returning a static value.</p> <p>This allows testing any function depending on tokenhex() in a predictable fashion. Prior to exiting from the context manager, the default behavior of tokenhex() is reestablished to eliminate any unexpected side effects that would affect other areas of the testing that may depend on the default behavior of token_hex().</p> <p>Unit test frameworks like unittest and pytest take this concept to a higher level of sophistication.</p> <p>With pytest, still using a lambda function, the same example becomes more elegant and concise :</p> <p>import secrets</p> <p>def gentoken():     return f'TOKEN'</p> <p>def testgenkey(monkeypatch):     monkeypatch.setattr('secrets.tokenhex', lambda _: 'feedfacecafebeef')     assert gentoken() == f\"TOKEN{'feedfacecafebeef'}\" With the pytest monkeypatch fixture, secrets.tokenhex() is overwritten with a lambda that will return a deterministic value, feedfacecafebeef, allowing to validate the test. The pytest monkeypatch fixture allows you to control the scope of the override. In the example above, invoking secrets.token_hex() in subsequent tests, without using monkey patching, would execute the normal implementation of this function.</p> <p>Executing the pytest test gives the following result:</p> <p>$ pytest testtoken.py -v ============================= test session starts ============================== platform linux -- Python 3.7.2, pytest-4.3.0, py-1.8.0, pluggy-0.9.0 cachedir: .pytestcache rootdir: /home/andre/AB/tools/bpython, inifile: collected 1 item</p> <p>testtoken.py::testgen_key PASSED                                       [100%]</p> <p>=========================== 1 passed in 0.01 seconds =========================== The test passes as we validated that the gen_token() was exercised, and the results were the expected ones in the context of the test.</p> <p>Remove ads Alternatives to Lambdas While there are great reasons to use lambda, there are instances where its use is frowned upon. So what are the alternatives?</p> <p>Higher-order functions like map(), filter(), and functools.reduce() can be converted to more elegant forms with slight twists of creativity, in particular with list comprehensions or generator expressions.</p> <p>To learn more about list comprehensions, check out When to Use a List Comprehension in Python. To learn more about generator expressions, check out How to Use Generators and yield in Python.</p> <p>Map The built-in function map() takes a function as a first argument and applies it to each of the elements of its second argument, an iterable. Examples of iterables are strings, lists, and tuples. For more information on iterables and iterators, check out Iterables and Iterators.</p> <p>map() returns an iterator corresponding to the transformed collection. As an example, if you wanted to transform a list of strings to a new list with each string capitalized, you could use map(), as follows:</p> <p>list(map(lambda x: x.capitalize(), ['cat', 'dog', 'cow'])) ['Cat', 'Dog', 'Cow'] You need to invoke list() to convert the iterator returned by map() into an expanded list that can be displayed in the Python shell interpreter.</p> <p>Using a list comprehension eliminates the need for defining and invoking the lambda function:</p> <p>[x.capitalize() for x in ['cat', 'dog', 'cow']] ['Cat', 'Dog', 'Cow'] Filter The built-in function filter(), another classic functional construct, can be converted into a list comprehension. It takes a predicate as a first argument and an iterable as a second argument. It builds an iterator containing all the elements of the initial collection that satisfies the predicate function. Here\u2019s an example that filters all the even numbers in a given list of integers:</p> <p>even = lambda x: x%2 == 0 list(filter(even, range(11))) [0, 2, 4, 6, 8, 10] Note that filter() returns an iterator, hence the need to invoke the built-in type list that constructs a list given an iterator.</p> <p>The implementation leveraging the list comprehension construct gives the following:</p> <p>[x for x in range(11) if x%2 == 0] [0, 2, 4, 6, 8, 10] Reduce Since Python 3, reduce() has gone from a built-in function to a functools module function. As map() and filter(), its first two arguments are respectively a function and an iterable. It may also take an initializer as a third argument that is used as the initial value of the resulting accumulator. For each element of the iterable, reduce() applies the function and accumulates the result that is returned when the iterable is exhausted.</p> <p>To apply reduce() to a list of pairs and calculate the sum of the first item of each pair, you could write this:</p> <p>import functools pairs = [(1, 'a'), (2, 'b'), (3, 'c')] functools.reduce(lambda acc, pair: acc + pair[0], pairs, 0) 6 A more idiomatic approach using a generator expression, as an argument to sum() in the example, is the following:</p> <p>pairs = [(1, 'a'), (2, 'b'), (3, 'c')] sum(x[0] for x in pairs) 6 A slightly different and possibly cleaner solution removes the need to explicitly access the first element of the pair and instead use unpacking:</p> <p>pairs = [(1, 'a'), (2, 'b'), (3, 'c')] sum(x for x, _ in pairs) 6 The use of underscore (_) is a Python convention indicating that you can ignore the second value of the pair.</p> <p>sum() takes a unique argument, so the generator expression does not need to be in parentheses.</p> <p>Are Lambdas Pythonic or Not? PEP 8, which is the style guide for Python code, reads:</p> <p>Always use a def statement instead of an assignment statement that binds a lambda expression directly to an identifier. (Source)</p> <p>This strongly discourages using lambda bound to an identifier, mainly where functions should be used and have more benefits. PEP 8 does not mention other usages of lambda. As you have seen in the previous sections, lambda functions may certainly have good uses, although they are limited.</p> <p>A possible way to answer the question is that lambda functions are perfectly Pythonic if there is nothing more Pythonic available. I\u2019m staying away from defining what \u201cPythonic\u201d means, leaving you with the definition that best suits your mindset, as well as your personal or your team\u2019s coding style.</p> <p>Beyond the narrow scope of Python lambda, How to Write Beautiful Python Code With PEP 8 is a great resource that you may want to check out regarding code style in Python.</p> <p>Conclusion You now know how to use Python lambda functions and can:</p> <p>Write Python lambdas and use anonymous functions Choose wisely between lambdas or normal Python functions Avoid excessive use of lambdas Use lambdas with higher-order functions or Python key functions If you have a penchant for mathematics, you may have some fun exploring the fascinating world of lambda calculus.</p> <p>Python lambdas are like salt. A pinch in your spam, ham, and eggs will enhance the flavors, but too much will spoil the dish.</p>"},{"location":"material/01_python/03_advanced/pathlib/","title":"9 - Pathlib","text":"<p>In questa lezione, vedremo come lavorare in Python direttamente con i percorsi (in inglese, path) di cartelle e librerie. In particolare, vedremo come scrivere e leggere i file, iterando lungo le cartelle e manipolando il file system del nostro computer.</p>"},{"location":"material/01_python/03_advanced/pathlib/#91-python-e-la-gestione-del-path","title":"9.1 - Python e la gestione del path","text":"<p>Come abbiamo visto anche nella lezione relativa all'I/O, lavorare ed interagire con i file presenti sul nostro elaboratore \u00e8 estremamente importante. Nei casi pi\u00f9 semplici, ci limiteremo a scrivere su o leggere da un file ma, a volte, potremmo avere a che fare con task pi\u00f9 complessi, come elencare tutti i file con una certa estensione presenti in una data cartella, o creare un file che non esiste.</p>"},{"location":"material/01_python/03_advanced/pathlib/#912-i-path-come-stringhe","title":"9.1.2 - I path come stringhe","text":"<p>In passato, il percorso di un file Python \u00e8 sempre stato rappresentato usando degli oggetti di tipo stringa. Tuttavia, appare evidente come il percorso di un file non sia realmente una stringa: ci\u00f2 ha comportato la necessit\u00e0 di \"spargere\" diverse funzionalit\u00e0 in diversi package della libreria standard, in particolare <code>os</code>, <code>glob</code> e <code>shutil</code>, con conseguente aumento delle righe di codice e degli import da utilizzare.</p> <p>Ad esempio, se volessimo spostare tutti i file di testo presenti nella cartella di lavoro in un'altra cartella chiamata <code>archivio</code> avremmo bisogno di usare tre import:</p> <pre><code>import glob\nimport os\nimport shutil\n\nfor file_testo in glob.glob('*.txt'):\n    nuovo_path = os.path.join('archivio', file_testo)\n    shutil.move(file_testo, nuovo_path)\n</code></pre> <p>Rappresentare un path come una stringa favorisce inoltre la discutibile pratica dell'utilizzo dei metodi normalmente utilizzati su oggetti di questo tipo. Ad esempio, potremmo pensare di unire il percorso della cartella nella quale ci troviamo attualmente al percorso di una sottocartella utilizzando l'operatore <code>+</code>:</p> <pre><code>path_sottocartella = os.getcwd() + '/sottocartella'\n</code></pre> <p>Tuttavia, questa pratica \u00e8 estremamente sconsigliata, dato che la rappresentazione del percorso sotto forma di stringa varia tra sistemi Windows ed Unix-like. In particolare, ricordiamo che Windows utilizza il backslash <code>\\</code> per articolare il percorso di una cartella o file, mentre i sistemi Unix-like usano il forward slash <code>/</code>.</p> <p>La funzione join()</p> <p>Per ovviare a questo problema, prima dell'introduzione di pathlib passato si utilizzava il metodo <code>join()</code> di <code>os.path</code>, che permette di unire due path usando il separatore adatto al sistema operativo in analisi.</p>"},{"location":"material/01_python/03_advanced/pathlib/#913-i-path-in-pathlib","title":"9.1.3 - I path in pathlib","text":"<p>Il modulo <code>pathlib</code> \u00e8 stato introdotto per la prima volta in Python 3.4 proprio per porre rimedio a questa complicata situazione. L'obiettivo di <code>pathlib</code>, quindi, \u00e8 quello di raccogliere tutte le funzionalit\u00e0 necessarie alla gestione del path di un file in un unico componente della libreria standard, ed in particolare mediante un oggetto di classe <code>Path</code>.</p>"},{"location":"material/01_python/03_advanced/pathlib/#92-la-classe-path","title":"9.2 - La classe Path","text":""},{"location":"material/01_python/03_advanced/pathlib/#921-creazione-di-un-path","title":"9.2.1 - Creazione di un path","text":"<p>Come abbiamo gi\u00e0 detto, <code>pathlib</code> mette a disposizione la classe <code>Path</code> per la gestione del path di un file. Per utilizzare un oggetto di questa classe, dovremo innanzitutto crearlo; potremo in tal senso farlo in diversi modi.</p> <p>Innanzitutto, possiamo usare il metodo <code>cwd()</code>, che restituisce il path della cartella di lavoro. Supponendo di essere nella cartella <code>Documents</code> dell'utente <code>user</code>, avremo:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; Path.cwd()\nWindowsPath('C:/Users/user/Documents')\n</code></pre> <p>Il metodo <code>home()</code>, invece, restiuir\u00e0 la cartella base per l'utente attuale. Ad esempio, supponendo che l'utente <code>user</code> sia loggato:</p> <pre><code>&gt;&gt;&gt; Path.home()\nWindowsPath('C:/Users/user')\n</code></pre> <p>Possiamo anche creare un path in maniera esplicita utilizzando una stringa:</p> <pre><code>&gt;&gt;&gt; Path('C:/Users/user/Documents')\nWindowsPath('C:/Users/user/Documents')\n</code></pre> <p>Windows ed il backslash</p> <p>Abbiamo gi\u00e0 visto che il separatore dei path in Windows \u00e8 il backslash. Tuttavia, questo viene usato anche come escape character per rappresentare caratteri che non \u00e8 possibile stampare altrimenti. In tal senso, per evitare l'insorgere di errori, \u00e8 possibile l'uso di raw string literal per rappresentare i percorsi Python. Ad esempio, nel caso precedente, scriveremo:</p> <pre><code>&gt;&gt;&gt; Path(r'C:\\Users\\user\\Documents')  \nWindowsPath('C:/Users/user/Documents')\n</code></pre> <p>Un altro modo per costruire un path \u00e8 unire le varie parti dello stesso usando l'operatore <code>/</code>:</p> <pre><code>&gt;&gt;&gt; pathlib.Path.home() / 'Documents'\nWindowsPath('C:/Users/user/Documents')\n</code></pre> <p>Da notare che l'operatore <code>/</code> permette di unire anche pi\u00f9 di un path, oltre che un insieme di oggetti di tipo <code>Path</code> e stringhe (a patto che, ovviamente, vi sia almeno un oggetto di tipo <code>Path</code>). In alternativa, possiamo ottenere lo stesso effetto con il metodo <code>joinpath()</code>:</p> <pre><code>&gt;&gt;&gt; Path.home().joinpath('Documents')     \nWindowsPath('C:/Users/user/Documents')\n</code></pre> <p>Path, WindowsPath e PosixPath</p> <p>Negli esempi precedenti, utilizzare la classe <code>Path</code> ha sempre portato in output un <code>WindowsPath</code>. Ci\u00f2 \u00e8 legato al fatto che questi esempi sono stati scritti su un sistema Windows; se si fosse utilizzaton un sistema Unix-like, i path risultanti sarebbero stati dei <code>PosixPath</code>. In pratica, la classe vera e propria che caratterizza il path dipende dal sistema operativo su cui viene eseguito il nostro programma.</p>"},{"location":"material/01_python/03_advanced/pathlib/#922-io-su-file","title":"9.2.2 I/O su file","text":""},{"location":"material/01_python/03_advanced/pathlib/#apertura-di-un-file","title":"Apertura di un file","text":"<p>Per leggere da o scrivere su un file Python si \u00e8 utilizza il metodo <code>open()</code> che, convenientemente, pu\u00f2 essere usato direttamente con gli oggetti di classe <code>Path</code>. Ad esempio, se volessimo leggere tutti i commenti in un file Python, potremmo usare la seguente funzione:</p> <pre><code>def leggi_commenti(file_path):\n    with open(file_path, 'r') as f:\n        commenti = [line.strip() for line in f if line.startswith('#')]\n    return '\\n'.join(commenti)\n\nleggi_commenti('test.py')\n</code></pre> <p>In realt\u00e0, possiamo anche chiamare la funzione <code>open()</code> messa a disposizione dall'oggetto <code>Path</code>:</p> <pre><code>with path.open(mode='r') as f:\n    # ...\n</code></pre> <p>In pratica, <code>Path.open()</code> chiama internamente la <code>open()</code>. Di conseguenza, l'utilizzo dell'una o dell'altra funzione \u00e8 semplicemente una questione di preferenze personali.</p>"},{"location":"material/01_python/03_advanced/pathlib/#lettura-e-scrittura","title":"Lettura e scrittura","text":"<p>Per le operazioni di lettura e scrittura su file <code>pathlib</code> mette a disposizione i metodi mostrati nella tabella 9.1.</p> Metodo File aperto come... Tipo di interazione <code>.read_text()</code> File di testo Contenuti restituiti come stringa <code>.read_bytes()</code> File binario Contenuti restituiti come binario <code>.write_text()</code> File di testo Scrittura di contenuti come stringa <code>.write_bytes()</code> File binario Scrittura di contenuti come binari <p>Tutti i metodi gestiscono autonomamente l'apertura e l'interazione con il file. Ad esempio, per leggere i contenuti di un file di testo:</p> <pre><code>&gt;&gt;&gt; path = Path.cwd() / 'test.txt'\n&gt;&gt;&gt; path.read_text()\n&lt;contenuti del file test.txt&gt;\n</code></pre> <p>I path possono essere specificati anche come semplici nomi di file. In questo caso, ovviamente, Python assumer\u00e0 che siano all'interno dell'attuale cartella di lavoro. Ad esempio, un'altra forma di esprimere il codice precedente \u00e8 questa:</p> <pre><code>&gt;&gt;&gt; Path('test.txt').read_text()\n&lt;contenuti del file test.md&gt;\n</code></pre>"},{"location":"material/01_python/03_advanced/pathlib/#percorso-di-un-file","title":"Percorso di un file","text":"<p>Per avere il percorso completo (assoluto) di un file possiamo usare il metodo resolve():</p> <pre><code>&gt;&gt;&gt; path = Path('test.txt')\n&gt;&gt;&gt; path.resolve()\nWindowsPath('C:/Users/user/Documents/test.txt')\n</code></pre>"},{"location":"material/01_python/03_advanced/pathlib/#93-attributi-di-un-path","title":"9.3 - Attributi di un path","text":"<p>TODO DA QUI</p> <p>Le diverse parti di un path sono disponibili sotto forma di property. Deglie sempi basilari includono:</p> <ul> <li>.name: il nome del fiel senza alcuna directory</li> <li>.parent: la cartella che contiene il file, o la cartella padre se il path \u00e8 una cartella</li> <li>.stem: il nome del file senza l'estensione</li> <li>.suffix: l'estensione del file</li> <li>.anchor: la parte del path prima delle cartelle</li> </ul> <p>Ecco degli esempio:</p> <pre><code>&gt;&gt;&gt; path\nPosixPath('/home/gahjelle/realpython/test.md')\n&gt;&gt;&gt; path.name\n'test.md'\n&gt;&gt;&gt; path.stem\n'test'\n&gt;&gt;&gt; path.suffix\n'.md'\n&gt;&gt;&gt; path.parent\nPosixPath('/home/gahjelle/realpython')\n&gt;&gt;&gt; path.parent.parent\nPosixPath('/home/gahjelle')\n&gt;&gt;&gt; path.anchor\n'/'\n</code></pre> <p>Da notare che .parent restituisce un nuovo oggetto di tipo Path, metnre le altre propriet\u00e0 restituiscono delle stringhe. Questo significa ad esempio che .parent pu\u00f2 essere concatenato come nell'ultimo esempio, o anche combinato con / per creare dei nuovi path:</p> <pre><code>&gt;&gt;&gt; path.parent.parent / ('new' + path.suffix)\nPosixPath('/home/gahjelle/new.md')\n</code></pre>"},{"location":"material/01_python/03_advanced/pathlib/#spostare-e-cancellare-dei-file","title":"Spostare e cancellare dei file","text":"<p>Attraverso pathlib, avremo accesso anche ad operazioni base a livello di file system, come muovere, aggiornare ed anche cancellare i file. Per la maggior parte, questi metodi non ci danno un feedback prima di spostare o cancellare informazioni, quindi conviene fare attenzione.</p> <p>Per spostar eun file, usiamo .replace(). Notiamo che se la destinazione esiste gi\u00e0, .replace() la sovrascriuver\u00e0. Sfortunatamente, pathlib non supporta esplcitiamente lo spsotamenteo safe dei file. Per evitrare di sovrascirvere il path di destinazione, il modo pi\u00f9 semplice \u00e8 quello di testare se la destinazione esiste prima di rimpiazare:</p> <pre><code>if not destination.exists():\n    source.replace(destination)\n</code></pre> <p>Tuttavia, questo lascia aperta la porta ad una possibile race condition. Un altro processo potrebbe agigungere un path di destinazione tra l'esecuzione dell'istruzione if ed il metodo .replace(). Se questo \u00e8 un problema, un podo pi\u00f9 semplice \u00e8 quello di aprire il percorso di destinazione per la creazione esclusiva e copiare esplicitamente i dati sorgenti:</p> <pre><code>with destination.open(mode='xb') as fid:\n    fid.write(source.read_bytes())\n</code></pre> <p>Questo codice lancer\u00e0 un <code>FileExistsError</code> se la destinazione esiste gi\u00e0. Tecnicamente, questo copia un file. Per effettuare lo spostamento, limitiamoci a cancellare il sorgente dopo che la copia \u00e8 stata fatta (vediamod i seguito). Assicuriamoci inoltre che non sia stata lanciata alcuna eccezione.</p> <p>Quando stiamo rinominando i file, dei meotdi utili potrebbero essere <code>.whit_name()</code> e <code>.with_suffix()</code>. Entrambi restituiscono il path originario ma con il noem o o il suffisso rimpiazzato, rispettivamente.</p> <p>Ad esempio:</p> <pre><code>&gt;&gt;&gt; path\nPosixPath('/home/gahjelle/realpython/test001.txt')\n&gt;&gt;&gt; path.with_suffix('.py')\nPosixPath('/home/gahjelle/realpython/test001.py')\n&gt;&gt;&gt; path.replace(path.with_suffix('.py'))\n</code></pre> <p>Le cartelle ed i file possono essere cancellati usando <code>.rmdir()</code> ed <code>.unlink()</code>, rispettivamente.</p>"},{"location":"material/01_python/03_advanced/pathlib/#esempi","title":"Esempi","text":"<p>Vediamo alcuni esempi di come usare <code>pathlib</code> per affrontare alcune semplici sfide.</p>"},{"location":"material/01_python/03_advanced/pathlib/#conteggio-dei-file","title":"Conteggio dei file","text":"<p>Ci sono alcuni modi differenti per elencare molti file. Il pi\u00f9 semplice \u00e8 il metodo <code>.iterdir()</code>, che itera su tutti i file in una data cartella. Il seguente esempio combina <code>.iterdir()</code> con la classe <code>collections.Counter</code> per contare quanti file ci sono di ogni tipo nella cartella attuale.</p> <pre><code>&gt;&gt;&gt; import collections\n&gt;&gt;&gt; collections.Counter(p.suffix for p in pathlib.Path.cwd().iterdir())\nCounter({'.md': 2, '.txt': 4, '.pdf': 2, '.py': 1})\n</code></pre> <p>Un elenco pi\u00f9 flessibile dei file pu\u00f2 essere creato con i metodi <code>.glob()</code> ed <code>.rglob()</code> (recursive glob). Ad esempio, <code>pathlib.Path.cwd().glob('*.txt')</code> restituisce tutti i file con estensione <code>.txt</code> nella cartella attuale. Il seguente codice conta soltanto i tipi di file che iniziano con <code>p</code>:</p> <pre><code>&gt;&gt;&gt; import collections\n&gt;&gt;&gt; collections.Counter(p.suffix for p in pathlib.Path.cwd().glob('*.p*'))\nCounter({'.pdf': 2, '.py': 1})\n</code></pre>"},{"location":"material/01_python/03_advanced/pathlib/#mostare-lalbero-di-una-cartella","title":"Mostare l'albero di una cartella","text":"<p>Il seguente esempio definisce una funzione, tree(), che mander\u00e0 a schermo un albero che rappresenta la gerarchia di file, con radice una certa cartella. Qui, vogliamo elencare le sottocartelle anche, per cui useremo il metodo <code>.rglob()</code>.</p> <pre><code>def tree(directory):\n    print(f'+ {directory}')\n    for path in sorted(directory.rglob('*')):\n        depth = len(path.relative_to(directory).parts)\n        spacer = '    ' * depth\n        print(f'{spacer}+ {path.name}')\n</code></pre> <p>Notiamo che dobbiamo sapere quanto lontano dalla cartella radice \u00e8 collocato un file. Per falro, usiamo per prima cosa <code>.relative_to()</code> per rappresentare un eprocros relativamente alla cartella radice. Quindi, contiamo il numero di cartelle (usando la propriet\u00e0 <code>.parts</code>) nella rappresentazione. QUando eseguita, qeusta funzione crea un albero visu9ale come il seguente:</p> <pre><code>&gt;&gt;&gt; tree(pathlib.Path.cwd())\n+ /home/gahjelle/realpython\n    + directory_1\n        + file_a.md\n    + directory_2\n        + file_a.md\n        + file_b.pdf\n        + file_c.py\n    + file_1.txt\n    + file_2.txt\n</code></pre>"},{"location":"material/01_python/03_advanced/pathlib/#trovare-lultimo-file-modificato","title":"Trovare l'ultimo file modificato","text":"<p>Im metodi <code>.iterdir()</code>, <code>.glob()</code>, ed <code>.rglob</code> sono ottimi per i generator e le list comprehension. Per trovare il file in una cartella modificato per ultimo, possiamo usar eil metodo <code>.stat()</code> per ottenere informaizoni circa i file sottostanti. Per esempio, <code>.stat().st_mtime</code> ci d\u00e0 il momento dell'ultima modifica ad un file:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; time, file_path = max((f.stat().st_mtime, f) for f in directory.iterdir())\n&gt;&gt;&gt; print(datetime.fromtimestamp(time), file_path)\n2018-03-23 19:23:56.977817 /home/gahjelle/realpython/test001.txt\n</code></pre> <p>Possiamo anche ottenere i contenuti del file che \u00e8 stato modificato epr un'ultimo con una simile espressione:</p> <pre><code>&gt;&gt;&gt; max((f.stat().st_mtime, f) for f in directory.iterdir())[1].read_text()\n&lt;the contents of the last modified file in directory&gt;\n</code></pre> <p>Il timestamp restituito dalle diverse propriet\u00e0 <code>.stat().st_</code> rappresenta i secondi dal primo gennaio 1970. Oltre a <code>datetime.fromtimestamp</code>, <code>time.localtime</code> o <code>time.ctime</code> possono essere usati per convertire il timestamp in qualcosa di pi\u00f9 utilizzabile.</p>"},{"location":"material/01_python/03_advanced/pathlib/#creare-un-nome-del-file-univoco","title":"Creare un nome del file univoco","text":"<p>L'ultimo esempio ci mostra come costruire un file con numero univoco basato su una tempalte. Per prima cosa, specifichiamo un pattern per il nome del file, con spazio per un contatore. Quindi, controlliamo l'esistenza dle percorso del file creato unendo una cartella ed il nome del file (con un valore per il contatore). SE esiste gi\u00e0, aumentiamo il contatore e proviamo di nuovo:</p> <pre><code>def unique_path(directory, name_pattern):\n    counter = 0\n    while True:\n        counter += 1\n        path = directory / name_pattern.format(counter)\n        if not path.exists():\n            return path\n\npath = unique_path(pathlib.Path.cwd(), 'test{:03d}.txt')\n</code></pre> <p>Se la cartella contiene gi\u00e0 i file test001.txt e test002.txt, il codice precedente imposter\u00e0 il path a test003.txt.</p>"},{"location":"material/01_python/03_advanced/pathlib/#differenze-tra-sistemi-operativi","title":"Differenze tra sistemi operativi","text":"<p>In precedenza, abbiamo notato che abbiamo istanziato pathlib.Path, ed abbiamo ottenuto in ritorno un WindowsPath o un PosixPath. Il tipo di oggetto dipende dal sistema operativo utilizzato. Questa feature rende semplice scrivere del codice cross-platform. E' possibile richiedere esplcitiamente un WindowsPath o un PosixPath, ma staremmo limitando il codice a quel sistema, senza alcun beneficio. Un path concreto come questo non pu\u00f2 essere usato su un sistema differente:</p> <pre><code>&gt;&gt;&gt; pathlib.WindowsPath('test.md')\nNotImplementedError: cannot instantiate 'WindowsPath' on your system\n</code></pre> <p>Ci sono delle volte dove abbiamo bisogno della rappresentazione di un path senza accedere al file system sottostante (nel qual caso potrebbe aver senso rappresentare un percorso Windows su un sistema non-Windows, o viceversa). Questo pu\u00f2 essere fatto con degli oggetti di tipo PurePath. Questi oggetti supportano le operazioni discusse nella sezione sui Path Components, ma non i metodi che accedono al file system.</p> <pre><code>&gt;&gt;&gt; path = pathlib.PureWindowsPath(r'C:\\Users\\gahjelle\\realpython\\file.txt')\n&gt;&gt;&gt; path.name\n'file.txt'\n&gt;&gt;&gt; path.parent\nPureWindowsPath('C:/Users/gahjelle/realpython')\n&gt;&gt;&gt; path.exists()\nAttributeError: 'PureWindowsPath' object has no attribute 'exists'\n</code></pre> <p>Possiamo istanziare direttaqmente un <code>PureWindowsPath</code>o un PurePosixPath su ttutti i sistemi. Istanziare un PurePath restituir\u00e0 uno di questi oggetti a seconda del sistema operativo utilizzato.</p>"},{"location":"material/01_python/03_advanced/pathlib/#path-come-oggetti-veri-e-propri","title":"Path come oggetti veri e propri","text":"<p>Nell'introduzione, abbiamo notato brevemente che i path non sono stringhe, ed una motivazione dietro a questo \u00e8 che pathlib rappresenta il file system cond egli oggetti veri e propri. Infatti, la documenbtazione ufficiale di pathlib \u00e8 intitolata \"pathlib - Object-oriented filesystem paths\". L'approccio orientato agli oggetti \u00e8 abbastanza visibile negli esempi precedenti (specialmente se lo mettiamo a confronto con la vecchia maniera usatga da <code>os.path</code>). Tuyttavia, vediamo qualche ultimo spunto.</p> <p>Indipendentemente dal sistema operativo utilizzato, i path sono rappresentati in stile Posix, con lo slash come separatore del path. Su Windows, vedremo qualcosa come questo:</p> <pre><code>&gt;&gt;&gt; pathlib.Path(r'C:\\Users\\gahjelle\\realpython\\file.txt')\nWindowsPath('C:/Users/gahjelle/realpython/file.txt')\n</code></pre> <p>Quando il path viene convertito in stringa, user\u00e0 la forma nativa, per esempio con delle backslash su Windows:</p> <pre><code>&gt;&gt;&gt; str(pathlib.Path(r'C:\\Users\\gahjelle\\realpython\\file.txt'))\n'C:\\\\Users\\\\gahjelle\\\\realpython\\\\file.txt'\n</code></pre> <p>Questo risulta essere particolarmente utile se stiamo usando una libreria che non sa come usare gli oggetti pathlib.Path. Qeusto \u00e8 un grosso problema sulle versioni di Python antecedendi la 3.6. Ad esempio, in Python 3.5, la libreria standard configparser pu\u00f2 usare solo dei path sotto forma di stringa per leggere dei file. Il modo di gestire questi casi \u00e8 fare la conversione ad una stringa in maniera esplcitia.</p> <pre><code>&gt;&gt;&gt; from configparser import ConfigParser\n&gt;&gt;&gt; path = pathlib.Path('config.txt')\n&gt;&gt;&gt; cfg = ConfigParser()\n&gt;&gt;&gt; cfg.read(path)                     # Error on Python &lt; 3.6\nTypeError: 'PosixPath' object is not iterable\n&gt;&gt;&gt; cfg.read(str(path))                # Works on Python &gt;= 3.4\n['config.txt']\n</code></pre> <p>In Python 3.6 e successovo \u00e8 raccomandabile usare <code>os.fpath()</code> invece di <code>str()</code> se dobbiamo fare una conversione esplciita. Questo \u00e8 pi\u00f9 sicuro, in quando lancer\u00e0 un errore se proviamo a convertire accidentalmente un oggetto che non \u00e8 un path.</p> <p>Probabilmente, la parte pi\u00f9 inusuale della libreria pathlibe \u00e8 data dall'uso dell'operatore <code>/</code>. Vediamo come \u00e8 implementato. Questo \u00e8 un esempio di un overloading di operatore: il comportamento di un operatore cambia a seconda del contesto. Python implementa l'overloading di operatori attraverso l'uso dei dunder metrhods, ovvero dei metodi circondati da un doppio underscore.</p> <p>L'operatore <code>/</code> \u00e8 definito dal metodo <code>.__truediv__()</code>. Infatti, se diamo un'occhiata al codice sorgente di <code>pathlib</code>, vedremo qualcosa come:</p> <pre><code>class PurePath(object):\n\n    def __truediv__(self, key):\n        return self._make_child((key,))\n</code></pre>"},{"location":"material/01_python/03_advanced/pathlib/#conclusion","title":"Conclusion","text":"<p>Da Python 3.4 in poi, pathlib \u00e8 stato reso disponibile nella libreria standard. Con pathlib, i path dei file possono essere rappresentati da oggetti Path veri e propri, invece di stringhe come in precedenza. Questi oggetti rendono l'uso dei path:</p> <ul> <li>pi\u00f9 facile da leggere, specie perch\u00e9 <code>/</code> \u00e8 usato per unire tra loro i path;</li> <li>pi\u00f9 potente, con la maggior parte dei metodi necessari e delle propriet\u00e0 disponibili direttamente sull'oggetto;</li> <li>pi\u00f9 consistenti tra sistemi operativi, in quanto le peculariet\u00e0 dei diversi sistemi sono nascoste dall'oggetto <code>Path</code>.</li> </ul>"},{"location":"material/01_python/03_advanced/reference/","title":"Passaggio per reference","text":"<p>Dopo aver guadagnato un po' di familiarit\u00e0 con Python, potremmo notare dei casi nei quali le nostre funzioni non modificano degli argometni come atteso. Specialmente se veniamo d aaltri linguaggi, vedremo che alcuni di questi gestiscono gli argometni di una funzione come dei riferimenti a variabili esistenti, il che \u00e8 conosciuto come passaggio per riferimento. Altri linguaggi, invece, trattano le due variabili come valori indipendenti, un approccio conosciuto come passaggio per valore.</p> <p>In questa lezione, vedremo:</p> <ul> <li>cosa significa passare per reference, e perch\u00e9 dovremmo volerlo fare;</li> <li>come il passaggio per refrence differisce sia dal passaggio per valore, sia dall'approccio unico di Python;</li> <li>come si comportano i funciton arguments in Python</li> <li>come possiamo usare certi tipi mutabili per effettuare il passaggio per reference in Python</li> <li>quali sono le best practice per replicare il passaggio per reference in Python</li> </ul>"},{"location":"material/01_python/03_advanced/reference/#cosa-e-il-passaggio-per-reference","title":"Cosa \u00e8 il passaggio per reference?","text":"<p>Prima di addentrarci nei dettagli tecnici del passaggio per reference, pu\u00f2 esserci d'aiuto dare uno sguardo ravvicinato al termine stesso \"rompendolo\" in pi\u00f9 componenti:</p> <ul> <li>passaggio significa fornire un arogmento ad una funzione;</li> <li>reference indca che l'argomento che stiamo passando alla funzione \u00e8 un riferimento ad una variabile che esiste gi\u00e0 in memoria, e non una copia indipendente di quella stessa variabile.</li> </ul> <p>Dal momento che stiamo dando alla funzione un riferimento ad una variabile esistente, tutte le operazioni effettuate su questo riferimento influenzeranno direttamente la variabile a cui si riferisce.</p> <p>Vediamo alcuni esempi di come questo funziona nella pratica.</p> <p>Vediamo come passare una variabile per reference in C#. NOtiamo l'uso della parola chiave ref nelle righe evidenziate:</p> <pre><code>using System;\n\n// Source:\n// https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/passing-parameters\nclass Program\n{\n    static void Main(string[] args)\n    {\n        int arg;\n\n        // Passing by reference.\n        // The value of arg in Main is changed.\n        arg = 4;\n        squareRef(ref arg);\n        Console.WriteLine(arg);\n        // Output: 16\n    }\n\n    static void squareRef(ref int refParameter)\n    {\n        refParameter *= refParameter;\n    }\n}\n</code></pre> <p>Come possiamo vedere, il <code>refParameter</code> di <code>squareRef()</code> deve essere dichiarato usando la parola chaive <code>ref</code>, e dobbiamo anche usare la parola chiave quando chiamiamo la funzione. Quindi, l'argomento sar\u00e0 passato per reference, e potr\u00e0 essere modificato in place.</p> <p>Python non ha una parola chiave <code>ref</code>, o un suo equivalente. SE provassimo a replicare l'esempio precedente in Python, avremmo risultati differenti:</p> <pre><code>&gt;&gt;&gt; def main():\n...     arg = 4\n...     square(arg)\n...     print(arg)\n...\n&gt;&gt;&gt; def square(n):\n...     n *= n\n...\n&gt;&gt;&gt; main()\n4\n</code></pre> <p>In questo caso, la variabile <code>arg</code> non viene alterata in place. Sembra che Python tratti l'argomento passato come valore singolo piuttosto che una reference ad una variabile esistente. Questo singifica che Python passa gli argomenti per valore piuttosto che per reference?</p> <p>Non proprio. Python passa gli argomenti n\u00e9 per reference n\u00e9 per valore, ma per assegnazione (assignment). Vedremo a breve rapidamente i dettalgi del passaggio per valore e per reference prima di guardare pi\u00f9 da vicino l'approccio di Python. Dopo questo, vedremo alcune delle best practice epr ottenere l'equivalente del passaggiuo per reference in Python.</p>"},{"location":"material/01_python/03_advanced/reference/#passaggio-per-valore-vs-passaggio-per-reference","title":"passaggio per valore vs. passaggio per reference","text":"<p>Quando passiamo degli argomenti di una funzione per reference, questi argomenti sono riferiti soltanto a valori gi\u00e0 esistenti. Di contro, quando passaimo gli argomenti per valore, questi diventano copie indipendenti dei valori originari.</p> <p>Torniamo brevemente all'esempio in C#, questa volta togliendo la parola chiave <code>ref</code>. Questo far\u00e0 s\u00ec che il programma usi il comportamento di default, ovvero il passaggio per valore:</p> <pre><code>using System;\n\n// Source:\n// https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/passing-parameters\nclass Program\n{\n    static void Main(string[] args)\n    {\n        int arg;\n\n        // Passing by value.\n        // The value of arg in Main is not changed.\n        arg = 4;\n        squareVal(arg);\n        Console.WriteLine(arg);\n        // Output: 4\n    }\n\n    static void squareVal(int valParameter)\n    {\n        valParameter *= valParameter;\n    }\n}\n</code></pre> <p>Qui, possiamo vedere che <code>squareVal()</code> non modifica la variabile originaria. Piuttosto, <code>valParameter</code> \u00e8 una copia indipendente della variabile originaria <code>arg</code>. Anche se questo combacia con il comportamento che vedremmo in Python, ricordiamo che Python non passa esattamente per valore. Proviamo ci\u00f2.</p> <p>La funzione integrata <code>id()</code> di Python restituisce un intero che rappresenta l'indirizzo di memoria dell'oggetto desiderato. Usando <code>id()</code>, possiamo verificare che:</p> <ul> <li>gli argomenti della funzione si riferiscono inizialmente allo stesso indirizzo delle loro variabili originarie</li> <li>riassegnare l'argomento nella funzione gli d\u00e0 un nuovo indirizzo, mentre la variabile originaria rimane immutata</li> </ul> <p>Nell'esempio successivo, notiamo che l'idnirizzo di <code>x</code> combacia inizialmente con quello di <code>n</code>, ma cambia dopo la riassegnazione, mentre l'indirizzo di <code>n</code> non cambia:</p> <pre><code>&gt;&gt;&gt; def main():\n...     n = 9001\n...     print(f\"Initial address of n: {id(n)}\")\n...     increment(n)\n...     print(f\"  Final address of n: {id(n)}\")\n...\n&gt;&gt;&gt; def increment(x):\n...     print(f\"Initial address of x: {id(x)}\")\n...     x += 1\n...     print(f\"  Final address of x: {id(x)}\")\n...\n&gt;&gt;&gt; main()\nInitial address of n: 140562586057840\nInitial address of x: 140562586057840\n  Final address of x: 140562586057968\n  Final address of n: 140562586057840\n</code></pre> <p>Il fatto che l'indirizzo iniziale di <code>n</code> ed <code>x</code> siano gli stessi quando invochiamo <code>increment()</code> prova che l'argomento <code>x</code> non sta venendo passato per valore. Altrimetni, <code>n</code> ed <code>x</code> avrebbero avuto indirizzi di memoria distinti.</p> <p>Prima di apprendere i dettagli di come Python gestisce gli argomenti, vediamo alcuni casi d'uso pratici del passaggio per reference.</p>"},{"location":"material/01_python/03_advanced/reference/#utilizzare-dei-costrutti-di-passaggio-per-referecne","title":"Utilizzare dei costrutti di passaggio per referecne","text":"<p>Passare variabili per reference \u00e8 una delle tante strategie che possiamo usare per implementare determianti pattern di programmazione. Anche se non \u00e8 sempre necessario, il passaggio per reference pu\u00f2 comunque essere utile.</p> <p>In questa sezione, vedremo tre dei pattern pi\u00f9 comuni per i quali il passaggio per reference \u00e8 un approccio pratico. Vedremo quindi come implementare ciascuno di questi pattern in Python.</p>"},{"location":"material/01_python/03_advanced/reference/#evitare-oggetti-duplicati","title":"Evitare oggetti duplicati","text":"<p>Come abbiamo visto, passare una variabile per valore casuer\u00e0 la creazione e memorizzazione di una copia di quel valore, Nei linguaggi in cui il comportamento di default \u00e8 il passaggio per valore, potremmo avere dei benefici in termini di performance nel passaggio per reference, specialmente qunado la variabile ha molti dati. Questo sar\u00e0 abbastanza evidente quando il codice gira su macchine con risorse limitate.</p> <p>In Python, tuttavia, questo non \u00e8 mai un problema. Veddemo il perch\u00e9 nella sezione successiva.</p>"},{"location":"material/01_python/03_advanced/reference/#restiture-valori-multipli","title":"Restiture valori multipli","text":"<p>Una delle applicazioni pi\u00f9 comuni del passaggio per reference \u00e8 creare auna funzione che alteri il valore dei parametri di riferimento restituendo un valore distinto. Possiamo modificare l'esempio del passaggio per reference in C# per illustrare questa tecnica:</p> <pre><code>using System;\n\nclass Program\n{\n    static void Main(string[] args)\n    {\n        int counter = 0;\n\n        // Passing by reference.\n        // The value of counter in Main is changed.\n        Console.WriteLine(greet(\"Alice\", ref counter));\n        Console.WriteLine(\"Counter is {0}\", counter);\n        Console.WriteLine(greet(\"Bob\", ref counter));\n        Console.WriteLine(\"Counter is {0}\", counter);\n        // Output:\n        // Hi, Alice!\n        // Counter is 1\n        // Hi, Bob!\n        // Counter is 2\n    }\n\n    static string greet(string name, ref int counter)\n    {\n        string greeting = \"Hi, \" + name + \"!\";\n        counter++;\n        return greeting;\n    }\n}\n</code></pre> <p>Nell'esempio precedente, <code>greet()</code> restituisce una stringa di benvenuto e modifica contestualmente il valore del contatore. Ora, proviamo a riprodurre questo quanto pi\u00f9 possibile in Python:</p> <pre><code>&gt;&gt;&gt; def main():\n...     counter = 0\n...     print(greet(\"Alice\", counter))\n...     print(f\"Counter is {counter}\")\n...     print(greet(\"Bob\", counter))\n...     print(f\"Counter is {counter}\")\n...\n&gt;&gt;&gt; def greet(name, counter):\n...     counter += 1\n...     return f\"Hi, {name}!\"\n...\n&gt;&gt;&gt; main()\nHi, Alice!\nCounter is 0\nHi, Bob!\nCounter is 0\n</code></pre> <p>Counter non viene incrementato nell'esempio precedente perch\u00e9, come abbiamo appreso in precedenza, Python non ha modo di passare i valori per reference. Quindi, come possiamo ottenere lo stesso risultato ottenuto in C#?</p> <p>In pratica, i parametri passati per riferimento in C# permettono alla funzione non solo di restituire un valore, ma anche di operare su parametri aggiuntivi. QUesto equivale a restituire valori multipli.</p> <p>Fortunatamente, Python supporta gi\u00e0 la restituzione di valori multipli. In senso stretto, una funzione Python che restituisce valori multipli restituisce in effetti una tupla che contiene ogni valore.</p> <pre><code>&gt;&gt;&gt; def multiple_return():\n...     return 1, 2\n...\n&gt;&gt;&gt; t = multiple_return()\n&gt;&gt;&gt; t  # A tuple\n(1, 2)\n\n&gt;&gt;&gt; # You can unpack the tuple into two variables:\n&gt;&gt;&gt; x, y = multiple_return()\n&gt;&gt;&gt; x\n1\n&gt;&gt;&gt; y\n2\n</code></pre> <p>Come possiamo vedere, per restiturie valori multipli, possiamo semplicemente usare la parola chiave <code>return</code>seguita da valori separati da virgole, o variabili.</p> <p>Grazie a questa tecnica, posisamo modificare l'istruzione di ritorno in <code>greet()</code> dal codice Python precedente per restituire sia un messaggio di benvenuto sia un contatore:</p> <pre><code>&gt;&gt;&gt; def main():\n...     counter = 0\n...     print(greet(\"Alice\", counter))\n...     print(f\"Counter is {counter}\")\n...     print(greet(\"Bob\", counter))\n...     print(f\"Counter is {counter}\")\n...\n&gt;&gt;&gt; def greet(name, counter):\n...     return f\"Hi, {name}!\", counter + 1\n...\n&gt;&gt;&gt; main()\n('Hi, Alice!', 1)\nCounter is 0\n('Hi, Bob!', 1)\nCounter is 0\n</code></pre> <p>Tuttavia, questo ancora non sembra giusto. Anche se adesso <code>greet()</code> restituisce pi\u00f9 valori, questi stanno venendo stampati come una tupla, il che non \u00e8 nostra intenzione. Inoltre, la variabile contatore originaria rimane a 0.</p> <p>Per pulire il nostro output ed ottenere i risutlati desiderati, dobbiamo reassegnare la variabile <code>counter</code> con ogni chiamata a <code>greet()</code>.</p> <pre><code>&gt;&gt;&gt; def main():\n...     counter = 0\n...     greeting, counter = greet(\"Alice\", counter)\n...     print(f\"{greeting}\\nCounter is {counter}\")\n...     greeting, counter = greet(\"Bob\", counter)\n...     print(f\"{greeting}\\nCounter is {counter}\")\n...\n&gt;&gt;&gt; def greet(name, counter):\n...     return f\"Hi, {name}!\", counter + 1\n...\n&gt;&gt;&gt; main()\nHi, Alice!\nCounter is 1\nHi, Bob!\nCounter is 2\n</code></pre> <p>Ora, dopo aver riassegnato ogni variabile con una chiamata a <code>greet()</code>, potremo vedere i risultati desiderati.</p> <p>Assegnare i valori di ritorno alle variabili \u00e8 il miglior modo di ottenere gli stessi risultati del passaggio per reference in Python. Vedremo il perch\u00e9, tra le altre cose, nella sezione sulle best pracitce.</p>"},{"location":"material/01_python/03_advanced/reference/#creazione-di-funzioni-condizionali-con-piu-valori-di-ritorno","title":"Creazione di funzioni condizionali con pi\u00f9 valori di ritorno","text":"<p>Questo \u00e8 un caso d'uso specifico di restituire pi\u00f9 valori nel quale la funzione pu\u00f2 essere usata in un'istruzione condizionale ed ha dei side effect aggiuntivi, come modificare una variabile esterna passata come argomento.</p> <p>Consideriamo la funzione standard <code>Int32.TryParse</code> in C#, che restituisce un valore booleano edmopera su una reference ad un argomento intero allo stesso tempo:</p> <pre><code>public static bool TryParse (string s, out int result);\n</code></pre> <p>Questa funzione prova a convertire una stringa in un intero a 32 bit cons egno usando la parola chiave <code>out</code>. Ci sono due possibili risultati:</p> <ul> <li>se il parsing ha successo, il parametro di output sar\u00e0 impostato all'intero risultatnte, e la funzione restituir\u00e0 <code>true</code></li> <li>se il parsing fallisce, allora il parmaetro di output sar\u00e0 impostato a <code>0</code>, e la funzione restituir\u00e0 falso</li> </ul> <p>Possiamo vedere questo nella pratica nel seguente esempio, che prova a convertire un certo numero di stringhe differenti:</p> <pre><code>using System;\n\n// Source:\n// https://docs.microsoft.com/en-us/dotnet/api/system.int32.tryparse?view=netcore-3.1#System_Int32_TryParse_System_String_System_Int32__\npublic class Example {\n    public static void Main() {\n        String[] values = { null, \"160519\", \"9432.0\", \"16,667\",\n                            \"   -322   \", \"+4302\", \"(100);\", \"01FA\" };\n        foreach (var value in values) {\n            int number;\n\n            if (Int32.TryParse(value, out number)) {\n                Console.WriteLine(\"Converted '{0}' to {1}.\", value, number);\n            }\n            else {\n                Console.WriteLine(\"Attempted conversion of '{0}' failed.\",\n                                   value ?? \"&lt;null&gt;\");\n            }\n        }\n    }\n}\n</code></pre> <p>Il codice precedente, che prova a convertrire delle stringhe formattate differentemente in interi mediante <code>TryParse()</code>, manda in output il seguente:</p> <pre><code>Attempted conversion of '&lt;null&gt;' failed.\nConverted '160519' to 160519.\nAttempted conversion of '9432.0' failed.\nAttempted conversion of '16,667' failed.\nConverted '   -322   ' to -322.\nConverted '+4302' to 4302.\nAttempted conversion of '(100);' failed.\nAttempted conversion of '01FA' failed.\n</code></pre> <p>Per implementare una funzione simile in Python, possiamo usare molteplici valori di ritorno come visto in precedenza:</p> <pre><code>def tryparse(string, base=10):\n    try:\n        return True, int(string, base=base)\n    except ValueError:\n        return False, None\n</code></pre> <p>Questa funzione <code>tryparse()</code> restituisce due valori. Il primo valore indica se la conversione ha avuto successo, ed il secondo contiene il risultato (o <code>None</code>, in caso di fallimento).</p> <p>Tuttavia, l'uso di questa funzione \u00e8 un po' CLUNKY, perch\u00e9 dobbiamo spacchettare il valore di ritorno ad ogni chiamata. Questo significa che non possiamo usare la funzione all'interno di un'istruzione if:</p> <pre><code>&gt;&gt;&gt; success, result = tryparse(\"123\")\n&gt;&gt;&gt; success\nTrue\n&gt;&gt;&gt; result\n123\n\n&gt;&gt;&gt; # We can make the check work\n&gt;&gt;&gt; # by accessing the first element of the returned tuple,\n&gt;&gt;&gt; # but there's no way to reassign the second element to `result`:\n&gt;&gt;&gt; if tryparse(\"456\")[0]:\n...     print(result)\n...\n123\n</code></pre> <p>Anche se in linea generale questo funziona restituendo pi\u00f9 valori, <code>tryparse()</code> non pu\u00f2 essere usato in un check condizionale. Questo significa che abbiamo dell'ulteriore lavoro da fare.</p> <p>Possiamo sfruttare la flessibilit\u00e0 di Python e semplificare la funzione per restituire un singolo valore di diversi tipi a seconda del fatto che la conversione abbia successo:</p> <pre><code>def tryparse(string, base=10):\n    try:\n        return int(string, base=base)\n    except ValueError:\n        return None\n</code></pre> <p>Grazie al fatto che le funzioni Python possono restituire diversi tipi di dato, possiamo usare questa funzione all'interno di un'istruzione condizionale. Ma come? Non dovremmo chiamare per prima la funzione, assegnare il suo valore di ritorno, e quindi controllare il valore stesso?</p> <p>Sfruttando la flessibilit\u00e0 di Python sui tipi degli oggetti, cos\u00ec come le nuove espressioni di assegnazione in Python 3.8, possiamo chiamare questa funzione semplificata all'interno di un'istruzione if ed ottenere il valore di ritorno se il controllo passa:</p> <pre><code>&gt;&gt;&gt; if (n := tryparse(\"123\")) is not None:\n...     print(n)\n...\n123\n&gt;&gt;&gt; if (n := tryparse(\"abc\")) is None:\n...     print(n)\n...\nNone\n\n&gt;&gt;&gt; # You can even do arithmetic!\n&gt;&gt;&gt; 10 * tryparse(\"10\")\n100\n\n&gt;&gt;&gt; # All the functionality of int() is available:\n&gt;&gt;&gt; 10 * tryparse(\"0a\", base=16)\n100\n\n&gt;&gt;&gt; # You can also embed the check within the arithmetic expression!\n&gt;&gt;&gt; 10 * (n if (n := tryparse(\"123\")) is not None else 1)\n1230\n&gt;&gt;&gt; 10 * (n if (n := tryparse(\"abc\")) is not None else 1)\n10\n</code></pre> <p>Notiamo che questa versione di <code>tryparse()</code> risulta essere anche pi\u00f9 potente della versione C#, permettendoci di suarla all'interno di istruzioni condizionali ed in espressioni aritmetiche.</p> <p>Con un po' di ingenuit\u00e0, abbiamo replicato un pattern specifico ed utile di passaggio per reference senza passare gli argomenti per reference. Infatti, dobbiamo di nuovo assegnare i valori di ritorno quando usiamo l'operatore di assegnazione (:=) ed usando il valore di ritorno direttamente nelle espressioni Python.</p> <p>Finora, abbiamo appreso quello che significa passaggio per reference, come differisce dal passaggio per valore, e come l'approccio di Python differisca da entrambi. Ora siamo pronti a dare un'occhiata ravvicinata a come Python gestisce gli argomenti di una funzione.</p>"},{"location":"material/01_python/03_advanced/reference/#passare-gli-argomenti-in-python","title":"Passare gli argomenti in Python","text":"<p>Python passa gli argomenti per assegnazione. Ci\u00f2 significa che quando chiamiamo una funzione Python, ogni argomento diventa una variabile alla quale viene assegnato il valore passato.</p> <p>Quindi, possiamo apprendere dettagli importanti su come Python gestisce gli argomenti della funzione comprendendo come il meccanismo di assegnazione stesso funziona, anche al di fuori delle funzioni.</p>"},{"location":"material/01_python/03_advanced/reference/#comprendere-lassegnazione-in-python","title":"Comprendere l'assegnazione in Python","text":"<p>La documentazione di Python per le istruzioni di assegnazione fornisce i seguenti dettagli:</p> <ul> <li>se l'obiettivo dell'assegnazione \u00e8 un identificatore, o il nome di una variabile, questo nome \u00e8 collegato all'oggetto. Per esempio, in <code>x = 2</code>, <code>x</code> \u00e8 il nome e <code>2</code> \u00e8 l'oggetto.</li> <li>se il nome \u00e8 gi\u00e0 collegato ad un oggetto separato, viene quindi ri-collegato al nuovo oggetto. Ad esempio, se <code>x</code> \u00e8 gi\u00e0 <code>2</code> ed abbiamo scritto <code>x = 3</code>, allora il nome della variabile <code>x</code> viene ri-assegnato a <code>3</code>.</li> </ul> <p>Tutti gli oggetti Python sono implementati secondo una certa struttura. UNa delle propriet\u00e0 di qeusta struttura \u00e8 un contatore che tiene traccia di qunati nomi sono stati collegati a questo oggetto.</p> <p>Nota</p> <p>Questo contatore \u00e8 chiamato reference counter perch\u00e9 tiene traccia di quante reference, o nomi, puntano allo stesso oggetto. Non confondiamo il reference counter con il concetto di passaggio per reference, in quanto i due sono incorrelati.</p> <p>La documentazione Python fornisce ulteriori dettagli sui reference counts.</p> <p>Rimaniamo all'esempio <code>x = 2</code> ed esaminiamo quello che accade quando assegnamo un valore ad una nuova varibile:</p> <ul> <li>se un oggetto rappresentativo del valore <code>2</code> esiste gi\u00e0, viene recuperato. Altrimenti, \u00e8 creato.</li> <li>il reference counter di questo oggetto viene incrementato</li> <li>un'entrata \u00e8 aggiunta nell'attuale namespace per collegare l'identificatore <code>x</code> all'oggetto che rappresenta <code>2</code>. Questa entrata \u00e8 nei fatti una coppia chiave-valore memorizzata in un dizionario. Una rappresentazione di questo dizionario \u00e8 restituita da <code>locals()</code> o <code>globals()</code>.</li> </ul> <p>Ecco quello chea ccade se riassegnamo <code>x</code> ad un diverso valore:</p> <ul> <li>il reference counter dell'oggetto rappresentante <code>2</code> viene decrementato</li> <li>il reference counter dell'oggetto che rappresenta il nuovo valore \u00e8 incrementato</li> <li>il dizionario per l'attuale namespace \u00e8 aggiornato per correlare <code>x</code> all'oggetto rappresentante il nuovo valore</li> </ul> <p>Python ci permette di ottenre il reference counter per valori arbitrari con la funzione <code>sys.getrefcount()</code>. Possiamo usarla per illustrare come l'assegnazione incemenrta e decrementa questi reference counter. Notiamo che l'interprete interattivo sfrutta il comportamento che dar\u00e0 risultati differenti, per cui dovremo eseguire il seguente codice da un file:</p> <pre><code>from sys import getrefcount\n\nprint(\"--- Before  assignment ---\")\nprint(f\"References to value_1: {getrefcount('value_1')}\")\nprint(f\"References to value_2: {getrefcount('value_2')}\")\nx = \"value_1\"\nprint(\"--- After   assignment ---\")\nprint(f\"References to value_1: {getrefcount('value_1')}\")\nprint(f\"References to value_2: {getrefcount('value_2')}\")\nx = \"value_2\"\nprint(\"--- After reassignment ---\")\nprint(f\"References to value_1: {getrefcount('value_1')}\")\nprint(f\"References to value_2: {getrefcount('value_2')}\")\n</code></pre> <p>Questo script mostrer\u00e0 il conteggio delle reference per ogni valore prima dell'assegnazione, dopo l'assegnazione, e dopo la riassegnazione:</p> <pre><code>--- Before  assignment ---\nReferences to value_1: 3\nReferences to value_2: 3\n--- After   assignment ---\nReferences to value_1: 4\nReferences to value_2: 3\n--- After reassignment ---\nReferences to value_1: 3\nReferences to value_2: 4\n</code></pre> <p>Questi risultai illustrano la relazione tra gli identificatori (nomi delle variabili) e gli oggetti Python che rapprentano valori distinti. Quando assegnamo pi\u00f9 variabili allo stesso valore, Python aumenta il reference counter per l'oggetto esistente ed aggiorna il namespace attuale piuttosto che creare oggetti duplicati in memroia.</p> <p>Vediamo adesso di esplorare come Python gestisce gli argomenti di una funzione.</p>"},{"location":"material/01_python/03_advanced/reference/#esplorazione-degli-argomenti-di-una-funzione","title":"Esplorazione degli argomenti di una funzione","text":"<p>Gli argomenti di una funzione in Python sono variabili locali. Cosa significa? Quello locale \u00e8 uno degli ambiti definiti in Python (ed in ogni linguaggio di programmazione). Questi ambiti sono rappresentati dai namespace dictionary menizioanti nella sezione precedente. Possiamo usare <code>locals()</code> e <code>globals()</code> per recuperare i namespace dictionary locali e globali, rispettivamnete.</p> <p>All'esecuzione, ogni funzione ha il suo proprio namespace locale:</p> <pre><code>&gt;&gt;&gt; def show_locals():\n...     my_local = True\n...     print(locals())\n...\n&gt;&gt;&gt; show_locals()\n{'my_local': True}\n</code></pre> <p>Usando <code>locals()</code>, possiamo dimostrare che gli argomenti della funzione diventano variabili regolai nel namespace locale della funzione. Aggiungiamo un argomento, <code>my_arg</code>, alla funzione:</p> <pre><code>&gt;&gt;&gt; def show_locals(my_arg):\n...     my_local = True\n...     print(locals())\n...\n&gt;&gt;&gt; show_locals(\"arg_value\")\n{'my_arg': 'arg_value', 'my_local': True}\n</code></pre> <p>Possiamo anche usare <code>sys.getrefcount()</code> per mostrare come gli argomenti di funzioni aumentano il conteggio delle reference per un oggetto:</p> <pre><code>&gt;&gt;&gt; from sys import getrefcount\n\n&gt;&gt;&gt; def show_refcount(my_arg):\n...     return getrefcount(my_arg)\n...\n&gt;&gt;&gt; getrefcount(\"my_value\")\n3\n&gt;&gt;&gt; show_refcount(\"my_value\")\n5\n</code></pre> <p>Lo script precedente scrive il conteggio delle reference per <code>my_value</code> per prima cosa all'esterno, poi all'interno di <code>show_refcount()</code>, mostrando un aumento del reference count non di uno, ma di due!</p> <p>Questo perch\u00e9, oltre a <code>show_refcount()</code> stesso, la chiamata a <code>sys.getrefcount()</code> all'interno di <code>show_refcount()</code> riceve anche <code>my_arg</code> come argomento. Questo piazza <code>my_arg</code> nel namespace locale per <code>sys.getrefcount()</code>, aggiungendo una reference extra a <code>my_value</code>.</p> <p>By examining namespaces and reference counts inside functions, you can see that function arguments work exactly like assignments: Python creates bindings in the function\u2019s local namespace between identifiers and Python objects that represent argument values. Each of these bindings increments the object\u2019s reference counter.</p> <p>Now you can see how Python passes arguments by assignment!</p> <p>Remove ads Replicating Pass by Reference With Python Having examined namespaces in the previous section, you may be asking why global hasn\u2019t been mentioned as one way to modify variables as if they were passed by reference:</p> <p>def square(): ...     # Not recommended! ...     global n ...     n *= n ... n = 4 square() n 16 Using the global statement generally takes away from the clarity of your code. It can create a number of issues, including the following:</p> <p>Free variables, seemingly unrelated to anything Functions without explicit arguments for said variables Functions that can\u2019t be used generically with other variables or arguments since they rely on a single global variable Lack of thread safety when using global variables Contrast the previous example with the following, which explicitly returns a value:</p> <p>def square(n): ...     return n * n ... square(4) 16 Much better! You avoid all potential issues with global variables, and by requiring an argument, you make your function clearer.</p> <p>Despite being neither a pass-by-reference language nor a pass-by-value language, Python suffers no shortcomings in that regard. Its flexibility more than meets the challenge.</p> <p>Best Practice: Return and Reassign You\u2019ve already touched on returning values from the function and reassigning them to a variable. For functions that operate on a single value, returning the value is much clearer than using a reference. Furthermore, since Python already uses pointers behind the scenes, there would be no additional performance benefits even if it were able to pass arguments by reference.</p> <p>Aim to write single-purpose functions that return one value, then (re)assign that value to variables, as in the following example:</p> <p>def square(n):     # Accept an argument, return a value.     return n * n</p> <p>x = 4 ...</p>"},{"location":"material/01_python/03_advanced/reference/#later-reassign-the-return-value","title":"Later, reassign the return value:","text":"<p>x = square(x) Returning and assigning values also makes your intention explicit and your code easier to understand and test.</p> <p>For functions that operate on multiple values, you\u2019ve already seen that Python is capable of returning a tuple of values. You even surpassed the elegance of Int32.TryParse() in C# thanks to Python\u2019s flexibility!</p> <p>If you need to operate on multiple values, then you can write single-purpose functions that return multiple values, then (re)assign those values to variables. Here\u2019s an example:</p> <p>def greet(name, counter):     # Return multiple values     return f\"Hi, {name}!\", counter + 1</p> <p>counter = 0 ...</p>"},{"location":"material/01_python/03_advanced/reference/#later-reassign-each-return-value-by-unpacking","title":"Later, reassign each return value by unpacking.","text":"<p>greeting, counter = greet(\"Alice\", counter) When calling a function that returns multiple values, you can assign multiple variables at the same time.</p> <p>Best Practice: Use Object Attributes Object attributes have their own place in Python\u2019s assignment strategy. Python\u2019s language reference for assignment statements states that if the target is an object\u2019s attribute that supports assignment, then the object will be asked to perform the assignment on that attribute. If you pass the object as an argument to a function, then its attributes can be modified in place.</p> <p>Write functions that accept objects with attributes, then operate directly on those attributes, as in the following example:</p> <p>It\u2019s worth repeating that you should make sure the attribute supports assignment! Here\u2019s the same example with namedtuple, whose attributes are read-only:</p> <p>from collections import namedtuple NS = namedtuple(\"NS\", \"n\") def square(instance): ...     instance.n *= instance.n ... ns = NS(4) ns.n 4 square(ns) Traceback (most recent call last):   File \"\", line 1, in    File \"\", line 2, in square AttributeError: can't set attribute Attempts to modify attributes that don\u2019t allow modification result in an AttributeError. <p>Additionally, you should be mindful of class attributes. They will remain unchanged, and an instance attribute will be created and modified:</p> <p>class NS: ...     n = 4 ... ns = NS() def square(instance): ...     instance.n *= instance.n ... ns.n 4 square(ns)</p>"},{"location":"material/01_python/03_advanced/reference/#for-the-purpose-of-this-example-lets-use-simplenamespace","title":"For the purpose of this example, let's use SimpleNamespace.","text":"<p>from types import SimpleNamespace</p>"},{"location":"material/01_python/03_advanced/reference/#simplenamespace-allows-us-to-set-arbitrary-attributes","title":"SimpleNamespace allows us to set arbitrary attributes.","text":""},{"location":"material/01_python/03_advanced/reference/#it-is-an-explicit-handy-replacement-for-class-x-pass","title":"It is an explicit, handy replacement for \"class X: pass\".","text":"<p>ns = SimpleNamespace()</p> <p>Remove ads Best Practice: Use Dictionaries and Lists Dictionaries in Python are a different object type than all other built-in types. They\u2019re referred to as mapping types. Python\u2019s documentation on mapping types provides some insight into the term:</p> <p>A mapping object maps hashable values to arbitrary objects. Mappings are mutable objects. There is currently only one standard mapping type, the dictionary. (Source)</p> <p>This tutorial doesn\u2019t cover how to implement a custom mapping type, but you can replicate pass by reference using the humble dictionary. Here\u2019s an example using a function that operates directly on dictionary elements:</p> <p>While lists aren\u2019t mapping types, you can use them in a similar way to dictionaries because of two important characteristics: subscriptability and mutability. These characteristics are worthy of a little more explanation, but let\u2019s first take a look at best practices for mimicking pass by reference using Python lists.</p> <p>To replicate pass by reference using lists, write a function that operates directly on list elements:</p> <p>Now let\u2019s explore subscriptability. An object is subscriptable when a subset of its structure can be accessed by index positions:</p> <p>subscriptable = [0, 1, 2]  # A list subscriptable[0] 0 subscriptable = (0, 1, 2)  # A tuple subscriptable[0] 0 subscriptable = \"012\"  # A string subscriptable[0] '0' notsubscriptable = {0, 1, 2}  # A set notsubscriptable[0] Traceback (most recent call last):   File \"\", line 1, in  TypeError: 'set' object is not subscriptable Lists, tuples, and strings are subscriptable, but sets are not. Attempting to access an element of an object that isn\u2019t subscriptable will raise a TypeError. <p>Mutability is a broader topic requiring additional exploration and documentation reference. To keep things short, an object is mutable if its structure can be changed in place rather than requiring reassignment:</p> <p>mutable = [0, 1, 2]  # A list mutable[0] = \"x\" mutable ['x', 1, 2]</p> <p>notmutable = (0, 1, 2)  # A tuple notmutable[0] = \"x\" Traceback (most recent call last):   File \"\", line 1, in  TypeError: 'tuple' object does not support item assignment <p>notmutable = \"012\"  # A string notmutable[0] = \"x\" Traceback (most recent call last):   File \"\", line 1, in  TypeError: 'str' object does not support item assignment <p>mutable = {0, 1, 2}  # A set mutable.remove(0) mutable.add(\"x\") mutable {1, 2, 'x'} Lists and sets are mutable, as are dictionaries and other mapping types. Strings and tuples are not mutable. Attempting to modify an element of an immutable object will raise a TypeError.</p> <p>Conclusion Python works differently from languages that support passing arguments by reference or by value. Function arguments become local variables assigned to each value that was passed to the function. But this doesn\u2019t prevent you from achieving the same results you\u2019d expect when passing arguments by reference in other languages.</p> <p>In this tutorial, you learned:</p> <p>How Python handles assigning values to variables How function arguments are passed by assignment in Python Why returning values is a best practice for replicating pass by reference How to use attributes, dictionaries, and lists as alternative best practices You also learned some additional best practices for replicating pass-by-reference constructs in Python. You can use this knowledge to implement patterns that have traditionally required support for passing by reference.</p> <p>To continue your Python journey, I encourage you to dive deeper into some of the related topics that you\u2019ve encountered here, such as mutability, assignment expressions, and Python namespaces and scope.</p> <p>Stay curious, and see you next time!</p>"},{"location":"material/01_python/03_advanced/reference/#define-a-function-to-operate-on-an-objects-attribute","title":"Define a function to operate on an object's attribute.","text":"<p>def square(instance): ...     instance.n *= instance.n ... ns.n = 4 square(ns) ns.n 16 Note that square() needs to be written to operate directly on an attribute, which will be modified without the need to reassign a return value.</p>"},{"location":"material/01_python/03_advanced/reference/#instance-attribute-is-modified","title":"Instance attribute is modified.","text":"<p>ns.n 16</p>"},{"location":"material/01_python/03_advanced/reference/#class-attribute-remains-unchanged","title":"Class attribute remains unchanged.","text":"<p>NS.n 4 Since class attributes remain unchanged when modified through a class instance, you\u2019ll need to remember to reference the instance attribute.</p>"},{"location":"material/01_python/03_advanced/reference/#dictionaries-are-mapping-types","title":"Dictionaries are mapping types.","text":"<p>mt = {\"n\": 4}</p>"},{"location":"material/01_python/03_advanced/reference/#define-a-function-to-operate-on-a-key","title":"Define a function to operate on a key:","text":"<p>def square(numdict): ...     numdict[\"n\"] *= num_dict[\"n\"] ... square(mt) mt {'n': 16} Since you\u2019re reassigning a value to a dictionary key, operating on dictionary elements is still a form of assignment. With dictionaries, you get the added practicality of accessing the modified value through the same dictionary object.</p>"},{"location":"material/01_python/03_advanced/reference/#lists-are-both-subscriptable-and-mutable","title":"Lists are both subscriptable and mutable.","text":"<p>sm = [4]</p>"},{"location":"material/01_python/03_advanced/reference/#define-a-function-to-operate-on-an-index","title":"Define a function to operate on an index:","text":"<p>def square(numlist): ...     numlist[0] *= num_list[0] ... square(sm) sm [16] Since you\u2019re reassigning a value to an element within the list, operating on list elements is still a form of assignment. Similar to dictionaries, lists allow you to access the modified value through the same list object.</p>"},{"location":"material/02_libs/01_jupyter/lecture.en/","title":"2.1 - Jupyter Lab","text":"<p>Accompanying Notebook</p> <p>For this lesson, there is an accompanying notebook available at this link.</p> <p>Until now, we have been running Python scripts directly from the command line. However, it is evident that this approach is limited, especially in data science applications.</p> <p>To overcome these limitations, the SciPy framework introduces Jupyter Lab, which is one of the most widely used tools by data analysts nowadays: the notebook.</p>"},{"location":"material/02_libs/01_jupyter/lecture.en/#anatomy-of-a-notebook","title":"Anatomy of a Notebook","text":"<p>A notebook is, in simple terms, an interactive environment that allows us to write and test our code. In particular, we can write one or more instructions and execute them separately from each other using the concept of cells, which are individual \"blocks\" of code.</p> <p>Tip</p> <p>Jupyter notebooks also allow us to include comments, descriptions, and equations using two well-known markup languages: Markdown and LaTeX.</p> <p>Now let's see how to create and use our first notebook.</p>"},{"location":"material/02_libs/01_jupyter/lecture.en/#installation-and-launching-jupyter-lab","title":"Installation and Launching Jupyter Lab","text":"<p>Installing a Library</p> <p>Remember that the various options for installing a library are described in detail in Appendix B.</p> <p>To install Jupyter Lab, we'll use <code>pip</code>, preferably within a virtual environment:</p> <pre><code>workon my-virtual-env\n(my-virtual-env) pip install jupyterlab\n</code></pre> <p>Unlike other libraries, Jupyter doesn't need to be imported explicitly. Instead, you can launch the interactive environment using the following command in the terminal:</p> <pre><code>jupyter lab\n</code></pre> <p>Importing iPython</p> <p>In theory, it is possible to import iPython and use the methods and classes provided like any other library. However, in practice, it is more common to use the interactive environment offered by Jupyter notebooks.</p>"},{"location":"material/02_libs/01_jupyter/lecture.en/#the-first-notebook","title":"The First Notebook","text":"<p>At this point, you will see a screen similar to the one shown in Figure 1.</p> <p> </p> Figure 1 - The introductory screen of Jupyter Lab <p>Let's create our first notebook by clicking on the Python 3 button in the Notebook menu. Once the notebook is created, we can start interacting with the environment. Before proceeding, let's define the name of our notebook from the left menu.</p> <p>Let's try doing something simple: create a function that adds two numeric variables and returns the result, and then call it with two different values.</p> <p>First, write the code for the function in the first cell:</p> <pre><code>def sum(a, b):\n    result = a + b\n    return result\n</code></pre> <p>To execute the code in the cell, press the <code>Play</code> button or use the keyboard shortcut <code>Shift+Enter</code>. Once the first cell is executed, Jupyter will automatically create a new cell. In the new cell, we can write the instructions to call the <code>sum()</code> function with different values.</p> <pre><code>sum(5, 7)\n</code></pre> <p>Execute the cell, and you will see the value returned by the function displayed below the cell.</p>"},{"location":"material/02_libs/01_jupyter/lecture.en/#other-useful-operations","title":"Other Useful Operations","text":"<p>Jupyter allows us to perform several useful operations, including:</p> <ul> <li>deleting an entire cell,</li> <li>inserting a cell above or below the currently selected cell,</li> <li>stopping the kernel,</li> <li>restarting</li> </ul> <p>the kernel.</p> <p>Let's focus for a moment on the last two operations. It may happen that you need to interrupt the current flow of execution of the instructions or restart the notebook. Since Jupyter is based on the concept of a kernel, which is responsible for executing the notebook, we say that we can interrupt or stop the kernel or restart it.</p> <p>Interrupting the kernel only stops the execution of the current cell. It does not result in any data loss, and you can continue running the code in the notebook at any time, either from the beginning of that cell or from within another cell. Restarting the kernel, on the other hand, completely \"freezes\" the execution, deleting all variables stored in memory. It is a true \"reset\" that should be used when, for example, you need to reorganize the code or when you have made too many modifications that lead to inconsistent results.</p>"},{"location":"material/02_libs/01_jupyter/lecture.en/#colab","title":"Colab","text":"<p>More and more often, scientific computing and data science code requires extensive computational resources that may not be available on our personal computers. To overcome this problem and allow anyone to experiment with the libraries we will see in this course, there is a tool called Colab provided by Google that allows us to run Jupyter notebooks for free.</p> <p>Of course, the free version has some limitations, but they are more than sufficient for our purposes.</p>"},{"location":"material/02_libs/01_jupyter/lecture/","title":"2.1 - Jupyter Lab","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Fino a questo momento ci siamo limitati a lanciare script Python direttamente da riga di comando. Tuttavia, \u00e8 evidente come questo approccio sia limitato, specialmente in applicazioni in ambito data science.</p> <p>Per ovviare a queste problematiche, all'interno del framework SciPy viene proposto Jupyter Lab,  che introduce uno tra gli strumenti pi\u00f9 utilizzati dai data analyst al giorno d'oggi, ovvero i notebook.</p>"},{"location":"material/02_libs/01_jupyter/lecture/#anatomia-di-un-notebook","title":"Anatomia di un notebook","text":"<p>Un notebook \u00e8, in poche parole, un ambiente interattivo che permette di scrivere e testare il nostro codice. In particolare, ptoremo scrivere una o pi\u00f9 istruzioni, ed eseguirle in maniera separata dalle altre mediante il meccanismo delle celle, che altro non sono se non dei singoli \"blocchi\" di codice.</p> <p>Suggerimento</p> <p>I notebook Jupyter ci permettono di inserire anche commenti, descrizioni ed equazioni utilizzando due linguaggi di markup molto noti, ovvero Markdown e Latex.</p> <p>Vediamo adesso come creare ed utilizzare il nostro primo notebook.</p>"},{"location":"material/02_libs/01_jupyter/lecture/#installazione-e-lancio-di-jupyter-lab","title":"Installazione e lancio di Jupyter Lab","text":"<p>Installazione di una libreria</p> <p>Ricordiamo che le diverse opzioni utilizzabili per installare una libreria sono descritte nel dettaglio nell'appendice B.</p> <p>Per installare Jupyter Lab, ricorriamo all'utilizzo di <code>pip</code>, preferibilmente all'interno di un ambiente virtuale:</p> <pre><code>workon my-virtual-env\n(my-virtual-env) pip install jupyterlab\n</code></pre> <p>A differenza delle altre librerie, Jupyter non andr\u00e0 (necessariamente) importato; infatti, \u00e8 possibile lanciare un ambiente interattivo utilizzando la seguente istruzione da riga di comando:</p> <pre><code>jupyter lab\n</code></pre> <p>Importare iPython</p> <p>In teoria \u00e8 possibile importare iPython ed utilizzare i metodi e le classi messe a disposizione come una qualsiasi libreria. Nei fatti, per\u00f2, molto spesso ci si limita ad utilizzare l'ambiente interattivo offerto dai notebook.</p>"},{"location":"material/02_libs/01_jupyter/lecture/#il-primo-notebook","title":"Il primo notebook","text":"<p>A questo punto ci troveremo davanti ad una schermata simile a quella mostrata in figura 1.</p> <p> </p> Figura 1 - La schermata introduttiva di Jupyter Lab <p>Creiamo il nostro primo notebook premendo il pulsante Python 3 nel menu Notebook. Una volta terminata la procedura, potremo iniziare ad interagire con l'ambiente. Prima di procedere, per\u00f2, definiamo il nome del nostro notebook dal menu a sinistra.</p> <p>Proviamo a fare qualcosa di semplice: creiamo una funzione che sommi due variabili di tipo numerico, restituendo il risultato, e chiamiamola su due diversi valori.</p> <p>Per prima cosa, scriviamo il codice della funzione all'interno della prima cella:</p> <pre><code>def somma(a, b):\n    somma = a + b\n    return somma\n</code></pre> <p>Per eseguire il codice all'interno della cella, premiamo il tasto <code>Play</code>, oppure la combinazione di tasti <code>Shift+Invio</code>. Una volta eseguita la prima cella, Jupyter ne creer\u00e0 in automatico un'altra; al suo interno, potremo scrivere le istruzioni necessarie a chiamare la funzione <code>somma()</code> su due diversi valori.</p> <pre><code>somma(5, 7)\n</code></pre> <p>Eseguiamo l'istruzione; noteremo che al di sotto della cella apparir\u00e0 il valore assunto dalla funzione.</p>"},{"location":"material/02_libs/01_jupyter/lecture/#altre-operazioni-utili","title":"Altre operazioni utili","text":"<p>Jupyter ci permette di effettuare una serie di operazioni utili, tra cui:</p> <ul> <li>cancellare un'intera cella;</li> <li>inserire una cella al di sopra o al di sotto di quella attualmente selezionata;</li> <li>stoppare il kernel;</li> <li>riavviare il kernel.</li> </ul> <p>Soffermiamoci per un attimo sulle ultime due operazioni. Pu\u00f2 capitare, infatti, che ci sia la necessit\u00e0 di interrompere il flusso attuale dell'esecuzione delle istruzioni, oppure ancora che sia necessario riavviare il notebook. Dato che Jupyter si basa sul concetto di kernel, il quale \u00e8 il responsabile per l'esecuzione del notebook, diremo in gergo che possiamo interrompere, o stoppare, il kernel, oppure ancora che possiamo riavviarlo.</p> <p>L'interruzione del kernel si limita a fermare l'esecuzione della cella attuale: ci\u00f2 non comporta alcuna perdita di dati, e potremo riprendere ad eseguire il codice nel notebook in ogni momento, sia dall'inizio di quella cella, sia dall'interno di un'altra. Il riavvio del kernel, invece, \"blocca\" completamente l'esecuzione, andando a cancellare anche le variabili presenti in memoria: si tratta, quindi, di un vero e proprio \"reset\", da utilizzare quando, ad esempio, abbiamo la necessit\u00e0 di riorganizzare il codice, oppure quando abbiamo effettuato un numero eccessivo di modifiche per le quali i risultati iniziano a non essere coerenti con le nostre attese.</p>"},{"location":"material/02_libs/01_jupyter/lecture/#colab","title":"Colab","text":"<p>Sempre pi\u00f9 spesso il codice per il calcolo scientifico e la data science richiede l'uso di risorse computazionali estese che, talvolta, non sono disponibili sui PC che utilizziamo. Per ovviare a questo problema, e permettere a chiunque di sperimentare con le librerie che vedremo in questo corso, esiste uno strumento chiamato Colab messo a disposizione da Google che ci permette di eseguire gratuitamente i nostri notebook Jupyter.</p> <p>Ovviamente, la versione gratuita comporta una serie di limitazioni, che tuttavia risultano essere pi\u00f9 che sufficienti per i nostri scopi.</p>"},{"location":"material/02_libs/02_numpy/01_intro.en/","title":"2.2.1 Introduction to NumPy","text":"<p>The NumPy library, derived from the fusion of *Num*erical and *Py*thon, is one of the most widely used libraries in scientific computing applications in Python.</p> <p>In practice, NumPy can be considered a de facto standard. In fact, the classes and methods provided by the library are extensively used in almost all other Python tools for mathematical, chemical, and physical sciences, as well as engineering.</p> <p>Let's start our examination with the installation procedure of the library.</p>"},{"location":"material/02_libs/02_numpy/01_intro.en/#installing-numpy","title":"Installing NumPy","text":"<p>Installing a Library</p> <p>As usual, remember that the various options for installing a library are described in detail in Appendix B.</p> <p>To install NumPy, we will use <code>pip</code>, preferably within a virtual environment:</p> <pre><code>workon my-virtual-env\n(my-virtual-env) pip install numpy\n</code></pre>"},{"location":"material/02_libs/02_numpy/01_intro.en/#introduction-to-numpy","title":"Introduction to NumPy","text":""},{"location":"material/02_libs/02_numpy/01_intro.en/#the-ndarray","title":"The <code>ndarray</code>","text":"<p>We have seen earlier that to use a package or module in our Python programs, we need to import it first:</p> <pre><code>import numpy as np\n</code></pre> <p>Once NumPy is imported, we can start using the main data structure of the library, which is the array, similar to those described in classical mathematical notation.</p> <p>Specifically, NumPy provides us with <code>ndarrays</code>, which are data structures capable of representing \\(n\\)-dimensional arrays containing homogeneous data.</p> <p>Note</p> <p><code>ndarray</code> is an abbreviation for n-dimensional array.</p> <p>The simplest way to create an array is to use the <code>array</code> constructor, passing it a list:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3])\n</code></pre>"},{"location":"material/02_libs/02_numpy/01_intro.en/#array-vs-lists","title":"Array vs. Lists","text":"<p>There are several differences between an array and a regular list; the main ones are summarized in the following table.</p> Feature <code>ndarray</code> List Size Fixed Not fixed Elements Homogeneous (same type) Heterogeneous (any type) Scope Algebraic operations General-purpose <p>In practice:</p> <ul> <li>an array has a fixed size, unlike a list. Changing its size will require creating a new array and deleting the original one;</li> <li>the elements of an array must be of the same type (this limitation does not apply to lists);</li> <li>arrays are specifically designed for algebraic operations, while lists are designed for general purposes.</li> </ul>"},{"location":"material/02_libs/02_numpy/01_intro.en/#numpy-and-algebraic-operations","title":"NumPy and Algebraic Operations","text":"<p>We have mentioned that NumPy arrays are specifically designed for algebraic operations. This is of great relevance to us. To understand it, let's look at a simple example, in which we multiply two row vectors element-wise.</p> <p>To perform the operation just described, we could use a <code>for</code> loop or a list comprehension:</p> <pre><code># for loop\nc = []\nfor i in range(len(a)):\n    c.append(a[i]*b[i])\n\n# list comprehension\nc = [a[i] * b[i] for i in range(len(a))]\n</code></pre> <p>The result of the operation will be correct in both cases. However, loops are computationally expensive: this means that, especially as the number of elements in the vectors increases, the cost will also increase.</p> <p>This could be mitigated by using a more efficient language, such as C. However, if we try to extend the calculation to two dimensions, the code becomes:</p> <pre><code>for i in range(len(a)):\n    for j in range(len(b)):\n        c.append(a[i][j]*b[i][j])\n</code></pre> <p>The number of nested loops will increase in proportion to the dimensionality of the arrays involved. This implies that for an \\(m\\)-dimensional array, we will have as many nested loops, with all the complexity that comes with it in terms of code complexity.</p> <p>And this is where NumPy comes to the rescue. In fact, to multiply two arrays of any dimensionality, we only need to use the following instruction:</p> <pre><code>c = a * b\n</code></pre> <p>Clearly, this syntax is much more concise and simple compared to using nested loops, and it closely resembles the notation used in \"real\" formulas found in textbooks.</p> <p>The use of this syntax is based on two fundamental concepts in NumPy:</p> <ul> <li>vectorization of the code, which allows us to write matrix operations without explicitly using loops;</li> <li>broadcasting, which allows us to use a common syntax regardless of the dimensionality of the arrays involved in the operations.</li> </ul>"},{"location":"material/02_libs/02_numpy/01_intro.en/#data-types","title":"Data Types","text":"<p>NumPy provides a range of primitive data types that overlap with the built-in types in Python and have a nearly perfect correspondence with those provided by the C language.</p> <p>These types are summarized in the table found in the reference.</p> <p>Two important aspects should be noted:</p> <ul> <li>the size of each data type depends on the platform (i.e., the operating system and processor) used;</li> <li>there is a range of types whose size is platform-independent, defined at this address. In general, it is advisable to refer to these types.</li> </ul>"},{"location":"material/02_libs/02_numpy/01_intro/","title":"2.2.1 Introduzione a NumPy","text":"<p>La libreria NumPy, nome derivante dalla crasi tra Numerical Python, \u00e8 una tra le pi\u00f9 utilizzate nelle applicazioni di calcolo scientifico in Python.</p> <p>Nella pratica, possiamo pensare a NumPy come ad uno standard de facto: infatti, le classi ed i metodi messi a disposizione dalla libreria sono estensivamente utilizzate nella quasi totalit\u00e0 degli altri tool Python per le scienze matematiche, chimiche e fisiche, oltre che per l'ingegneria.</p> <p>Partiamo nella nostra disamina dalla procedura di installazione della libreria.</p>"},{"location":"material/02_libs/02_numpy/01_intro/#installare-numpy","title":"Installare NumPy","text":"<p>Installazione di una libreria</p> <p>Al solito, ricordiamo che le diverse opzioni utilizzabili per installare una libreria sono descritte nel dettaglio nell'appendice B.</p> <p>Per installare NumPy, ricorriamo all'utilizzo di <code>pip</code>, preferibilmente all'interno di un ambiente virtuale:</p> <pre><code>workon my-virtual-env\n(my-virtual-env) pip install numpy\n</code></pre>"},{"location":"material/02_libs/02_numpy/01_intro/#introduzione-a-numpy","title":"Introduzione a NumPy","text":""},{"location":"material/02_libs/02_numpy/01_intro/#gli-ndarray","title":"Gli <code>ndarray</code>","text":"<p>Abbiamo visto in precedenza che per usare un package o un modulo Python all'interno dei nostri programmi dovremo per prima cosa importarlo:</p> <pre><code>import numpy as np\n</code></pre> <p>Una volta importato NumPy, potremo passare ad utilizzare la struttura dati \"principe\" della libreria, ovvero l'array, analogo a quelli descritti dalla classica formulazione matematica.</p> <p>Nello specifico, NumPy ci mette a disposizione gli <code>ndarray</code>, ovvero delle strutture dati in grado di rappresentare array ad \\(n\\) dimensioni, contenenti dati di tipo omogeneo.</p> <p>Nota</p> <p>Anche <code>ndarray</code> \u00e8 un'abbreviazione che sta per n-dimensional array.</p> <p>Il metodo pi\u00f9 semplice per creare un array \u00e8 usare il costruttore <code>array</code> a cui viene passata una lista:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3])\n</code></pre>"},{"location":"material/02_libs/02_numpy/01_intro/#array-vs-liste","title":"Array vs liste","text":"<p>Sono diverse le differenze che intercorrono tra un array ed una classica lista; le principali sono riassunte nella seguente tabella.</p> Caratteristica <code>ndarray</code> Lista Dimensione Fissa Non fissa Elementi Omogenei (stesso tipo) Eterogenei (qualsiasi tipo) Ambito Operazioni algebriche General-purpose <p>In pratica:</p> <ul> <li>un array ha dimensione fissa, a differenza della lista. Cambiarne la dimensione comporter\u00e0 quindi la creazione di un nuovo array, e la cancellazione di quello originario;</li> <li>gli elementi di un array devono essere dello stesso tipo (tale limitazione non vale ovviamente per le liste);</li> <li>gli array sono pensati specificamente per le operazioni algebriche, laddove le liste sono pensate per degli scopi generici.</li> </ul>"},{"location":"material/02_libs/02_numpy/01_intro/#numpy-e-le-operazioni-algebriche","title":"NumPy e le operazioni algebriche","text":"<p>Abbiamo detto che gli array NumPy sono progettati specificamente per le operazioni algebriche. Ovviamente, ci\u00f2 assume una notevole rilevanza ai nostri fini. Per capirlo, facciamo un esemplice esempio, nel quale moltiplichiamo tra loro due vettori riga elemento-per-elemento.</p> <p>Per effettuare l'operazione appena descritta potremmo usare un ciclo <code>for</code> o una list comprehension:</p> <pre><code># ciclo for\nc = []\nfor i in range(len(a)):\n    c.append(a[i]*b[i])\n\n# list comprehension\nc = [a[i] * b[i] for i in range(len(a))]\n</code></pre> <p>Il risultato dell'operazione sar\u00e0 in entrambi i casi corretto. Tuttavia, i cicli sono computazionalmente costosi: ci\u00f2 significa che, specialmente all'aumentare del numero di elementi contenuti nei vettori, sar\u00e0 necessario pagare un costo crescente.</p> <p>Questo potrebbe essere in qualche modo arginato dal ricorso ad un linguaggio pi\u00f9 efficiente, come ad esempio il C; tuttavia, provando ad estendere il calcolo a due dimensioni, il codice diverr\u00e0:</p> <pre><code>for i in range(len(a)):\n    for j in range(len(b)):\n        c.append(a[i][j]*b[i][j])\n</code></pre> <p>Il numero di cicli annidati aumenter\u00e0 ovviamente in maniera direttamente proporzionale alla dimensionalit\u00e0 degli array coinvolti. Ci\u00f2 implica che per un array ad \\(m\\) dimensioni avremo altrettanti cicli annidati, con tutto ci\u00f2 che ne consegue in termini di complessit\u00e0 di codice.</p> <p>Ed \u00e8 proprio in questa situazione che NumPy ci viene in aiuto. Infatti, per moltiplicare due array di qualsiasi dimensionalit\u00e0 ci basta usare la seguente istruzione:</p> <pre><code>c = a * b\n</code></pre> <p>Evidentemente, una sintassi di questo tipo risulta essere molto pi\u00f9 concisa e semplice rispetto all'uso dei cicli annidati, ed \u00e8 inoltre molto simile a quella che possiamo trovare sulle formule \"reali\" usate nei libri di testo.</p> <p>L'uso di questa sintassi si esplicita in due concetti fondamentali sui quali risulta essere basato NumPy:</p> <ul> <li>la vettorizzazione del codice, ovvero la possibilit\u00e0 di scrivere istruzioni matriciali senza usare esplicitamente dei cicli;</li> <li>il broadcasting, che riguarda la possibilit\u00e0 di usare una sintassi comune ed indipendente dalla dimensionalit\u00e0 degli array coinvolti nelle operazioni.</li> </ul>"},{"location":"material/02_libs/02_numpy/01_intro/#tipi-di-dato","title":"Tipi di dato","text":"<p>NumPy mette a disposizione una serie di tipi di dato primitivi che vanno a sovrapporsi a quelli built-in di Python, e che trovano una corrispondenza praticamente perfetta con quelli messi a disposizione dal linguaggio C.</p> <p>Questi sono riassunti nella tabella presente sulla reference.</p> <p>E' importante sottolineare due aspetti:</p> <ul> <li>la dimensione di ciascun dato dipende dalla piattaforma (ovvero, sistema operativo e processore) utilizzata;</li> <li>\u00e8 disponibile una serie di tipi la cui dimensione \u00e8 indipendente dalla piattaforma, definiti a questo indirizzo. In generale, il consiglio \u00e8 di far riferimento proprio a questi ultimi.</li> </ul>"},{"location":"material/02_libs/02_numpy/02_array.en/","title":"2.2.2 - Arrays","text":"<p>In the previous lesson we introduced the concept of arrays, which are the \"central\" data structure in the NumPy ecosystem. In this lesson, we will delve deeper into their main aspects and characteristics.</p>"},{"location":"material/02_libs/02_numpy/02_array.en/#arrays-and-lists","title":"Arrays and Lists","text":"<p>At first glance, arrays may appear very similar to traditional lists. However, as we have seen before, there are several notable differences that can be summarized by stating that it is preferable to use an array when performing mathematical operations on homogeneous data.</p> <p>NumPy arrays are instances of the <code>ndarray</code> class, which stands for \\(n\\)-dimensional array. With this class, we can represent data structures with an arbitrary number of dimensions, including vectors, matrices, and tensors.</p> <p>The first step in using an array, as mentioned earlier, is to create it. In this regard, there are several methods, but let's remember the \"simplest\" one, which involves using the <code>array()</code> constructor and passing it a list of elements of the same type:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 4, 5, 6])\n&gt;&gt;&gt; a\narray([1, 2, 3, 4, 5, 6])\n</code></pre> <p>By passing a list whose elements are themselves lists, we can obtain a multidimensional array as output:</p> <pre><code>&gt;&gt;&gt; b = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; b\narray([[1, 2, 3],\n       [4, 5, 6]])\n</code></pre> <p>Finally, note that arrays are not necessarily numeric. We can, for example, create an array of strings:</p> <pre><code>&gt;&gt;&gt; c = np.array([\"s1\", \"s2\"])\n&gt;&gt;&gt; c\narray(['s1', 's2'], dtype='&lt;U2')\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array.en/#heterogeneous-arrays","title":"Heterogeneous Arrays","text":"<p>We previously mentioned that arrays, unlike lists, must contain homogeneous data. So what would happen if we tried to pass a list of heterogeneous data types to the <code>array()</code> method? Let's start by verifying what happens, for example, when using an integer and a float.</p> <pre><code>&gt;&gt;&gt; d = np.array([1, 1.])\n&gt;&gt;&gt; d\narray([1., 1.])\n</code></pre> <p>We immediately notice that an implicit and automatic type conversion has been performed, and all the values passed have been converted to the float format.</p> <p>It's also interesting to see what happens when we try to pass a list containing a number and a string:</p> <pre><code>&gt;&gt;&gt; e = np.array([1, \"s3\"])\n&gt;&gt;&gt; e\narray(['1', 's3'], dtype='&lt;U11')\n</code></pre> <p>We can see that, in this case as well, a type conversion has been performed, this time converting the integer to a string.</p> <p>Upcasting</p> <p>The rule to keep in mind is that NumPy (and Python in general) follows the principle of upcasting: in other words, when a conversion between different data types is necessary, the type with the highest precision is chosen, minimizing the risk of information loss.</p>"},{"location":"material/02_libs/02_numpy/02_array.en/#the-number-of-elements-in-an-array","title":"The number of elements in an array","text":"<p>NumPy arrays have a fixed size and can contain a fixed number of objects of a certain type. To define (or know) this value, we use a property called <code>shape</code>, which roughly represents the shape of the array. The shape of an array is actually a tuple of non-negative integers, each of which determines the number of elements for each dimension of the array.</p> <p>Let's create an array that represents a \\(2 \\times 3\\) matrix, meaning two rows and three columns:</p> <pre><code>&gt;&gt;&gt; a = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; a\narray([[1, 2, 3],\n       [4, 5, 6]])\n</code></pre> <p>Let's see what value the <code>shape</code> property takes for this array:</p> <pre><code>&gt;&gt;&gt; a.shape\n(2, 3)\n</code></pre> <p>As expected, our array has a size of two in the first dimension (i.e., the number of rows) and three in the second dimension (i.e., the number of columns).</p>"},{"location":"material/02_libs/02_numpy/02_array.en/#other-methods-to-create-an-array","title":"Other methods to create an array","text":"<p>In addition to the method seen before, we can create an array by directly using the constructor of the <code>ndarray</code> class:</p> <pre><code>&gt;&gt;&gt; a = np.ndarray([3, 3])       # or a = np.ndarray(shape=(3, 3))\n&gt;&gt;&gt; a\narray([[0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n       [0.00000000e+000, 0.00000000e+000, 3.02368175e-321],\n       [6.69431255e+151, 1.68534231e+246, 6.69431467e+151]])\n</code></pre> <p>Note that the constructor accepts a list containing the shape of the array, which in this case becomes \\(3 \\times 3\\).</p> <p>Note</p> <p>Notice how the numbers with which the array is \"filled\" are currently random.</p> <p>In addition to this basic technique, there are several ways to create arrays of a certain type. Let's briefly see them.</p>"},{"location":"material/02_libs/02_numpy/02_array.en/#arrays-with-zero-and-unit-values","title":"Arrays with zero and unit values","text":"<p>We can create an array of arbitrary dimensions where all elements are equal to 1. To do this, we use the <code>ones()</code> function:</p> <pre><code>&gt;&gt;&gt; u = np.ones(shape=(3, 3))\n&gt;&gt;&gt; u\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n</code></pre> <p>Similarly, we can create arrays of arbitrary dimensions where all elements are equal to zero using the <code>zeros()</code> function:</p> <pre><code>&gt;&gt;&gt; z = np.zeros(shape=(3, 3))\n&gt;&gt;&gt; z\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array.en/#empty-arrays","title":"Empty Arrays","text":"<p>We can create an empty array using the <code>empty()</code> function:</p> <pre><code>&gt;&gt;&gt; e = np.empty(shape=(3, 3))\n&gt;&gt;&gt; e\narray([[0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n       [0.00000000e+000, 0.00000000e+000, 1.67982320e-321],\n       [5.96555652e-302, 1.14188703e-104, 9.91401238e-278]])\n</code></pre> <p>This function can be useful when we want to preallocate space for an array.</p> <p>Note</p> <p>The observant ones may notice that the array generated by <code>empty()</code> is not actually empty but contains random values. In this sense, it produces results equivalent to directly using the <code>ndarray()</code> constructor.</p>"},{"location":"material/02_libs/02_numpy/02_array.en/#identity-matrix","title":"Identity Matrix","text":"<p>We can create an identity matrix using the <code>eye()</code> function:</p> <pre><code>&gt;&gt;&gt; i = np.eye(3)\n&gt;&gt;&gt; i\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n</code></pre> <p>Attention</p> <p>In this case, we cannot pass a tuple or list to indicate the dimensions of the array. However, we can specify both the number of rows (using the first parameter) and the number of columns (using the second parameter).</p>"},{"location":"material/02_libs/02_numpy/02_array.en/#diagonal-matrices","title":"Diagonal Matrices","text":"<p>The <code>diag()</code> function is used both to create a diagonal matrix from a vector (which will be the diagonal of the matrix) and to extract the diagonal of a matrix. To understand this duality, let's first consider having a row vector with three elements that we want to transform in such a way that it behaves as the diagonal of a matrix.</p> <pre><code>&gt;&gt;&gt; x = np.array([5, 2, 3])\n&gt;&gt;&gt; x\narray([5, 2, 3])\n</code></pre> <p>We can create a diagonal matrix from this vector by passing it as a parameter to the <code>diag()</code> function:</p> <pre><code>&gt;&gt;&gt; d = np.diag(x)\n&gt;&gt;&gt; d\narray([[5, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n</code></pre> <p>Now let's see how to address the dual problem. Suppose we have an array and we want to extract its diagonal:</p> <pre><code>&gt;&gt;&gt; x = np.array([[5, 5, 5], [2, 1, 3], [4, 3, 6]])\n&gt;&gt;&gt; x\narray([[5, 2, 2],\n       [2, 1, 3],\n       [4, 3, 6]])\n</code></pre> <p>To do this, we need to use the <code>diag()</code> function again:</p> <pre><code>&gt;&gt;&gt; d = np.diag(x)\n&gt;&gt;&gt; d\narray([5, 1, 6])\n</code></pre> <p>Hint</p> <p>The fact that the <code>diag()</code> function is used for dual operations can cause confusion. However, just remember that passing a vector returns a matrix, while passing a matrix returns a vector, and that's it.</p> <p>Attention</p> <p>Obviously, the <code>diag()</code> function only accepts one-dimensional (vectors) and two-dimensional (matrices) inputs!</p>"},{"location":"material/02_libs/02_numpy/02_array.en/#triangular-matrices","title":"Triangular Matrices","text":"<p>We conclude this brief overview by showing two methods to extract the triangular matrix, both upper and lower.</p> <p>Suppose we have the previously defined matrix <code>x</code>. To extract the upper triangular matrix, we use the <code>triu()</code> function:</p> <pre><code>&gt;&gt;&gt; tu = np.triu(x)\n&gt;&gt;&gt; tu\narray([[5, 2, 2],\n       [0, 1, 3],\n       [0, 0, 6]])\n</code></pre> <p>To extract the lower triangular matrix, we use the <code>tril()</code> function:</p> <pre><code>&gt;&gt;&gt; tl = np.tril(x)\n&gt;&gt;&gt; tl\narray([[5, 0, 0],\n       [2, 1, 0],\n       [4, 3, 6]])\n</code></pre> <p>Hint</p> <p>In this case, the <code>tril()</code> and <code>triu()</code> functions can be applied to n-dimensional arrays. Furthermore, the dimensions of the array do not need to have the same size.</p>"},{"location":"material/02_libs/02_numpy/02_array.en/#accessing-array-elements","title":"Accessing Array Elements","text":"<p>Just like lists, the most straightforward way to access the value of an element in an array is to use the <code>[]</code> operator, specifying the index of the element you want to access. For example, to select the first element of a vector:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; a[0]\n1\n</code></pre> <p>In the case of n-dimensional arrays, you need to specify the index for each dimension of the array. For example, for a two-dimensional array, you can select the element in the first row and first column using the following syntax:</p> <pre><code>&gt;&gt;&gt; b = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; b[0][0]\n1\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array.en/#boolean-masks","title":"Boolean Masks","text":"<p>We can access a subset of elements in an array using a \"mask,\" which is another array of the same dimensions as the original array, containing boolean values only. By doing so, we extract only the elements whose corresponding position in the mask has a value of <code>True</code>. For example, we can select all elements belonging to the first column of the array <code>b</code>:</p> <pre><code>&gt;&gt;&gt; mask = np.array([[True, False], [True, False]])\n&gt;&gt;&gt; b[mask]\narray([1, 3])\n</code></pre> <p>Similarly, we can choose all elements that satisfy a certain logical/mathematical condition:</p> <pre><code>&gt;&gt;&gt; mask = (b &gt; 2)\n&gt;&gt;&gt; mask\narray([[False, False],\n       [ True,  True]])\n&gt;&gt;&gt; b[mask]\narray([3, 4])\n</code></pre> <p>It's interesting to note that the previous notation can be further simplified using logical relations:</p> <pre><code>&gt;&gt;&gt; b[b &gt; 2]\narray([3, 4])\n</code></pre> <p>If desired, we can adapt the previous form to use arbitrarily complex expressions:</p> <pre><code>&gt;&gt;&gt; b[b % 2 == 0]\narray([2, 4])\n&gt;&gt;&gt; b[(b &gt; 1) &amp; (b &lt; 4)]\narray([2, 3])\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array.en/#array-slicing","title":"Array Slicing","text":"<p>Just like lists, arrays also support slicing operations:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; a[0:2]\narray([1, 2])\n</code></pre> <p>For multidimensional arrays, slicing is done along the nth dimension of the array. This concept is easy to understand when visualizing an n-dimensional array as an array of arrays:</p> <pre><code>&gt;&gt;&gt; b\narray([[1, 2],\n       [3, 4]])\n&gt;&gt;&gt; b[0:1]              # Slicing is done along the second dimension\narray([[1, 2]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array.en/#the-nonzero-function","title":"The <code>nonzero()</code> Function","text":"<p>We can use the <code>nonzero()</code> function to select the elements and indices of an array whose value is not zero. For example:</p> <pre><code>&gt;&gt;&gt; x = np.array([[3, 0, 0], [0, 4, 0], [5, 6, 0]])\n&gt;&gt;&gt; x\narray([[3, 0, 0],\n       [0, 4, 0],\n       [5, 6, 0]])\n&gt;&gt;&gt; np.nonzero(x)\n(array([0, 1, 2, 2], dtype=int64), array([0, 1, 0, 1], dtype=int64))\n</code></pre> <p>The <code>nonzero()</code> function returns a tuple with the row and column indices of the non-zero elements. In particular, the resulting tuple will have a number of elements equal to each dimension of the input array <code>x</code>, and the \\(i\\)-th array will indicate the indices relative to the \\(i\\)-th dimension. In this case, the first array represents the indices related to the first dimension of the non-zero values (in this case, the row indices), while the second array represents the indices related to the second dimension (column indices). Therefore, we have the following non-zero elements:</p> Row Index Column Index Value 0 0 3 1 1 4 2 0 5 2 1 6 <p>Obtaining a list of tuples</p> <p>We can obtain a list of tuples representing the pairs of indices for the non-zero elements by using the <code>zip</code> function:</p> <pre><code>&gt;&gt;&gt; s = np.nonzero(tarry)\n&gt;&gt;&gt; s\n(array([0, 1, 2, 2], dtype=int64), array([0, 1, 0, 1], dtype=int64))\n&gt;&gt;&gt; coords = list(zip(s[0], s[1]))\n&gt;&gt;&gt; coords\n[(0, 0), (1, 1), (2, 0), (2, 1)]\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array.en/#fancy-indexing","title":"Fancy Indexing","text":"<p>We conclude this lesson by discussing a very interesting technique called fancy indexing, which involves using an array of indices to access multiple elements simultaneously. For example:</p> <pre><code>&gt;&gt;&gt; rand = np.random.RandomState(42)\n&gt;&gt;&gt; x = rand.randint(100, size=10)\n&gt;&gt;&gt; indexes = np.array([[1, 4],[5, 2]])\n&gt;&gt;&gt; x\narray([51, 92, 14, 71, 60, 20, 82, 86, 74, 74])\n&gt;&gt;&gt; x[indexes]\narray([[92, 60],\n       [20, 14]])\n</code></pre> <p>In the above code, we are:</p> <ol> <li>using the <code>randint()</code> function to generate an array of random integers between 0 and 100;</li> <li>creating a two-dimensional array <code>indexes</code>;</li> <li>using fancy indexing to return an array with the dimensions of <code>indexes</code> and the elements of <code>x</code> taken from the positions indicated by <code>indexes</code>.</li> </ol> <p>The power of fancy indexing lies in the fact that we can not only easily access multiple elements of an array in a single operation, but we can also rearrange these elements as desired!</p>"},{"location":"material/02_libs/02_numpy/02_array/","title":"2.2.2 - Gli array","text":"<p>Nella lezione precedente abbiamo introdotto il concetto gli array, ovvero la struttura dati \"centrale\" nell'ecosistema di NumPy. In questa lezione ne approfondiremo aspetti e caratteristiche principali.</p>"},{"location":"material/02_libs/02_numpy/02_array/#array-e-liste","title":"Array e liste","text":"<p>Di primo acchito, l'impressione che si pu\u00f2 avere osservando gli array \u00e8 che questi siano molto simili alle classiche liste. Tuttavia, come abbiamo gi\u00e0 visto, esistono diverse differenze notevoli, riassumibili in linea di massima affermando che \u00e8 preferibile usare un array quando si devono svolgere operazioni di tipo matematico su dati omogenei.</p> <p>Gli array NumPy sono istanze della classe <code>ndarray</code>, crasi che sta per \\(n\\)-dimensional array. Mediante questa classe possiamo rappresentare strutture dati con un numero arbitrario di dimensioni, ovvero vettori, matrici e tensori.</p> <p>Il primo passo per utilizzare un array \u00e8, come accennato in precedenza, crearlo. In tal senso, ci sono diversi metodi, ma ricordiamo di seguito quello pi\u00f9 \"semplice\", che prevede l'uso del costruttore <code>array()</code> a cui passare una lista di elementi dello stesso tipo:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 4, 5, 6])\n&gt;&gt;&gt; a\narray([1, 2, 3, 4, 5, 6])\n</code></pre> <p>Passando invece una lista i cui elementi sono a loro volta delle liste, potremo ottenere in uscita un array multidimensionale:</p> <pre><code>&gt;&gt;&gt; b = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; b\narray([[1, 2, 3],\n       [4, 5, 6]])\n</code></pre> <p>Notiamo infine che gli array non sono necessariamente numerici. Possiamo, ad esempio, creare un array di stringhe:</p> <pre><code>&gt;&gt;&gt; c = np.array([\"s1\", \"s2\"])\n&gt;&gt;&gt; c\narray(['s1', 's2'], dtype='&lt;U2')\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array/#array-eterogenei","title":"Array eterogenei","text":"<p>In precedenza si \u00e8 accennato al fatto che gli array, a differenza delle liste, debbano contenere dati omogenei. Cosa succederebbe quindi se provassimo a passare al metodo <code>array()</code> una lista composta da dati di tipo eterogeneo? Partiamo verificando cosa accade ad esempio usando un intero ed un float.</p> <pre><code>&gt;&gt;&gt; d = np.array([1, 1.])\n&gt;&gt;&gt; d\narray([1., 1.])\n</code></pre> <p>Notiamo subito che \u00e8 stata effettuata in maniera implicita ed automatica un'operazione di conversione di tipo, e tutti i valori passati sono stati convertiti in formato float.</p> <p>Interessante \u00e8 anche valutare cosa accade se proviamo a passare una lista contenente un numero ed una stringa:</p> <pre><code>&gt;&gt;&gt; e = np.array([1, \"s3\"])\n&gt;&gt;&gt; e\narray(['1', 's3'], dtype='&lt;U11')\n</code></pre> <p>Notiamo come anche in questo caso sia stata effettuata una conversione di tipo, passando stavolta da intero a stringa.</p> <p>Upcasting</p> <p>La regola da tenere a mente \u00e8 che NumPy (e, in generale, Python) seguono il principio dell'upcasting: in altre parole, quando deve essere fatta una conversione tra diversi tipi di dati, si fa in modo di scegliere il tipo a pi\u00f9 alta precisione, minimizzando i rischi di perdita di informazioni.</p>"},{"location":"material/02_libs/02_numpy/02_array/#il-numero-di-elementi-di-un-array","title":"Il numero di elementi di un array","text":"<p>Gli array NumPy hanno dimensione prefissata, e sono quindi in grado di contenere un numero fisso di oggetti di un certo tipo. Per definire (o conoscere) questo valore si utilizza una propriet\u00e0 chiamata <code>shape</code> che, a grandi linee, rappresenta la forma dell'array. La shape di un array \u00e8 in pratica una tupla di numeri interi, ovviamente non negativi, ciascuno dei quali determina il numero di elementi per ciascuna delle dimensioni dell'array.</p> <p>Creiamo ad esempio un array che rappresenti una matrice \\(2 \\times 3\\), ovvero a due righe e tre colonne:</p> <pre><code>&gt;&gt;&gt; a = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; a\narray([[1, 2, 3],\n       [4, 5, 6]])\n</code></pre> <p>Vediamo che valore assume la propriet\u00e0 <code>shape</code> di questo array:</p> <pre><code>&gt;&gt;&gt; a.shape\n(2, 3)\n</code></pre> <p>Come ci aspettavamo, il nostro array ha cardinalit\u00e0 due sulla prima dimensione (ovvero il numero di righe) e tre sulla seconda (ovvero il numero di colonne).</p>"},{"location":"material/02_libs/02_numpy/02_array/#altri-metodi-per-creare-un-array","title":"Altri metodi per creare un array","text":"<p>Oltre al metodo visto in precedenza, possiamo creare un array utilizzando direttamente il costruttore della classe <code>ndarray</code>:</p> <pre><code>&gt;&gt;&gt; a = np.ndarray([3, 3])       # oppure a = np.ndarray(shape=(3, 3))\n&gt;&gt;&gt; a\narray([[0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n       [0.00000000e+000, 0.00000000e+000, 3.02368175e-321],\n       [6.69431255e+151, 1.68534231e+246, 6.69431467e+151]])\n</code></pre> <p>Notiamo che il costruttore accetta una lista contenente la shape dell'array, che in questo caso diverr\u00e0 un \\(3 \\times 3\\).</p> <p>Nota</p> <p>Notiamo come i numeri con cui viene \"riempito\" l'array sono al momento casuali.</p> <p>Oltre a questa tecnica base, esistono diversi modi per creare array di un certo tipo. Vediamoli in breve.</p>"},{"location":"material/02_libs/02_numpy/02_array/#array-con-valori-zero-ed-unitari","title":"Array con valori zero ed unitari","text":"<p>Possiamo creare un array di dimensioni arbitrarie in cui tutti gli elementi sono pari ad 1. Per farlo, usiamo la funzione <code>ones()</code>:</p> <pre><code>&gt;&gt;&gt; u = np.ones(shape=(3, 3))\n&gt;&gt;&gt; u\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n</code></pre> <p>In modo simile, possiamo creare array di dimensioni arbitrarie in cui tutti gli elementi sono pari a zero mediante la funzione <code>zeros()</code>:</p> <pre><code>&gt;&gt;&gt; z = np.zeros(shape=(3, 3))\n&gt;&gt;&gt; z\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array/#array-vuoti","title":"Array vuoti","text":"<p>Possiamo creare un array vuoto mediante la funzione <code>empty()</code>:</p> <pre><code>&gt;&gt;&gt; e = np.empty(shape=(3, 3))\n&gt;&gt;&gt; e\narray([[0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n       [0.00000000e+000, 0.00000000e+000, 1.67982320e-321],\n       [5.96555652e-302, 1.14188703e-104, 9.91401238e-278]])\n</code></pre> <p>Questa funzione pu\u00f2 risultare utile quando vogliamo preallocare spazio per un array.</p> <p>Nota</p> <p>I pi\u00f9 attenti avranno notato che, in realt\u00e0, l'array generato da <code>empty()</code> non \u00e8 vuoto, ma contiene valori casuali. In tal senso, d\u00e0 risultati equivalenti all'uso diretto del costruttore <code>ndarray()</code>.</p>"},{"location":"material/02_libs/02_numpy/02_array/#matrice-identita","title":"Matrice identit\u00e0","text":"<p>Possiamo creare una matrice identit\u00e0 usando la funzione <code>eye()</code>:</p> <pre><code>&gt;&gt;&gt; i = np.eye(3)\n&gt;&gt;&gt; i\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n</code></pre> <p>Attenzione</p> <p>In questo caso, notiamo come non si possa passare una tupla o una lista per indicare le dimensioni dell'array. Tuttavia, possiamo specificare sia il numero delle righe (con il primo parametro) che il numero delle colonne (con il secondo parametro).</p>"},{"location":"material/02_libs/02_numpy/02_array/#matrici-diagonali","title":"Matrici diagonali","text":"<p>La funzione <code>diag()</code> viene usata sia per creare una matrice diagonale a partire da un vettore (che, ovviamente, sar\u00e0 poi la diagonale della matrice), sia per estrarre la diagonale di una matrice. Per capire questa dualit\u00e0, immaginiamo per prima cosa di avere a disposizione un vettore riga a tre elementi, che vogliamo trasformare in modo tale che si comporti come la diagonale di una matrice.</p> <pre><code>&gt;&gt;&gt; x = np.array([5, 2, 3])\n&gt;&gt;&gt; x\narray([5, 2, 3])\n</code></pre> <p>Potremo creare una matrice diagonale a partire da questo vettore passandolo come parametro alla funzione <code>diag()</code>:</p> <pre><code>&gt;&gt;&gt; d = np.diag(x)\n&gt;&gt;&gt; d\narray([[5, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n</code></pre> <p>Vediamo invece come affrontare il problema duale. Immaginiamo di avere quindi un array, e volerne estrarre la diagonale:</p> <pre><code>&gt;&gt;&gt; x = np.array([[5, 5, 5], [2, 1, 3], [4, 3, 6]])\n&gt;&gt;&gt; x\narray([[5, 2, 2],\n       [2, 1, 3],\n       [4, 3, 6]])\n</code></pre> <p>Per farlo, dovremo anche questa volta usare la funzione <code>diag()</code>:</p> <pre><code>&gt;&gt;&gt; d = np.diag(x)\n&gt;&gt;&gt; d\narray([5, 1, 6])\n</code></pre> <p>Suggerimento</p> <p>Il fatto che la funzione <code>diag()</code> sia usata per operazioni duali pu\u00f2, a ragione, causare confusione. Basta per\u00f2 ricordare che passando un vettore si ottiene una matrice, mentre passando una matrice si ottiene un vettore, ed il gioco \u00e8 fatto.</p> <p>Attenzione</p> <p>Ovviamente, la funzione <code>diag()</code> accetta solo input monodimensionali (vettori) e bidimensionali (matrici)!</p>"},{"location":"material/02_libs/02_numpy/02_array/#matrici-triangolari","title":"Matrici triangolari","text":"<p>Concludiamo questa breve carrellata mostrando due metodi in grado di estrarre la matrice triangolare, rispettivamente superiore ed inferiore.</p> <p>Supponiamo di avere la matrice x definita in precedenza. Per estrarre la matrice triangolare superiore, dovremo usare la funzione <code>triu()</code>:</p> <pre><code>&gt;&gt;&gt; tu = np.triu(x)\n&gt;&gt;&gt; tu\narray([[5, 2, 2],\n       [0, 1, 3],\n       [0, 0, 6]])\n</code></pre> <p>Per estrarre invece la matrice triangolare inferiore, dovremo usare la funzione <code>tril()</code>:</p> <pre><code>&gt;&gt;&gt; tl = np.tril(x)\n&gt;&gt;&gt; tl\narray([[5, 0, 0],\n       [2, 1, 0],\n       [4, 3, 6]])\n</code></pre> <p>Suggerimento</p> <p>In questo caso, le funzioni <code>tril()</code> e <code>triu()</code> possono tranquillamente essere applicate agli array n-dimensionali. Inoltre, non \u00e8 richiesto le diverse dimensioni dell'array abbiano la stessa cardinalit\u00e0.</p>"},{"location":"material/02_libs/02_numpy/02_array/#accesso-agli-elementi-di-un-array","title":"Accesso agli elementi di un array","text":"<p>Cos\u00ec come per le liste, il modo pi\u00f9 immediato per accedere al valore di un elemento in un array \u00e8 usare l'operatore <code>[]</code>, specificando contestualmente l'indice dell'elemento cui si vuole accedere. Ad esempio, per selezionare il primo elemento di un vettore:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; a[0]\n1\n</code></pre> <p>Nel caso di array ad \\(n\\) dimensioni, \u00e8 necessario indicare l'indice per ciascuna delle dimensioni dell'array. Ad esempio, per un array bidimensionale potremmo selezionare l'elemento alla prima riga e prima colonna con una sintassi di questo tipo:</p> <pre><code>&gt;&gt;&gt; b = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; b[0][0]\n1\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array/#maschere-booleane","title":"Maschere booleane","text":"<p>Possiamo accedere ad un sottoinsieme di elementi dell'array mediante una \"maschera\", ovvero un altro array, di dimensioni uguali a quelle di partenza, al cui interno sono presenti esclusivamente dei valori booleani. Cos\u00ec facendo, estrarremo soltanto gli elementi la cui corrispondente posizione all'interno della maschera ha valore <code>True</code>. Ad esempio, possiamo selezionare tutti gli elementi appartenenti alla prima colonna dell'array <code>b</code>:</p> <pre><code>&gt;&gt;&gt; mask = np.array([[True, False], [True, False]])\n&gt;&gt;&gt; b[mask]\narray([1, 3])\n</code></pre> <p>Ancora, possiamo scegliere tutti gli elementi che soddisfano una certa condizione logico/matematica:</p> <pre><code>&gt;&gt;&gt; mask = (b &gt; 2)\n&gt;&gt;&gt; mask\narray([[False, False],\n       [ True,  True]])\n&gt;&gt;&gt; b[mask]\narray([3, 4])\n</code></pre> <p>Interessante notare come la precedente notazione possa essere ulteriormente sintetizzata usando delle relazioni logiche:</p> <pre><code>&gt;&gt;&gt; b[b &gt; 2]\narray([3, 4])\n</code></pre> <p>Volendo, possiamo adattare la forma precedente all'uso di espressioni arbitrariamente complesse:</p> <pre><code>&gt;&gt;&gt; b[b%2 == 0]\narray([2, 4])\n&gt;&gt;&gt; b[(b &gt; 1) &amp; (b &lt; 4)]\narray([2, 3])\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array/#slicing-degli-array","title":"Slicing degli array","text":"<p>Cos\u00ec come le liste, anche gli array consentono le operazioni di slicing:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; a[0:2]\narray([1, 2])\n</code></pre> <p>Per gli array multidimensionali, lo slicing si intende sulla \\(n\\)-ma dimensione dell'array. Questo concetto \u00e8 facile da comprendere se si visualizza l'array ad \\(n\\)-dimensioni come un array di array:</p> <pre><code>&gt;&gt;&gt; b\narray([[1, 2],\n       [3, 4]])\n&gt;&gt;&gt; b[0:1]              # Lo slicing avviene sulla seconda dimensione\narray([[1, 2]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array/#la-funzione-nonzero","title":"La funzione <code>nonzero()</code>","text":"<p>Possiamo usare la funzione <code>nonzero()</code> per selezionare gli elementi e gli indici di un array il cui valore non sia pari a zero. Ad esempio:</p> <pre><code>&gt;&gt;&gt; x = np.array([[3, 0, 0], [0, 4, 0], [5, 6, 0]])\n&gt;&gt;&gt; x\narray([[3, 0, 0],\n       [0, 4, 0],\n       [5, 6, 0]])\n&gt;&gt;&gt; np.nonzero(x)\n(array([0, 1, 2, 2], dtype=int64), array([0, 1, 0, 1], dtype=int64))\n</code></pre> <p>La funzione <code>nonzero()</code> restituisce una tupla con gli indici per riga e colonna degli elementi diversi da zero. In particolare, la tupla risultante avr\u00e0 un numero di elementi pari a ciascuna delle dimensioni dell'array <code>x</code> di ingresso, e l'\\(i\\)-mo vettore individuer\u00e0 gli indici relativi alla \\(i\\)-ma dimensione. Ad esempio, in questo caso, il primo array rappresenta gli indici relativi alla prima dimensione dei valori non nulli (in questo caso, gli indici di riga), mentre il secondo gli indici relativi alla seconda dimensione (indici di colonna). Notiamo quindi che avremo i seguenti elementi diversi da zero:</p> Indice di riga Indice di colonna Valore 0 0 3 1 1 4 2 0 5 2 1 6 <p>Ottenere una lista di tuple</p> <p>Possiamo ottenere una lista di tuple rappresentative delle coppie di indici per gli elementi non nulli sfruttando la funzione <code>zip</code>:</p> <pre><code>&gt;&gt;&gt; s = np.nonzero(tarry)\n&gt;&gt;&gt; s\n(array([0, 1, 2, 2], dtype=int64), array([0, 1, 0, 1], dtype=int64))\n&gt;&gt;&gt; coords = list(zip(s[0], s[1]))\n&gt;&gt;&gt; coords\n[(0, 0), (1, 1), (2, 0), (2, 1)]\n</code></pre>"},{"location":"material/02_libs/02_numpy/02_array/#fancy-indexing","title":"Fancy indexing","text":"<p>Chiudiamo questa lezione parlando di una tecnica molto interessante chiamata fancy indexing, consistente nell'usare un array di indici per accedere a pi\u00f9 elementi contemporaneamente. Ad esempio:</p> <pre><code>&gt;&gt;&gt; rand = np.random.RandomState(42)\n&gt;&gt;&gt; x = rand.randint(100, size=10)\n&gt;&gt;&gt; indexes = np.array([[1, 4],[5, 2]])\n&gt;&gt;&gt; x\narray([51, 92, 14, 71, 60, 20, 82, 86, 74, 74])\n&gt;&gt;&gt; x[indexes]\narray([[92, 60],\n       [20, 14]])\n</code></pre> <p>Nel codice precedente, stiamo:</p> <ol> <li>usando la funzione <code>randint()</code> per generare un array di numeri interi casuali compresi tra 0 e 100;</li> <li>generando un array bidimensionale <code>indexes</code>;</li> <li>restituendo, mediante il fancy indexing, un array con le dimensioni di <code>indexes</code> e gli elementi di <code>x</code> presi nelle posizioni indicate da <code>indexes</code>.</li> </ol> <p>La potenza del fancy indexing sta proprio in questo: non solo siamo in grado di accedere facilmente a pi\u00f9 elementi di un array mediante un'unica operazione, ma possiamo anche ridisporre questi elementi come pi\u00f9 ci aggrada!</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/","title":"2.2.3 - Basic Operations on Arrays","text":"<p>After introducing arrays in the previous lesson, let's continue our discussion on arrays by exploring a series of fundamental operations that can be performed on them.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#basic-algebraic-operations","title":"Basic Algebraic Operations","text":"<p>Let's start with basic algebraic operations. For example, we can add two arrays element-wise using the <code>+</code> operator:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2])\n&gt;&gt;&gt; b = np.array([3, 4])\n&gt;&gt;&gt; a + b\narray([4, 6])\n</code></pre> <p>Similarly, we can perform element-wise subtraction, multiplication, and division using the mathematical operators <code>-</code>, <code>*</code>, and <code>/</code>:</p> <pre><code>&gt;&gt;&gt; a - b\narray([-2, -2])\n&gt;&gt;&gt; a * b\narray([3, 8])\n&gt;&gt;&gt; a / b\narray([0.33333333, 0.5])\n&gt;&gt;&gt; b / a\narray([3., 2.])\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#the-numpysum-function","title":"The <code>numpy.sum()</code> function","text":"<p>The <code>numpy.sum()</code> function allows us to sum all elements of an array. For example, to sum all elements of a vector:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; a.sum()\n10\n</code></pre> <p>In the case of a multidimensional array, we can specify the <code>axis</code> parameter, which indicates the axis along which the elements are summed. The value passed can be an integer, in which case the sum is performed along the \\(i\\)-th dimension, or a tuple of integers, in which case the sum is performed on each of the specified dimensions. Let's see some examples.</p> <p>Suppose we have a two-dimensional array of size \\(2 \\times 3\\) (i.e., two rows and three columns).</p> <pre><code>&gt;&gt;&gt; b = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; b\narray([[1, 2, 3],\n       [4, 5, 6]])\n</code></pre> <p>Let's determine which dimension corresponds to rows and which dimension corresponds to columns. We can use the <code>shape</code> attribute for that.</p> <pre><code>&gt;&gt;&gt; b.shape\n(2, 3)\n</code></pre> <p>In the <code>shape</code> tuple, we can see that the first element corresponds to the number of rows, and the second element corresponds to the number of columns. Let's sum the array along the rows by setting <code>axis=0</code>:</p> <pre><code>&gt;&gt;&gt; b.sum(axis=0)\narray([5, 7, 9])\n</code></pre> <p>Similarly, we can sum along the columns by setting <code>axis=1</code>:</p> <pre><code>&gt;&gt;&gt; b.sum(axis=1)\narray([6, 15])\n</code></pre> <p>Let's see what happens in higher dimensions. Let's create a three-dimensional array:</p> <pre><code>&gt;&gt;&gt; c = np.random.rand(2, 3, 4)\n&gt;&gt;&gt; c\narray([[[0.92287179, 0.46394039, 0.87332474, 0.74359334],\n        [0.52656447, 0.6055654 , 0.74493945, 0.9349442 ],\n        [0.90911935, 0.01961204, 0.66304527, 0.3025307 ]],\n\n       [[0.86562763, 0.37544415, 0.4984404 , 0.74486371],\n        [0.10910642, 0.24617353, 0.6237486 , 0.26432378],\n        [0.37713232, 0.08804626, 0.21283048, 0.41133527]]])\n&gt;&gt;&gt; c.shape\n(2, 3, 4)\n</code></pre> <p>Now let's sum all elements along the rows and columns:</p> <pre><code>&gt;&gt;&gt; c.sum(axis=(0, 1))\narray([3.71042197, 1.79878176, 3.61632893, 3.401591  ])\n</code></pre> <p>We have seen the versatility of the <code>sum()</code> function and its ability to operate on arrays of any dimension.</p> <p>The <code>axis</code> parameter</p> <p>We will encounter the <code>axis</code> parameter in every application involving NumPy and related libraries.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#the-dot-function","title":"The <code>dot()</code> function","text":"<p>The <code>dot()</code> function allows us to perform matrix multiplication row by column. Specifically, the function is called on the matrix on the left in the multiplication, while the passed parameter is the matrix on the right.</p> <p>For example, we can multiply a row vector by a column vector, resulting in a scalar:</p> <pre><code>&gt;&gt;&gt; a = np.array([[1, 2]])\n&gt;&gt;&gt; b = np.array([[3], [4]])\n&gt;&gt;&gt; a.dot(b)\narray([11])\n</code></pre> <p>Conversely, multiplying a column vector by a row vector will result in a matrix:</p> <pre><code>&gt;&gt;&gt; b.dot(a)\narray([[3, 6],\n       [4, 8]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#the-numpysort-function","title":"The <code>numpy.sort()</code> function","text":"<p>The <code>numpy.sort()</code> function allows us to sort the elements of an array. For example:</p> <pre><code>&gt;&gt;&gt; arr = np.array([2, 1, 5, 3, 7, 4, 6, 8])\n&gt;&gt;&gt; np.sort(arr)\narray([1, 2, 3, 4, 5, 6, 7, 8])\n</code></pre> <p>The sorting is done in ascending order (from the smallest to the largest element). In the case of an \\(n\\)-dimensional array, we can also specify the axis along which the sorting occurs by specifying the <code>axis</code> parameter. For example:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[2, 3, 1], [4, 2, 6], [7, 5, 1]])\n&gt;&gt;&gt; mat\narray([[2, 3, 1],\n       [4, 2, 6],\n       [7, 5, 1]])\n</code></pre> <p>To sort along the rows:</p> <pre><code>&gt;&gt;&gt; np.sort(mat, axis=0)\narray([[2, 2, 1],\n       [4, 3, 1],\n       [7, 5, 6]])\n</code></pre> <p>While to sort along the columns:</p> <pre><code>&gt;&gt;&gt; np.sort(mat, axis=1)\narray([[1, 2, 3],\n       [2, 4, 6],\n       [1, 5, 7]])\n</code></pre> <p>We can also specify different sorting algorithms using the <code>kind</code> argument, which allows us to choose between quicksort, mergesort, and heapsort.</p> <p>Inplace sorting</p> <p>Keen observers will notice that the <code>sort()</code> function can be called directly on the <code>mat</code> object (<code>mat.sort()</code>) or by using NumPy (<code>np.sort()</code>). In the former case, the matrix is modified inplace, while in the latter case, it is not modified, and a copy is returned.</p> <p>Other sorting functions</p> <p>There are also other functions for sorting an array, such as <code>argsort()</code>, <code>lexsort()</code>, <code>searchsorted()</code>, and <code>partition()</code>.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#the-dot-function_1","title":"The <code>dot()</code> function","text":"<p>The <code>dot()</code> function allows us to perform matrix multiplication row by column. Specifically, the function is called on the matrix on the left in the multiplication, while the passed parameter is the matrix on the right.</p> <p>For example, we can multiply a row vector by a column vector, resulting in a scalar:</p> <pre><code>&gt;&gt;&gt; a = np.array([[1, 2]])\n&gt;&gt;&gt; b = np.array([[3], [4]])\n&gt;&gt;&gt; a.dot(b)\narray([11])\n</code></pre> <p>Conversely, multiplying a column vector by a row vector will result in a matrix:</p> <pre><code>&gt;&gt;&gt; b.dot(a)\narray([[3, 6],\n       [4, 8]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#the-numpysort-function_1","title":"The <code>numpy.sort()</code> function","text":"<p>The <code>numpy.sort()</code> function allows us to sort the elements of an array. For example:</p> <pre><code>&gt;&gt;&gt; arr = np.array([2, 1, 5, 3, 7, 4, 6, 8])\n&gt;&gt;&gt; np.sort(arr)\narray([1, 2, 3, 4, 5, 6, 7, 8])\n</code></pre> <p>The sorting is done in ascending order (from the smallest to the largest element). In the case of an \\(n\\)-dimensional array, we can also specify the axis along which the sorting occurs by specifying the <code>axis</code> parameter. For example:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[2, 3, 1], [4, 2, 6], [7, 5, 1]])\n&gt;&gt;&gt; mat\narray([[2, 3, 1],\n       [4, 2, 6],\n       [7, 5, 1]])\n</code></pre> <p>To sort along the rows:</p> <pre><code>&gt;&gt;&gt; np.sort(mat, axis=0)\narray([[2, 2, 1],\n       [4, 3, 1],\n       [7, 5, 6]])\n</code></pre> <p>While to sort along the columns:</p> <pre><code>&gt;&gt;&gt; np.sort(mat, axis=1)\narray([[1, 2, 3],\n       [2, 4, 6],\n       [1, 5, 7]])\n</code></pre> <p>We can also specify different sorting algorithms using the <code>kind</code> argument, which allows us to choose between quicksort, mergesort, and heapsort.</p> <p>Inplace sorting</p> <p>Keen observers will notice that the <code>sort()</code> function can be called directly on the <code>mat</code> object (<code>mat.sort()</code>) or by using NumPy (<code>np.sort()</code>). In the former case, the matrix is modified inplace, while in the latter case, it is not modified, and a copy is returned.</p> <p>Other sorting functions</p> <p>There are also other functions for sorting an array, such as <code>argsort()</code>, <code>lexsort()</code>, <code>searchsorted()</code>, and <code>partition()</code>.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#multidimensional-arrays-and-delete","title":"Multidimensional Arrays and <code>delete()</code>","text":"<p>The <code>delete()</code> function can also be used on multidimensional arrays. In this case, it is necessary to specify the axis along which to operate.</p> <p>For example, if we want to remove the first row from the following array, we need to set the <code>axis</code> parameter to <code>0</code>:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; np.delete(mat, 0, 0)\narray([[4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <p>On the other hand, if we want to remove the first column, we need to pass the value <code>1</code>:</p> <pre><code>&gt;&gt;&gt; np.delete(mat, 0, 1)\narray([[2, 3],\n       [5, 6],\n       [8, 9]])\n</code></pre> <p>If we do not specify a value for the <code>axis</code> parameter, we get this result:</p> <pre><code>&gt;&gt;&gt; np.delete(mat, 0)\narray([2, 3, 4, 5, 6, 7, 8, 9])\n</code></pre> <p>In other words, by not specifying a value for <code>axis</code>, we remove the first element of the \"flattened\" array.</p> <p>Other Element Removal Techniques</p> <p>There are other ways to remove elements from an array. For example, you could use the <code>slice(start, stop, step)</code> function, which creates a <code>slice</code> object on the indices ranging from <code>start</code> to <code>stop</code> with a step of <code>step</code>. This can be used for purposes similar to the previous methods; for example:</p> <pre><code>&gt;&gt;&gt; np.delete(a, slice(0, 2, 1))\narray([3, 4])\n</code></pre> <p>Another way is to use a boolean mask:</p> <pre><code>&gt;&gt;&gt; mask = np.array([[True, False, True], [False, False, True], [False, True, True]])\n&gt;&gt;&gt; mtrx[mask]\narray([1, 3, 6, 8, 9])\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#the-numpyinsert-function","title":"The <code>numpy.insert()</code> Function","text":"<p>The <code>numpy.insert()</code> function allows us to insert an element into an array. The parameters accepted by the function are:</p> <ul> <li><code>arr</code>: the array on which we want to perform the insertion operation;</li> <li><code>obj</code>: the indices at which to insert the new values;</li> <li><code>values</code>: the values to insert at the indices specified by <code>obj</code>;</li> <li><code>axis</code>: the axis along which we want to operate.</li> </ul> <p>For example, to insert a new row into the previous matrix, we need to specify the row index (<code>3</code>), the elements of the row to insert (<code>[10, 11, 12]</code>), and the axis (<code>0</code>):</p> <pre><code>&gt;&gt;&gt; np.insert(mat, 3, [10, 11, 12], 0)\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\n</code></pre> <p>By changing the axis to <code>1</code>, the insertion is performed on the columns:</p> <pre><code>&gt;&gt;&gt; np.insert(mat, 3, [10, 11, 12], 1)\narray([[ 1,  2,  3, 10],\n       [ 4,  5,  6, 11],\n       [ 7,\n\n  8,  9, 12]])\n</code></pre> <p>By not specifying any axis, the specified element is inserted into the flattened matrix:</p> <pre><code>&gt;&gt;&gt; np.insert(mat, 9, 10)\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#the-numpyappend-function","title":"The <code>numpy.append()</code> Function","text":"<p>The <code>numpy.append()</code> function allows us to append the specified values to the end of an array. The parameters accepted by the function are:</p> <ul> <li><code>arr</code>: the array on which we want to perform the append operation;</li> <li><code>values</code>: the values to append to the array;</li> <li><code>axis</code>: the axis along which we want to operate.</li> </ul> <p>By not specifying the axis, the concatenation is performed on the flattened matrix:</p> <pre><code>&gt;&gt;&gt; np.append(mat, [[10, 11, 12]])\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n</code></pre> <p>If we specify the value <code>0</code> for the <code>axis</code> parameter, the concatenation is performed along the rows:</p> <pre><code>&gt;&gt;&gt; np.append(mat, [[10, 11, 12]], axis=0)\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\n</code></pre> <p>If we specify the value <code>1</code> for the <code>axis</code> parameter, the concatenation is performed along the columns:</p> <pre><code>&gt;&gt;&gt; np.append(mat, [[10], [11], [12]], axis=1)\narray([[ 1,  2,  3, 10],\n       [ 4,  5,  6, 11],\n       [ 7,  8,  9, 12]])\n</code></pre> <p>Attention</p> <p>In the last instruction, we used a column vector, while in the penultimate one, we used a row vector.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#array-dimensions-and-shape","title":"Array Dimensions and Shape","text":"<p>There are several properties of an array that describe its dimensions and shape.</p> <p>Referring back to our matrix <code>mat</code>, we can determine the number of axes using the <code>ndarray.ndim</code> attribute:</p> <pre><code>&gt;&gt;&gt; mat.ndim\n2\n</code></pre> <p>The number of elements is defined by the <code>ndarray.size</code> attribute:</p> <pre><code>&gt;&gt;&gt; mat.size\n9\n</code></pre> <p>The <code>ndarray.shape</code> attribute, which we briefly saw earlier, returns a tuple of integers indicating the number of elements along each axis of the array:</p> <pre><code>&gt;&gt;&gt; mat.shape\n(3, 3)\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#modifying-the-dimensions-of-an-array","title":"Modifying the Dimensions of an Array","text":"<p>We can modify the dimensions of an array using the <code>numpy.reshape()</code> function. The parameters passed to the function are:</p> <ul> <li><code>arr</code>: the array to modify the dimensions of;</li> <li><code>new_shape</code>: the new dimensions of the array.</li> </ul> <p>If we wanted to change the dimensions of a \\(4 \\times 4\\) matrix to \\(2 \\times 8\\), we could use the <code>reshape()</code> function as follows:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n&gt;&gt;&gt; np.reshape(mat, (2, 8))\narray([[ 1,  2,  3,  4,  5,  6,  7,  8],\n       [ 9, 10, 11, 12, 13, 14, 15, 16]])\n</code></pre> <p>Tip</p> <p>An alternative syntax is as follows:</p> <pre><code>&gt;&gt;&gt; mat.reshape((2, 8))\narray([[ 1,  2,  3,  4,  5,  6,  7,  8],\n       [ 9, 10, 11, 12, 13, 14, 15, 16]])\n</code></pre> <p>This means that the <code>reshape()</code> function is available both as a NumPy library function and as a method on <code>ndarray</code> objects.</p> <p>Attention</p> <p>The new dimensions of the array must be consistent with those of the original array!</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals.en/#flattening-an-array","title":"Flattening an Array","text":"<p>We have already seen the flattening of an array, which is automatically performed in certain situations (such as when calling <code>delete()</code> or <code>insert()</code> without specifying the <code>axis</code> parameter). However, we can use the <code>numpy.flatten()</code> function to manually perform this operation:</p> <pre><code>&gt;&gt;&gt; mat.flatten()\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n</code></pre> <p>Another way to flatten an array is to use the <code>numpy.ravel()</code> function.</p> <p>The <code>ravel()</code> and <code>flatten()</code> functions may appear to be similar. However, there is a fundamental difference: <code>flatten()</code> will always return a copy of the original array, while <code>ravel()</code>, if possible, will return a view, which is a variable that points to the same memory address as the original object.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals/","title":"2.2.3 - Operazioni fondamentali sugli array","text":"<p>Dopo averli introdotti nella lezione precedente, proseguiamo nella nostra trattazione sugli array andando a vedere tutta quella serie di operazioni fondamentali che \u00e8 possibile effettuarvi.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#operazioni-algebriche-di-base","title":"Operazioni algebriche di base","text":"<p>Partiamo dalle operazioni algebriche di base. Ad esempio, possiamo sommare elemento per elemento due array mediante l'operatore <code>+</code>:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2])\n&gt;&gt;&gt; b = np.array([3, 4])\n&gt;&gt;&gt; a + b\narray([4, 6])\n</code></pre> <p>Contestualmente, l'utilizzo degli operatori matematici di sottrazione, moltiplicazione e divisione ci permetter\u00e0 di effettuare le omologhe operazioni, sempre elemento per elemento.</p> <pre><code>&gt;&gt;&gt; a - b\narray([-2, -2])\n&gt;&gt;&gt; a * b\narray([3, 8])\n&gt;&gt;&gt; a / b\narray([0.33333333, 0.5])\n&gt;&gt;&gt; b / a\narray([3., 2.])\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#la-funzione-numpysum","title":"La funzione <code>numpy.sum()</code>","text":"<p>La funzione <code>numpy.sum()</code> ci permette di sommare tutti gli elementi di un array. Ad esempio, per sommare tutti gli elementi di un vettore:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; a.sum()\n10\n</code></pre> <p>In caso di array multidimensionale, potremo specificare il parametro <code>axis</code>, che indica l'asse lungo il quale gli elementi vengono sommati. In particolare, il valore passato pu\u00f2 essere un intero a valore \\(i\\), nel qual caso si andr\u00e0 ad effettuare la somma lungo la dimensione \\(i\\)-ma, o una tupla di interi, in qual caso la somma sar\u00e0 effettuata su ognuna delle dimensioni specificate. Facciamo alcuni esempi.</p> <p>Supponiamo di avere un array bidimensionale di dimensioni \\(2 \\times 3\\) (ovvero, a due righe e tre colonne).</p> <pre><code>&gt;&gt;&gt; b = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; b\narray([[1, 2, 3],\n       [4, 5, 6]])\n</code></pre> <p>Cerchiamo di capire a quale dimensione sono associate le righe, ed a quale le colonne. Per farlo, usiamo l'attributo <code>shape</code>.</p> <pre><code>&gt;&gt;&gt; b.shape\n(2, 3)\n</code></pre> <p>Nella tupla associata a <code>shape</code>, notiamo come il primo elemento sia associato al numero di righe, mentre il secondo al numero di colonne. Proviamo quindi a sommare l'array per righe, passando il parametro <code>axis=0</code>:</p> <pre><code>&gt;&gt;&gt; b.sum(axis=0)\narray([5, 7, 9])\n</code></pre> <p>Intuitivamente, dato che il secondo elemento \u00e8 associato al numero di colonne, la somma per colonne avr\u00e0 luogo valorizzando <code>axis</code> ad <code>1</code>:</p> <pre><code>&gt;&gt;&gt; b.sum(axis=1)\narray([6, 15])\n</code></pre> <p>Proviamo a vedere cosa accade in pi\u00f9 dimensioni. Creiamo un array tridimensionale:</p> <pre><code>&gt;&gt;&gt; c = np.random.rand(2, 3, 4)\n&gt;&gt;&gt; c\narray([[[0.92287179, 0.46394039, 0.87332474, 0.74359334],\n        [0.52656447, 0.6055654 , 0.74493945, 0.9349442 ],\n        [0.90911935, 0.01961204, 0.66304527, 0.3025307 ]],\n\n       [[0.86562763, 0.37544415, 0.4984404 , 0.74486371],\n        [0.10910642, 0.24617353, 0.6237486 , 0.26432378],\n        [0.37713232, 0.08804626, 0.21283048, 0.41133527]]])\n&gt;&gt;&gt; c.shape\n(2, 3, 4)\n</code></pre> <p>Proviamo adesso ad effettuare la somma di tutti gli elementi per riga e colonna:</p> <pre><code>&gt;&gt;&gt; c.sum(axis=(0, 1))\narray([3.71042197, 1.79878176, 3.61632893, 3.401591  ])\n</code></pre> <p>Abbiamo quindi visto la versatilit\u00e0 della funzione <code>sum()</code>, e la capacit\u00e0 della stessa di operare su array a qualsiasi dimensionalit\u00e0.</p> <p>Il parametro <code>axis</code></p> <p>Ritroveremo il parametro <code>axis</code> in ogni applicazione che coinvolge NumPy e le librerie basate su di essa.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#la-funzione-dot","title":"La funzione <code>dot()</code>","text":"<p>La funzione <code>dot()</code> ci permette di effettuare l'operazione di moltiplicazione matriciale riga per colonna. In particolare, la funzione sar\u00e0 invocata sulla matrice a sinistra nella moltiplicazione, mentre il parametro passato sar\u00e0 la matrice a destra.</p> <p>Ad esempio, possiamo provare a moltiplicare un vettore riga per un vettore colonna, ottenendo un intero:</p> <pre><code>&gt;&gt;&gt; a = np.array([[1, 2]])\n&gt;&gt;&gt; b = np.array([[3], [4]])\n&gt;&gt;&gt; a.dot(b)\narray([11])\n</code></pre> <p>Al contrario, moltiplicando un vettore colonna per un vettore riga, otterremo una matrice:</p> <pre><code>&gt;&gt;&gt; b.dot(a)\narray([[3, 6],\n       [4, 8]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#la-funzione-numpysort","title":"La funzione <code>numpy.sort()</code>","text":"<p>La funzione <code>numpy.sort()</code> permette di ordinare gli elementi di un array. Ad esempio:</p> <pre><code>&gt;&gt;&gt; arr = np.array([2, 1, 5, 3, 7, 4, 6, 8])\n&gt;&gt;&gt; np.sort(arr)\narray([1, 2, 3, 4, 5, 6, 7, 8])\n</code></pre> <p>L'ordine avviene maniera ascendente (ovvero dall'elemento pi\u00f9 piccolo al pi\u00f9 grande). In caso di array \\(n\\)-dimensionale, possiamo anche specificare l'asse lungo il quale avviene l'ordinamento, specificando il parametro axis. Ad esempio:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[2, 3, 1], [4, 2, 6], [7, 5, 1]])\n&gt;&gt;&gt; mat\narray([[2, 3, 1],\n       [4, 2, 6],\n       [7, 5, 1]])\n</code></pre> <p>Per ordinare lungo le righe:</p> <pre><code>&gt;&gt;&gt; np.sort(mat, axis=0)\narray([[2, 2, 1],\n       [4, 3, 1],\n       [7, 5, 6]])\n</code></pre> <p>Mentre per ordinare lungo le colonne:</p> <pre><code>&gt;&gt;&gt; np.sort(mat, axis=1)\narray([[1, 2, 3],\n       [2, 4, 6],\n       [1, 5, 7]])\n</code></pre> <p>Possiamo anche specificare diversi algoritmi di ordinamento mediante l'argomento <code>kind</code>, che ci permette di scegliere tra il quick sort, il merge sort e l'heap sort.</p> <p>Ordinamento inplace</p> <p>I pi\u00f9 attenti noteranno che la funzione <code>sort()</code> pu\u00f2 essere chiamata direttamente sull'oggetto <code>mat</code> (<code>mat.sort()</code>) oppure da NumPy (<code>np.sort()</code>). Nel primo caso, la matrice viene modificata inplace, mentre nel secondo caso non lo \u00e8, e ne viene modificata una copia.</p> <p>Altre funzioni per l'ordinamento</p> <p>Esistono anche altre funzioni per l'ordinamento di un array, come <code>argsort()</code>, <code>lexsort()</code>, <code>searchsorted()</code> e <code>partition()</code>.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#concatenazione-di-due-array","title":"Concatenazione di due array","text":"<p>Per concatenare due array \u00e8 necessario usare la funzione <code>concatenate()</code>:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; b = np.array([5, 6, 7, 8])\n&gt;&gt;&gt; np.concatenate((a, b))\narray([1, 2, 3, 4, 5, 6, 7, 8])\n</code></pre> <p>Si pu\u00f2 anche in questo caso usare il parametro <code>axis</code> per specificare l'asse lungo quale concatenare due diversi array:</p> <pre><code>&gt;&gt;&gt; x = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; y = np.array([[5, 6], [7, 8]])\n&gt;&gt;&gt; np.concatenate((x, y), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6],\n       [7, 8]])\n&gt;&gt;&gt; np.concatenate((x, y), axis=1)\narray([[1, 2, 5, 6],\n       [3, 4, 7, 8]])\n</code></pre> <p>Ovviamente, le dimensioni degli array devono essere coerenti affinch\u00e9 vengano concatenati. Ad esempio:</p> <pre><code>&gt;&gt;&gt; z = np.array([[9, 10]])\n</code></pre> <p>la concatenazione per righe \u00e8 ammissibile:</p> <pre><code>&gt;&gt;&gt; np.concatenate((x, z), axis=0)\narray([[ 1,  2],\n       [ 3,  4],\n       [ 9, 10]])\n</code></pre> <p>mentre la concatenazione per colonne lancer\u00e0 un errore:</p> <pre><code>&gt;&gt;&gt; np.concatenate((x, z), axis=1)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"&lt;__array_function__ internals&gt;\", line 5, in concatenate\nValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 2 and the array at index 1 has size 1\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#rimozione-ed-inserimento-di-elementi-in-un-array","title":"Rimozione ed inserimento di elementi in un array","text":"<p>Vediamo brevemente come aggiungere o rimuovere un elemento in un array.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#la-funzione-numpydelete","title":"La funzione <code>numpy.delete()</code>","text":"<p>La funzione <code>numpy.delete()</code> ci permette di rimuovere uno o pi\u00f9 elementi di un array specificandone gli indici. La funzione accetta i seguenti parametri:</p> <ul> <li><code>arr</code>: l'array sul quale vogliamo effettuare l'operazione di rimozione;</li> <li><code>obj</code>: gli indici degli elementi da rimuovere;</li> <li><code>axis</code>: l'asse su cui vogliamo operare.</li> </ul> <p>Ad esempio, immaginiamo di voler rimuovere il primo elemento di un vettore:</p> <pre><code>&gt;&gt;&gt; arr = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; np.delete(arr, 0)\narray([2, 3, 4])\n</code></pre> <p>La funzione pu\u00f2 essere anche applicata su pi\u00f9 indici usando una sequenza:</p> <pre><code>&gt;&gt;&gt; np.delete(arr, range(2))\narray([3, 4])\n</code></pre> <p>Possiamo anche usare lo slicing:</p> <pre><code>&gt;&gt;&gt; idx = range(4)\n&gt;&gt;&gt; np.delete(arr, idx[0:2])\narray([3, 4])\n</code></pre> <p>Modifiche inplace</p> <p>Nei casi precedenti la modifica non avviene inplace, per cui <code>arr</code> non sar\u00e0 modificato.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#array-multidimensionali-e-delete","title":"Array multidimensionali e <code>delete()</code>","text":"<p>La funzione <code>delete()</code> pu\u00f2 essere usata anche su array multidimensionali. In questo caso, \u00e8 opportuno specificare l'asse su cui operare.</p> <p>Ad esempio, se vogliamo rimuovere la prima riga dal seguente array, dobbiamo dare il valore <code>0</code> al parametro <code>axis</code>:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; np.delete(mat, 0, 0)\narray([[4, 5, 6],\n       [7, 8, 9]])\n</code></pre> <p>Invece, se vogliamo rimuovere la prima colonna, dobbiamo passare il valore <code>1</code>:</p> <pre><code>&gt;&gt;&gt; np.delete(mat, 0, 1)\narray([[2, 3],\n       [5, 6],\n       [8, 9]])\n</code></pre> <p>Se non specificassimo alcun valore per il parametro <code>axis</code>, otterremmo questo risultato:</p> <pre><code>&gt;&gt;&gt; np.delete(mat, 0)\narray([2, 3, 4, 5, 6, 7, 8, 9])\n</code></pre> <p>In altre parole, non specificando un valore per <code>axis</code>, rimuoveremmo il primo elemento dell'array \"vettorizzato\".</p> <p>Altre tecniche di rimozione elementi</p> <p>Esistono altri modi di rimuovere elementi da un array. Ad esempio, si potrebbe usare la funzione <code>slice(start, stop, step)</code>, che crea un oggetto di classe <code>slice</code> sugli indici che vanno da <code>start</code> a <code>stop</code> con passo <code>step</code>. Questo pu\u00f2 essere usato per scopi analoghi ai precedenti; ad esempio:</p> <pre><code>&gt;&gt;&gt; np.delete(a, slice(0, 2, 1))\narray([3, 4])\n</code></pre> <p>Un altro modo \u00e8 usare una maschera booleana:</p> <pre><code>&gt;&gt;&gt; mask = np.array([[True, False, True], [False, False, True], [False, True, True]])\n&gt;&gt;&gt; mtrx[mask]\narray([1, 3, 6, 8, 9])\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#la-funzione-numpyinsert","title":"La funzione <code>numpy.insert()</code>","text":"<p>La funzione <code>numpy.insert()</code> permette di inserire un elemento all'interno di un array. I parametri accettati dalla funzione sono:</p> <ul> <li><code>arr</code>: l'array sul quale vogliamo effettuare l'operazione di inserzione;</li> <li><code>obj</code>: gli indici su cui inserire i nuovi valori;</li> <li><code>values</code>: i valori da inserire agli indici specificati da <code>obj</code>;</li> <li><code>axis</code>: l'asse su cui vogliamo operare.</li> </ul> <p>Ad esempio, per inserire una nuova riga nella matrice precedente, dovremo specificare l'indice di riga (<code>3</code>), gli elementi della riga da inserire (<code>[10, 11, 12]</code>) e l'asse (<code>0</code>):</p> <pre><code>&gt;&gt;&gt; np.insert(mat, 3, [10, 11, 12], 0)\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\n</code></pre> <p>Cambiando l'asse in <code>1</code>, si effettua l'inserzione sulle colonne:</p> <pre><code>&gt;&gt;&gt; np.insert(mat, 3, [10, 11, 12], 1)\narray([[ 1,  2,  3, 10],\n       [ 4,  5,  6, 11],\n       [ 7,  8,  9, 12]])\n</code></pre> <p>Non specificando alcun asse, infine, si inserisce l'elemento specificato nella matrice vettorizzata:</p> <pre><code>&gt;&gt;&gt; np.insert(mat, 9, 10)\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#la-funzione-numpyappend","title":"La funzione <code>numpy.append()</code>","text":"<p>La funzione <code>numpy.append()</code> permette di inserire in coda ad un array i valori specificati. I parametri accettati dalla funzione sono:</p> <ul> <li><code>arr</code>: l'array sul quale vogliamo effettuare l'operazione di inserzione;</li> <li><code>values</code>: i valori da inserire in coda all'array;</li> <li><code>axis</code>: l'asse su cui vogliamo operare.</li> </ul> <p>Al solito, non specificando l'asse effettuiamo la concatenazione sulla matrice vettorizzata:</p> <pre><code>&gt;&gt;&gt; np.append(mat, [[10, 11, 12]])\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n</code></pre> <p>Se specifichiamo il valore 0 sul parametro <code>axis</code>, effettuiamo la concatenazione per righe:</p> <pre><code>&gt;&gt;&gt; np.append(mat, [[10, 11, 12]], axis=0)\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\n</code></pre> <p>Se specifichiamo il valore 1 sul parametro <code>axis</code>, invece, effettuiamo la concatenazione per colonne:</p> <pre><code>&gt;&gt;&gt; np.append(mat, [[10], [11], [12]], axis=1)\narray([[ 1,  2,  3, 10],\n       [ 4,  5,  6, 11],\n       [ 7,  8,  9, 12]])\n</code></pre> <p>Attenzione</p> <p>Nell'ultima istruzione, abbiamo usato un vettore colonna, mentre nella penultima un vettore riga.</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#dimensioni-e-forma-di-un-array","title":"Dimensioni e forma di un array","text":"<p>Esistono diverse propriet\u00e0 di un array che ne descrivono dimensioni e forma.</p> <p>Tornando alla nostra matrice <code>mat</code>, possiamo conoscere il numero di assi mediante l'attributo <code>ndarray.ndim</code>:</p> <pre><code>&gt;&gt;&gt; mat.ndim\n2\n</code></pre> <p>Il numero di elementi \u00e8 invece definito dall'attributo <code>ndarray.size</code>:</p> <pre><code>&gt;&gt;&gt; mat.size\n9\n</code></pre> <p>L'attributo <code>ndarray.shape</code>, che abbiamo brevemente visto in precedenza, restituisce invece una tupla di interi che indica il numero di elementi per ciascuno degli assi dell'array:</p> <pre><code>&gt;&gt;&gt; mat.shape\n(3, 3)\n</code></pre>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#modificare-le-dimensioni-di-un-array","title":"Modificare le dimensioni di un array","text":"<p>Possiamo modificare le dimensioni di un array mediante la funzione <code>numpy.reshape()</code>. I parametri passati alla funzione sono:</p> <ul> <li><code>arr</code>: l'array di cui modificare le dimensioni;</li> <li><code>new_shape</code>: le nuove dimensioni dell'array.</li> </ul> <p>Se volessimo modificare le dimensioni di una matrice da \\(4 \\times 4\\) a \\(2 \\times 8\\), potremmo usare la funzione <code>reshape()</code> come segue:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n&gt;&gt;&gt; np.reshape(mat, (2, 8))\narray([[ 1,  2,  3,  4,  5,  6,  7,  8],\n       [ 9, 10, 11, 12, 13, 14, 15, 16]])\n</code></pre> <p>Suggerimento</p> <p>Una forma alternativa \u00e8 la seguente:</p> <pre><code>&gt;&gt;&gt; mat.reshape((2, 8))\narray([[ 1,  2,  3,  4,  5,  6,  7,  8],\n       [ 9, 10, 11, 12, 13, 14, 15, 16]])\n</code></pre> <p>Ci\u00f2 significa che la funzione <code>reshape()</code> \u00e8 sia disponibile nella libreria NumPy, sia come metodo sugli oggetti di classe <code>ndarray</code>.</p> <p>Attenzione</p> <p>Le nuove dimensioni dell'array devono essere coerenti con quelle dell'array di partenza!</p>"},{"location":"material/02_libs/02_numpy/03_fundamentals/#flattening-di-un-array","title":"Flattening di un array","text":"<p>Abbiamo gi\u00e0 visto in precedenza la vettorizzazione di un array, effettuata in automatico in alcune situazioni (come ad esempio la chiamata di <code>delete()</code> o <code>insert()</code> senza specificare il parametro <code>axis</code>). Tuttavia, possiamo usare la funzione <code>numpy.flatten()</code> per effettuare manualmente questa operazione:</p> <pre><code>&gt;&gt;&gt; mat.flatten()\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n</code></pre> <p>Un altro modo per effettuare la vettorizzazione \u00e8 utilizzare la funzione <code>numpy.ravel()</code>.</p> <p>Le funzioni <code>ravel()</code> e <code>flatten()</code> possono apparentemente sembrare analoghe. Tuttavia, vi \u00e8 una differenza fondamentale: infatti, <code>flatten()</code> restituir\u00e0 sempre una copia dell'array originario, mentre <code>ravel()</code>, laddove possibile, restituir\u00e0 una vista, ovvero una variabile che punta allo stesso indirizzo di memoria dell'oggetto originario.</p>"},{"location":"material/02_libs/02_numpy/04_algebra.en/","title":"2.2.4 - Algebraic Operations in NumPy","text":"<p>Accompanying Notebook</p> <p>For this lesson, there is an accompanying notebook available at this link.</p> <p>After covering the fundamental operations in NumPy, let's delve into linear algebra operations, some of which are integrated into the <code>linalg</code> package.</p> <p>The examples we will see in the following sections all involve the use of this package, so it is necessary to import it before proceeding with the lesson.</p> <pre><code>from numpy import linalg\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#matrix-transpose","title":"Matrix Transpose","text":"<p>In reality, the first operation we will describe does not require the use of the <code>linalg</code> module, and it is the operation that allows us to take the transpose of a matrix.</p> <p>Recall that the transpose \\(A^T\\) of a matrix \\(A\\) is defined as the matrix in which the element with index \\((i, j)\\) is the element with indices \\((j, i)\\) of the original matrix. In practice:</p> \\[ (A^T)_{ij} = A_{ji} \\forall A \\in \\mathbb{R}^{m,n}, 1 \\leq i \\leq m, 1 \\leq j \\leq n \\] <p>To accomplish this, we simply use the <code>numpy.transpose()</code> function.</p> <pre><code>&gt;&gt;&gt; x = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; np.transpose(x)\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#matrix-inverse","title":"Matrix Inverse","text":"<p>The calculation of the matrix inverse, on the other hand, requires the use of the <code>linalg.inv()</code> function, which takes the matrix to be inverted as a parameter.</p> <p>Recall that the inverse matrix \\(A_{inv}\\) of an invertible matrix \\(A\\) is the matrix for which the following relationship holds:</p> \\[ A_{inv}A=AA_{inv}=I \\] <p>In other words, the matrix product between \\(A\\) and its inverse is commutative and equal to the identity matrix of the same rank.</p> <p>For example:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[5, 0, 0], [0, 2, 0], [0, 0, 4]])\n&gt;&gt;&gt; linalg.inv(mat)\narray([[0.2 , 0.  , 0.  ],\n       [0.  , 0.5 , 0.  ],\n       [0.  , 0.  , 0.25]])\n</code></pre> <p>Obviously, the matrix <code>mat</code> must be invertible, that is, it must be square and have maximum rank. If we were to pass a rectangular matrix, an error of type <code>LinAlgError</code> would be raised:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; linalg.inv(mat)\nTraceback (most recent call last):\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#vector-and-matrix-products","title":"Vector and Matrix Products","text":""},{"location":"material/02_libs/02_numpy/04_algebra.en/#the-dot-function","title":"The <code>dot()</code> function","text":"<p>In the previous lesson, we saw an example of using the <code>dot(a, b)</code> function to calculate the matrix product between arrays <code>a</code> and <code>b</code>. This function follows all the rules of matrix calculations, as summarized in the following table.</p> Dimensionality of <code>a</code> Dimensionality of <code>b</code> Result One-dimensional (vector) One-dimensional (vector) Dot product Two-dimensional (matrix) Two-dimensional (matrix) Matrix product Scalar \\(n\\)-dimensional Dot product with \\(n\\)-dimensional array \\(n\\)-dimensional Scalar Dot product with \\(n\\)-dimensional array <p>Note that:</p> <ul> <li>when multiplying two vectors, the result is the dot product;</li> <li>when multiplying two matrices, the result is the matrix product, and it is recommended to use the <code>matmul()</code> function instead;</li> <li>when multiplying a scalar and a matrix, it is recommended to use the <code>multiply()</code> function or alternatively, the <code>*</code> operator.</li> </ul> <p>In the lesson notebook, we will see some examples of matrix products.</p> <p>Multiplication of Multidimensional Arrays</p> <p>When both arrays to be multiplied are \\(n\\)-dimensional, additional rules apply, which can be found at this link.</p>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#inner-product","title":"Inner Product","text":"<p>Recall that for two arbitrary one-dimensional vectors \\(a = [a_{1}, \\ldots, a_{j}]\\) and \\(b = [b_{1}, \\ldots, b_{j}]\\), both with \\(j\\) elements, the dot product or inner product is given by:</p> \\[ p = \\sum_{i=1}^j a_{i} \\cdot b_{i} \\] <p>To calculate the dot product between vectors <code>a</code> and <code>b</code>, we can use the <code>numpy.inner(a, b)</code> function:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3])\n&gt;&gt;&gt; b = np.array([4, 5, 6])\n&gt;&gt;&gt; np.inner(a, b)\n32\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#the-functions-inner-and-dot","title":"The Functions <code>inner()</code> and <code>dot()</code>","text":"<p>An attentive reader may have noticed that, in practice, for monodimensional vectors, the <code>inner()</code> and <code>dot()</code> functions return the same result:</p> <pre><code>np.inner(a, b)      # Output: 32\na.dot(b)            # Output: 32, same as b.dot()\n</code></pre> <p>The difference between the two functions becomes apparent when using arrays with dimensions greater than 1:</p> <pre><code>&gt;&gt;&gt; a = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; b = np.array([[5, 6], [7, 8]])\n&gt;&gt;&gt; np.inner(a, b)   # Result with inner()\narray([[17, 23],\n       [39, 53]])\n&gt;&gt;&gt; a.dot(b)   # Result with dot()\narray([[19, 22],\n       [43, 50]])\n</code></pre> <p>In practice, referring back to the documentation:</p> <ul> <li>Regarding the <code>dot()</code> function, it is equivalent to <code>matmul()</code> and represents a matrix multiplication. In the case of monodimensional vectors, it is equivalent to the dot product. For \\(n\\) dimensions, it is the sum of the products between the last dimension of the first vector and the dimensions ranging from 2 to \\(n\\) of the second vector.</li> <li>Regarding the <code>inner()</code> function, it represents the dot product for monodimensional vectors. In the case of \\(n\\) dimensions, it represents the sum of the products along the last dimension.</li> </ul> <p>In other words:</p> <pre><code>a.dot(b) == sum(a[i, :] * b[:, j])\nnp.inner(a, b) == sum(a[i, :] * b[j, :])\n</code></pre> <p>Referring to the previous example:</p> \\[ \\text{dot} = \\left(\\begin{array}{cc} 1 &amp; 2\\\\ 3 &amp; 4 \\end{array}\\right) \\left(\\begin{array}{cc} 5 &amp; 6\\\\ 7 &amp; 8 \\end{array}\\right) = \\\\ = \\left(\\begin{array}{cc} 1 \\cdot 5 + 2 \\cdot 7 &amp; 1 \\cdot 6 + 2 \\cdot 8   \\\\ 3 \\cdot 5 + 4 \\cdot 7 &amp; 3 \\cdot 6 + 4 \\cdot 8 \\end{array}\\right) = \\left(\\begin{array}{cc} 19 &amp; 22\\\\ 43 &amp; 50 \\end{array}\\right) \\] \\[ \\text{inner} = \\left(\\begin{array}{cc} 1 &amp; 2\\\\ 3 &amp; 4 \\end{array}\\right) \\left(\\begin{array}{cc} 5 &amp; 6\\\\ 7 &amp; 8 \\end{array}\\right) = \\\\ = \\left(\\begin{array}{cc} 1 \\cdot 5 + 2 \\cdot 6 &amp; 1 \\cdot 7 + 2 \\cdot 8   \\\\ 3 \\cdot 5 + 4 \\cdot 6 &amp; 3 \\cdot 7 + 4 \\cdot 8 \\end{array}\\right) = \\left(\\begin{array}{cc} 17 &amp; 23\\\\ 39 &amp; 53 \\end{array}\\right) \\]"},{"location":"material/02_libs/02_numpy/04_algebra.en/#outer-product","title":"Outer Product","text":"<p>The outer product between two vectors \\(a = [a_1, a_2, \\ldots, a_j]\\) and \\(b = [b_1, b_2, \\ldots, b_j]\\) is defined as the matrix \\(P\\) where:</p> <p>$$ P = \\left[     \\begin{array}{ccc}</p> <pre><code>    a_1 \\cdot b_1 &amp; \\ldots &amp; a_1 \\cdot b_n \\\\\n    \\vdots        &amp; \\ddots &amp; \\vdots        \\\\\n    a_n \\cdot b_1 &amp; \\ldots &amp; a_n \\cdot b_n\n\\end{array}\n</code></pre> <p>\\right] $$</p> <p>To calculate it, NumPy provides the function <code>numpy.outer(a, b)</code>.</p> <p>For example:</p> <pre><code>a = np.array([1, 2])\nb = np.array([3, 4])\n\nnp.outer(a, b)\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#the-matmul-function","title":"The <code>matmul()</code> Function","text":"<p>When we talked about the <code>dot()</code> function, we saw how it is possible to use it to perform matrix multiplication between matrices <code>mat_1</code> and <code>mat_2</code>. However, there is another possibility, which is also the recommended one, which is to use the <code>numpy.matmul()</code> function:</p> <pre><code>a = np.array([[1, 2], [3, 4]])\nb = np.array([[5, 6], [7, 8]])\nnp.matmul(a, b)\n</code></pre> <p>The <code>matmul()</code> function has a fundamental difference compared to the <code>dot()</code> function, as it does not accept scalars as input (although it is possible to pass vectors and \\(n\\)-dimensional arrays). There is actually another important difference that concerns \\(n\\)-dimensional operations, but we will not discuss it here.</p> <p>The <code>@</code> Operator</p> <p>The <code>@</code> operator is delegated to direct multiplication of two-dimensional arrays and is used \"under the hood\" by the <code>matmul()</code> function.</p>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#matrix-power","title":"Matrix Power","text":"<p>The <code>np.linalg.matrix_power(a, n)</code> function allows us to raise matrix <code>a</code> to the power of <code>n</code>. For example:</p> <pre><code>np.linalg.matrix_power(a, 5)\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#eigenvalues-and-eigenvectors","title":"Eigenvalues and Eigenvectors","text":"<p>To calculate the eigenvalues and eigenvectors of a matrix, NumPy provides the <code>np.linalg.eig()</code> function, which returns the eigenvalues and right eigenvectors of a square matrix:</p> <pre><code>(v, w) = np.linalg.eig(a)\nv\nw\n</code></pre> <p>There are also other functions for calculating eigenvalues and eigenvectors. In particular:</p> <ul> <li><code>np.linalg.eigh()</code> calculates the eigenvalues and eigenvectors of a real or complex symmetric matrix;</li> <li><code>np.linalg.eigvals()</code> calculates the eigenvalues of a square matrix;</li> <li><code>np.linalg.eigvalsh()</code> calculates the eigenvalues of a real or complex symmetric matrix.</li> </ul>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#norm-rank-determinant-and-trace","title":"Norm, Rank, Determinant, and Trace","text":"<p>The <code>np.linalg.norm(a)</code> function allows us to calculate the norm of a matrix. Optionally, we can specify three parameters:</p> <ul> <li><code>ord</code>, which represents the order of the norm to calculate (by default, the Frobenius norm is calculated);</li> <li><code>axis</code>, which indicates the axis (or axes, in the case of a multidimensional array) to operate on;</li> <li><code>keepdims</code>, used to optionally return the axis along which the norm is calculated.</li> </ul> <p>To calculate the Frobenius norm of matrix <code>mat</code>, we can use this syntax:</p> <pre><code>np.linalg.norm(a)\n</code></pre> <p>To calculate the determinant, rank, and trace of a matrix, we can use the functions <code>np.linalg.det()</code>, <code>np.linalg.matrix_rank()</code>, and <code>np.trace()</code>. For example:</p> <pre><code>np.linalg.det(a)\nnp.linalg.matrix_rank(a\n\n)\nnp.trace(a)\n</code></pre> <p>The <code>trace()</code> function can also be used to calculate the sum of the upper/lower diagonals by specifying the <code>offset</code> parameter. For example:</p> <pre><code>np.trace(mtrx, offset=1)\nnp.trace(mtrx, offset=-1)\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#solving-linear-systems-of-equations","title":"Solving Linear Systems of Equations","text":"<p>We conclude this (necessarily) brief overview of linear algebra operations with the function <code>np.linalg.solve(a, b)</code>, which allows us to solve a system of linear equations where matrix <code>a</code> is the coefficient matrix, and vector <code>b</code> is the vector of known terms. For example:</p> <pre><code>b = np.array([3, 2, 3])\nnp.linalg.solve(mat, b)\n</code></pre> <p>Of course, the matrix <code>a</code> must be square, and the vector <code>b</code> must have exactly <code>n</code> elements, where <code>n</code> is the order of <code>a</code>!</p> <p>Operations on Tensors</p> <p>NumPy provides the N-dimensional tensor version of some functions. In particular, we mention the tensor product function <code>np.tensordot()</code> and the function for solving tensor equations <code>np.linalg.tensorsolve()</code>.</p>"},{"location":"material/02_libs/02_numpy/04_algebra.en/#reference","title":"Reference","text":"<p>As mentioned several times, this overview has been necessarily brief and limited. For a more complete overview, the recommendation is always to refer to the excellent NumPy documentation.</p>"},{"location":"material/02_libs/02_numpy/04_algebra/","title":"2.2.4 - Operazioni algebriche in NumPy","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Dopo aver trattato le operazioni fondamentali in NumPy, facciamo un breve approfondimento sulle operazioni di algebra lineare, alcune delle quali sono integrate nel package <code>linalg</code>.</p> <p>Gli esempi che vedremo nel seguito prevederanno tutti l'uso di questo package, per cui \u00e8 necessario importarlo prima di procedere nella lezione.</p> <pre><code>from numpy import linalg\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra/#matrice-trasposta","title":"Matrice trasposta","text":"<p>In realt\u00e0, la prima operazione che descriveremo non richiede l'uso del modulo <code>linalg</code>, ed \u00e8 quella che ci permette di effettuare la trasposta di una matrice. </p> <p>Ricordiamo che la trasposta \\(A^T\\) di una matrice \\(A\\) \u00e8 definita come la matrice in cui il generico elemento con indice \\((i,j)\\) \u00e8 l'elemento con indici \\((j,i)\\) della matrice originaria. In pratica:</p> \\[ (A^T)_{ij} = A_{ji} \\forall A \\in \\mathbb{R}^{m,n}, 1 \\leq i \\leq m, 1 \\leq j \\leq n \\] <p>Per farlo, dovremo semplicemente usare la funzione <code>numpy.transpose()</code>.</p> <pre><code>&gt;&gt;&gt; x = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; np.transpose(x)\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra/#matrice-inversa","title":"Matrice inversa","text":"<p>Il calcolo della matrice inversa, invece, prevede l'utilizzo della funzione <code>linalg.inv()</code>, che accetta come parametro la matrice da invertire. </p> <p>Ricordiamo che la matrice inversa \\(A_{inv}\\) di una matrice invertibile \\(A\\) \u00e8 quella matrice per cui vale la seguente relazione:</p> \\[ A_{inv}A=AA_{inv}=I \\] <p>In altri termini, il prodotto matriciale tra \\(A\\) e la sua inversa \u00e8 commutativo e pari alla matrice identit\u00e0 di eguale rango.</p> <p>Ad esempio:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[5, 0, 0], [0, 2, 0], [0, 0, 4]])\n&gt;&gt;&gt; linalg.inv(mat)\narray([[0.2 , 0.  , 0.  ],\n       [0.  , 0.5 , 0.  ],\n       [0.  , 0.  , 0.25]])\n</code></pre> <p>Ovviamente, la matrice <code>mat</code> deve essere invertibile, ovvero quadrata e a rango massimo. Nel caso passassimo una matrice rettangolare, infatti, verrebbe lanciato un errore di tipo <code>LinAlgError</code>:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; linalg.inv(mat)\nTraceback (most recent call last):\nnumpy.linalg.LinAlgError: Last 2 dimensions of the array must be square\n</code></pre> <p>Lo stesso accade se <code>mat</code> \u00e8 singolare:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[1, 1, 1], [2, 2, 2], [0, 0, 1]])\n&gt;&gt;&gt; linalg.inv(mat)\nTraceback (most recent call last):\nnumpy.linalg.LinAlgError: Singular matrix\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra/#prodotti-vettoriali-e-matriciali","title":"Prodotti vettoriali e matriciali","text":""},{"location":"material/02_libs/02_numpy/04_algebra/#la-funzione-dot","title":"La funzione <code>dot()</code>","text":"<p>Nella scorsa lezione abbiamo visto un esempio di utilizzo della funzione <code>dot(a, b)</code> utilizzata per calcolare il prodotto matriciale tra gli array <code>a</code> e <code>b</code>. A questa funzione si applicano tutte le regole del calcolo matriciale, cos\u00ec come riassunto nella seguente tabella.</p> Dimensionalit\u00e0 <code>a</code> Dimensionalit\u00e0 <code>b</code> Risultato Monodimensionale (vettore) Monodimensionale (vettore) Prodotto scalare Bidimensionale (matrice) Bidimensionale (matrice) Prodotto matriciale Scalare \\(n\\)-dimensionale Prodotto scalare per array \\(n\\)-dimensionale \\(n\\)-dimensionale Scalare Prodotto scalare per array \\(n\\)-dimensionale <p>Notiamo che:</p> <ul> <li>in caso di moltiplicazione di due vettori, il risultato sar\u00e0 il prodotto scalare;</li> <li>in caso di moltiplicazione di due matrici, il risultato sar\u00e0 il prodotto matriciale e, di conseguenza, si consiglia di preferire la funzione <code>matmul()</code>;</li> <li>in caso di prodotto tra scalare e matrice, si consiglia di utilizzare la funzione <code>multiply()</code> o, in alternativa, l'operatore <code>*</code>.</li> </ul> <p>Nel notebook della lezione vedremo alcuni esempi di prodotti matriciali.</p> <p>Moltiplicazione di array multidimensionali</p> <p>Nel caso entrambi gli array da moltiplicare siano \\(n\\)-dimensionali, si applicano altre regole, che \u00e8 possibile recuperare a questo indirizzo.</p>"},{"location":"material/02_libs/02_numpy/04_algebra/#prodotto-interno","title":"Prodotto interno","text":"<p>Ricordiamo che per due generici vettori monodimensionali \\(a = [a_{1}, \\ldots, a_{j}], b = [b_{1}, \\ldots, b_{j}]\\), entrambi a \\(j\\) elementi, il prodotto scalare, o prodotto interno, \u00e8 dato da:</p> \\[ p = \\sum_{i=1}^j a_{i} \\cdot b_{i} \\] <p>Per calcolare il prodotto scalare tra i vettori <code>a</code> e <code>b</code> possiamo usare la funzione <code>numpy.inner(a, b)</code>:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, 3])\n&gt;&gt;&gt; b = np.array([4, 5, 6])\n&gt;&gt;&gt; np.inner(a, b)\n32\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra/#le-funzioni-inner-e-dot","title":"Le Funzioni <code>inner()</code> e <code>dot()</code>","text":"<p>Un lettore attento avr\u00e0 notato che, nella pratica, per vettori monodimensionali, le funzioni <code>inner()</code> e <code>dot()</code> restituiscono lo stesso risultato:</p> <pre><code>np.inner(a, b)      # Output: 32\na.dot(b)            # Output: 32, stesso di b.dot()\n</code></pre> <p>La differenza tra le due funzioni \u00e8 visibile quando si utilizzano array a dimensionalit\u00e0 maggiore di 1:</p> <pre><code>&gt;&gt;&gt; a = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; b = np.array([[5, 6], [7, 8]])\n&gt;&gt;&gt; np.inner(a, b)   # Risultato con inner()\narray([[17, 23],\n      [39, 53]])\n&gt;&gt;&gt; a.dot(b)   # Risultato con dot()\narray([[19, 22],\n      [43, 50]])\n</code></pre> <p>In pratica, riprendendo la documentazione:</p> <ul> <li>per quello che riguarda la funzione <code>dot()</code>, questa \u00e8 equivalente a <code>matmul()</code>, e quindi rappresenta una moltiplicazione matriciale che, nel caso di vettori monodimensionali, equivale al prodotto vettoriale, mentre per \\(n\\) dimensioni \u00e8 la somma dei prodotti tra l'ultima dimensione del primo vettore e delle dimensioni che vanno da \\(2\\) ad \\(n\\) del secondo;</li> <li>per quello che riguarda la funzione <code>inner()</code>, rappresenta il prodotto vettoriale nel caso ad una dimensione, mentre nel caso di \\(n\\) dimensioni rappresenta la somma dei prodotti lungo l'ultima dimensione.</li> </ul> <p>In altri termini:</p> <pre><code>a.dot(b) == sum(a[i, :] * b[:, j])\nnp.inner(a, b) == sum(a[i, :] * b[j, :])\n</code></pre> <p>Andando a rapportare il tutto all'esempio precedente:</p> \\[ dot = \\left(\\begin{array}{cc} 1 &amp; 2\\\\ 3 &amp; 4 \\end{array}\\right) \\left(\\begin{array}{cc} 5 &amp; 6\\\\ 7 &amp; 8 \\end{array}\\right) = \\\\ = \\left(\\begin{array}{cc} 1 \\cdot 5 + 2 \\cdot 7 &amp; 1 \\cdot 6 + 2 \\cdot 8   \\\\ 3 \\cdot 5 + 4 \\cdot 7 &amp; 1 \\cdot 6 + 4 \\cdot 8 \\end{array}\\right) = \\left(\\begin{array}{cc} 19 &amp; 22\\\\ 43 &amp; 50 \\end{array}\\right) \\] \\[ inner = \\left(\\begin{array}{cc} 1 &amp; 2\\\\ 3 &amp; 4 \\end{array}\\right) \\left(\\begin{array}{cc} 5 &amp; 6\\\\ 7 &amp; 8 \\end{array}\\right) = \\\\ = \\left(\\begin{array}{cc} 17 &amp; 23\\\\ 39 &amp; 53 \\end{array}\\right) \\]"},{"location":"material/02_libs/02_numpy/04_algebra/#prodotto-esterno","title":"Prodotto esterno","text":"<p>Il prodotto esterno tra due vettori \\(a = [a_1, a_2, \\ldots, a_j]\\) e \\(b = [b_1, b_2, \\ldots, b_j]\\) \u00e8 definito come la matrice \\(P\\) tale per cui:</p> \\[ P = \\left[     \\begin{array}{ccc}         a_1 \\cdot b_1 &amp; \\ldots &amp; a_1 \\cdot b_n \\\\         \\vdots        &amp; \\ddots &amp; \\vdots        \\\\         a_n \\cdot b_1 &amp; \\ldots &amp; a_n \\cdot b_n     \\end{array} \\right] \\] <p>Per calcolarlo, NumPy ci mette a disposizione la funzione <code>numpy.outer(a, b)</code>.</p> <p>Ad esempio:</p> <pre><code>a = np.array([1, 2])\nb = np.array([3, 4])\n\nnp.outer(a, b)\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra/#la-funzione-matmul","title":"La funzione <code>matmul</code>","text":"<p>Quando abbiamo parlato della funzione <code>dot()</code> abbiamo visto come sia possibile usarla per effettuare il prodotto matriciale tra le matrici <code>mat_1</code> e <code>mat_2</code>. Tuttavia, esiste un'altra possibilit\u00e0, che \u00e8 anche quella consigliata, ovvero usare la funzione <code>numpy.matmul()</code>:</p> <pre><code>&gt;&gt;&gt; a = np.array([[1, 2], [3, 4]])\n&gt;&gt;&gt; b = np.array([[5, 6], [7, 8]])\n&gt;&gt;&gt; np.matmul(a, b)\narray([[19, 22],\n       [43, 50]])\n</code></pre> <p>La funzione <code>matmul()</code> ha una differenza fondamentale rispetto alla funzione <code>dot()</code>, in quanto non accetta scalari come parametro (anche se \u00e8 possibile passare vettori ed array \\(n\\)-dimensionali). Esiste in realt\u00e0 un'altra differenza importante, che riguarda le operazioni \\(n\\)-dimensionali, ma che non tratteremo in questa sede.</p> <p>L'operatore <code>@</code></p> <p>L'operatore <code>@</code> \u00e8 delegato alla moltiplicazione diretta di array bidimensionali, ed \u00e8 utilizzato \"sotto al cofano\" dalla funzione <code>matmul()</code>.</p>"},{"location":"material/02_libs/02_numpy/04_algebra/#potenza-di-matrice","title":"Potenza di matrice","text":"<p>La funzione <code>np.linalg.matrix_power(a, n)</code> permette di elevare a potenza <code>n</code> della matrice <code>a</code>. Ad esempio:</p> <pre><code>&gt;&gt;&gt; linalg.matrix_power(a, 5)\narray([[1069, 1558],\n       [2337, 3406]])\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra/#autovalori-ed-autovettori","title":"Autovalori ed autovettori","text":"<p>Per calcolare gli autovalori e gli autovettori di una matrice, NumPy ci mette a disposizione la funzione <code>np.linalg.eig()</code>, che restituisce gli autovalori e gli autovettori destri di una matrice quadrata:</p> <pre><code>&gt;&gt;&gt; (v, w) = linalg.eig(a)\n&gt;&gt;&gt; v\narray([-0.37228132,  5.37228132])\n&gt;&gt;&gt; w\narray([[-0.82456484, -0.41597356],\n       [ 0.56576746, -0.90937671]])\n</code></pre> <p>Esistono anche altre funzioni per il calcolo degli autovalori ed autovettori. In particolare:</p> <ul> <li><code>np.linalg.eigh()</code> calcola gli autovettori e gli autovettori di una matrice simmetrica a valori reali o complessi;</li> <li><code>np.linalg.eigvals()</code> calcola gli autovalori di una matrice quadrata;</li> <li><code>np.linalg.eigvalsh()</code> calcola gli autovalori di una matrice simmetrica a valori reali o complessi.</li> </ul>"},{"location":"material/02_libs/02_numpy/04_algebra/#norma-rango-determinante-e-traccia","title":"Norma, rango, determinante e traccia","text":"<p>La funzione <code>np.linalg.norm(a)</code> ci permette di calcolare la norma di una matrice. Opzionalmente, possiamo specificare tre parametri, ovvero:</p> <ul> <li><code>ord</code>, che rappresenta l'ordine della norma da calcolare (di default, viene calcolata la norma di Frobenius);</li> <li><code>axis</code>, che indica l'asse (o gli assi, in caso di array multidimensionale) su cui operare;</li> <li><code>keepdims</code>, usata per restituire, opzionalmente, l'asse su cui viene calcolata la norma.</li> </ul> <p>Per calcolare la norma di Frobenius della matrice <code>mat</code> possiamo usare questa sintassi:</p> <pre><code>&gt;&gt;&gt; linalg.norm(a)\n5.477225575051661\n</code></pre> <p>Per calcolare determinante, rango e traccia di una matrice mediante le funzioni <code>np.linalg.det()</code>, <code>np.linalg.matrix_rank()</code> e <code>np.trace()</code>. Ad esempio:</p> <pre><code>&gt;&gt;&gt; linalg.det(a)\n-2.0000000000000004\n&gt;&gt;&gt; linalg.matrix_rank(a)\n2\n&gt;&gt;&gt; np.trace(a)\n5\n</code></pre> <p>La <code>trace()</code> pu\u00f2 anche essere usata per calcolare la sommatoria delle sovra/sotto diagonali specificando il parametro <code>offset</code>. Ad esempio:</p> <pre><code>&gt;&gt;&gt; mtrx = np.array([[ 5,  2,  9], [ 2,  3,  1], [ 4, -2, 12]])\n&gt;&gt;&gt; np.trace(mtrx, offset=1)\n3\n&gt;&gt;&gt; np.trace(mtrx, offset=-1)\n0\n</code></pre>"},{"location":"material/02_libs/02_numpy/04_algebra/#risoluzione-di-sistemi-di-equazioni-lineari","title":"Risoluzione di sistemi di equazioni lineari","text":"<p>Chiudiamo questa (necessariamente breve!) carrellata sulle operazioni di algebra lineare con la funzione <code>np.linalg.solve(a, b)</code>, che permette di risolvere un sistema di equazioni lineari nel quale la matrice <code>a</code> \u00e8 la matrice dei coefficienti, mentre il vettore <code>b</code> \u00e8 il vettore dei termini noti. Ad esempio:</p> <pre><code>&gt;&gt;&gt; b = np.array([3, 2, 3])\n&gt;&gt;&gt; linalg.solve(mat, b)\narray([-7.5,  4.5,  3.5])\n</code></pre> <p>Ovviamente, la matrice <code>a</code> deve essere quadrata, mentre il vettore <code>b</code> deve avere esattamente <code>n</code> elementi, con <code>n</code> ordine di <code>a</code>!</p> <p>Operazioni su tensori</p> <p>NumPy mette a disposizione la versione per tensori ad \\(N\\) dimensioni di alcune funzioni. In particolare, ricordiamo la funzione per il prodotto tensoriale (<code>np.tensordot()</code>) e quella per risolvere equazioni tensoriali (<code>np.linalg.tensorsolve()</code>).</p>"},{"location":"material/02_libs/02_numpy/04_algebra/#reference","title":"Reference","text":"<p>Come detto pi\u00f9 volte, la nostra carrellata \u00e8 stata inevitabilmente molto breve e limitata. Per una panoramica pi\u00f9 completa, il consiglio \u00e8 sempre quello di riferirsi all'ottima documentazione di NumPy.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials.en/","title":"2.2.5 - Polynomial Operations in NumPy","text":"<p>Accompanying Notebook</p> <p>For this lesson, there is an accompanying notebook available here.</p> <p>As we saw in the previous lesson, NumPy offers a wide range of functions for matrix calculations. However, it can also be used for other purposes, including polynomial calculations, through the <code>numpy.polynomial</code> module. Let's see some of the main uses of this module.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials.en/#the-polynomial-class","title":"The <code>Polynomial</code> Class","text":"<p>Let's imagine we have two different polynomials, which are not associated with any physical meaning. The two polynomials are expressed by the following equations:</p> \\[ \\begin{cases} p_1: y = 2x + 1 \\\\ p_2: y = x^2 + 3x + 2 \\end{cases} \\] <p>NumPy allows us to represent a polynomial using objects of the <code>Polynomial</code> class. In particular, an object of this type can be instantiated from the coefficients of the polynomial (but not only that). For example, considering the polynomials mentioned above and their coefficients, we can write:</p> <pre><code>&gt;&gt;&gt; from numpy.polynomial import Polynomial\n&gt;&gt;&gt; p_1_coef = [2, 1]       # coefficient list for p_1\n&gt;&gt;&gt; p_2_coef = [1, 3, 2]    # coefficient list for p_2\n&gt;&gt;&gt; p_1 = Polynomial(p_1_coef[::-1])\n&gt;&gt;&gt; p_2 = Polynomial(p_2_coef[::-1])\n</code></pre> <p>Order of Coefficients</p> <p>The most important (and counterintuitive) feature of the <code>Polynomial</code> class (and all polynomial handling methods in NumPy) is that the coefficients are treated in ascending order. In practice, the constant term (the term for \\(x^0\\)) is considered first, followed by the coefficient of the first degree, the second degree, and so on. For this reason, in the above code, the coefficient list is reversed.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials.en/#polynomial-addition","title":"Polynomial Addition","text":"<p>To add two polynomials, you can use the <code>polyadd()</code> method, which takes two vectors <code>c1</code> and <code>c2</code> representing the coefficients of the two polynomials to be added. For example, to add two polynomials, you can write:</p> <pre><code>&gt;&gt;&gt; from numpy.polynomial import polynomial as P\n&gt;&gt;&gt; c1 = [1, 2, 3]\n&gt;&gt;&gt; c2 = [2, 5, 1]\n&gt;&gt;&gt; poly_sum = P.polyadd(c1, c2)\n</code></pre> <p>The result of this operation will be a numpy array with values <code>[3, 7, 4]</code>, equivalent to the polynomial \\(4x^2 + 7x + 3\\).</p>"},{"location":"material/02_libs/02_numpy/05_polynomials.en/#polynomial-subtraction","title":"Polynomial Subtraction","text":"<p>Subtracting two polynomials is possible using the <code>polysub()</code> function, which works similarly to <code>polyadd()</code>, but the result will be the polynomial resulting from subtracting the coefficients of <code>c2</code> from <code>c1</code>.</p> <pre><code>&gt;&gt;&gt; poly_sub\n\n = P.polysub(c1, c2)\n</code></pre>"},{"location":"material/02_libs/02_numpy/05_polynomials.en/#polynomial-multiplication","title":"Polynomial Multiplication","text":"<p>The above considerations can be extended to polynomial multiplication using the <code>polymul()</code> function:</p> <pre><code>&gt;&gt;&gt; poly_mul = P.polymul(c1, c2)\n</code></pre> <p>If you need to multiply a polynomial by an independent variable \\(x\\), you can use the <code>polymulx()</code> function.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials.en/#polynomial-division","title":"Polynomial Division","text":"<p>Division between polynomials is a slightly more complex operation than the others, and it requires the use of the <code>polydiv()</code> function, which will return two arrays this time: the first represents the coefficients of the quotient polynomial, and the second represents the coefficients of the remainder polynomial. For example:</p> <pre><code>&gt;&gt;&gt; poly_q, poly_r = P.polydiv(c1, c2)\n</code></pre> <p>Again, the coefficients are ordered from the lowest degree to the highest degree.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials.en/#polynomial-power","title":"Polynomial Power","text":"<p>To raise a polynomial to a power, you can use the <code>polypow()</code> function, which accepts two parameters: a coefficient array <code>c</code> as the first argument, and a scalar <code>pow</code> as the second argument, representing the power to which the polynomial should be raised. For example:</p> <pre><code>&gt;&gt;&gt; poly_pow = P.polypow(c1, 2)\narray([0., 0., 4., 4., 1.])\n</code></pre>"},{"location":"material/02_libs/02_numpy/05_polynomials.en/#value-of-a-polynomial","title":"Value of a Polynomial","text":"<p>The value \\(y\\) assumed by a polynomial \\(p\\) at a certain value of \\(x\\) can be determined using the <code>polyval()</code> function, which takes an integer (or a list of integers) <code>x</code> and a polynomial <code>p</code> as arguments. For example, to evaluate the value of \\(y\\) for \\(x \\in [1, 2]\\) on the line represented by the polynomial <code>c1</code>:</p> <pre><code>&gt;&gt;&gt; P.polyval([1, 2], c1.coef)\n</code></pre> <p>Coefficients and Polynomial</p> <p>It is important to note that we are not using an object of the <code>Polynomial</code> class, but rather the coefficients of the polynomial, which can be accessed through the <code>coef</code> property.</p> <p>Operations with Dimensionality 2 and 3</p> <p>In case you need to evaluate polynomials in two and three dimensions, NumPy provides the functions <code>polyval2d()</code> and <code>polyval3d()</code>.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials.en/#derivative-and-integral-of-polynomial-functions","title":"Derivative and Integral of Polynomial Functions","text":"<p>We conclude this brief overview with two methods capable of calculating the derivative and integral of a polynomial function.</p> <p>The <code>numpy.polyder()</code> function allows you to calculate the <code>m</code>th derivative (specified as the second argument, defaulting to <code>1</code>) of the coefficients of the polynomial <code>p</code> (specified as the first argument). For example, to calculate the derivative of <code>c1</code>:</p> <pre><code>&gt;&gt;&gt; P.polyder(c1.coef)\n</code></pre> <p>The dual method is <code>numpy.polyint()</code>, which computes the <code>m</code>th integral of the coefficients of the polynomial <code>p</code>:</p> <pre><code>&gt;&gt;&gt; P.polyint(c1.coef)\n</code></pre>"},{"location":"material/02_libs/02_numpy/05_polynomials/","title":"2.2.5 - Operazioni polinomiali in NumPy","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Come abbiamo visto nella scorsa lezione, NumPy ci offre un'ampia gamma di funzioni per il calcolo matriciale. Tuttavia, \u00e8 anche possibile utilizzarlo per altri scopi, non ultimo il calcolo polinomiale, mediante il modulo <code>numpy.polynomial</code>. Vediamo quindi alcuni tra i principali utilizzi di questo modulo.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials/#la-classe-polynomial","title":"La classe <code>Polynomial</code>","text":"<p>Immaginiamo di avere due diversi polinomi, cui non associamo alcun significato fisico. I due sono espressi dalle seguenti equazioni:</p> \\[ \\begin{cases} p_1: y = 2x + 1 \\\\ p_2: y = x^2 + 3x + 2 \\end{cases} \\] <p>NumPy ci permette di rappresentare il polinomio mediante gli oggetti di classe <code>Polynomial</code>. In particolare, un oggetto di questo tipo pu\u00f2 essere istanziato a partire dai coefficienti del polinomio (ma non solo). Ad esempio, considerando i polinomi visti in precedenza, assieme ai loro coefficienti, possiamo scrivere:</p> <pre><code>&gt;&gt;&gt; from numpy.polynomial import Polynomial\n&gt;&gt;&gt; p_1_coef = [2, 1]       # lista dei coefficienti per p_1\n&gt;&gt;&gt; p_2_coef = [1, 3, 2]    # lista dei coefficienti per p_2\n&gt;&gt;&gt; p_1 = Polynomial(p_1_coef[::-1])\n&gt;&gt;&gt; p_2 = Polynomial(p_2_coef[::-1])\n</code></pre> <p>Ordine dei coefficienti</p> <p>La caratteristica pi\u00f9 importante (e controintuitiva) della classe <code>Polynomial</code> (e di tutti i metodi di gestione dei polinomi in NumPy) \u00e8 che i coefficienti vengono trattati in ordine crescente. In pratica, viene considerato innanzitutto il termine noto (ovvero il termine per \\(x^0\\)), poi il coefficiente di primo grado, quello di secondo, e cos\u00ec via. Per questo motivo, nel codice precedente viene considerata la lista dei coefficienti in ordine inverso.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials/#somma-di-polinomi","title":"Somma di polinomi","text":"<p>Per sommare due polinomi, \u00e8 possibile utilizzare il metodo <code>polyadd()</code>, il quale accetta come parametri due vettori <code>c1</code> e <code>c2</code> rappresentativi dei coefficienti dei due polinomi da sommare. Ad esempio, volendo sommare due polinomi, potremo scrivere:</p> <pre><code>&gt;&gt;&gt; from numpy.polynomial import polynomial as P\n&gt;&gt;&gt; c1 = [1, 2, 3]\n&gt;&gt;&gt; c2 = [2, 5, 1]\n&gt;&gt;&gt; poly_sum = P.polyadd(c1, c2)\n</code></pre> <p>Il risultato di questa operazione sar\u00e0 un array numpy a valori <code>[3, 7, 4]</code>, equivalente al polinomio \\(4x^2 + 7x + 3\\).</p>"},{"location":"material/02_libs/02_numpy/05_polynomials/#sottrazione-di-polinomi","title":"Sottrazione di polinomi","text":"<p>La sottrazione tra due polinomi \u00e8 possibile mediante la funzione <code>polysub()</code>, il cui funzionamento \u00e8 molto simile a quello di <code>polyadd()</code>, con l'ovvia differenza che il risultato sar\u00e0 il polinomio risultante dalla differenza tra i coefficienti di <code>c1</code> e <code>c2</code>.</p> <pre><code>&gt;&gt;&gt; poly_sub = P.polysub(c1, c2)\n</code></pre>"},{"location":"material/02_libs/02_numpy/05_polynomials/#moltiplicazione-di-polinomi","title":"Moltiplicazione di polinomi","text":"<p>Le considerazioni precedenti possono essere estese alla moltiplicazione tra polinomi mediante la funzione <code>polymul()</code>:</p> <pre><code>&gt;&gt;&gt; poly_mul = P.polymul(c1, c2)\n</code></pre> <p>Nel caso si debba moltiplicare un polinomio per una variabile indipendente \\(x\\), andr\u00e0 utilizzata la funzione <code>polymulx()</code>.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials/#divisione-tra-polinomi","title":"Divisione tra polinomi","text":"<p>La divisione tra polinomi \u00e8 un'operazione leggermente pi\u00f9 complessa delle altre, e prevede l'uso della funzione <code>polydiv()</code>, che restituir\u00e0 stavolta due array: il primo rappresenta i coefficienti del polinomio quoziente, mentre il secondo i coefficienti del polinomio resto. Ad esempio:</p> <pre><code>&gt;&gt;&gt; poly_q, poly_r = P.polydiv(c1, c2)\n</code></pre> <p>Anche in questo caso, i coefficienti sono ordinati da quello a grado pi\u00f9 basso a quello a grado pi\u00f9 alto.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials/#elevazione-a-potenza","title":"Elevazione a potenza","text":"<p>Per elevare a potenza un polinomio, possiamo usare la funzione <code>polypow()</code>, la quale accetta due parametri: un vettore dei coefficienti <code>c</code> al primo argomento, ed uno scalare <code>pow</code> al secondo, rappresentativo della potenza alla quale effettuare l'elevazione. Ad esempio:</p> <pre><code>&gt;&gt;&gt; poly_pow = P.polypow(c1, 2)\narray([0., 0., 4., 4., 1.])\n</code></pre>"},{"location":"material/02_libs/02_numpy/05_polynomials/#valore-assunto-da-un-polinomio","title":"Valore assunto da un polinomio","text":"<p>Il valore \\(y\\) assunto da un polinomio \\(p\\) ad un certo valore di \\(x\\) pu\u00f2 essere determinato mediante la funzione <code>polyval()</code>, che accetta come argomento un intero (o una lista di interi) <code>x</code> ed un polinomio <code>p</code>. Ad esempio, volendo valutare il valore assunto da \\(y\\) per \\(x \\in [1, 2]\\) sulla retta rappresentata dal polinomio <code>c1</code>:</p> <pre><code>&gt;&gt;&gt; P.polyval([1, 2], c1.coef)\n</code></pre> <p>Coefficienti e polinomio</p> <p>E' molto importante notare come non stiamo usando un oggetto di classe <code>Polynomial</code>, ma i coefficienti dello stesso, estraibili accedendo alla propriet\u00e0 <code>coef</code>.</p> <p>Operazioni a dimensionalit\u00e0 2 e 3</p> <p>Nel caso occorra valutare i polinomi nei casi a due e tre dimensioni, NumPy ci mette a disposizione le funzioni <code>polyval2d()</code> e <code>polyval3d()</code>.</p>"},{"location":"material/02_libs/02_numpy/05_polynomials/#derivata-ed-integrale-di-funzioni-polinomiali","title":"Derivata ed integrale di funzioni polinomiali","text":"<p>Concludiamo questa breve carrellata con due metodi in grado di calcolare, rispettivamente, la derivata e l'integrale di una funzione polinomiale.</p> <p>La funzione <code>numpy.polyder()</code>, infatti, permette di calcolare la derivata di ordine <code>m</code> (passato come secondo argomento, di default pari ad <code>1</code>) dei coefficienti del polinomio <code>p</code> (passati come primo argomento). Ad esempio, per calcolare la derivata di <code>c1</code>:</p> <pre><code>&gt;&gt;&gt; P.polyder(c1.coef)\n</code></pre> <p>Il metodo duale \u00e8 <code>numpy.polyint()</code>, che (prevedibilmente) calcola l'integrale di ordine <code>m</code> dei coefficienti del polinomio <code>p</code>:</p> <pre><code>&gt;&gt;&gt; P.polyint(c1.coef)\n</code></pre>"},{"location":"material/02_libs/02_numpy/06_statistics.en/","title":"2.2.6 - Statistics in NumPy","text":"<p>Accompanying Notebook</p> <p>For this lesson, there is an accompanying notebook available at this address.</p> <p>In addition to polynomials and algebraic operations, NumPy provides various useful functions for statistical calculations. In this lesson, we will see some of them.</p>"},{"location":"material/02_libs/02_numpy/06_statistics.en/#minimum-and-maximum-of-an-array","title":"Minimum and maximum of an array","text":"<p>First, let's look at two useful functions to determine the maximum and minimum values of an array, specifically <code>numpy.amax()</code> and <code>numpy.amin()</code>, which allow us to find the maximum along a specific direction of an array.</p> <p>For example, if we wanted to find the minimum and maximum of a randomly generated vector using the <code>default_rng()</code> function:</p> <pre><code>&gt;&gt;&gt; rng = np.random.default_rng(42)\n&gt;&gt;&gt; a = rng.integers(low=0, high=10, size=5)\n&gt;&gt;&gt; np.amin(a)\n&gt;&gt;&gt; np.amax(a)\n</code></pre> <p>For a matrix, and generally for any \\(N\\)-dimensional array, the procedure is similar:</p> <pre><code>&gt;&gt;&gt; b = rng.integers(low=0, high=10, size=(3, 3))\n&gt;&gt;&gt; np.amin(b)\n&gt;&gt;&gt; np.amax(b)\n</code></pre> <p>If we wanted to find the maximum and minimum along the columns of <code>b</code>, we would need to specify the <code>axis</code> parameter, which would be set to <code>0</code>:</p> <pre><code>&gt;&gt;&gt; np.amin(b, axis=0)\n&gt;&gt;&gt; np.amax(b, axis=0)\n</code></pre> <p>Similarly, to find the minimum and maximum along the rows, we would change the value of <code>axis</code> to <code>1</code>:</p> <pre><code>&gt;&gt;&gt; np.amin(b, axis=1)\n&gt;&gt;&gt; np.amax(b, axis=1)\n</code></pre> <p>We can also specify a tuple for the value of the <code>axis</code> parameter; in this case, the search for the maximum or minimum will be performed along all the axes specified by the tuple. For example, by specifying <code>(0, 1)</code>, we will search for the minimum (or maximum) element in the matrix:</p> <pre><code>&gt;&gt;&gt; np.amin(b, axis=(0, 1))\n&gt;&gt;&gt; np.amax(b, axis=(0, 1))\n</code></pre> <p>The <code>argmax</code> and <code>argmin</code> functions</p> <p>The [<code>numpy.argmax()</code>] and [<code>numpy.argmin()</code>] functions allow us to find the index of the maximum and minimum values, respectively.</p>"},{"location":"material/02_libs/02_numpy/06_statistics.en/#percentile-quantile-and-quartiles","title":"Percentile, Quantile, and Quartiles","text":"<p>The term percentile refers to a value that indicates the position of a data point within a distribution, or in other words, the percentage of data points that are below a certain value within the distribution itself.</p> <p>To calculate the percentile for a given value \\(p\\), we need to follow these steps:</p> <ol> <li>Sort the \\(n\\) values of the distribution in ascending order.</li> <li>If \\(p\\) is the percentile, calculate the product \\(k = n \\cdot p \\cdot \\frac{1}{100}\\).</li> <li>If \\(k\\) is an integer, the percentile is the average of the \\(k\\)-th and \\((k+1)\\)-th values of the sorted data.</li> <li>If \\(k\\) is not an integer, round \\(k\\) up to the nearest integer, and choose the corresponding value from the sorted data.</li> </ol> <p>Let's consider an example. Suppose we have the following distribution of values:</p> \\[ D = [20, 30, 10, 50, 40, 100, 90, 60, 80, 70] \\] <p>We want to calculate the \\(30\\)th percentile, so \\(p=30\\). Let's follow the steps described above.</p> <ol> <li>We sort the data, obtaining the vector \\(D_{sorted} = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\\).</li> <li>The value of \\(k\\) is \\(n \\cdot p\\), where \\(n=10\\) and \\(p=0.30\\), so \\(k=3\\).</li> <li>Since \\(k\\) is an integer, the \\(30\\)th percentile is the average of \\(30\\) and \\(40\\), which is \\(35\\).</li> </ol> <p>The concept of quantile is related to that of percentile, but it generalizes it in some way. Specifically, when we talk about quantiles, we define a partition of the value distribution into a certain number of intervals. It is clear that if we have \\(100\\) intervals, the quantile coincides with the percentile.</p> <p>A notable partition is the one that divides the value distribution into four parts, called quartiles, as follows:</p> <ul> <li>The first quartile collects all elements below the \\(25\\)th percentile (denoted as \\(Q_1\\)).</li> <li>The second quartile collects elements between the \\(25\\)th and \\(50\\)th percentiles (denoted as \\(Q_2\\)).</li> <li>The third quartile contains elements between the \\(50\\)th and \\(75\\)th percentiles (denoted as \\(Q_3\\)).</li> <li>The last quartile includes elements beyond \\(Q_3\\).</li> </ul> <p>In other words, if \\(n\\) is the total number of data points in the distribution, then:</p> \\[ \\begin{align} &amp; Q_1 = \\frac{n + 1}{4}\\\\ &amp; Q_2 = \\frac{n + 1}{2}\\\\ &amp; Q_3 = \\frac{n + 1}{4} \\cdot 3 \\end{align} \\] <p>Quartiles are particularly important when characterizing a non-parametric distribution, which cannot be represented in terms of simple statistical parameters such as a normal distribution. In these cases, the distribution can be characterized non-parametrically by the median (the \\(50\\)th percentile), the standard deviation, and the interquartile range, which is given by \\(Q3-Q1\\).</p>"},{"location":"material/02_libs/02_numpy/06_statistics.en/#the-numpypercentile-and-numpyquantile-functions","title":"The <code>numpy.percentile()</code> and <code>numpy.quantile()</code> functions","text":"<p>For example, let's consider an ordered vector of elements from \\(1\\) to $10</p> <p>$ and calculate the \\(50\\)th percentile using the <code>numpy.percentile()</code> function in NumPy.</p> <pre><code>&gt;&gt;&gt; a = np.arange(1, 11)\n&gt;&gt;&gt; np.percentile(a, 50)\n</code></pre> <p>There are different ways to calculate the q-percentile, so it is advisable to consult the reference and the article Sample quantiles in statistical packages by Hyndman, R. J., &amp; Fan, Y.</p> <p>In reality, the <code>percentile()</code> function uses the calculation of the median for the \\(50\\)th percentile, so it is equivalent to the <code>median()</code> function. In this specific case, there may be a deviation from the expected result due to interpolation errors introduced by NumPy:</p> <pre><code>np.percentile(a, 50)\n</code></pre> <p>The concept of quantile is similar to that of percentile, but in this case, we are not dealing with percentage values, but with normalized values between \\(0\\) and \\(1\\). Therefore, if we use the <code>numpy.quantile()</code> function as before:</p> <pre><code>&gt;&gt;&gt; np.quantile(a, .5)\n</code></pre> <p>Both the <code>percentile()</code> and <code>quantile()</code> functions have an optional parameter <code>axis</code>. For example:</p> <pre><code>&gt;&gt;&gt; np.percentile(b, 50, axis=0)\n&gt;&gt;&gt; np.percentile(b, 50, axis=1)\n</code></pre> <p>As expected, setting the <code>axis</code> parameter to <code>0</code> will calculate the percentile for each column, while using <code>1</code> will calculate the percentile for each row.</p>"},{"location":"material/02_libs/02_numpy/06_statistics.en/#arithmetic-mean-and-weighted-mean","title":"Arithmetic Mean and Weighted Mean","text":"<p>For calculating the average value of a NumPy array, two methods are available. The first method is the <code>numpy.average(a, weights)</code> function, which is used to calculate a weighted average of the elements in <code>a</code>, weighted by the elements in <code>weights</code> (assuming the dimensions of the two arrays are consistent).</p> <p>The calculation performed by NumPy with the <code>average()</code> function is as follows:</p> <pre><code>&gt;&gt;&gt; avg = sum(a * weights) / sum(weights)\n</code></pre> <p>For example, if we want to assign a higher weight to the first and fourth elements of a randomly generated array <code>a</code>, we can do the following:</p> <pre><code>&gt;&gt;&gt; w = np.array([3, 1, 1, 3, 1])\n&gt;&gt;&gt; np.average(a, weights=w)\n3.2222222222222223\n</code></pre> <p>The result slightly deviates from the simple arithmetic mean calculated as:</p> <pre><code>&gt;&gt;&gt; np.average(a)\n4.2\n</code></pre> <p>Keep in mind that the mean is weighted by the sum of the weight values!</p> <p>The <code>numpy.mean(a)</code> function, on the other hand, represents the arithmetic mean of the elements in an array and is equivalent to the <code>average(a)</code> function without specifying the weights vector. For example:</p> <pre><code>&gt;&gt;&gt; np.mean(a)\n4.2\n</code></pre> <p>We can also specify the <code>axis</code> parameter in this case:</p> <pre><code>&gt;&gt;&gt; np.mean(b, axis=0)\narray([6.33333333, 2.33333333, 6.        ])\n&gt;&gt;&gt; np.mean(b, axis=1)\narray([4.66666667, 2.33333333, 7.66666667])\n</code></pre>"},{"location":"material/02_libs/02_numpy/06_statistics.en/#variance-and-standard-deviation","title":"Variance and Standard Deviation","text":"<p>The functions <code>numpy.std(a)</code> and <code>numpy.var(a)</code> are used to calculate the standard deviation and variance of a vector:</p> <pre><code>&gt;&gt;&gt; np.std(a)\n2.4\n&gt;&gt;&gt; np.var(a)\n5.76\n</code></pre> <p>Similarly, we can specify the axes along which to perform the desired operation:</p> <pre><code>&gt;&gt;&gt; np.var(b, axis=0)\narray([ 9.55555556, 10.88888889,  0.66666667])\n&gt;&gt;&gt; np.var(b, axis=1)\narray([11.55555556,  4.22222222,  0.88888889])\n</code></pre>"},{"location":"material/02_libs/02_numpy/06_statistics.en/#histogram","title":"Histogram","text":"<p>An histogram provides a graphical representation of the values contained in a vector by grouping them into a certain number of partitions (bins).</p> <p>For example, a possible representation with two bins for the vector \\(A = [1, 2, 3, 4]\\) would be the vector \\([2, 2]\\). This is because the two bins divide the range of values taken by \\(A\\) into two parts, with the first bin including the elements \\(1\\) and \\(2\\), and the second bin including the elements \\(3\\) and \\(4\\). Once the bins are calculated, they are \"filled\" by counting the number of elements in each bin, resulting in the vector \\([2, 2]\\).</p> <p>It is possible to specify not only the number of bins but also their limits, which may not coincide with the limits of the vector.</p> <p>NumPy allows us to obtain the histogram of a vector using the set of functions [<code>numpy.histogram(a, bins, range)</code>](https://numpy</p> <p>.org/doc/stable/reference/generated/numpy.histogram.html), which calculates the (one-dimensional) histogram of the array <code>a</code> based on the number of bins (optional) and the range (optional). For example:</p> <pre><code>&gt;&gt;&gt; h, b = np.histogram(a)\n</code></pre> <p>In this case, we have used the default value of <code>bins</code>, which is 10.</p> <p>Note that the <code>histogram()</code> function returns two values: the first represents the values taken by the histogram (i.e., the number of elements falling into each bin), and the second represents the bin limits.</p>"},{"location":"material/02_libs/02_numpy/06_statistics/","title":"2.2.6 - Statistica in NumPy","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Oltre ai polinomi ed alle operazioni algebriche, NumPy ci mette a disposizione varie funzioni utili per i calcoli statistici; in questa lezione ne vedremo alcune.</p>"},{"location":"material/02_libs/02_numpy/06_statistics/#minimo-e-massimo-di-un-array","title":"Minimo e massimo di un array","text":"<p>Per prima cosa, vediamo due funzioni utili a deterinare il valore massimo e minimo per un array, in particolare <code>numpy.amax()</code> e <code>numpy.amin()</code>, che permettono di individuare il massimo lungo una particolare direzione di un array.</p> <p>Ad esempio, se volessimo trovare il minimo ed il massimo di un vettore generato casualmente usando la funzione <code>default_rng()</code>:</p> <pre><code>&gt;&gt;&gt; rng = np.random.default_rng(42)\n&gt;&gt;&gt; a = rng.integers(low=0, high=10, size=5)\n&gt;&gt;&gt; np.amin(a)\n&gt;&gt;&gt; np.amax(a)\n</code></pre> <p>Per una matrice, ed in generale per ogni array \\(N\\)-dimensionale, il procedimento da seguire \u00e8 analogo:</p> <pre><code>&gt;&gt;&gt; b = rng.integers(low=0, high=10, size=(3, 3))\n&gt;&gt;&gt; np.amin(b)\n&gt;&gt;&gt; np.amax(b)\n</code></pre> <p>Se volessimo individuare il massimo ed il minimo sulle colonne di <code>b</code>, dovremmo specificare il parametro <code>axis</code>, che assumer\u00e0 valore pari a <code>0</code>:</p> <pre><code>&gt;&gt;&gt; np.amin(b, axis=0)\n&gt;&gt;&gt; np.amax(b, axis=0)\n</code></pre> <p>Ovviamente, per trovare il minimo ed il massimo per riga, dovremo cambiare il valore di <code>axis</code> in <code>1</code>:</p> <pre><code>&gt;&gt;&gt; np.amin(b, axis=1)\n&gt;&gt;&gt; np.amax(b, axis=1)\n</code></pre> <p>Possiamo anche specificare una tupla per il valore del parametro <code>axis</code>; in tal caso, la ricerca del massimo o del minimo avverr\u00e0 lungo tutti gli assi specificati dalla tupla. Ad esempio, specificando <code>(0, 1)</code>, effettueremo la ricerca del minimo (o del massimo) elemento nella matrice:</p> <pre><code>&gt;&gt;&gt; np.amin(b, axis=(0, 1))\n&gt;&gt;&gt; np.amax(b, axis=(0, 1))\n</code></pre> <p>Le funzioni <code>argmax</code> ed <code>argmin</code></p> <p>Le funzioni [<code>numpy.argmax()</code>] e [<code>numpy.argmin()</code>] permettono di individuare l'indice del valore massimo e minimo, rispettivamente.</p>"},{"location":"material/02_libs/02_numpy/06_statistics/#percentile-quantile-e-quartili","title":"Percentile, quantile, e quartili","text":"<p>Con il termine percentile definiamo un valore che indica la posizione di un dato all'interno di una distribuzione o, in altre parole, la percentuale di dati che si trovano al di sotto di un determinato valore all'interno della distribuzione stessa. </p> <p>Per calcolare il percentile per un dato valore \\(p\\), dovremo seguire questi passaggi:</p> <ol> <li>Ordinare gli \\(n\\) valori della distribuzione in ordine crescente.</li> <li>Se \\(p\\) \u00e8 il percentile, calcolare il prodotto \\(k = n \\cdot p \\cdot \\frac{1}{100}\\).</li> <li>Se \\(k\\) \u00e8 un intero, il percentile \u00e8 dato dalla media tra il \\(k\\)-esimo ed il \\(k+1\\)-esimo valore dei dati ordinati.</li> <li>Se \\(k\\) non \u00e8 un intero, si arrotonda \\(k\\) per eccesso al primo intero successivo, scegliendo il corrispondente valore dei dati ordinati.</li> </ol> <p>Facciamo un esempio. Supponiamo di avere la seguente distribuzione di valori:</p> \\[ D = [20, 30, 10, 50, 40, 100, 90, 60, 80, 70] \\] <p>Vogliamo calcolare il \\(30\\)-percentile, quindi \\(p=30\\). Proviamo a seguire i passaggi sopra descritti.</p> <ol> <li>Ordiniamo i dati, ottenendo il vettore \\(D_{ord} = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\\).</li> <li>Il valore di \\(k\\) \u00e8 pari ad \\(n * p\\), con \\(n=10\\) e \\(p=0.30\\), per cui \\(k=3\\).</li> <li>Dato che \\(k\\) \u00e8 un intero, il \\(30\\)-percentile \u00e8 dato dalla media tra \\(30\\) e \\(40\\), ovvero \\(35\\).</li> </ol> <p>Il concetto di quantile \u00e8 collegato a quello di percentile, che in qualche modo generalizza. In particolare, quando si parla di quantile, si definisce una suddivisione della distribuzione dei valori in un certo numero di intervalli: appare chiaro che se questi intervalli sono \\(100\\) il quantile coincide con il percentile.</p> <p>Una suddivisione notevole \u00e8 quella che prevede la ripartizione della distribuzione dei valori in quattro parti, dette quartili, ripartiti come segue:</p> <ul> <li>il primo quartile \u00e8 quello che raccoglie tutti gli elementi che vanno al di sotto del \\(25\\)-percentile (indicato con \\(Q_1\\));</li> <li>il secondo quartile \u00e8 quello che raccoglie gli elementi che vanno dal \\(25\\)-percentile al \\(50\\)-percentile (indicato con \\(Q_2\\));</li> <li>il terzo quartile \u00e8 quello che contiene gli elementi che vanno dal \\(50\\)-percentile al \\(75\\)-percentile (indicato con \\(Q_3\\));</li> <li>l'ultimo quartile racchidue gli elementi che sono oltre \\(Q_3\\).</li> </ul> <p>In altri termini, se \\(n\\) \u00e8 il numero totale di dati nella distribuzione, allora:</p> \\[ \\begin{align} &amp; Q_1 = \\frac{n + 1}{4}\\\\ &amp; Q_2 = \\frac{n + 1}{2}\\\\ &amp; Q_3 = \\frac{n + 1}{4} \\cdot 3 \\end{align} \\] <p>I quartili sono importanti soprattutto quando si va a caratterizzate una distribuzione non parametrica, ovvero non rappresentabile in termini di semplici parametri statistici come, ad esempio, una distribuzione normale. In questi casi, infatti, la distribuzione pu\u00f2 essere caratterizzata in maniera non parametrica mediante la mediana (ovvero, il \\(50\\)-percentile), la deviazione standard, ed il range interquartile, dato da \\(Q3-Q1\\).</p>"},{"location":"material/02_libs/02_numpy/06_statistics/#le-funzioni-numpypercentile-e-numpyquantile","title":"Le funzioni <code>numpy.percentile()</code> e <code>numpy.quantile()</code>","text":"<p>Per fare un esempio, supponiamo di avere un vettore ordinato di elementi che vanno da \\(1\\) a \\(10\\), e di calcolare il \\(50\\)-percentile mediante la funzione <code>numpy.percentile()</code> di NumPy.</p> <pre><code>&gt;&gt;&gt; a = np.arange(1, 11)\n&gt;&gt;&gt; np.percentile(a, 50)\n</code></pre> <p>Esistono diversi modi di calcolare il q-percentile; in tal senso, \u00e8 opportuno consultare la reference e l'articolo Sample quantiles in statistical packages di Hyndman, R. J., &amp; Fan, Y.</p> <p>In realt\u00e0, la funzione <code>percentile()</code> usa, per il \\(50\\)-percentile, il calcolo della mediana, per cui \u00e8 equivalente alla funzione <code>median()</code>. In questo caso specifico, avremo un discostamento dal risultato atteso, dovuto ad errori di interpolazione introdotti da NumPy:</p> <pre><code>np.percentile(a, 50)\n</code></pre> <p>Il concetto di quantile \u00e8 analogo a quello di percentile; tuttavia, in questo caso, non abbiamo a che fare con valori percentuali, bens\u00ec con valori normalizzati tra \\(0\\) e \\(1\\). Per cui, se usassimo la funzione <code>numpy.quantile()</code> come in precedenza:</p> <pre><code>&gt;&gt;&gt; np.quantile(a, .5)\n</code></pre> <p>Anche le funzioni <code>percentile()</code> e <code>quantile()</code> prevedono come argomento opzionale il parametro <code>axis</code>. Ad esempio:</p> <pre><code>&gt;&gt;&gt; np.percentile(b, 50, axis=0)\n&gt;&gt;&gt; np.percentile(b, 50, axis=1)\n</code></pre> <p>Come previsto, dando il valore <code>0</code> al parametro <code>axis</code> avremo il calcolo del percentile su ciascuna colonna, mentre passando il valore <code>1</code> avremo il calcolo del percentile su ciascuna riga.</p>"},{"location":"material/02_libs/02_numpy/06_statistics/#media-aritmetica-e-media-pesata","title":"Media aritmetica e media pesata","text":"<p>Per il calcolo del valore medio di un array NumPy ci mette a disposizione due metodi. Il primo \u00e8 la funzione <code>numpy.average(a, weights)</code>, che viene usata per calcolare una media pesata degli elementi di <code>a</code> ponderati per gli elementi di <code>weights</code> (a patto che, ovviamente, le dimensioni dei due array siano coerenti).</p> <p>Il calcolo che viene effettuato da NumPy con la funzione <code>average()</code> \u00e8 quindi il seguente:</p> <pre><code>&gt;&gt;&gt; avg = sum(a * weights) / sum(weights)\n</code></pre> <p>Per cui, se volessimo assegnare un peso maggiore al primo ed al quarto elemento di un array <code>a</code> generato casualmente, potremmo fare come segue:</p> <pre><code>&gt;&gt;&gt; w = np.array([3, 1, 1, 3, 1])\n&gt;&gt;&gt; np.average(a, weights=w)\n3.2222222222222223\n</code></pre> <p>Il risultato si discosta leggermente dalla semplice media, calcolata come:</p> <pre><code>&gt;&gt;&gt; np.average(a)\n4.2\n</code></pre> <p>Suggerimento</p> <p>Teniamo sempre a mente che la media \u00e8 ponderata per la sommatoria dei valori assunti dai pesi!</p> <p>La funzione <code>numpy.mean(a)</code> \u00e8 invece rappresentativa della media aritmetica degli elementi di un array, ed equivale alla funzione <code>average(a)</code> senza la specifica del vettore dei pesi. Ad esempio:</p> <pre><code>&gt;&gt;&gt; np.mean(a)\n4.2\n</code></pre> <p>Concludiamo ricordando che anche in questo caso possiamo specificare il valore del parametro <code>axis</code>:</p> <pre><code>&gt;&gt;&gt; np.mean(b, axis=0)\narray([6.33333333, 2.33333333, 6.        ])\n&gt;&gt;&gt; np.mean(b, axis=1)\narray([4.66666667, 2.33333333, 7.66666667])\n</code></pre>"},{"location":"material/02_libs/02_numpy/06_statistics/#varianza-e-deviazione-standard","title":"Varianza e deviazione standard","text":"<p>Non possono mancare le funzioni <code>numpy.std(a)</code> e <code>numpy.var(a)</code>, dedicate al calcolo della deviazione standard e della varianza di un vettore:</p> <pre><code>&gt;&gt;&gt; np.std(a)\n2.4\n&gt;&gt;&gt; np.var(a)\n5.76\n</code></pre> <p>Anche in questo caso, possiamo specificare gli assi lungo i quali effettuare l'operazione desiderata:</p> <pre><code>&gt;&gt;&gt; np.var(b, axis=0)\narray([ 9.55555556, 10.88888889,  0.66666667])\n&gt;&gt;&gt; np.var(b, axis=1)\narray([11.55555556,  4.22222222,  0.88888889])\n</code></pre>"},{"location":"material/02_libs/02_numpy/06_statistics/#istogramma","title":"Istogramma","text":"<p>Un istogramma offre una visualizzazione grafica dei valori contenuti in un vettore, raggruppandoli all'interno di un certo numero di partizioni (bin).</p> <p>Ad esempio, una possibile rappresentazione a due partizioni del vettore \\(A = [1, 2, 3, 4]\\) \u00e8 data dal vettore \\([2, 2]\\). Questo si spiega col fatto che le due partizioni suddividono il range di valori assunti da \\(A\\) in due parti, con la prima inerente gli elementi \\(1\\) e \\(2\\), e la seconda gli elementi \\(3\\) e \\(4\\). Una volta calcolate le partizioni, queste andranno \"riempite\" contando il numero di elementi presenti in ciascuna partizione, il che ci riporta al vettore \\([2, 2]\\).</p> <p>Nota</p> <p>Ovviamente, \u00e8 possibile specificare, oltre al numero di partizioni, anche gli estremi delle stesse, che potrebbero non coincidere con quelli del vettore.</p> <p>NumPy ci permette di ottenere l'istogramma di un vettore mediante l'insieme di funzioni <code>numpy.histogram(a, bins, range)</code>, che ci permette di calcolare l'istogramma (monodimensionale) dell'array <code>a</code> in funzione del numero di partizioni (opzionale) e del range (opzionale). Ad esempio:</p> <pre><code>&gt;&gt;&gt; h, b = np.histogram(a)\n</code></pre> <p>In questo caso, abbiamo lasciato il valore di default di <code>bins</code>, ovvero 10.</p> <p>Notiamo che la funzione <code>histogram()</code> restituisce due valori: il primo \u00e8 dato dai valori assunti dall'istogramma (ovvero dal numero di elementi che ricade in ciascun bin), mentre il secondo \u00e8 dato dai limiti di ogni bin.</p>"},{"location":"material/02_libs/02_numpy/exercises/exercises/","title":"Esercitazione 2.1 - NumPy","text":""},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizi-sugli-array","title":"Esercizi sugli array","text":""},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizio-211","title":"Esercizio 2.1.1","text":"<p>Scrivere una funzione che restituisca il prodotto riga per colonna di due vettori <code>v1</code> e <code>v2</code>. Utilizzare in primis una list comprehension, verificando anche che la lunghezza dei due vettori sia coerente. Valutare inoltre il tempo necessario all'esecuzione utilizzando la libreria <code>time</code>.</p> <p>Effettuare la stessa operazione in NumPy, valutando contestualmente il tempo necessario in entrambi i casi.</p>"},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizio-212","title":"Esercizio 2.1.2","text":"<p>Scrivere la funzione <code>crea_array(dim_1, dim_2, val_min, val_max)</code> che crea array di dimensione arbitraria <code>dim_1</code> \\(\\times\\) <code>dim_2</code> composti da numeri interi casuali compresi tra <code>val_min</code> e <code>val_max</code>. Di default, la funzione dovr\u00e0 creare dei vettori riga. Utilizzare il package <code>random</code>.</p> <p>Provare ad effettuare la stessa operazione in NumPy.</p>"},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizio-213","title":"Esercizio 2.1.3","text":"<p>Scrivere la funzione <code>rettifica(array)</code> che restituisce un array analogo a quello in ingresso, ma con tutti i valori negativi \"rettificati\" a \\(0\\).</p>"},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizi-sulle-operazioni-algebriche","title":"Esercizi sulle operazioni algebriche","text":""},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizio-214","title":"Esercizio 2.1.4","text":"<p>Verificare che il prodotto tra una matrice invertibile e la sua inversa sia la matrice identit\u00e0.</p>"},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizio-215","title":"Esercizio 2.1.5","text":"<p>Scrivere la funzione <code>calcola_determinante()</code> che accetta come parametro in ingresso una matrice \\(2 \\times 2\\) e ne calcola il determinante. Gestire opportunamente il caso in cui la matrice in ingresso sia difforme dalle indicazioni fornite in precedenza, o che non la matrice non sia invertibile.</p>"},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizio-216","title":"Esercizio 2.1.6","text":"<p>Scrivere la funzione <code>inverti_se_invertibile(mat)</code> che, data una matrice bidimensionale, restituisca l'inversa soltanto se <code>mat</code> \u00e8 bidimensionale, quadrata, e il determinante \u00e8 diverso da zero. Utilizzare un'unica istruzione condizionale.</p>"},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizi-sulle-operazioni-polinomiali-in-numpy","title":"Esercizi sulle operazioni polinomiali in NumPy","text":""},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizio-217","title":"Esercizio 2.1.7","text":"<p>Scrivere la funzione <code>somma_polinomi()</code> che accetta come parametri due polinomi di grandezza arbitraria, sommandoli tra loro. Trattiamo i polinomi come liste; in particolare, all'\\(i\\)-mo elemento della lista corrisponder\u00e0 il coefficiente di \\(i\\)-mo grado del polinomio.</p>"},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizio-218","title":"Esercizio 2.1.8","text":"<p>Usare una lista per scrivere la funzione <code>calcola_media(array, pesi)</code> che restituisce il valor medio di un array. Il valore di default del parametro <code>pesi</code> dovr\u00e0 essere una lista vuota. Nel caso che <code>pesi=[]</code>, dovr\u00e0 essere calcolata una media aritmetica; in caso contrario, si dovr\u00e0 verificare la coerenza delle dimensioni di <code>array</code> e <code>pesi</code>, e restituire la media pesata.</p>"},{"location":"material/02_libs/02_numpy/exercises/exercises/#esercizio-219","title":"Esercizio 2.1.9","text":"<p>Scrivere la funzione <code>descrivi(array)</code> che permette di descrivere un array in termini non parametrici, individuando mediana, deviazione standard e range interquartile (ovvero tra il 25-percentile ed il 75-percentile).</p>"},{"location":"material/02_libs/02_numpy/exercises/solutions/","title":"Esercitazione 2.1 - NumPy (Soluzioni)","text":"<p>Soluzioni</p> <p>L'implementazione delle soluzioni \u00e8 disponibile questo notebook.</p>"},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizi-sugli-array","title":"Esercizi sugli array","text":""},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizio-211","title":"Esercizio 2.1.1","text":"<p>Definiamo innanzitutto la funzione <code>riga_per_colonna</code>, la quale accetta due array in ingresso e, se le dimensioni sono coerenti, effettua la moltiplicazione riga per colonna.</p> <p>Una possibile forma per la funzione \u00e8 la seguente:</p> <pre><code>def riga_per_colonna(v1, v2):\n    tic = time()\n    if v1.shape[0] == 1:\n        if v2.shape[1] == 1 and v1.shape[1] == v2.shape[0]:\n            prod = sum([v1[0][i] * v2[i] for i in range(v2.shape[0])])\n    elif v2.shape[0] == 1:\n        if v1.shape[1] == 1 and v2.shape[1] == v1.shape[0]:\n            prod = sum([v1[i] * v2[0][i] for i in range(v1.shape[0])])\n    else:\n        return 'Le dimensioni non sono coerenti!'\n    toc = time()\n    return prod, toc - tic\n</code></pre> <p>In particolare:</p> <ul> <li>alla riga 2, lanciamo il timer di inizio chiamata a funzione;</li> <li>alle righe 3-4, se il vettore <code>v1</code> \u00e8 un vettore riga, allora andiamo a controllare che il vettore <code>v2</code> sia un vettore colonna;</li> <li>alla riga 5, se il controllo precedente \u00e8 andato per il verso giusto, usiamo una list comprehension per fare il prodotto riga per colonna;</li> <li>alle righe 6-8, effettuiamo le operazioni duali alle precedenti;</li> <li>alla riga 11, lanciamo il timer di fine chiamata a funzione;</li> <li>alla riga 12, restituiamo il prodotto ed il tempo trascorso.</li> </ul> <p>Proviamo la nostra funzione:</p> <pre><code>v1 = np.array([[1,2,3,4]])\nv2 = np.array([[1],[2],[3],[4]])\n\nres, elapsed = riga_per_colonna(v1, v2)\nprint(elapsed)\n</code></pre> <p>L'equivalente operazione in NumPy \u00e8 data da:</p> <pre><code>res = np.dot(v1, v2)\n</code></pre>"},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizio-212","title":"Esercizio 2.1.2","text":"<p>Una possibile soluzione \u00e8 la seguente:</p> <pre><code>def crea_array(dim_1, dim_2=1, val_min=0, val_max=100):\n    rows = [[randint(val_min, val_max) for i in range(dim_2)] for j in range(dim_1)]\n    return np.array(rows)\n</code></pre> <p>Alla riga 2 utilizziamo due list comprehension, l'una annidata nell'altra. In particolare, nella list comprehension pi\u00f9 interna, andremo a generare <code>dim_2</code> valori interi casuali compresi tra <code>val_min</code> e <code>val_max</code>, mentre in quella pi\u00f9 esterna ripeteremo l'operazione definita dalla lista pi\u00f9 interna <code>dim_1</code> volte. Il valore ottenuto \u00e8 quindi restituito alla riga 2.1.</p> <p>Con NumPy potremo ovviamente utilizzare il metodo <code>randint</code>:</p> <pre><code>from numpy import random\n\na_1 = random.randint(0, 100, (4, 1))\na_2 = random.randint(0, 100, (2, 2))\n</code></pre>"},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizio-213","title":"Esercizio 2.1.3","text":"<p>Per risolvere questo problema, possiamo sfruttare il concetto di maschera booleana. In particolare, se provassimo a scrivere un'espressione del tipo:</p> <pre><code>&gt;&gt;&gt; a = np.array([1, 2, -1, 2])\n&gt;&gt;&gt; [a &lt; 0]\n</code></pre> <p>l'interprete ci restituirebbe una maschera fatta di soli booleani:</p> <pre><code>[array([False, False,  True, False])]\n</code></pre> <p>Questa maschera pu\u00f2 essere utilizzata per accedere agli elementi dell'array <code>a</code> il cui corrispondente elemento nella maschera \u00e8 a <code>True</code>. In pratica, nell'esempio precedente, accederemo esclusivamente all'elemento in posizione \\(3\\):</p> <pre><code>&gt;&gt;&gt; a[a &lt; 0]\narray([-1])\n</code></pre> <p>Ovviamente, possiamo utilizzare questa maschera anche per assegnare dei nuovi valori agli elementi acceduti. Quindi, scrivendo:</p> <pre><code>&gt;&gt;&gt; a[a &lt; 0] = 0\n</code></pre> <p>andremo a modificare l'array <code>a</code> come segue:</p> <pre><code>array([1, 2, 0, 2])\n</code></pre> <p>Di conseguenza, potremo scrivere la funzione rettifica come segue:</p> <pre><code>def rettifica(array):\n    array[array &lt; 0] = 0\n    return array\n</code></pre>"},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizi-sulle-operazioni-algebriche","title":"Esercizi sulle operazioni algebriche","text":""},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizio-214","title":"Esercizio 2.1.4","text":"<p>Per verificare questo assunto ci basta utilizzare la funzione <code>inv</code> per calcolare la matrice inversa:</p> <pre><code>mat = np.array([[5, 0, 1], [0, 2, 2], [0, 0, 3]])\nmat_inv = np.linalg.inv(mat)\n</code></pre> <p>Conseguentemente, utilizzando la funzione <code>dot</code>, potremo fare il prodotto matriciale tra <code>mat</code>e <code>mat_inv</code>, verificando che sia pari alla matrice identit\u00e0 (in questo caso di ordine 3):</p> <pre><code>np.eye(3) == mat.dot(mat_inv)\n</code></pre>"},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizio-215","title":"Esercizio 2.1.5","text":"<p>Ricordiamo che il calcolo del determinante di una matrice \\(2 \\times 2\\) \u00e8 dato dalla differenza tra il prodotto degli elementi sulla diagonale e quello dei restanti elementi.</p> <p>Per cui, la funzione <code>calcola_determinante()</code> potr\u00e0 essere scritta come segue:</p> <p>```py linenums=1\" def calcola_determinante(mat):     if len(mat.shape) == 2 and mat.shape[0] == mat.shape[1] and mat.shape[0] == 2:         return mat[0][0] * mat[1][1] - mat[0][1] * mat[1][0]     raise ValueError('La matrice non ha le dimensioni attese.') <pre><code>In particolare:\n\n* alla riga 2, verifichiamo che la matrice sia bidimensionale ed abbia dimensioni $2 \\times 2$;\n* alla riga 3, calcoliamo il determinante;\n* alla riga 4, lanciamo un errore nel caso la matrice non abbia dimensioni $2 \\times 2$.\n\n### Esercizio 2.1.6\n\nIn questo caso, potremo utilizzare i metodi messi a disposizione da NumPy. Tuttavia, dovremo verificare contemporaneamente che:\n\n* `shape` di `mat` sia pari a `2` (e, quindi, la matrice sia bidimensionale);\n* la prima dimensione sia uguale alla seconda;\n* che il determinante sia diverso da zero (e, quindi, la matrice risulti essere invertibile).\n\nDi seguito, una possibile soluzione:\n\n```py\ndef inverti_se_invertibile(mat):\n    if len(mat.shape) == 2 \\\n        and mat.shape[0] == mat.shape[1] \\\n        and linalg.det(mat) != 0:\n        return linalg.inv(mat)\n    raise ValueError('La matrice passata non \u00e8 invertibile.')\n</code></pre></p>"},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizi-sulle-operazioni-polinomiali-in-numpy","title":"Esercizi sulle operazioni polinomiali in NumPy","text":""},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizio-217","title":"Esercizio 2.1.7","text":"<p>Per prima cosa, dovremo verificare le lunghezze dei polinomi e, qualora queste non siano coerenti, andare ad inserire un numero di coefficienti adeguato.</p> <pre><code>def somma_polinomi(pol_1, pol_2):\n    if len(pol_1) &lt; len(pol_2):\n        while len(pol_1) &lt; len(pol_2):\n            pol_1.insert(0, 0)\n    elif len(pol_2) &lt; len(pol_1):\n        while len(pol_2) &lt; len(pol_1):\n            pol_2.insert(0, 0)\n    return [(pol_1[i] + pol_2[i]) for i in range(len(pol_1))]\n</code></pre> <p>In pratica:</p> <ul> <li>alle righe 2 - 5, verifichiamo se la lunghezza di pol1 \u00e8 inferiore a quella di pol2 e, se questo \u00e8 vero, andiamo ad inserire tanti zeri quanti sono i coefficienti \"mancanti\";</li> <li>alle righe 6 - 8, effettuiamo la stessa operazione a polinomi invertiti;</li> <li>alla riga 9, andiamo a restituire la somma elemento per elemento dei coefficienti del polinomio.</li> </ul>"},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizio-218","title":"Esercizio 2.1.8","text":"<p>Una possibile soluzione \u00e8 la seguente:</p> <pre><code>def calcola_media(array, pesi=[]):\n    if len(pesi) == 0:\n        return sum(array) / len(array)\n    else:\n        if len(pesi) == len(array):\n            return sum([(pesi[i] * array[i]) for i in range(len(array))]) / len(array)\n    raise ValueError('La lunghezza dei pesi non corrisponde a quella degli array.')\n\ncalcola_media([5, 4, 5])\ncalcola_media([5, 4, 5], [0, 1, 0])\ncalcola_media([5, 4, 5], [0, 1])\n</code></pre> <p>In pratica:</p> <ul> <li>alla riga 2, verifichiamo che <code>pesi</code> sia una lista vuota;</li> <li>alla riga 3, calcoliamo la media aritmetica come somma degli elementi di <code>array</code> diviso la lunghezza dello stesso;</li> <li>nel caso <code>pesi</code> non sia una lista vuota, alla riga 5 viene verificato che <code>pesi</code> ed <code>array</code> abbiano la stessa lunghezza;</li> <li>se ci\u00f2 avviene, alla riga 6 viene creata una list comprehension moltiplicando l'\\(i\\)-mo elemento di <code>pesi</code> per il corrispondente elemento di <code>array</code>; questa sar\u00e0 quindi suddiviso per il numero di elementi di <code>array</code>.</li> </ul>"},{"location":"material/02_libs/02_numpy/exercises/solutions/#esercizio-219","title":"Esercizio 2.1.9","text":"<p>La funzione <code>descrivi</code> pu\u00f2 essere definita come segue:</p> <pre><code>def descrivi(array):\n    return (\n        np.median(array),\n        np.std(array),\n        np.percentile(array, 25) - np.percentile(array, 75))\n\ndescrivi(np.array([3, 5, 3, 2, 1, 8]))\n</code></pre> <p>In pratica, la funzione restituisce mediana, deviazione standard e range interquartile usando le rispettive funzioni NumPy, e restituendo il tutto in una tupla.</p>"},{"location":"material/02_libs/03_pandas/01_intro.en/","title":"2.3.1 - Introduction to Pandas","text":"<p>Accompanying Notebook</p> <p>For this lesson, there is an accompanying notebook available at this link.</p> <p>Accompanying Notebook</p> <p>For this lesson, there is an accompanying notebook available at this link.</p> <p>Accompanying Notebook</p> <p>For this lesson, there is an accompanying notebook available at this link.</p> <p>Pandas is used for reading and processing data from various sources such as CSV or Excel files, as well as text files and databases. Let's briefly see how to use this library, keeping in mind that we will delve deeper into its workings in subsequent lessons.</p>"},{"location":"material/02_libs/03_pandas/01_intro.en/#installation-and-configuration-of-pandas","title":"Installation and Configuration of Pandas","text":"<p>As usual, the first step is to install the library in our working environment:</p> <pre><code>pip install pandas\n</code></pre> <p>We can then import the library into our scripts or notebooks:</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"material/02_libs/03_pandas/01_intro.en/#pandas-and-data-handling","title":"Pandas and Data Handling","text":"<p>Pandas primarily handles structured data in a tabular form, similar to what is commonly found in spreadsheets or databases. These types of data are among the most prevalent and widely used in data analysis, excluding images. To model such data, Pandas provides a specific structure called <code>DataFrame</code>.</p> <p>Dataframes are structures designed to hold data of various types. They are typically organized into rows and columns, similar to how spreadsheets and databases are organized. It is important to note that, conventionally, each row represents a sample in the dataset, while columns are associated with the values of different characteristics or features for each sample.</p> <p>Let's use the Titanic dataset as an example, which is one of the most commonly used datasets for experimentation. First, we generate a dataframe representing the data contained in the dataset by reading the <code>titanic.csv</code> file, which can be downloaded from this link. To read the data, we will use the <code>read_csv()</code> method, passing the relative path of the file:</p> <pre><code>df = pd.read_csv('titanic.csv')\n</code></pre> <p>We can use the <code>head()</code> method to display the first five rows of the dataframe.</p> <pre><code>&gt;&gt;&gt; df.head()\n</code></pre> <pre><code>   PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked\n0            1         0       3                            Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   7.2500   NaN        S\n1            2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0          PC 17599  71.2833   C85        C\n2            3         1       \n\n 3                             Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S\n3            4         1        1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803  53.1000  C123        S\n4            5         0       3                           Allen, Mr. William Henry    male  35.0      0      0            373450   8.0500   NaN        S\n</code></pre> <p>We can quickly see that each passenger is associated with features, from which we can infer their type (we will verify this shortly):</p> Feature Description Type PassengerId Unique identifier of the passenger Integer Survived Indicates if the passenger survived Integer/Boolean Pclass Represents the passenger's class Integer Name Full name of the passenger String Sex Gender of the passenger String Age Age of the passenger Decimal SibSp Abbreviation for \"Siblings/Spouses,\" represents the number of siblings/spouses aboard for each passenger Integer Parch Abbreviation for \"Parents/Children,\" represents the number of parents/children aboard for each passenger Integer Ticket Represents the ticket identifier of the passenger String Fare Represents the fare paid by the passenger Decimal Cabin Represents the cabin where the passenger stayed String Embarked Represents the point of embarkation for the passenger String <p>Let's verify that our assumptions about the data types are correct. We can do this by using the <code>dtypes</code> property of the dataframe:</p> <pre><code>&gt;&gt;&gt; df.dtypes\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n</code></pre> <p>We immediately notice the presence of three column types: <code>int64</code>, <code>float64</code>, and <code>object</code>. The first two are self-explanatory, but it's worth mentioning the <code>object</code> type, which is automatically associated with all strings.</p> <p>Tip</p> <p>Using the <code>object</code> type often poses several issues in the subsequent data analysis phase. Therefore, it might be a good idea to parameterize the <code>read_csv()</code> function using the <code>dtype</code> parameter, which accepts a dictionary specifying the type of one or more columns. For example, if we wanted to specify that names are strings, we could use the <code>string</code> type:</p> <pre><code>&gt;&gt;&gt; types = {'Name': 'string'}\n&gt;&gt;&gt; df = pd.read_csv('train.csv', dtype=types)\n&gt;&gt;&gt; df.dtypes\n# ...\nName            string\n# ...\n</code></pre> <p>It is clear that the dataset provides numerous properties for each embarked passenger. These properties can be used for in-depth analysis of the data from different perspectives, which we will discuss more extensively later.</p>"},{"location":"material/02_libs/03_pandas/01_intro/","title":"2.3.1 - Introduzione a Pandas","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Pandas viene usata per la lettura ed elaborazione dei dati provenienti da sorgenti di vario tipo, come ad esempio file CSV o Excel, ma anche file di testo e database. Vediamo quindi brevemente come usare la libreria, tenendo presente che ne approfondiremo il funzionamento anche durante le lezioni successive.</p>"},{"location":"material/02_libs/03_pandas/01_intro/#installazione-e-configurazione-di-pandas","title":"Installazione e configurazione di Pandas","text":"<p>Al solito, il primo passo \u00e8 sempre quello di installare la libreria nel nostro ambiente di lavoro:</p> <pre><code>pip install pandas\n</code></pre> <p>Possiamo quindi importare la libreria all'interno dei nostri script o notebook:</p> <pre><code>import pandas as pd\n</code></pre>"},{"location":"material/02_libs/03_pandas/01_intro/#pandas-e-la-gestione-dei-dati","title":"Pandas e la gestione dei dati","text":"<p>Pandas gestisce prevalentemente dati strutturati sotto forma tabellare, ossia simili a quelli comunemente contenuti all'interno dei fogli di calcolo o nei database. Questi dati sono sicuramente tra i pi\u00f9 diffusi ed utilizzati nel contesto dell'analisi dei dati, ovviamente escludendo le immagini: in tal senso, per modellarli, Pandas ci mette a disposizione un'apposita struttura denominata <code>DataFrame</code>.</p> <p>I dataframe sono quindi delle strutture atte a contenere dati di ogni tipo. Questi sono normalmente organizzati in righe e colonne, in maniera del tutto analoga a quella in cui sono organizzati i fogli di calcolo ed i database. Importante anche sottolineare come, per convenzione, le singole righe rappresentino i campioni del dataset, mentre le colonne siano associati ai valori assunti dalle diverse caratteristiche, o feature, di ciascun campione.</p> <p>Facciamo un esempio usando il dataset Titanic, che \u00e8 uno tra i pi\u00f9 utilizzati a scopi di sperimentazione. Per prima cosa, generiamo un dataframe rappresentativo dei dati contenuti nel dataset, leggendo il file <code>titanic.csv</code>, che possiamo scaricare a questo link. Per leggere i dati, dovremo utilizzare il metodo <code>read_csv()</code>, cui passeremo il percorso relativo del file:</p> <pre><code>df = pd.read_csv('titanic.csv')\n</code></pre> <p>Usiamo il metodo <code>head()</code> per mostrare a schermo le prime cinque righe del dataframe.</p> <pre><code>&gt;&gt;&gt; df.head()\n</code></pre> <pre><code>   PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked\n0            1         0       3                            Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   7.2500   NaN        S\n1            2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0          PC 17599  71.2833   C85        C\n2            3         1       3                             Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S\n3            4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803  53.1000  C123        S\n4            5         0       3                           Allen, Mr. William Henry    male  35.0      0      0            373450   8.0500   NaN        S\n</code></pre> <p>Vediamo rapidamente che ad ogni passeggero sono associate delle feature, di cui possiamo inferire il tipo (lo verificheremo a breve):</p> Feature Descrizione Tipo PassengerId Identificativo univoco del passeggero. Intero Survived Stabilisce se il passeggero \u00e8 sopravvissuto. Intero/booleano Pclass Rappresenta la classe del passeggero Intero Name Nome completo del passeggero Stringa Sex Genere del passeggero Stringa Age Et\u00e0 del passeggero Decimale SibSp Crasi di \"Siblings/Spouses\", rappresenta il numero di fratelli/sorelle/coniugi a bordo per ogni passeggero Intero Parch Crasi di \"Parents/Children\", rappresenta il numero di genitori/figli a bordo per ogni passeggero Intero Ticket Rappresenta l'identificativo per il ticket del passeggero. Stringa Tariffa Rappresenta la tariffa pagata dal passeggero. Decimale Cabin Rappresenta la cabina in cui allogiava il passeggero. Stringa Embarked Rappresenta il punto di imbarco del passeggero. Stringa <p>Verifichiamo che le nostre ipotesi sul tipo di dato siano corrette; per farlo, possiamo usare la propriet\u00e0 <code>dtypes</code> del dataframe:</p> <pre><code>&gt;&gt;&gt; df.dtypes\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n</code></pre> <p>Notiamo subito la presenza di tre tipi di colonna, ovvero <code>int64</code>, <code>float64</code> e <code>object</code>. Laddove i primi due sono autoesplicativi, merita una particolare menzione il tipo <code>object</code>, che viene associato automaticamente a tutte le stringhe.</p> <p>Suggerimento</p> <p>Normalmente, usare il tipo <code>object</code> comporta diversi problemi nella successiva fase di analisi dei dati. Potrebbe quindi essere una buona idea parametrizzare la funzione <code>read_csv()</code> mediante il parametro <code>dtype</code>, che accetta un dizionario che specifica il tipo di una o pi\u00f9 colonne. Ad esempio, se volessimo specificare che i nomi sono delle stringhe, potremmo usare il tipo <code>string</code>:</p> <pre><code>&gt;&gt;&gt; types = {'Name': 'string'}\n&gt;&gt;&gt; df = pd.read_csv('train.csv', dtype=types)\n&gt;&gt;&gt; df.dtypes\n# ...\nName            string\n# ...\n</code></pre> <p>Appare chiaro come il dataset ci illustri numerose propriet\u00e0 per ogni passeggero imbarcato. Queste potranno quindi essere utilizzate per un'analisi approfondita della struttura dei dati sotto diversi aspetti e punti di vista; ne parleremo pi\u00f9 estesamente nel seguito.</p>"},{"location":"material/02_libs/03_pandas/02_series.en/","title":"2.3.2 - Series","text":""},{"location":"material/02_libs/03_pandas/02_series.en/#series","title":"Series","text":"<p>In the previous lesson, we saw that each DataFrame is composed of different columns, each representing a specific feature. In practice, Pandas offers a way to extract each of these columns individually using the <code>Series</code> class. For example, we can extract the series related to the numeric passenger IDs:</p> <pre><code>names = df['Name']\nnames.head()\n\n# Output\n0                              Braund, Mr. Owen Harris\n1    Cumings, Mrs. John Bradley (Florence Briggs Th...\n2                               Heikkinen, Miss. Laina\n3         Futrelle, Mrs. Jacques Heath (Lily May Peel)\n4                             Allen, Mr. William Henry\nName: Name, dtype: object\n</code></pre>"},{"location":"material/02_libs/03_pandas/02_series.en/#accessing-elements-of-a-series","title":"Accessing elements of a series","text":"<p>We can access a single element of a series using indexing. Each sample within the series is associated with an increasing numerical index starting from 0. Therefore, we can access the \\(i\\)-th element of the series by referring to the \\(i-1\\) index, similar to accessing elements in lists or sequences.</p> <pre><code>&gt;&gt;&gt; names[0]\n'Braund, Mr. Owen Harris'\n</code></pre> <p>Note</p> <p>Indexing can also be used to set the value associated with a specific index of the series.</p>"},{"location":"material/02_libs/03_pandas/02_series.en/#accessing-elements-of-the-dataframe","title":"Accessing elements of the dataframe","text":"<p>Accessing elements of the dataframe can be done in different ways. First, we can access the specific value of a feature for a given sample using chained indexing:</p> <pre><code>&gt;&gt;&gt; df['Age'][1]\n38\n</code></pre> <p>Alternatively, we can use the <code>loc(row_idx, col)</code> function. When called on a <code>DataFrame</code> object, it allows us to access the value of the <code>col</code> feature for the element at position <code>row_idx</code>:</p> <pre><code>&gt;&gt;&gt; df.loc[1, 'Age']\n38.0\n</code></pre> <p>The <code>loc()</code> function can also operate on data slices:</p> <pre><code>&gt;&gt;&gt; df.loc[1:5, 'Age']\n1    38.0\n2    26.0\n3    35.0\n4    35.0\n5     NaN\n</code></pre> <p>or on sets of features:</p> <pre><code>&gt;&gt;&gt; df.loc[1:5, ['Age', 'Sex']]\n   Age     Sex\n1  38.0  female\n2  26.0  female\n3  35.0  female\n4  35.0    male\n5   NaN    male\n</code></pre> <p>It's important to note that the <code>loc()</code> function operates on row indices. In this case, our dataframe has integer row indices assigned automatically during the dataframe's creation. If we decide to use a column of the dataframe as an index, we can use the <code>set_index()</code> method:</p> <pre><code>df = df.set_index('Ticket')\n</code></pre> <p>Note that functions operate on the value, not the reference. Therefore, if we omit the assignment, <code>df</code> remains unchanged. To avoid using the assignment operation every time, we can set the <code>inplace</code> parameter to <code>True</code>:</p> <pre><code>df.set_index('Ticket', inplace=True)\n</code></pre> <p>Alternatively, we can directly set the index in the <code>read_csv()</code> method by setting the <code>index_col</code> parameter:</p> <pre><code>df = pd.read_csv('titanic.csv', index_col='Ticket')\n</code></pre> <p>In this case, the <code>loc()</code> function should be used by providing the new row indices as read parameters</p> <p>. For example:</p> <pre><code>&gt;&gt;&gt; df.loc['STON/O2. 3101282', 'Name']\n'Heikkinen, Miss. Laina'\n</code></pre> <p>In addition to the <code>loc()</code> function, Pandas provides the <code>iloc()</code> function, which allows us to select a subset of dataframe samples using integer indices (<code>i</code> in <code>iloc()</code> stands for integer):</p> <pre><code>&gt;&gt;&gt; df.iloc[2:5, 2:4]\n                  Pclass                                          Name\nTicket                                                                \nSTON/O2. 3101282       3                        Heikkinen, Miss. Laina\n113803                 1  Futrelle, Mrs. Jacques Heath (Lily May Peel)\n373450                 3                      Allen, Mr. William Henry\n</code></pre>"},{"location":"material/02_libs/03_pandas/02_series.en/#boolean-masks","title":"Boolean masks","text":"<p>Suppose we want to select only adult males from the Titanic dataset. We can use a boolean logic statement to achieve this:</p> <pre><code>&gt;&gt;&gt; men = df[(df['Age'] &gt; 18) &amp; (df['Sex'] == 'male')]\n&gt;&gt;&gt; men.head()\n    PassengerId  Survived  Pclass                            Name   Sex   Age\n0             1         0       3         Braund, Mr. Owen Harris  male  22.0   \n4             5         0       3        Allen, Mr. William Henry  male  35.0   \n6             7         0       1         McCarthy, Mr. Timothy J  male  54.0   \n12           13         0       3  Saundercock, Mr. William Henry  male  20.0   \n13           14         0       3     Andersson, Mr. Anders Johan  male  39.0   \n</code></pre> <p>In practice, we are filtering the dataset based on the logical <code>AND</code> between two conditions:</p> <ul> <li><code>df['Age'] &gt; 18</code>: this condition generates a boolean mask that is <code>True</code> only if the age for that passenger is greater than 18 years.</li> <li><code>df['Sex'] == 'male'</code>: this condition generates a boolean mask that is true only if the passenger's gender is male.</li> </ul>"},{"location":"material/02_libs/03_pandas/02_series.en/#the-groupby-function","title":"The <code>groupby</code> function","text":"<p>We can use the <code>groupby</code> function to group sets of data that are usually relevant to categories.</p> <p>For example, we can group passengers by gender:</p> <pre><code>&gt;&gt;&gt; df.groupby(['Sex'])\n</code></pre> <p>We can also extract statistics from these groupings. Let's see the average age of female and male passengers:</p> <pre><code>&gt;&gt;&gt; df.groupby(['Sex'])['Age'].mean()\nSex\nfemale    27.915709\nmale      30.726645\nName: Age, dtype: float64\n</code></pre>"},{"location":"material/02_libs/03_pandas/02_series/","title":"2.3.2 - Le Series","text":""},{"location":"material/02_libs/03_pandas/02_series/#le-series","title":"Le Series","text":"<p>Nella lezione precedente abbiamo visto come ogni DataFrame sia in realt\u00e0 composto da diverse colonne, ciascuna rappresentativa di una feature specifica. Nella pratica, Pandas ci offre un modo per estrarre singolarmente ciascuna di queste colonne mediante la classe <code>Series</code>. Ad esempio, potremmo estrarre la serie relativa agli identificativi numerici dei passeggeri:</p> <pre><code>names = df['Name']\nnames.head()\n\n# Output restituito\n0                              Braund, Mr. Owen Harris\n1    Cumings, Mrs. John Bradley (Florence Briggs Th...\n2                               Heikkinen, Miss. Laina\n3         Futrelle, Mrs. Jacques Heath (Lily May Peel)\n4                             Allen, Mr. William Henry\nName: Name, dtype: object\n</code></pre>"},{"location":"material/02_libs/03_pandas/02_series/#accesso-agli-elementi-di-una-serie","title":"Accesso agli elementi di una serie","text":"<p>Possiamo accedere ad un singolo elemento di una serie mediante una classica procedura di indicizzazione. Notiamo infatti come ogni campione all'interno della serie sia associato ad un indice numerico crescente il cui valore iniziale \u00e8 pari a 0; pertanto, possiamo accedere all'\\(i\\)-mo elemento della serie richiamando l'\\(i-1\\)-mo indice, esattamente come accade per le liste o le sequenze.</p> <pre><code>&gt;&gt;&gt; names[0]\n'Braund, Mr. Owen Harris'\n</code></pre> <p>Nota</p> <p>L'indicizzazione pu\u00f2 essere anche usata per impostare il valore associato ad uno specifico indice della serie.</p>"},{"location":"material/02_libs/03_pandas/02_series/#accesso-agli-elementi-del-dataframe","title":"Accesso agli elementi del dataframe","text":"<p>L'accesso agli elementi del dataframe pu\u00f2 avvenire attraverso diverse modalit\u00e0. In primo luogo, possiamo accedere allo specifico valore di una feature di un dato campione mediante il chained indexing:</p> <pre><code>&gt;&gt;&gt; df['Age'][1]\n38\n</code></pre> <p>In alternativa, \u00e8 possibile usare la funzione <code>loc(row_idx, col)</code> che, se chiamata su un oggetto di tipo <code>DataFrame</code>, ci permette di accedere al valore assunto dalla feature <code>col</code> per l'elemento in posizione <code>row_idx</code>:</p> <pre><code>&gt;&gt;&gt; df.loc[1, ('Age')]\n38.0\n</code></pre> <p>La funzione <code>loc()</code> pu\u00f2 operare anche su delle slice di dati:</p> <pre><code>&gt;&gt;&gt; df.loc[1:5, ('Age')]\n1    38.0\n2    26.0\n3    35.0\n4    35.0\n5     NaN\n</code></pre> <p>o su insiemi di feature:</p> <pre><code>&gt;&gt;&gt; df.loc[1:5, ('Age', 'Sex')]\n   Age     Sex\n1  38.0  female\n2  26.0  female\n3  35.0  female\n4  35.0    male\n5   NaN    male\n</code></pre> <p>Sottolineamo che la funzione <code>loc()</code> opera sugli indici di riga. In questo caso, il nostro dataframe ha degli indici di riga interi, assegnati automaticamente in fase di lettura del dataframe. Nel caso decidessimo di usare una colonna del dataframe come indice, potremmo usare il metodo <code>set_index()</code>:</p> <pre><code>df = df.set_index('Ticket')\n</code></pre> <p>Notiamo che, come al solito, le funzioni lavorano sul valore, e non sulla reference. Di conseguenza, se omettessimo l'assegnazione, <code>df</code> rimarrebbe invariato. Un modo per evitare di usare ogni volta l'operazione di assegnazione \u00e8 quello di impostare il parametro <code>inplace</code> a <code>True</code>:</p> <pre><code>df.set_index('Ticket', inplace=True)\n</code></pre> <p>In alternativa, possiamo decidere di impostare l'indice direttamente nel metodo <code>read_csv()</code> impostando il parametro <code>index_col</code>:</p> <pre><code>df = pd.read_csv('titanic.csv', index_col='Ticket')\n</code></pre> <p>In questo caso, la funzione <code>loc()</code> dovr\u00e0 essere utilizzata usando come parametri di lettura per righe i nuovi indici. Ad esempio:</p> <pre><code>&gt;&gt;&gt; df.loc['STON/O2. 3101282', 'Name']\n'Heikkinen, Miss. Laina'\n</code></pre> <p>Oltre alla funzione <code>loc()</code> Pandas ci mette a disposizione la funzione <code>iloc()</code>, la quale ci offre la possibilit\u00e0 di selezionare un sottoinsieme di campion del dataframe mediante indici interi (da cui la <code>i</code>):</p> <pre><code>&gt;&gt;&gt; df.iloc[2:5, 2:4]\n                  Pclass                                          Name\nTicket                                                                \nSTON/O2. 3101282       3                        Heikkinen, Miss. Laina\n113803                 1  Futrelle, Mrs. Jacques Heath (Lily May Peel)\n373450                 3                      Allen, Mr. William Henry\n</code></pre>"},{"location":"material/02_libs/03_pandas/02_series/#maschere-booleane","title":"Maschere booleane","text":"<p>Supponiamo di voler selezionare soltanto gli uomini maggiorenni presenti nel dataset del Titanic. Per farlo, possiamo usare un'istruzione che implementi delle logiche di tipo booleano:</p> <pre><code>&gt;&gt;&gt; men = df[(df['Age'] &gt; 18) &amp; (df['Sex'] == 'male')]\n&gt;&gt;&gt; men.head()\n    PassengerId  Survived  Pclass                            Name   Sex   Age\n0             1         0       3         Braund, Mr. Owen Harris  male  22.0   \n4             5         0       3        Allen, Mr. William Henry  male  35.0   \n6             7         0       1         McCarthy, Mr. Timothy J  male  54.0   \n12           13         0       3  Saundercock, Mr. William Henry  male  20.0   \n13           14         0       3     Andersson, Mr. Anders Johan  male  39.0   \n</code></pre> <p>Nella pratica, stiamo filtrando il dataset in base all'<code>AND</code> logico tra due condizioni:</p> <ul> <li><code>df['Age'] &gt; 18</code>: questa condizione genera una maschera booleana che \u00e8 <code>True</code> soltanto se l'et\u00e0 per quel passeggero \u00e8 maggiore di 18 anni;</li> <li><code>df['Sex'] == 'male'</code>: questa condizione genera una maschera booleana che \u00e8 vera soltanto se il genere del passeggero \u00e8 maschile.</li> </ul>"},{"location":"material/02_libs/03_pandas/02_series/#la-funzione-groupby","title":"La funzione <code>groupby</code>","text":"<p>Possiamo sfruttare la funzione <code>groupby</code> per raggruppare insiemi di dati (normalmente pertinenti a categorie).</p> <p>Ad esempio, potremmo raggruppare i passeggeri per genere:</p> <pre><code>&gt;&gt;&gt; df.groupby(['Sex'])\n</code></pre> <p>Possiamo ovviamente estrarre delle statistiche a partire da questi raggruppamenti. Vediamo, ad esempio, l'et\u00e0 media dei passeggeri di sesso femminile e maschile:</p> <pre><code>&gt;&gt;&gt; df.groupby(['Sex'])['Age'].mean()\nSex\nfemale    27.915709\nmale      30.726645\nName: Age, dtype: float64\n</code></pre>"},{"location":"material/02_libs/03_pandas/03_io.en/","title":"2.3.3 - I/O in Pandas","text":""},{"location":"material/02_libs/03_pandas/03_io.en/#reading-data-from-heterogeneous-sources","title":"Reading data from heterogeneous sources","text":"<p>In our first example, we used the <code>read_csv</code> function to create a DataFrame from data stored in a CSV file. However, Pandas supports many other formats.</p> <p>For example, we could try reading an Excel file:</p> <pre><code>df = pd.read_excel('data.xlsx')\n</code></pre> <p>Attention</p> <p>To read (and write) from (to) Excel, you need to install the <code>openpyxl</code> library (<code>pip install openpyxl</code>).</p> <p>Alternatively, you can read a JSON file or directly from a database:</p> <pre><code>df = pd.read_json('data.json')\ndf = pd.read_sql(SQL_QUERY)\n</code></pre> <p>There is a complete list of the available (numerous) functions, which can be found in the reference. In general, however, the syntax is always <code>read_*(data_source)</code>, where <code>*</code> is replaced with the type of data source (<code>csv</code>, <code>excel</code>, etc.).</p>"},{"location":"material/02_libs/03_pandas/03_io.en/#writing-data-to-heterogeneous-destinations","title":"Writing data to heterogeneous destinations","text":"<p>We can also write a DataFrame to a file using the dual functions to <code>read_</code>, which use the prefix <code>to_</code> followed by the destination file extension. For example, we can write a CSV file using the <code>to_csv</code> method:</p> <pre><code>df.to_csv('train.xlsx')\n</code></pre>"},{"location":"material/02_libs/03_pandas/03_io.en/#adding-features-and-data","title":"Adding features and data","text":"<p>Now let's say we want to add a new feature to an existing DataFrame. To do this, we start by creating a DataFrame from scratch:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame([1,2,3,4,5], columns=['one'])\n   one\n0    1\n1    2\n2    3\n3    4\n4    5\n</code></pre> <p>We can add a new column simply by using the assignment operator and specifying its name:</p> <pre><code>&gt;&gt;&gt; df['two'] = df['one'] * 2\n   one  two\n0    1    2\n1    2    4\n2    3    6\n3    4    8\n4    5   10\n</code></pre> <p>We can then insert new samples at the end of the DataFrame. To do this, we first need to create a new DataFrame with dimensions consistent with the existing one, and then use the <code>concat()</code> function:</p> <pre><code>&gt;&gt;&gt; df_add = pd.DataFrame([[6,7]], columns=['one', 'two'])\n&gt;&gt;&gt; df = pd.concat([df, df_add])\n   one  two\n0    1    2\n1    2    4\n2    3    6\n3    4    8\n4    5   10\n0    6    7\n</code></pre> <p>Note that the <code>concat()</code> function accepts the <code>axis</code> parameter, among others. If it is equal to zero (as it is by default), <code>concat()</code> performs row-wise concatenation. If it is equal to 1, concatenation is performed column-wise. However, it is important to note that concatenation also occurs when the dimensions are not completely consistent. In fact, if we were to perform column-wise concatenation, the result would be:</p> <pre><code>&gt;&gt;&gt; pd.concat([df, df_add], axis=1)\n   one  two  one  two\n0    1    2 \n\n 6.0  7.0\n1    2    4  NaN  NaN\n2    3    6  NaN  NaN\n3    4    8  NaN  NaN\n4    5   10  NaN  NaN\n</code></pre> <p>The values corresponding to rows with indices 1 to 4, which obviously do not exist, would automatically be set to NaN, which stands for Not a Number.</p>"},{"location":"material/02_libs/03_pandas/03_io/","title":"2.3.3 - I/O in Pandas","text":""},{"location":"material/02_libs/03_pandas/03_io/#lettura-di-dati-da-sorgenti-eterogenee","title":"Lettura di dati da sorgenti eterogenee","text":"<p>Nel nostro primo esempio abbiamo usato la funzione <code>read_csv</code> per creare un dataframe partendo dai dati memorizzati in un file in formato CSV. Tuttavia, Pandas supporta molti altri formati.</p> <p>Ad esempio, potremmo provare a leggere un file Excel:</p> <pre><code>df = pd.read_excel('dati.xlsx')\n</code></pre> <p>Attenzione</p> <p>Per leggere (e scrivere) da (su) Excel \u00e8 necessario installare la libreria <code>openpyxl</code> (<code>pip install openpyxl</code>).</p> <p>In alternativa, pu\u00f2 essere letto un file in formato JSON, oppure ancora direttamente un database:</p> <pre><code>df = pd.read_json('dati.json')\ndf = pd.read_sql(SQL_QUERY)\n</code></pre> <p>Esiste un elenco completo delle (numerose) funzioni disponibili, che possono essere individuate sulla reference. In generale, comunque, la sintassi \u00e8 sempre <code>read_*(data_source)</code>, con <code>*</code> da sostituire con il tipo di sorgente dati (<code>csv</code>, <code>excel</code>, etc.).</p>"},{"location":"material/02_libs/03_pandas/03_io/#scrittura-di-dati-su-destinazioni-eterogenee","title":"Scrittura di dati su destinazioni eterogenee","text":"<p>Possiamo anche scrivere un dataframe su file mediante le funzioni duali alle <code>read_</code>, che usano il suffisso <code>to_</code> seguito dall'estensione del file destinazione. Ad esempio, potremmo scrivere un file CSV con il metodo <code>to_csv</code>:</p> <pre><code>df.to_csv('train.xlsx')\n</code></pre>"},{"location":"material/02_libs/03_pandas/03_io/#aggiunta-di-feature-e-dati","title":"Aggiunta di feature e dati","text":"<p>Immaginiamo adesso di voler aggiungere una nuova feature ad un dataframe gi\u00e0 esistente. Per farlo, iniziamo creando un dataframe da zero:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame([1,2,3,4,5], columns=['one'])\n   one\n0    1\n1    2\n2    3\n3    4\n4    5\n</code></pre> <p>Possiamo aggiungere una nuova colonna semplicemente usando l'operatore di assegnazione e specificandone il nome:</p> <pre><code>&gt;&gt;&gt; df['two'] = df['one'] * 2\n   one  two\n0    1    2\n1    2    4\n2    3    6\n3    4    8\n4    5   10\n</code></pre> <p>Possiamo poi inserire nuovi campioni in coda al dataframe. Per farlo, dovremo prima creare un nuovo dataframe dalle dimensioni coerenti con quello gi\u00e0 esistente, e poi usare la funzione <code>concat()</code>:</p> <pre><code>&gt;&gt;&gt; df_add = pd.DataFrame([[6,7]], columns=['one', 'two'])\n&gt;&gt;&gt; df = pd.concat([df, df_add])\n   one  two\n0    1    2\n1    2    4\n2    3    6\n3    4    8\n4    5   10\n0    6    7\n</code></pre> <p>Notiamo che la funzione <code>concat()</code> accetta, tra gli altri, il parametro <code>axis</code>. Se questo \u00e8 uguale a zero (come lo \u00e8 di default), la <code>concat()</code> effettua la concatenazione per righe; se \u00e8 pari ad 1, invece, la concatenazione avviene per colonne. Tuttavia, \u00e8 importante sottolineare come la concatenazione avvenga anche nel caso le misure non siano completamente coerenti: infatti, se provassimo ad effettuare una concatenazione per colonne, avremmo un risultato del tipo:</p> <pre><code>&gt;&gt;&gt; pd.concat([df, df_add], axis=1)\n   one  two  one  two\n0    1    2  6.0  7.0\n1    2    4  NaN  NaN\n2    3    6  NaN  NaN\n3    4    8  NaN  NaN\n4    5   10  NaN  NaN\n</code></pre> <p>I valori relativi alle righe con indice che va da 1 a 4, che ovviamente non saranno presenti, saranno automaticamente impostati a NaN, acronimo di Not a Number.</p>"},{"location":"material/02_libs/03_pandas/04_functions.en/","title":"2.3.4 - DataFrame Manipulation","text":""},{"location":"material/02_libs/03_pandas/04_functions.en/#dataframe-visualization","title":"DataFrame Visualization","text":"<p>Pandas provides native support for Matplotlib to visualize the data contained in a DataFrame.</p> <p>In this regard, we can use the <code>plot()</code> function on a series or an entire DataFrame. For example, we can plot the ages of the passengers:</p> <pre><code>df['Age'].plot()\nplt.show()\n</code></pre> <p>This will produce the result shown in figure 1:</p> <p> </p> Figure 1 - Ages of the passengers in the Titanic dataset <p>We can also plot the entire <code>DataFrame</code>:</p> <pre><code>df.plot()\nplt.show()\n</code></pre> <p>This will result in figure 2:</p> <p> </p> Figure 2 - The Titanic dataset (in a plot) <p>Of course, we can also use Pandas to plot other types of charts, such as histograms. To do this, we use the appropriate sub-functions of <code>plot</code>:</p> <pre><code>df['Age'].plot.hist()\nplt.show()\n</code></pre> <p>The result is shown in figure 3.</p> <p> </p> Figure 3 - Histogram of the ages of the passengers <p>Pandas and Seaborn</p> <p>Pandas integrates naturally with the Seaborn library, which we will cover in one of the upcoming lessons.</p>"},{"location":"material/02_libs/03_pandas/04_functions.en/#handling-missing-values","title":"Handling Missing Values","text":"<p>Anyone who has tried at least once to perform a data acquisition campaign knows well that it is (almost) never a straightforward procedure. For example, a sensor failure could result in the loss of a certain set of data for a period of time, or a user of our system could omit their age, which would then not be included in the final dataset.</p> <p>These events put us in a \"sticky\" situation related to the management of datasets that contain one or more missing values. For example, let's take the first six rows of the Titanic dataset:</p> <p>```sh    PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked 0            1         0       3                            Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   7.2500   NaN        S 1            2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0          PC 17599  71.2833   C85        C 2            3         1       3                             Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S 3            4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803  53.1000  C123        S 4            5         0       3                           Allen, Mr. William Henry    male  35.0      0      0            373450   8.0500   NaN        S 5            6         0       3                                   Moran, Mr. James    male  </p> <p>.</p>"},{"location":"material/02_libs/03_pandas/04_functions/","title":"2.3.4 - Manipolazione dei DataFrame","text":""},{"location":"material/02_libs/03_pandas/04_functions/#visualizzazione-dei-dataframe","title":"Visualizzazione dei DataFrame","text":"<p>Pandas ci offre un supporto nativo a Matplotlib per permettere la visualizzazione dei dati contenuti all'interno di un dataframe.</p> <p>In tal senso, possiamo usare la funzione <code>plot()</code> su una serie o su un intero dataframe; ad esempio, potremmo plottare le et\u00e0 dei passeggeri:</p> <pre><code>df['Age'].plot()\nplt.show()\n</code></pre> <p>ottenendo il risultato mostrato in figura 1:</p> <p> </p> Figura 1 - Le et\u00e0 dei passeggeri nel dataset Titanic <p>Possiamo anche fare il plot dell'intero <code>DataFrame</code>:</p> <pre><code>df.plot()\nplt.show()\n</code></pre> <p>che risulter\u00e0 nella figura 2:</p> <p> </p> Figura 2 - Il dataset Titanic (in grafico) <p>Ovviamente, \u00e8 possibile usare Pandas anche per fare il plot di altri tipi di grafico, come ad esempio gli istogrammi. Per farlo, si usano le apposite sotto-funzioni di <code>plot</code>:</p> <pre><code>df['Age'].plot.hist()\nplt.show()\n</code></pre> <p>Il risultato \u00e8 mostrato in figura 3.</p> <p> </p> Figura 3 - Istogramma delle et\u00e0 dei passeggeri <p>Pandas e Seaborn</p> <p>Pandas si integra in maniera naturale anche con la libreria Seaborn, di cui tratteremo in una delle prossime lezioni.</p>"},{"location":"material/02_libs/03_pandas/04_functions/#gestione-dei-valori-mancanti","title":"Gestione dei valori mancanti","text":"<p>Chiunque abbia provato almeno una volta ad effettuare una campagna di acquisizione dati sa bene come questa non sia (quasi) mai una procedura in cui va tutto per il verso giusto. Ad esempio, potremmo avere un guasto ad un sensore, il quale comporterebbe la perdita di un determinato insieme di dati per un certo periodo; oppure, un utente del nostro sistema potrebbe omettere la sua et\u00e0, che quindi non risulterebbe poi nell'insieme di dati finali.</p> <p>Queste evenienze ci pongono di fronte ad una situazione \"spinosa\", legata alla gestione di dataset con al loro interno uno o pi\u00f9 valori mancanti. Ad esempio, riprendiamo le prime sei righe del dataset Titanic:</p> <pre><code>   PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked\n0            1         0       3                            Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   7.2500   NaN        S\n1            2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0          PC 17599  71.2833   C85        C\n2            3         1       3                             Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S\n3            4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803  53.1000  C123        S\n4            5         0       3                           Allen, Mr. William Henry    male  35.0      0      0            373450   8.0500   NaN        S\n5            6         0       3                                   Moran, Mr. James    male   NaN      0      0            330877   8.4583   NaN        Q\n</code></pre> <p>Notiamo come, per la feature <code>Cabin</code>, sia associato ai passeggeri <code>1</code>, <code>3</code>, <code>5</code> e <code>6</code> il valore <code>NaN</code>, cos\u00ec come per il parametro <code>Age</code> del passeggero <code>6</code>. Questo indica che, nel dataset originario, il valore corrispondente risulta essere assente: <code>NaN</code>, infatti, \u00e8 un acronimo che sta per Not a Number, e viene per convenzione utilizzato come placeholder per tutte le feature di cui non \u00e8 possibile leggere il valore a partire dal dataset iniziale.</p> <p>La presenza (o, per meglio dire assenza) di questi valori comporta l'impossibilit\u00e0 di utilizzare alcuni degli algoritmi che vedremo successivamente, in quanto questi prevedono la valorizzazione integrale di ciascun campione presente nel dataset. Per ovviare a questa problematica, Pandas ci mette a disposizione principalmente due alternative.</p>"},{"location":"material/02_libs/03_pandas/04_functions/#opzione-1-rimozione-dei-dati-mancanti","title":"Opzione 1: rimozione dei dati mancanti","text":"<p>La prima alternativa offerta da Pandas sta nella rimozione dei dati mancanti usando la funzione <code>dropna()</code>, che ci permette di eliminare i campioni o le feature dove sono presenti valori mancanti.</p> <p>In particolare, per eliminare i campioni che presentano dei valori mancanti, dovremo impostare il parametro <code>axis</code> a <code>0</code> (e, quindi, lavorare sulle righe):</p> <pre><code>&gt;&gt;&gt; df.dropna(axis=0)\n     PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch    Ticket     Fare        Cabin Embarked\n1              2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0  PC 17599  71.2833          C85        C\n3              4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0    113803  53.1000         C123        S\n6              7         0       1                            McCarthy, Mr. Timothy J    male  54.0      0      0     17463  51.8625          E46        S\n10            11         1       3                    Sandstrom, Miss. Marguerite Rut  female   4.0      1      1   PP 9549  16.7000           G6        S\n11            12         1       1                           Bonnell, Miss. Elizabeth  female  58.0      0      0    113783  26.5500         C103        S\n</code></pre> <p>Per eliminare invece una feature che presenta uno o pi\u00f9 dati mancanti nella sua interezza, agiamo per colonne, impostando <code>axis=1</code>:</p> <pre><code>&gt;&gt;&gt; df.dropna(axis=1)\n     PassengerId  Survived  Pclass                                               Name     Sex  SibSp  Parch            Ticket     Fare\n0              1         0       3                            Braund, Mr. Owen Harris    male      1      0         A/5 21171   7.2500\n1              2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female      1      0          PC 17599  71.2833\n2              3         1       3                             Heikkinen, Miss. Laina  female      0      0  STON/O2. 3101282   7.9250\n3              4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female      1      0            113803  53.1000\n4              5         0       3                           Allen, Mr. William Henry    male      0      0            373450   8.0500\n</code></pre> <p>Soglie per l'eliminazione di un campione o di una feature</p> <p>Eliminare interamente un campione (o addirittura una feature) per la presenza di un unico valore mancante potrebbe risultare una misura draconiana. Per evitare di far questo, possiamo decidere di impostare il parametro <code>how</code> ad <code>all</code>, nel qual caso il campione/feature sar\u00e0 eliminato solo se tutti i valori risultano essere mancanti:</p> <pre><code>&gt;&gt;&gt; df.dropna(axis=1, how='all')\n</code></pre> <p>In alternativa, possiamo impostare il parametro <code>thresh</code>, che ci permette di impostare una soglia minima di valori mancanti per eliminare il campione o la feature. Ad esempio, se volessimo eliminare tutte quelle feature che presentano almeno il \\(50\\%\\) di valori mancanti, dovremmo scrivere:</p> <pre><code>&gt;&gt;&gt; df.dropna(axis=1, thresh=0.5)\n</code></pre>"},{"location":"material/02_libs/03_pandas/04_functions/#opzione-2-riempimento-dei-dati-mancanti","title":"Opzione 2: riempimento dei dati mancanti","text":"<p>L'altra opzione messa a disposizione da Pandas \u00e8 quella del riempimento dei dati mancanti. Questa strada \u00e8 percorribile usando la funzione <code>fillna()</code>.</p> <p>Nell'interpretazione base, possiamo usare <code>fillna()</code> parametrizzandolo con il valore che vogliamo utilizzare. Ad esempio:</p> <pre><code>&gt;&gt;&gt; df.fillna(0) \n     PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked\n0              1         0       3                            Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   7.2500     0        S\n1              2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0          PC 17599  71.2833   C85        C\n2              3         1       3                             Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   7.9250     0        S\n3              4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803  53.1000  C123        S\n4              5         0       3                           Allen, Mr. William Henry    male  35.0      0      0            373450   8.0500     0        S\n</code></pre> <p>In questo caso, andremo ad associare il valore <code>0</code> a tutti i <code>NaN</code>. Un utilizzo pi\u00f9 raffinato prevede la specifica di un metodo di riempimento dei dati. Ad esempio, potremmo indicare a Pandas di riempire i <code>NaN</code> con i valori immediatamente precedenti:</p> <pre><code>&gt;&gt;&gt; df.fillna(method='ffill')\n     PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked\n0              1         0       3                            Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   7.2500   NaN        S\n1              2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0          PC 17599  71.2833   C85        C\n2              3         1       3                             Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   7.9250   C85        S\n3              4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803  53.1000  C123        S\n4              5         0       3                           Allen, Mr. William Henry    male  35.0      0      0            373450   8.0500  C123        S\n</code></pre> <p>In questo caso, notiamo come il valore di <code>Cabin</code> per la prima riga non venga riempito, perch\u00e9 non vi sono dati \"precedenti\" da cui attingere. Potremmo per\u00f2 porvi rimedio utilizzando in cascata due chiamate a <code>fillna()</code>:</p> <pre><code>&gt;&gt;&gt; df.fillna(method='ffill').fillna(method='bfill')\n     PassengerId  Survived  Pclass                                               Name     Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked\n0              1         0       3                            Braund, Mr. Owen Harris    male  22.0      1      0         A/5 21171   7.2500   C85        S\n1              2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1      0          PC 17599  71.2833   C85        C\n2              3         1       3                             Heikkinen, Miss. Laina  female  26.0      0      0  STON/O2. 3101282   7.9250   C85        S\n3              4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1      0            113803  53.1000  C123        S\n4              5         0       3                           Allen, Mr. William Henry    male  35.0      0      0            373450   8.0500  C123        S\n</code></pre> <p>Questa volta, al primo indice, <code>Cabin</code> diventa pari al valore del secondo indice, grazie alla chiamata in cascata di un <code>ffill</code> e di un <code>bfill</code> (ovvero, un riempimento \"all'indietro\"). Da notare anche l'influenza del parametro <code>axis</code>. Negli esempi precedenti, infatti, abbiamo ragionato per colonne; tuttavia, possiamo anche ragionare per righe, specificando <code>axis=1</code>:</p> <pre><code>&gt;&gt;&gt; df.fillna(method='ffill', axis=1)\n    PassengerId Survived Pclass                                               Name     Sex     Age SibSp Parch            Ticket     Fare  Cabin Embarked\n0             1        0      3                            Braund, Mr. Owen Harris    male    22.0     1     0         A/5 21171     7.25   7.25        S\n1             2        1      1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female    38.0     1     0          PC 17599  71.2833    C85        C\n2             3        1      3                             Heikkinen, Miss. Laina  female    26.0     0     0  STON/O2. 3101282    7.925  7.925        S\n3             4        1      1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female    35.0     1     0            113803     53.1   C123        S\n4             5        0      3                           Allen, Mr. William Henry    male    35.0     0     0            373450     8.05   8.05        S\n</code></pre> <p>Vediamo come la feature <code>Cabin</code> venga stavolta valorizzata con il valore di <code>Fare</code>.</p> <p>Suggerimento</p> <p>Va da s\u00e9 che la precedente trattazione sia a meno scopo illustrativo. Non ha infatti senso andare ad assegnare a <code>Cabin</code> un valore numerico, o far propagare i valori delle cabine precedenti/successive. In questo caso, infatti, potrebbe essere conveniente eliminare del tutto la feature o il campione. Come vedremo quando parleremo di Scikit-Learn, l'assegnazione dei valori mancanti ha significato soprattutto quando vi sono delle relazioni, dirette o indirette, tra i diversi campioni presenti nel dataset.</p>"},{"location":"material/02_libs/03_pandas/04_functions/#operazioni-statistiche-sui-dataframe","title":"Operazioni statistiche sui dataframe","text":"<p>Pandas ci mette a disposizione delle funzioni, simili a quelle offerte da NumPy, per calcolare delle statistiche per ciascuna delle colonne presenti in un DataFrame. Ad esempio, possiamo calcolare la media usando la funzione <code>mean()</code>:</p> <pre><code>&gt;&gt;&gt; df.mean()\nPassengerId    446.000000\nSurvived         0.383838\nPclass           2.308642\nAge             29.699118\nSibSp            0.523008\nParch            0.381594\nFare            32.204208\ndtype: float64\n</code></pre> <p>Ovviamente, esistono funzioni anche per calcolare varianza (<code>var()</code>), mediana (<code>median()</code>), deviazione standard (<code>std()</code>), e via discorrendo.</p> <p>Particolarmente interessante \u00e8 la funzione <code>describe()</code>, che ci mosta tutte le statistiche pi\u00f9 significative per ognuna delle feature considerate.</p> <pre><code>&gt;&gt;&gt; df.describe()\n       PassengerId    Survived      Pclass         Age       SibSp       Parch        Fare\ncount   891.000000  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\nmean    446.000000    0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\nstd     257.353842    0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\nmin       1.000000    0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n25%     223.500000    0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n50%     446.000000    0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n75%     668.500000    1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\nmax     891.000000    1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n</code></pre>"},{"location":"material/02_libs/04_visualization/01_matplotlib.en/","title":"01 matplotlib.en","text":"<p>Coming soon...</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/","title":"2.4.1 - Matplotlib","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Nelle lezioni precedenti, ci siamo limitati a visualizzare i risultati ottenuti usando l'output fornito dalla riga di comando o dal notebook Jupyter/Colab. Tuttavia, \u00e8 chiaro come questo modo di procedere sia giocoforza limitante: cosa ne \u00e8 di tutti i coloratissimi grafici che possiamo ammirare in siti ed articoli scientifici? Saranno per caso relegati esclusivamente al mondo di Excel?</p> <p>In realt\u00e0, per ottenerli dovremo necessariamente integrare il nostro ambiente di lavoro con altre librerie. Ne esistono diverse, ma la pi\u00f9 utilizzata \u00e8 senza dubbio Matplotlib, cui si pu\u00f2 affiancare Seaborn, che tratteremo in una delle prossime lezioni.</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/#setup-della-libreria","title":"Setup della libreria","text":"<p>Prima di utilizzare Matplotlib, dovremo ovviamente installare la libreria. Per farlo, abbiamo al solito le opzioni mostrate in appendice; di seguito, riportiamo l'opzione di installazione tramite <code>pip</code>:</p> <pre><code>pip install matplotlib\n</code></pre> <p>Passiamo poi ad importare la libreria all'interno del nostro programma. In particolare, il package pi\u00f9 utilizzato \u00e8 <code>pyplot</code> che, come dice la documentazione, altro non \u00e8 che un insieme di funzioni (palesemente) ispirate a MATLAB. Useremo quindi un alias per questo package:</p> <pre><code>import matplotlib.pylot as plt\n</code></pre> <p>Saremo a questo punto pronti per utilizzare le funzioni messe a disposizione da Matplotlib.</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/#il-primo-plot","title":"Il primo plot","text":"<p>Per creare il nostro primo plot, utilizziamo il seguente codice:</p> <pre><code>rng = np.random.default_rng(42)\nx = np.arange(1, 6)\ny = rng.integers(low=0, high=10, size=5)\nfig, ax = plt.subplots()\nax.plot(x, y)\nplt.show()\n</code></pre> <p>In particolare:</p> <ul> <li>alla riga 1, creiamo un generatore di numeri casuali;</li> <li>alla riga 2, definiamo tutti i valori di <code>x</code> compresi nell'intervallo tra 1 e 5 usando la funzione <code>numpy.arange()</code>;</li> <li>alla riga 3, definiamo tutti i valori di <code>y</code> come valori interi casuali compresi tra <code>0</code> e <code>10</code>;</li> <li>alla riga 4, creiamo una <code>figure</code> ed un <code>axes</code> mediante il metodo <code>subplots()</code>;</li> <li>alla riga 5, effettuiamo il plot su <code>ax</code>, mettendo come ascissa i valori di <code>x</code>, e come ordinata quelli di <code>y</code>;</li> <li>alla riga 6, chiamiamo il metodo <code>show()</code> per mostrare a schermo il grafico ottenuto.</li> </ul> <p>Se tutto \u00e8 andato per il verso giusto, dovremmo vedere a schermo l'immagine mostrata nella figura 1.</p> <p> </p> Figura 1 - Un semplice plot in Matplotlib <p>Suggerimento</p> <p>Se avete seguito pedissequamente il tutorial, a schermo dovrebbe essere visualizzata esattamente l'immagine mostrata nella figura 1. Questo perch\u00e9 al generatore di numeri (pseudo) casuali viene passato il parametro <code>seed</code>, usato come base per la generazione degli stessi, che risulteranno quindi essere sempre gli stessi, indipendentemente dall'iterazione.</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/#figure-ed-assi","title":"Figure ed assi","text":"<p>L'esempio precedente ci permette di illustrare in poche righe di codice tutti i concetti su cui si basa Matplotlib. Tuttavia, \u00e8 opportuno scendere maggiormente nel dettaglio.</p> <p>In particolare, alla base del funzionamento di Matplotlib ci sono quattro classi fondamentali.</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/#la-classe-figure","title":"La classe <code>Figure</code>","text":"<p>Per prima cosa, ci sono le <code>Figure</code>, rappresentative dell'intera area mostrata a schermo da Matplotlib. Un oggetto di questa classe conterr\u00e0 un numero arbitrario di elementi, permettendone visualizzazione e contestuale manipolazione.</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/#la-classe-axes","title":"La classe <code>Axes</code>","text":"<p>Gli oggetti di classe <code>Axes</code>, rappresentano l'area della <code>Figure</code> all'interno della quale saranno visualizzati i dati. La relazione tra <code>Figure</code> ed <code>Axes</code> \u00e8 strettamente gerarchica: in pratica, una <code>Figure</code> pu\u00f2 avere diversi <code>Axes</code>, ma ogni <code>Axes</code> appartiene esclusivamente ad una <code>Figure</code>.</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/#la-classe-axis","title":"La classe <code>Axis</code>","text":"<p>All'interno di un oggetto <code>Axes</code> troviamo poi due o tre oggetti di tipo <code>Axis</code>, ognuno dei quali rappresenta l'asse vero e proprio. In altri termini, avremo due <code>Axis</code> per i plot bidimensionali, rappresentativi degli assi \\(x\\) ed \\(y\\), ed un terzo <code>Axis</code> per i plot tridimensionali, rappresentativo ovviamente dell'asse \\(z\\). Gli oggetti <code>Axis</code> ci permettono quindi di definire gli intervalli dati, l'eventuale griglia, e via discorrendo.</p> <p><code>Axes</code> ed <code>Axis</code></p> <p>Fate attenzione a non confondere gli <code>Axes</code> con gli <code>Axis</code>! In pratica: gli <code>Axes</code> sono i singoli plot, mentre gli <code>Axis</code> sono gli assi contenuti in ciascun plot.</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/#la-classe-artist","title":"La classe <code>Artist</code>","text":"<p>L'ultimo concetto fondamentale di Matplotlib \u00e8 quello degli artist, oggetti derivati dalla classe base <code>Artist</code>, delegata al rendering vero e proprio dei plot.</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/#rivisitazione-dellesempio-precedente","title":"Rivisitazione dell'esempio precedente","text":"<p>Torniamo brevemente al precedente snippet. La funzione <code>subplots()</code> ci servir\u00e0 quindi a creare un oggetto di tipo <code>Figure</code> assieme agli <code>Axes</code> desiderati:</p> <pre><code>fig, ax = plt.subplots()\n</code></pre> <p>A questo punto, possiamo plottare i valori di <code>x</code> ed <code>y</code> su nostro oggetto <code>Axes</code>, che conterr\u00e0 a sua volta due <code>Axis</code>, uno per l'asse delle \\(x\\), e l'altro per l'asse delle \\(y\\):</p> <pre><code>ax.plot(x, y)\n</code></pre> <p>Vediamo adesso qualche esempio pi\u00f9 significativo</p>"},{"location":"material/02_libs/04_visualization/01_matplotlib/#esempio-1-plot-di-piu-funzioni","title":"Esempio 1: Plot di pi\u00f9 funzioni","text":"<p>L'obiettivo di questo esempio \u00e8 mostrare su uno stesso <code>Axes</code> due diverse funzioni. In particolare, scegliamo come funzioni da rappresentare una retta ed una funzione seno.</p> <p>Per prima cosa, definiamo i nostri dati:</p> <pre><code>x = np.arange(0., 10., 0.01)\ny_1 = 1 + 2 * x\ny_2 = np.sin(x)\n</code></pre> <p>Alla riga 1, definiamo l'intervallo sull'asse \\(x\\) come quello compreso tra \\(0\\) e \\(10\\) e campionato a passo \\(0.01\\). Cos\u00ec facendo, avremo un campionamento molto fitto, con un totale di \\(1000\\) punti considerati. Alla riga 2 definiamo la nostra retta, la cui equazione sar\u00e0 \\(y = 2x + 1\\), mentre alla riga 3 andremo a definire la funzione seno.</p> <p>Adesso, creiamo la nostra <code>Figure</code> con relativo <code>Axes</code>, ed effettuiamo il plot di entrambe le funzioni.</p> <pre><code>fig, ax = plt.subplots()\nax.plot(x, y_1, label='Retta')\nax.plot(x, y_2, label='Funzione sinusoidale')\n</code></pre> <p>Notiamo la presenza del parametro <code>label</code> che indica l'etichetta assegnata ai due plot; questa sar\u00e0 utilizzata successivamente per generare la legenda. Passiamo adesso ad impostare il titolo e le label sugli assi \\(x\\) e \\(y\\) usando rispettivamente le funzioni <code>set_title()</code>, <code>set_xlabel()</code> e <code>set_ylabel()</code>:</p> <pre><code>ax.set_title('Plot di due funzioni matematiche')\nax.set_xlabel('Asse x')\nax.set_ylabel('Asse y')\n</code></pre> <p>Usiamo adesso la funzione <code>grid()</code> per mostrare una griglia sulla figura, e la funzione <code>legend()</code> per far apparire la legenda che descrive le funzioni visualizzate.</p> <pre><code>ax.legend()\nax.grid()\n</code></pre> <p>In ultimo, mostriamo a schermo la figura con la funzione <code>show()</code>:</p> <pre><code>plt.show()\n</code></pre> <p>Il risultato ottenuto \u00e8 mostrato in figura 2.</p> <p> </p> Figura 2 - Plot di due funzioni"},{"location":"material/02_libs/04_visualization/01_matplotlib/#esempio-2-subplot-multipli","title":"Esempio 2: Subplot multipli","text":"<p>Alle volte, pu\u00f2 essere conveniente affiancare pi\u00f9 subplot all'interno di un unico plot principale. In tal senso, abbiamo in precedenza detto che possiamo definire pi\u00f9 <code>Axes</code> per un'unica <code>Figure</code>; per farlo, possiamo parametrizzare la funzione <code>subplots(i, j)</code>, in maniera tale che vengano creati \\(i \\times j\\) plot all'interno della stessa figura.</p> <p>Per creare 2 subplot in \"riga\", ad esempio, parametrizzeremo <code>subplots()</code> come segue:</p> <pre><code>fig, (ax_1, ax_2) = plt.subplots(2, 1)\n</code></pre> <p>Notiamo la presenza di due <code>Axes</code>, rispettivamente <code>ax_1</code> ed <code>ax_2</code>. Ognuno di questi potr\u00e0 essere trattato come descritto in precedenza.</p> <p>Per prima cosa, usiamo la funzione <code>suptitle()</code> sulla <code>Figure</code> <code>fig</code> per dare un titolo all'intero plot:</p> <pre><code>fig.suptitle('Due subplot di pi\u00f9 funzioni matematiche')\n</code></pre> <p>A questo punto, procediamo ad effettuare i plot sui relativi assi:</p> <pre><code># Primo subplot\nax_1.plot(x, y_1, label='Retta')\nax_1.set_xlabel('Asse x')\nax_1.set_ylabel('Asse y')\nax_1.legend()\nax_1.grid()\n# Secondo subplot\nax_2.plot(x, y_2, label='Funzione sinusoidale')\nax_2.set_xlabel('Asse x')\nax_2.set_ylabel('Asse y')\nax_2.legend()\nax_2.grid()\n</code></pre> <p>Mostriamo quindi a schermo il plot:</p> <pre><code>plt.show()\n</code></pre> <p>Il risultato sar\u00e0 simile a quello mostrato in figura 3:</p> <p> </p> Figura 3 - Subplot multipli"},{"location":"material/02_libs/04_visualization/01_matplotlib/#esempio-3-istogramma","title":"Esempio 3: Istogramma","text":"<p>Abbiamo gi\u00e0 parlato degli istogrammi in NumPy. Tuttavia, un istogramma raggiunge la massima espressivit\u00e0 possibile quando ne si utilizza la rappresentazione visiva. In tal senso, Matplotlib ci offre una funzione apposita chiamata <code>hist()</code>.</p> <p>Proviamo ad utilizzarla. Per farlo, creiamo in primis un vettore di \\(1000\\) numeri interi casuali compresi tra \\(0\\) e \\(100\\).</p> <pre><code>x = rng.integers(low=0, high=100, size=1000)\n</code></pre> <p>Al solito, creiamo la nostra figura, ed usiamo la funzione <code>hist()</code> passandogli il vettore <code>x</code> creato in precedenza e il parametro <code>density</code>, che ci permetter\u00e0 di normalizzare l'istogramma (ovvero, fare in modo tale che la sommatoria dei singoli bin sia esattamente pari ad 1).</p> <pre><code>fig, ax = plt.subplots()\nax.hist(x, edgecolor='black', linewidth=1.2, density=True)\n</code></pre> <p>Notiamo anche l'uso dei parametri <code>edgecolor</code>, che permette di impostare il colore del bordo di ciascuna barra dell'istogramma, e <code>linewidth</code>, che consente di specificarne lo spessore.</p> <p>Al solito, usiamo i metodi opportuni per impostare titolo e label degli assi, e mostriamo la figura.</p> <pre><code>ax.set_xlabel('Bin')\nax.set_ylabel('Conteggio dei singoli elementi')\nax.set_title('Esempio di istogramma')\n\nplt.show()\n</code></pre> <p>Il risultato sar\u00e0 simile a quello mostrato nella figura 4.</p> <p> </p> Figura 4 - Istogramma"},{"location":"material/02_libs/04_visualization/01_matplotlib/#esempio-4-scatter-plot","title":"Esempio 4: Scatter plot","text":"<p>Lo scatter plot \u00e8 una modalit\u00e0 alternativa di mostrare la distribuzione dei dati lungo gli assi \\(x\\), \\(y\\) ed, opzionalmente, \\(z\\). In particolare, questo tipo di visualizzazione differisce dal normale plot in quanto i singoli punti dati non sono collegati da una linea, ma rappresentati in modo discreto, ed \u00e8 particolarmente utile quando si vuole valutare visivamente la distribuzione di una serie di dati in due o tre dimensioni. Ovviamente, Matplotlib ci mette a disposizione un'apposita funzione per la visualizzazione di questo tipo di plot, chiamata <code>scatter()</code>.</p> <p>Il funzionamento \u00e8 totalmente analogo al <code>plot()</code>. Ad esempio, considerando i valori di <code>x</code> ed <code>y</code> utilizzati nel primo esempio del tutorial, potremmo generare uno scatter plot come segue:</p> <pre><code>rng = np.random.default_rng(42)\nx = np.arange(1, 6)\ny = rng.integers(low=0, high=10, size=5)\nfig, ax = plt.subplots()\nax.set_xlabel('Asse x')\nax.set_ylabel('Asse y')\nax.set_title('Esempio di scatter plot')\nax.plot(x, y)\nplt.show()\n</code></pre> <p>Il risultato sar\u00e0 simile a quello mostrato in figura 5.</p> <p> </p> Figura 5 - Scatter plot"},{"location":"material/02_libs/04_visualization/01_matplotlib/#note-finali-salvataggio-e-chiusura-di-una-figura","title":"Note finali: salvataggio e chiusura di una figura","text":"<p>Chiudiamo questa breve carrellata specificando che, per salvare una figura, \u00e8 necessario usare la funzione <code>savefig()</code>, che accetta come primo parametro il nome del file su cui salveremo l'immagine:</p> <pre><code>plt.savefig('file_name.png')\n</code></pre> <p>Oltre a questo, dei parametri molto utili da utilizzare sono <code>dpi</code>, che ci permette di specificare la risoluzione della figura, e <code>bbox_inches</code>, per il quale il suggerimento \u00e8 di impostarlo a <code>tight</code> per evitare eccessivi spazi bianchi attorno alla figura (o, al contrario, il taglio di parti delle label).</p> <p>Infine, facciamo un cenno alla funzione <code>close()</code>, delegata alla chiusura di un oggetto di tipo <code>Figure</code>:</p> <pre><code>plt.close()\n</code></pre> <p>La funzione <code>close()</code> assume particolare rilevanza nel momento in cui vogliamo, ad esempio, plottare diverse figure al variare di alcune condizioni, come ad esempio in un ciclo. Infatti, se non chiudessimo la figura attuale al termine di ciascuna iterazione, Matplotlib andrebbe a disegnare la nuova funzione sempre sulla stessa figura. Ad esempio:</p> <pre><code>for i in range(5):\n    plt.plot(x, y*i)\nplt.show()\n</code></pre> <p>In questo caso, dato che non stiamo chiudendo la figura, avremo un risultato come visto in figura 6.</p> <p> </p> Figura 6 - Plot senza l'invocazione di `close()` <p>Se invece chiamassimo sempre la <code>close()</code>, all'ultima iterazione avremmo il plot mostrato in figura 7.</p> <pre><code>for i in range(5):\n    plt.plot(x, y*i)\n    plt.show()\n    plt.close()\n</code></pre> <p> </p> Figura 7 - Plot con l'invocazione di `close()` <p>Nella prossima lezione, approfondiremo l'utilizzo di una libreria discendente da Matplotlib, ovvero Seaborn.</p>"},{"location":"material/02_libs/04_visualization/02_seaborn.en/","title":"02 seaborn.en","text":"<p>Coming soon...</p>"},{"location":"material/02_libs/04_visualization/02_seaborn/","title":"2.4.2 - Seaborn","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Seaborn \u00e8 una libreria che estende Matplotlib aggiungendone diverse funzionalit\u00e0, tutte nell'ottica della data analysis, e sulla scia di quello che abbiamo presentato in Pandas in una delle precedenti lezioni. Ci\u00f2 permette quindi di mantenere un'interfaccia molto simile a quella di Matplotlib, estendendone al contempo le possibilit\u00e0. Vediamo qualche esempio.</p>"},{"location":"material/02_libs/04_visualization/02_seaborn/#setup-della-libreria","title":"Setup della libreria","text":"<p>Come in ogni altro caso, partiamo dall'installazione della libreria, che potr\u00e0 essere fatta usando il seguente comando:</p> <pre><code>pip install seaborn\n</code></pre> <p>Una volta completata l'installazione, potremo importare Seaborn mediante un alias:</p> <pre><code>import seaborn as sns\n</code></pre>"},{"location":"material/02_libs/04_visualization/02_seaborn/#analisi-esplorativa-dei-dati","title":"Analisi esplorativa dei dati","text":"<p>Come abbiamo detto in precedenza, Seaborn risulta utile in diverse situazioni collegate alla data analysis.</p> <p>In tal senso, supponiamo di voler effettuare un'analisi dei dati esplorativa (Exploratory Data Analysis, o EDA), visualizzando le relazioni che intercorrono tra le diverse feature presenti all'interno di un certo dataset. Nel nostro esempio, utilizzeremo il dataset tips, contenuto all'interno di Seaborn, il quale descrive le caratteristiche dei clienti di un generico ristorante, ed in particolare:</p> <ul> <li>total_bill: il conto totale;</li> <li>tip: la mancia lasciata dal cliente pagante;</li> <li>sex: il genere del cliente pagante;</li> <li>smoker: il fatto che il tavolo sia o meno in zona fumatori;</li> <li>day: la giornata nella quale il pasto \u00e8 stato consumato;</li> <li>time: la parte della giornata nella il pasto \u00e8 stato consumato (in particolare, pranzo o cena);</li> <li>size: il numero di clienti seduti al tavolo.</li> </ul> <p>Partiamo caricando il dataset. Per farlo, utilizziamo il metodo <code>load_dataset()</code>, che ci permette di scaricare rapidamente il dataset passato come parametro, salvandolo in un apposito <code>DataFrame</code>:</p> <pre><code>tips = sns.load_dataset('tips')\n</code></pre> <p>I dataset</p> <p>L'elenco dei dataset integrati in Seaborn \u00e8 presente a questo indirizzo.</p> <p>Partiamo ispezionando il <code>DataFrame</code> appena scaricato mediante il comando <code>head()</code>. Vediamo che gli elementi precedentemente descritti sono organizzati in questo modo:</p> <pre><code>  total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4\n</code></pre> <p>La struttura del <code>DataFrame</code> \u00e8 quindi la seguente:</p> <ul> <li>ogni riga \u00e8 associata ad una specifica ordinazione;</li> <li>le colonne sono associate rispettivamente a conto (<code>total_bill</code>), mancia (<code>tip</code>), genere (<code>sex</code>), fumatore (<code>smoker</code>), giorno (<code>day</code>), orario (<code>time</code>) e numero di attendenti (<code>size</code>), come descritto in precedenza.</li> </ul>"},{"location":"material/02_libs/04_visualization/02_seaborn/#visualizzare-le-relazioni-tra-dati","title":"Visualizzare le relazioni tra dati","text":"<p>La prima funzione che andremmo ad utilizzare \u00e8 la <code>relplot()</code>, la quale ci permette di analizzare rapidamente le relazioni intercorrenti tra le diverse feature del dataset. Ad esempio, possiamo vedere come cambiano il conto e la mancia al variare della giornata:</p> <pre><code>sns.relplot(\n    data=tips,\n    x='total_bill',\n    y='tip',\n    col='day')\n</code></pre> <p>In particolare:</p> <ul> <li>il parametro <code>data</code> viene valorizzato con <code>tips</code>, ovvero il <code>DataFrame</code> che contiene i dati che vogliamo analizzare;</li> <li>il parametro <code>x</code>, che definisce i dati visualizzati sull'asse delle ascisse, sar\u00e0 parametrizzato a <code>total_bill</code>, ovvero il nome della feature che vogliamo visualizzare sul suddetto asse;</li> <li>in maniera simile al precedente, <code>y</code>, che definisce i dati visualizzati sull'asse delle ordinate, sar\u00e0 parametrizzato a <code>tip</code>;</li> <li>il parametro <code>col</code>, invece, generer\u00e0 tanti grafici quante sono le possibili valorizzazioni della colonna <code>day</code>, ognuno dei quali corrispondente all'andamento dei parametri indicati su <code>x</code> ed <code>y</code> per quello specifico giorno.</li> </ul> <p>I risultati sono mostrati nella figura 1.</p> <p> </p> Figura 1 - Comparazione tra conto e mance al variare del giorno in Seaborn <p>Proviamo adesso a valutare come cambia il rapporto conto/mance in base al sesso:</p> <pre><code>sns.relplot(\n    data=tips,\n    x='total_bill',\n    y='tip',\n    col='sex',\n    size='tip')\n</code></pre> <p>I risultati sono mostrati in figura 2. Notiamo come, impostando il parametro <code>size</code>, potremo modificare la dimensione di ciascun punto, rendendola direttamente proporzionale alla mancia data.</p> <p> </p> Figura 2 - Comparazione tra conto e mance al variare del sesso. La dimensione di ciascun punto \u00e8 data dall'entit\u00e0 della mancia <p>Una funzione simile alla <code>relplot()</code> \u00e8 la <code>lmplot()</code>, la quale offre anche un'approssimazione ai minimi quadrati dei dati impostati sugli assi cartesiani. Ad esempio, proviamo a mostrare lo stesso rapporto illustrato in figura 2 con la <code>lmplot()</code>:</p> <pre><code>sns.lmplot(\n    data=tips,\n    x='total_bill',\n    y='tip',\n    col='time',\n    hue='day')\n</code></pre> <p>Da notare come in questo caso abbiamo specificato il parametro <code>hue</code>, che regola la tinta dei punti dati. I risultati sono mostrati in figura 3.</p> <p> </p> Figura 3 - Comparazione tra conto e mance con approssimazione dei dati ai minimi quadrati"},{"location":"material/02_libs/04_visualization/02_seaborn/#analisi-della-distribuzione-dati","title":"Analisi della distribuzione dati","text":"<p>Seaborn ci permette di effettuare un'analisi della distribuzione delle variabili all'interno del dataset sotto analisi. Per farlo, usiamo la funzione <code>displot()</code>, che ci permette di visualizzare la distribuzione dei dati sulla base di determinate condizioni sfruttando un istogramma.</p> <p>Ad esempio, potremmo visualizzare la distribuzione dei clienti in base al loro genere ed al momento della giornata in cui effettuano la consumazione:</p> <pre><code>sns.displot(\n    data=tips,\n    x='sex',\n    col='time',\n    kde=True)\n</code></pre> <p>Il risultato \u00e8 mostrato in figura 4:</p> <p> </p> Figura 4 - Distribuzione dei clienti in base al loro genere ed al momento della giornata in cui viene effettuata la consumazione <p>Specificando il parametro <code>kde</code>, \u00e8 possibile ottenere un'approssimazione della distribuzione mediante kernel density estimation, come mostrato in figura 5.</p> <p> </p> Figura 5 - Distribuzione dei clienti in base al loro genere ed al momento della giornata in cui viene effettuata la consumazione. La visualizzazione sfrutta la KDE"},{"location":"material/02_libs/04_visualization/02_seaborn/#plot-di-dati-categorici","title":"Plot di dati categorici","text":"<p>Seaborn offre anche dei plot specializzati per la creazione e visualizzazione di dati (o feature) di tipo categorico, ovvero dati appartenenti ad una tra diverse possibili categorie. In tal senso, un esempio di feature categorica \u00e8 il genere dei clienti del ristorante, che nel dataset sono soltanto uomini o donne.</p> <p>Dati categorici e numerici</p> <p>Oltre ai dati categorici, esistono i dati numerici che, ovviamente, rappresentano dei numeri.</p> <p>I plot di questo tipo possono essere generati mediante la funzione <code>catplot()</code>, delegata alla definizione di plot a diversi livelli di granularit\u00e0, come ad esempio i violin plot.</p> <pre><code>sns.catplot(\n    data=tips,\n    kind='violin',\n    x='day',\n    y='tip',\n    hue='sex',\n    split=True)\n</code></pre> <p>In particolare, il grafico mostrato in figura 6 descrive la distribuzione delle mance giorno per giorno al variare del genere del cliente.</p> <p> </p> Figura 6 - *Violin plot* descrivente la distribuzione delle mance giorno per giorno al variare del sesso dell'avventore <p>Catplot con dati non categorici</p> <p>In realt\u00e0, \u00e8 possibile usare la <code>catplot()</code> con dati numerici. Tuttavia, vi \u00e8 un elevato rischio che il risultato non sia interpretabile, in quanto la funzione assegner\u00e0 una categoria ad ogni possibile valore assunto dalla feature di riferimento, il che ovviamente comporter\u00e0 l'illeggibilit\u00e0 del grafico nel caso di valori reali.</p>"},{"location":"material/02_libs/04_visualization/02_seaborn/#heatmap","title":"Heatmap","text":"<p>Un'ultima funzione che vale la pena menzionare \u00e8 quella che ci permette di visualizzare le heatmap, ovvero delle strutture grafiche che ci permettono di visualizzare gli intervalli in cui ricadono i valori di diversi tipi di matrici. Questa funzione \u00e8, per l'appunto, chiamata <code>heatmap()</code>, e richiede in ingresso almeno il parametro relativo alla matrice da cui sar\u00e0 estratta la figura. Ad esempio:</p> <pre><code>ar = np.array([[5, 12], [4, 3]])\nsns.heatmap(\n    ar,\n    cmap='jet',\n    annot=True,\n    xticklabels=False,\n    yticklabels=False)\n</code></pre> <p>Nella precedente invocazione della funzione <code>heatmap()</code> specifichiamo i parametri indicati in modo da passare un array (o similari) come primo argomento, seguito da una colormap, ovvero i colori da utilizzare. Specifichiamo inoltre che vogliamo inserire i valori dell'array su ciascuna delle celle dell'heatmap (mediante il parametro <code>annot</code>) e che non vogliamo visualizzare i label sugli assi \\(x\\) e \\(y\\) (<code>xticklabels</code> ed <code>yticklabels</code> rispettivamente). Otterremo il risultato mostrato in figura 7:</p> <p> </p> Figura 7 - Un esempio di heatmap <p>Usi delle heatmap</p> <p>Le heatmap sono molto utili in diverse situazioni, tra cui la descrizione dei risultati degli algoritmi di machine learning mediante le matrici di confusione, l'analisi di correlazione, e la visualizzazione delle mappe di attivazione in caso di interpretabilit\u00e0 delle reti neurali.</p>"},{"location":"material/02_libs/04_visualization/exercises/exercises/","title":"Esercitazione 4 - Visualizzazione dei dati in Python","text":""},{"location":"material/02_libs/04_visualization/exercises/exercises/#esercizio-41","title":"Esercizio 4.1","text":"<p>Utilizzare Seaborn per visualizzare come si distribuisce l'et\u00e0 dei diversi passeggeri del Titanic sulla base del loro genere. Visualizzare inoltre il rapporto tra l'et\u00e0 ed il numero di familiari, sempre sulla base del genere.</p>"},{"location":"material/02_libs/04_visualization/exercises/exercises/#esercizio-42","title":"Esercizio 4.2","text":"<p>Effettuiamo un'analisi esplorativa del dataset Titanic. In particolare, partiamo da una serie di domande di ricerca, e sfruttiamo Pandas e Seaborn per rispondere in maniera qualitativa.</p>"},{"location":"material/02_libs/04_visualization/exercises/exercises/#esercizio-43","title":"Esercizio 4.3","text":"<p>Effettuiamo un'analoga analisi sul dataset Tips.</p>"},{"location":"material/02_libs/04_visualization/exercises/solutions/","title":"Esercitazione 4 - Visualizzazione dei dati in Python (Soluzioni)","text":"<p>Soluzioni</p> <p>L'implementazione delle soluzioni \u00e8 disponibile questo notebook.</p>"},{"location":"material/02_libs/04_visualization/exercises/solutions/#esercizio-41","title":"Esercizio 4.1","text":"<p>Per prima cosa, leggiamo il dataframe:</p> <pre><code>df = pd.read_csv('titanic.csv')\n</code></pre> <p>Per visualizzare la distribuzione dell'et\u00e0 dei diversi passeggeri del Titanic in base al loro genere usiamo un <code>displot()</code>:</p> <pre><code>sns.displot(\n    data=df,\n    x='Age',\n    col='Sex')\n</code></pre> <p>Per visualizzare inoltre il rapporto tra et\u00e0 e numero di fratelli/sorelle/coniugi in base al genere del passeggero usiamo un <code>catplot()</code>:</p> <pre><code>sns.catplot(\n    data=df,\n    kind='violin',\n    x='SibSp',\n    y='Age',\n    hue='Sex',\n    split=True)\n</code></pre>"},{"location":"material/02_libs/05_scipy/exercises/","title":"E11 - Introduzione a SciPy","text":""},{"location":"material/02_libs/05_scipy/exercises/#esercizio-e111","title":"Esercizio E11.1","text":"<p>Scrivere una funzione che restituisca <code>True</code> se la matrice passata in ingresso \u00e8 invertibile, <code>False</code> altrimenti. Usare SciPy.</p>"},{"location":"material/02_libs/05_scipy/exercises/#soluzione-s101","title":"Soluzione S10.1","text":"<p>Ecco una possibile soluzione:</p> <pre><code>from scipy import linalg\n\ndef invertibile(mat):\n    \"\"\" Usiamo un operatore ternario.\n\n    Il risultato \u00e8 analogo alla seguente:\n    if linalg.det(mat) != 0.:\n        return True\n    else:\n        return False\n    \"\"\"\n    return True if linalg.det(mat) != 0. else False\n</code></pre>"},{"location":"material/02_libs/05_scipy/exercises/#esercizio-e112","title":"Esercizio E11.2","text":"<p>Scrivere una classe che, incorporando la funzione precedente, permetta di invertire una matrice.</p>"},{"location":"material/02_libs/05_scipy/exercises/#soluzione-s112","title":"Soluzione S11.2","text":"<p>Ecco una possibile soluzione:</p> <pre><code>from scipy import linalg\nimport warnings\n\nclass InversoreMatrici():\n\n    def __init__(self, mat):\n        self.mat = mat\n        self.invertibilita = mat\n\n    @property\n    def mat(self):\n        return self.__mat\n\n    @mat.setter\n    def mat(self, value):\n        if value is None:\n            raise ValueError('La matrice non pu\u00f2 essere nulla')\n        self.__mat = value\n\n    @property\n    def inv(self):\n        return self.__inv\n\n    @inv.setter\n    def inv(self, value):\n        if value is None:\n            raise ValueError(\"L'inversa non pu\u00f2 essere nulla\")\n        self.__inv = value\n\n    @property\n    def invertibilita(self):\n        return self.__invertibilita\n\n    @invertibilita.setter\n    def invertibilita(self, value):\n        if value is None:\n            raise ValueError(\"La determinazione dell'invertibilit\u00e0 non pu\u00f2 essere nulla\")\n        self.__invertibilita = True if linalg.det(value) != 0. else False\n\n    def inverti(self):\n        if self.invertibilita:\n            self.inv = linalg.inv(self.mat)\n        else:\n            warnings.warn('La matrice non \u00e8 invertibile')\n\na = np.array([[1, 2], [2, 5]])\ni = InversoreMatrici(a)\ni.inverti()\ni.inv\n</code></pre>"},{"location":"material/02_libs/05_scipy/lecture/","title":"6 - L'ecosistema SciPy","text":"<p>Ai lettori pi\u00f9 attenti pu\u00f2 apparire evidente come tutte le librerie viste finora facciano parte di una sorta di \"ecosistema\" pensato per permettere un'interazione tra tipi e classi il quanto pi\u00f9 possibile \"semplice\" e coesa.</p> <p>Questo \u00e8 dovuto al fatto che librerie come NumPy, Matplotlib, Pandas e Seaborn fanno tutte parte di un unico ecosistema chiamato SciPy, pensato per dare delle fondamenta comuni su cui costruire l'intera disciplina del calcolo scientifico in Python.</p> <p>Tuttavia, abbiamo omesso una delle librerie fondamentali di questo ecosistema, talmente importante che prende il nome del framework stesso: ovviamente, stiamo parlando della libreria SciPy.</p>"},{"location":"material/02_libs/05_scipy/lecture/#la-libreria-scipy","title":"La libreria SciPy","text":"<p>La libreria SciPy presenta un vastissimo insieme di algoritmi e funzioni matematiche costruite a partire dagli oggetti definiti da NumPy.</p> <p>Al solito, la libreria va installata usando, ad esempio <code>pip</code>:</p> <pre><code>pip install scipy\n</code></pre> <p>In questa giocoforza brevissima introduzione, vedremo alcune delle potenzialit\u00e0 di SciPy, basandoci su un paio di casi d'uso (pi\u00f9 o meno) reali.</p>"},{"location":"material/02_libs/05_scipy/lecture/#validazione-empirica-di-due-distribuzioni","title":"Validazione empirica di due distribuzioni","text":"<p>Proviamo a vedere come viene visualizzato il valore (teorico) assunto da due distribuzioni di probabilit\u00e0 \"classiche\", ovvero la distribuzione uniforme e quella normale.</p> <p>Vediamo come comparare visivamente il valore teorico assunto da due distribuzioni di probabilit\u00e0 \"standard\" (ovvero la uniforme e la normale) e l'istogramma ottenuto a partire da un elevato numero di elementi generati casualmente ma appartenenti a quella distribuzione.</p> <p>In primis, iniziamo importando i moduli <code>norm</code> ed <code>uniform</code> dal package <code>stats</code>, atti a modellare tutte le istruzioni riguardanti le distribuzioni normali ed uniformi:</p> <pre><code>from scipy.stats import norm, uniform\n</code></pre> <p>Generiamo adesso 100 campioni equidistanziati e compresi tra l'1 ed il 99 percentile delle distribuzioni:</p> <pre><code>x_1 = np.linspace(norm.ppf(0.01), norm.ppf(0.99), 100)\nx_2 = np.linspace(uniform.ppf(0.01), uniform.ppf(0.99), 100)\n</code></pre> <p>Stiamo usando la funzione <code>linspace()</code> per generare dei campioni equidistanti tra loro, compresi tra <code>dist.ppf(0.01)</code> e <code>dist.ppf(0.99)</code>. L'oggetto <code>dist</code> pu\u00f2 essere sia <code>norm</code> che <code>uniform</code>, mentre <code>ppf(0.01)</code> rappresenta l'1-percentile della distribuzione (e, analogamente, <code>ppf(0.99)</code> rappresenta il 99-percentile). In parole povere, stiamo generando cento campioni equidistanti tra l'1-percentile ed il 99-percentile della distribuzione <code>dist</code>.</p> <p>Successivamente, utilizziamo la funzione <code>rvs()</code> per generare casualmente un \"gran\" numero di valori che per\u00f2 siano distribuiti secondo le due distribuzioni considerate:</p> <pre><code>r_1 = norm.rvs(size=1000)\nr_2 = uniform.rvs(size=1000)\n</code></pre> <p>A questo punto, possiamo plottare l'istogramma dei valori <code>r_i</code>, e verificare che segua la distribuzione di probabilit\u00e0 <code>pdf(x)</code> per ciascuno dei due tipi di distribuzione. Ricordiamo di inserire il valore <code>density=True</code> per normalizzare l'istogramma.</p> <p>Il risultato dovrebbe essere simile a quello mostrato in figura:</p> <p></p>"},{"location":"material/02_libs/05_scipy/lecture/#calcolo-del-determinante-e-dellinversa","title":"Calcolo del determinante e dell'inversa","text":"<p>SciPy offre anche la possibilit\u00e0 di effettuare calcoli algebrici grazie ad un numero di funzioni molto pi\u00f9 elevato rispetto a quelle presenti in NumPy.</p> <p>Per fare un rapido esempio, vediamo come \u00e8 possibile calcolare il determinante e l'inversa di una matrice.</p> <pre><code>from scipy import linalg\n\n# ... matrice mat creata sotto forma di array NumPy\n\n# Determinante\nd = linalg.det(mat)\n# Inversa\ni = linalg.inv(mat)\n</code></pre> <p>Nota</p> <p>E' molto semplice notare come la sintassi richiami quella di NumPy e, in realt\u00e0, anche il funzionamento sia il medesimo, per cui \u00e8 possibile usare indifferentemente entrambe le librerie. Dove SciPy \"spicca\" \u00e8 in tutte quelle funzioni che non sono presenti in NumPy.</p>"},{"location":"material/02_libs/05_scipy/lecture/#filtraggio-di-un-segnale","title":"Filtraggio di un segnale","text":"<p>SciPy ha al suo interno diverse librerie per l'elaborazione dei segnali a diverse dimensionalit\u00e0.</p> <p>Per fare un esempio, proviamo ad utilizzare un filtro di Savitzky-Golay su un array monodimensionale mediante la funzione <code>savgol_filter()</code>.</p> <p>Creiamo un array casuale mediante NumPy:</p> <pre><code>noisy = np.random.normal(0, 1, size=(100,))\n</code></pre> <p>Filtriamo questo segnale usando un filtro di Savitzky-Golay con finestra di lunghezza pari a 7 campioni e mediante un polinomio approssimante di secondo grado:</p> <pre><code>from scipy.signal import savgol_filter\n\nfiltered = savgol_filter(noisy, 7, 2)\n</code></pre> <p>Creiamo il plot:</p> <pre><code>plt.plot(np.arange(100), noisy, alpha=0.5, label='Segnale originale')\nplt.plot(np.arange(100), filtered, label='Segnale filtrato')\nplt.grid()\nplt.legend()\nplt.show()\n</code></pre> <p>Otterremo un risultato simile a quello mostrato in figura:</p> <p></p>"},{"location":"material/03_ml/01_intro/01_intro_ai/","title":"3.1.1 - Introduzione all'intelligenza artificiale","text":"<p>Partiamo dalla definizione che d\u00e0 l'Enciclopedia Treccani dell'Intelligenza Artificiale (IA), ovvero:</p> <p>Definizione di IA</p> <p>L'Intelligenza artificiale \u00e8 una disciplina che studia se ed in che modo si possono riprodurre i processi dei cervelli biologici mediante l'uso di un computer.</p>"},{"location":"material/03_ml/01_intro/01_intro_ai/#tipologie-di-intelligenza-artificiale","title":"Tipologie di Intelligenza artificiale","text":"<p>L'IA pu\u00f2 essere classificata secondo tre diverse categorie.</p>"},{"location":"material/03_ml/01_intro/01_intro_ai/#ia-ristretta-debole","title":"IA Ristretta (Debole)","text":"<p>L'IA Ristretta, anche detta Debole, \u00e8 progettata per svolgere compiti specifici e limitati. Esempi classici includono:</p> <ul> <li>Assistenti virtuali, come Siri o Alexa, che rispondono a domande generiche o controllano dispositivi smart.</li> <li>Sistemi di raccomandazione, che possono essere implementati su piattaforme come Amazon, Spotify o Netflix, e che si concentrano sul suggerire all'utente prodotti, musica o prodotti.</li> <li>Chatbot, utilizzati per conversare con l'utente in ambiti specifici (come ad esempio quelli di assistenza per determinati siti) oppure generici (come ChatGPT).</li> </ul>"},{"location":"material/03_ml/01_intro/01_intro_ai/#ia-generale-forte","title":"IA Generale (Forte)","text":"<p>Quello dell'IA Generale \u00e8 un concetto al momento teorico che rappresenta un'intelligenza artificiale con capacit\u00e0 cognitive simili a quelle umane, e quindi in grado di comprendere, apprendere ed applicare conoscenze in contesti eterogenei. Attualmente (siamo nel 2025!) non esistono IA di questo tipo, anche se lo sviluppo di un'IA Forte \u00e8 oggetto di diversi progetti di ricerca di frontiera.</p>"},{"location":"material/03_ml/01_intro/01_intro_ai/#superintelligenza","title":"Superintelligenza","text":"<p>Il concetto di Superintelligenza \u00e8 un ipotetico concetto di forma di IA in grado di superare l'intelligenza umana sotto ogni punto di vista. Questa forma di IA potrebbe avere delle capacit\u00e0 di auto-miglioramento, risolvendo problemi complessi che sono al di fuori delle capacit\u00e0 di comprensione umana. Tuttavia, al momento, questo concetto di IA \u00e8 esclusivamente fantascientifico.</p> <p>Una volta definita l'IA, possiamo definire un sottocampo della stessa, ovvero il machine learning.</p>"},{"location":"material/03_ml/01_intro/02_intro_ml/","title":"3.1.1 - Introduzione al machine learning","text":"<p>Il machine learning \u00e8 l'insieme di approcci alla base di alcune tra le pi\u00f9 importanti e diffuse tecnologie odierne. Le sue applicazioni sono molteplici: si va dagli strumenti di traduzione automatica, che tutti utilizziamo pi\u00f9 o meno spesso, ai chatbot, ormai estremamente evoluti, passando per sistemi di videosorveglianza e riconoscimento facciale, e tanto altro ancora. In altre parole, la pervasiva diffusione del machine learning ha offerto un'alternativa pi\u00f9 efficace alla risoluzione di problemi che, un tempo, venivano trattati esclusivamente mediante complessi modelli di equazioni matematiche.</p> <p>Volendo riassumere quello che \u00e8 il \"flusso di lavoro\" di un algoritmo di machine learning, possiamo dire che questo \u00e8 il procedimento che permette ad un modello di imparare a fare predizioni significative a partire da un insieme di dati. In altri termini:</p> <p>Modello di machine learning</p> <p>Un modello di machine learning rappresenta la relazione matematica intercorrente tra i dati che il sistema derivante utilizza per effettuare predizioni.</p> <p>Come esempio, immaginiamo di creare un software che effettui la predizione del quantitativo di pioggia che cadr\u00e0 in una zona. Per farlo, possiamo usare due approcci:</p> <ul> <li>nell'approccio tradizionale, creeremo una rappresentazione fisica dell'atmosfera e della superficie terrestre, risolvendo equazioni estremamente complesse come le Navier-Stokes;</li> <li>nell'approccio basato sul machine learning, daremo ad un modello un quantitativo adeguato (e, molto spesso, enorme) di dati riguardanti le condizioni meteorologiche, fino a che il modello stesso non apprender\u00e0 le relazioni sottostanti i diversi pattern di feature meteorologiche che permettono di produrre diversi quantitativi di pioggia.</li> </ul> <p>In entrambi i casi, una volta completata l'implementazione (per l'approccio tradizionale) o l'addestramento (per l'approccio basato su machine learning) passeremo al software i dati sulla condizione meteorologica attuale, per poi predire il quantitativo di pioggia previsto.</p> <p>Perch\u00e9, dunque, preferire il machine learning agli approcci tradizionali? Per rispondere a questa domanda, ci viene in aiuto il principio di indeterminazione discendente dal paradosso del demone di Laplace, il quale afferma che l'unico modo di predire lo stato di un sistema chiuso, pur conoscendone ogni dettaglio, \u00e8 quello di essere esterno al sistema stesso. Ci\u00f2 comporta che, per complesso che sia, nessun modello a scatola trasparente \u00e8 in grado di modellare con precisione la realt\u00e0: di conseguenza, i modelli fisici sono giocoforza limitati sia dal numero di variabili da considerare, sia dalla natura dell'universo fisico.</p> <p>Un modello di machine learning, ovviamente, \u00e8 soggetto agli stessi vincoli dei modelli tradizionali. Tuttavia, il modello non deriva le sue predizioni da complesse equazioni matematiche, bens\u00ec deriva la relazione dai dati sotto osservazione. Di conseguenza, con un numero adeguato di dati, e rispettando le opportune ipotesi di funzionamento, un modello di machine learning questo offre una rappresentazione della realt\u00e0 pi\u00f9 \"vera\" di quella offerta da qualsiasi modello classico.</p>"},{"location":"material/03_ml/01_intro/02_intro_ml/#machine-learning-deep-learning-ia","title":"Machine learning, deep learning, IA...","text":"<p>Prendiamoci un attimo per cercare di comprendere la differenza tra discipline apparentemente sovrapposte, quali machine learning, deep learning, ed intelligenza artificiale. In particolare, tutte queste discipline discendono da un \"antenato\" comune, ovvero la statistica, che altro non \u00e8 se non una branca della matematica che si occupa di descrivere le caratteristiche di una popolazione di dati (statistica descrittiva), eventualmente effettuando delle operazioni di inferenza a partire da una certa popolazione (statistica inferenziale)<sup>1</sup>.</p> <p>La statistica \u00e8 ampiamente utilizzata dall'intelligenza artificiale, il cui compito \u00e8 quello di creare dei sistemi in grado di svolgere dei compiti che, in condizioni normali, richiedono l'interazione con degli esseri umani. Dal punto di vista \"filosofico\", esistono due tipologie di intelligenza artificiale: la prima \u00e8 la cosiddetta IA ristretta, o debole, progettate per eseguire dei compiti ben precisi, come ad esempio individuare i volti in una foto, o rispondere a delle domande testuali. La seconda, invece, \u00e8 chiamata IA generale, o forte, e rappresenta un'intelligenza che possiede capacit\u00e0 cognitive eterogenee ed assimilabili a quelle presenti negli organismi che riteniamo comunemente \"intelligenti\", come ad esempio scimmie o delfini (e, se non vi sentite particolarmente misantropi, esseri umani). </p> <p>Il machine learning \u00e8 invece una branca dell'intelligenza artificiale che si concentra sullo sviluppo di algoritmi data-driven, ovvero che permettono ad una macchina di apprendere un determinato comportamento direttamente dai dati a disposizione del modello. A sua volta, il deep learning, che vedremo in seguito, \u00e8 una branca del machine learning che utilizza uno specifico tipo di algoritmi, ovvero le deep neural network, per mimare la struttura del cervello umano e trattare, di conseguenza, problemi estremamente complessi e sfaccettati.</p> <p>E ChatGPT?</p> <p>Quello dell'IA generale \u00e8, nel 2025, ancora un concetto teorico. Non esiste niente di simile e, nonostante gli incredibili sviluppi che abbiamo avuto negli ultimi anni, non raggiungeremo questo livello ancora per un po' di tempo. Va da s\u00e8, quindi, che ChatGPT et similia non rappresentano esempi di IA generale, quanto piuttosto di IA debole applicata a contesti specifici di generazione di linguaggio naturale, sviluppata utilizzando degli approcci basati sul deep learning.</p>"},{"location":"material/03_ml/01_intro/02_intro_ml/#tipi-di-sistemi-di-machine-learning","title":"Tipi di sistemi di machine learning","text":"<p>I sistemi di machine learning ricadono in tre diverse categorie, distinte sulla base di come \"apprendono\" a fare determinate predizioni.</p>"},{"location":"material/03_ml/01_intro/02_intro_ml/#sistemi-ad-apprendimento-supervisionato","title":"Sistemi ad apprendimento supervisionato","text":"<p>I sistemi ad apprendimento supervisionato (supervised learning) effettuano una predizione dopo aver appreso le relazioni intercorrenti tra un numero pi\u00f9 o meno grande di dati ed i corrispondenti valori da predire. Per intenderci, un sistema di questo tipo \u00e8 un po' come uno studente di matematica che, dopo aver appreso i metodi per la risoluzione di un problema di analisi mediante la risoluzione di un gran numero degli stessi, si prepara a sostenere l'esame.</p> <p>Perch\u00e9 supervisionato?</p> <p>L'appellativo supervisionato deriva dal fatto che \u00e8 (di solito) un esperto di dominio a fornire al sistema i dati con i risultati corretti.</p> <p>I pi\u00f9 importanti approcci all'apprendimento supervisionato sono la regressione e la classificazione.</p>"},{"location":"material/03_ml/01_intro/02_intro_ml/#modelli-di-regressione","title":"Modelli di regressione","text":"<p>Un modello di regressione predice un valore numerico. Ad esempio, un modello meteorologico di regressione potrebbe predire il quantitativo di pioggia in millimetri, mentre un altro modello di regressione potrebbe valutare l'andamento dei prezzi delle propriet\u00e0 immobiliari sulla base di dati come i metri quadri, la posizione e le caratteristiche della casa, nonch\u00e9 la situazione attuale dei mercati finanziario ed immobiliare.</p>"},{"location":"material/03_ml/01_intro/02_intro_ml/#modelli-di-classificazione","title":"Modelli di classificazione","text":"<p>A differenza dei modelli di regressione, il cui output \u00e8 rappresentato da un numero, i modelli di classificazione restituiscono in uscita un valore che stabilisce la possibilit\u00e0 che un certo campione appartenga ad una data categoria. Ad esempio, un modello di classificazione potrebbe essere usato per predire se un'email \u00e8 un messaggio di spam, o se una foto contiene invece un gatto o un cane.</p> <p>Esistono due macrocategorie di modelli di classificazione, ovvero quelli binari e quelli multiclasse. In particolare, i modelli di classificazione binaria distinguono esclusivamente tra due valori: ad esempio, un modello di classificazione delle email potrebbe indicare se il messaggio \u00e8 di spam o meno. I modelli di classificazione multiclasse invece riescono a distinguere tra pi\u00f9 classi: ad esempio, il nostro modello di riconoscimento delle foto potrebbe riconoscere oggetti di \"classe\" gatto, cane, gallina ed oca.</p> <p>Parleremo pi\u00f9 approfonditamente di questi modelli in una delle lezioni successive.</p>"},{"location":"material/03_ml/01_intro/02_intro_ml/#sistemi-ad-apprendimento-non-supervisionato","title":"Sistemi ad apprendimento non supervisionato","text":"<p>I sistemi di apprendimento non supervisionato compiono delle predizioni a partire da dati che non contengono alcuna informazione sulla classe di appartenenza o sul valore di regressione. In pratica, i modelli non supervisionati hanno il compito di identificare pattern significativi direttamente nei dati, senza alcun \"indizio\" a priori, ma limitandosi ad inferire automaticamente le proprie regole.</p> <p>Algoritmi comunemente utilizzati in tal senso sono quelli di clustering, nei quali il modello individua come i dati vanno a \"disporsi\" utilizzando delle regole basate su distanze o capacit\u00e0 di \"agglomerarsi\".</p> <p>Il clustering differisce dagli algoritmi supervisionati, ed in particolare dalla classificazione, principalmente perch\u00e9 le categorie non sono definite a priori da un esperto di dominio. Ad esempio, un algoritmo di clustering potrebbe raggruppare i campioni in un dataset meteo sulla base esclusivamente delle temperature, rivelando delle suddivisioni che definiscono le diverse stagioni, oppure ancora gli orari del giorno. Sar\u00e0 poi nostro compito \"provare\" a dare un nome a questi cluster sulla base della nostra interpretazione del dataset.</p>"},{"location":"material/03_ml/01_intro/02_intro_ml/#sistemi-di-reinforcement-learning","title":"Sistemi di reinforcement learning","text":"<p>I sistemi di reinforcement learning effettuano delle predizioni a partire da ricompense o penalit\u00e0 basate sulle azioni effettuate da un agente all'interno di un ambiente. Sulla base di queste osservazioni, il sistema di reinforcement learning genera una policy che definisce la strategia migliore per raggiungere lo scopo prefissato.</p> <p>Le applicazioni dei sistemi di questo tipo sono varie, e spaziano dall'addestramento dei robot per svolgere task anche complessi, alla creazione di programmi come Alpha Go che sfidino (e battano) gli umani al gioco del Go.</p> <ol> <li> <p>Statistics versus machine learning, Nature\u00a0\u21a9</p> </li> </ol>"},{"location":"material/03_ml/01_intro/03_framing/","title":"3.1.2 - Definizione del problema","text":"<p>La definizione di un problema \u00e8, come prevedibile, il primo passo per la sua risoluzione. Pensiamoci un attimo: se non abbiamo una chiara idea del problema da affrontare, come possiamo pensare di risolverlo?</p> <p>Il primo step per affrontare il problema \u00e8 quindi analizzarlo, isolando gli elementi essenziali da utilizzare nella sua risoluzione: andr\u00e0 fatto uno studio di fattibilit\u00e0, determinando se il problema \u00e8 risolvibile o meno; saranno poi forniti un chiaro insieme di obiettivi, assieme ai criteri ed ai vincoli da rispettare nella risoluzione. Approfondiamo questi aspetti.</p>"},{"location":"material/03_ml/01_intro/03_framing/#analisi-del-problema","title":"Analisi del problema","text":"<p>Partiamo analizzando il problema, definendo quindi sia i dati in ingresso, sia quello che vogliamo ottenere a valle della risoluzione. Facciamo un paio di esempi concreti:</p> <ul> <li>Predittore di precipitazioni: il nostro primo problema prevede la predizione delle precipitazioni orarie. In questo caso, l'obiettivo \u00e8 quello di creare un software/hardware che, data la collezione storica delle precipitazioni, sia in grado di predire l'entit\u00e0 delle stesse, a partire dalla zona e dal periodo dell'anno, con un buon grado di confidenza.</li> <li>Spam detector: il nostro secondo problema ci chiede di capire se una mail \u00e8 di spam. L'obiettivo sar\u00e0 quindi la creazione di un software che, una volta ricevuta una mail, sia in grado di estrapolarne le informazioni che permettano di classificarla come legittima (o meno).</li> </ul> <p>Una possibile schematizzazione \u00e8 nella seguente tabella.</p> Applicazione Obiettivo del problema Output atteso Input Previsioni meteo Calcolare le precipitazioni orarie in una determinata zona Predizione delle precipitazione orarie Storico delle precipitazioni, localit\u00e0, situazione attuale Spam detector Individuare lo spam Alert per un possibile spam Mail sotto analisi, esempi di mail di spam e legittime <p>Una volta individuati obiettivo, input ed output, dovremo verificare che il problema sia risolvibile mediante un algoritmo di machine learning. In particolare, dovremo verificare la presenza di un'adeguata quantit\u00e0 di dati rappresentativi del fenomeno osservato, e decidere quale approccio utilizzare tra classificazione, regressione e clustering. In questa nostra analisi, inoltre, dovremo tenere conto del cosiddetto rasoio di Occam: infatti, alle volte, potrebbe non essere necessario utilizzare un algoritmo di machine learning per risolvere il problema sotto analisi.</p> <p>Machine learning? No, grazie!</p> <p>Ad eempio, nonostante un semplice sistema massa - molla - smorzatore possa essere modellato tramite il machine learning, \u00e8 decisamente pi\u00f9 produttivo usare delle semplici relazioni fisiche.</p> <p>Una volta analizzato il problema, e verificata la necessit\u00e0 (e possibilit\u00e0) di usare il machine learning, dovremo passare alla fase successiva, che prevede l'analisi e lo studio dei dati a nostra disposizione.</p>"},{"location":"material/03_ml/01_intro/03_framing/#comprensione-dei-dati","title":"Comprensione dei dati","text":"<p>La disponibilit\u00e0 di un'adeguata quantit\u00e0 di dati caratterizzanti il fenomeno sotto analisi sono alla base del corretto funzionamento degli algoritmi di machine learning. Per effettuare delle predizioni efficaci, infatti, abbiamo bisogno di usare dati di quantit\u00e0 e qualit\u00e0 adeguate, in grado quindi di garantire un elevato potere predittivo. In particolare, i nostri dati dovranno essere:</p> <ul> <li>abbondanti: pi\u00f9 esempi rilevanti abbiamo a disposizione, maggiori saranno gli aspetti che potremo caratterizzare e, di conseguenza, migliore sar\u00e0 il potere risolutivo del nostro modello;</li> <li>consistenti: raccogliere dati usando criteri e strumenti ben determinati e coerenti permetter\u00e0 una migliore campagna di acquisizione. Ad esempio, il modello meteorologico ottenuto usando dati raccolti ogni giorno per cento anni sar\u00e0 pi\u00f9 preciso di quello ottenuto raccogliendo dati una volta l'anno per lo stesso periodo di tempo;</li> <li>affidabili: la sorgente dei nostri dati dovr\u00e0 essere affidabile. Ad esempio, l'igrometro ed il barometro dovranno essere ben tarati ed accessibili, mentre l'insieme di email usate per caratterizzare lo spam dovr\u00e0 essere acquisita da utenti verificati;</li> <li>disponibili: dovremo verificare che non vi siano parti del nostro dataset completamente omesse e, qualora queste siano presenti, che non riguardino aspetti fondamentali e non trascurabili del fenomeno sotto analisi. Ad esempio, un dataset nel quale i valori di temperatura, umidit\u00e0 e pressione sono presi soltanto nei giorni soleggiati avr\u00e0 ben poca utilit\u00e0 nella predizione delle precipitazioni;</li> <li>corretti: i dati andrebbero verificati in termini sia di valori delle feature, che potrebbero essere errati a causa di starature o malfunzionamenti dei sensori, oppure ancora in caso di perdita di dettagli del testo analizzato, sia di label assegnate. In quest'ultimo caso, infatti, un esperto di dominio potrebbe erroneamente contrassegnare come spam una mail legittima, o viceversa, e ci\u00f2 andrebbe ovviamente ad inficiare le performance del nostro modello;</li> <li>rappresentativi: il dataset dovrebbe descrivere in maniera completa il fenomeno sottostante, riflettendone accuratamente aspetti e caratteristiche. Utilizzare un dataset non rappresentativo inficier\u00e0 negativamente le performance predittive del modello.</li> </ul>"},{"location":"material/03_ml/01_intro/03_framing/#scelta-del-modello","title":"Scelta del modello","text":"<p>L'ultimo step \u00e8 la scelta del tipo di modello da utilizzare, valutando ad esempio tra classificatore, regressore ed algoritmo di clustering.</p> <p>Per la nostra applicazione meteo, ad esempio, predire il quantitativo di pioggia che cadr\u00e0 in un determinato luogo \u00e8 un chiaro problema di regressione, nel senso che date \\(n\\) variabili indipendenti cercheremo di predire una variabile dipendente in uscita.</p> <p>Regressione univariata e multivariata</p> <p>In questo caso, la regressione si dice univariata a causa del fatto che si sta predicendo un'unica variabile dipendente. Se provassimo a predire (ad esempio) anche la temperatura, avremmo a che fare con una regressione multivariata.</p> <p>Nel caso dell'applicazione mail, dato che stiamo cercando di valutare se un messaggio \u00e8 classificabile o meno come spam, avremo a che fare con un problema di classificazione binaria.</p> <p>Una volta determinato il tipo di problema, dovremo scegliere l'algoritmo da utilizzare sulla base dei dati, oltre che la metrica con la quale valutare i risultati ottenuti (in accordo al modello scelto). In particolare, la metrica, ed il valore da essa assunto, dipendono strettamente dall'ambito applicativo: se, ad esempio, un errore del \\(10\\%\\) potrebbe non essere estremamente importante nell'applicazione mail, questo diventerebbe estremamente rilevante (e potenzialmente pericoloso) nella stima dei millimetri di pioggia che cadranno in una certa zona.</p>"},{"location":"material/03_ml/01_intro/03_framing/#problemi-multiclasse-e-multioutput","title":"Problemi multiclasse e multioutput","text":"<p>Esistono diverse tipologie di problemi sulla base dell'ordine sia dei nostri dati, sia delle nostre label. In particolare, distinguiamo i seguenti.</p>"},{"location":"material/03_ml/01_intro/03_framing/#classificazione-multiclasse","title":"Classificazione multiclasse","text":"<p>In questo task di classificazione, i dati possono appartenere ad una ed una sola tra pi\u00f9 di due classi. Un esempio \u00e8 un classificatore che identifica il tipo di frutta mostrato in un'immagine, discernendo tra mele, pere ed uva.</p>"},{"location":"material/03_ml/01_intro/03_framing/#classificazione-multioutput","title":"Classificazione multioutput","text":"<p>In questo tipo di classificazione, ogni campione \u00e8 associato ad \\(m\\) label prese da \\(n\\) diverse classi, con \\(0 &lt; m \\leq n\\). Un esempio pu\u00f2 essere la predizione dei topic di riferimento di un documento, presi tra scienza, tecnologia, cultura e religione: possono esserci infatti documenti che parlano sia di cultura che di religione, o anche di religione e scienza, e cos\u00ec via.</p>"},{"location":"material/03_ml/01_intro/03_framing/#classificazione-multiclassemultioutput","title":"Classificazione multiclasse/multioutput","text":"<p>In questo tipo di classificazione, nota anche come multitask classification, ad ogni campione \u00e8 associato un insieme di propriet\u00e0, per ciascuna delle quali sar\u00e0 possibile scegliere tra pi\u00f9 di due classi. Per fare un esempio, immaginiamo di voler associare i parametri specie e razza ad una serie di immagini di grandi felini: la specie potr\u00e0 variare tra leone, leopardo, tigre, e cos\u00ec via, mentre la razza varier\u00e0 per ciascuna specie, ad esempio, leone senegalese e leone indiano per i leoni, tigre siberiana e tigre cinese meridionale per le tigri, e via dicendo.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/","title":"3.1.3 - Preparazione dei dati","text":"<p>Nella lezione precedente abbiamo evidenziato come uno dei passi fondamentali nella definizione di un algoritmo di machine learning sia la definizione dei dati sui quali il modello deve essere addestrato. Infatti, per ottenere delle buone predizioni, dovremo opportunamente costruire un dataset; questa operazione avviene in due step, ovvero campionamento e preparazione dei dati.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#campionamento-del-dataset","title":"Campionamento del dataset","text":"<p>Il primo problema da affrontare \u00e8 quello relativo al campionamento, ovvero alla raccolta dei dati che andranno a comporre il nostro dataset. In particolare, dovremo tenere conto di due aspetti, ovvero il numero dei dati e la loro qualit\u00e0.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#dimensione-del-dataset","title":"Dimensione del dataset","text":"<p>Il primo aspetto di cui tenere conto riguarda il numero dei dati da raccogliere, che influenza direttamente la dimensione del dataset. In tal senso, non esiste una regola vera e propria per deteterminare il quantitativo di dati sufficiente ad addestrare un modello in modo adeguato; in generale, tuttavia, potremo dire che questa quantit\u00e0 deve essere almeno un ordine di grandezza superiore rispetto al numero dei parametri utilizzati dal modello. A scopo puramente esemplificativo, usare una rete neurale con \\(100\\) neuroni implicher\u00e0 la presenza di almeno \\(100\\) pesi e \\(100\\) bias, per cui dovremo avere indicativamente almeno \\(2000\\) campioni a disposizione.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#qualita-del-dataset","title":"Qualit\u00e0 del dataset","text":"<p>Parafrasando una vecchia pubblicit\u00e0 degli anni '90, la quantit\u00e0 (potenza) \u00e8 niente senza qualit\u00e0 (controllo). In altri termini, avere a disposizione grandi quantit\u00e0 di dati non basta se questi non sono anche significativi nella caratterizzazione del fenomeno sotto osservazione.</p> <p>Per comprendere questo concetto, cerchiamo di usare un approccio empirico. Riprendiamo il nostro modello di predizione delle precipitazioni, ed immaginiamo di doverlo addestrare scegliendo tra due dataset. In particolare:</p> <ul> <li>il dataset A contiene un dato relativo ai valori di temperatura e pressione campionato ogni \\(15\\) minuti negli ultimi \\(100\\) anni esclusivamente per il mese di luglio;</li> <li>il dataset B contiene un unico dato giornaliero per i valori di temperatura e pressione per ciascun giorno degli ultimi \\(100\\) anni.</li> </ul> <p>Possiamo quindi facilmente verificare che il dataset A contiene \\(297.600\\) campioni (pari a \\(4 \\cdot 24 \\cdot 31 \\cdot 100\\), ovvero il numero di campioni per ora moltiplicati per il numero di ore e per il numero di giorni presenti a luglio), mentre il dataset B contiene solo \\(365 \\cdot 100 = 36.500\\) campioni.</p> <p>Tuttavia, la qualit\u00e0 del dataset B risulta essere migliore rispetto a quella del dataset A: infatti, nonostante quest'ultimo abbia quasi dieci volte pi\u00f9 campioni del primo, non potr\u00e0 essere utilizzato per stimare le precipitazioni in mesi differenti da luglio.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#preparazione-dei-dati","title":"Preparazione dei dati","text":"<p>Una volta acquisito in maniera appropriata il dataset, dovremo preparare i dati in esso contenuto. </p> <p>Anonimizzazione dei dati</p> <p>Lo step \"zero\" della preparazione del dataset \u00e8 quello legato all'anonimizzazione dei dati, che assume fondamentale importanza in taluni contesti, come ad esempio quello medico o finanziario. Presupporremo, nel prosieguo, che questo passo venga effettuato di default.</p> <p>Questa preparazione prevede gli step riassunti nel prosieguo.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#step-1-data-cleaning","title":"Step 1: Data cleaning","text":"<p>Abbiamo visto come la qualit\u00e0 ed affidabilit\u00e0 del dataset siano essenziali per garantire adeguate performance del modello addestrato. In tal senso, \u00e8 necessario determinare diversi fattori, ad esempio:</p> <ul> <li>presenza di errori nel labelling: il lavoro svolto dall'esperto di dominio nel labeling pu\u00f2, alle volte, essere subottimale, e presentare errori grossolani ed evidenti. In questo caso, potrebbe essere necessario effettuare un'ulteriore procedura di etichettatura;</li> <li>presenza di rumore: \u00e8 importante valutare se i dati sono affetti da rumore o da trend evidenti. Per esempio, potremmo notare che tutte le letture di un sensore sono affette da un certo punto in avanti da un certo offset, legato magari ad una staratura dello stesso;</li> <li>assenza di dati: immaginando che un sensore risulti danneggiato, potrebbe verificarsi la situazione per cui le letture per lo stesso non siano disponibili da un certo istante in avanti. Questo fenomeno, che abbiamo gi\u00e0 analizzato quando abbiamo parlato della manipolazione dei DataFrame in Pandas, va adeguatamente affrontato;</li> <li>valori contrastanti o duplicati: data la dinamicit\u00e0 delle campagne di acquisizione, specialmente se lunghe, potrebbero esserci delle incoerenze nei dati campionati. Ad esempio, lungo l'arco della campagna, un sensore di temperatura non funzionante potrebbe essere stato sostituito con un altro che, per\u00f2, a differenza del primo effettua le letture in gradi Kelvin.</li> </ul> <p>Se si presenta una di queste occorrenza, \u00e8 necessario valutare una strategia di data cleaning. Ne abbiamo brevemente parlato in precedenza analizzando le funzioni <code>dropna()</code> e <code>fillna()</code>; ne vedremo delle altre nel proseguo.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#step-2-bilanciamento-del-dataset","title":"Step 2: Bilanciamento del dataset","text":"<p>Quando si affronta un problema di classificazione, esiste la (tutt'altro che remota) possibilit\u00e0 che il dataset acquisito presenti diverse proporzioni nel raggruppamento delle classi. In altre parole, supponendo la presenza di due classi \\(X\\) ed \\(Y\\), potremmo avere il \\(70\\%\\) di campioni appartenenti alla classe \\(X\\), ed il restante \\(30\\%\\) appartenenti alla classe \\(Y\\).</p> <p>Ci\u00f2 comporta che avremo una classe maggioritaria, ovvero \\(X\\), che avr\u00e0 un maggior numero di campioni rispetto alla minoritaria \\(Y\\), la quale, prevedibilmente, sar\u00e0 rappresentata da un numero limitato di campioni. Un dataset in cui si verifica questo fenomeno \u00e8 detto sbilanciato.</p> <p>Possiamo quantificare, in maniera approssimativa, l'entit\u00e0 dello sbilanciamento del dataset, rifacendoci alla seguente tabella.</p> Grado di sbilanciamento \\(\\%\\) di campioni di classi minoritarie Leggero 20-40 \\(\\%\\) del datset Moderato 1-20 \\(\\%\\) del dataset Estremo &lt; 1 \\(\\%\\) del dataset <p>Per comprendere a fondo quali siano gli effetti dello sbilanciamento del dataset, possiamo tornare al nostro spam detector. Per addestrarlo, usiamo un dataset di qeusto tipo:</p> Mail non spam Mail spam Numero di mail \\(5\\) \\(995\\) Percentuale \\(0.5 \\%\\) \\(99.5 \\%\\) <p>Notiamo subito che abbiamo un numero molto limitato di mail di spam. Di conseguenza, l'addestramento avverr\u00e0 per la maggior parte su mail \"normali\"; inoltre, un numero cos\u00ec esiguo di mail di spam non potr\u00e0 descrivere adeguatamente tutti i casi in cui ci si trova di fronte a questo fenomeno.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#data-balancing","title":"Data balancing","text":"<p>Una maniera efficace per trattare situazioni di questo tipo \u00e8 quella di adottare una tecnica di data balancing (ovvero, bilanciamento dei dati). Ne esistono diverse, pi\u00f9 o meno efficaci; tra queste, la pi\u00f9 semplice riguarda la rimozione di un certo numero di campioni della classe maggioritaria (downsampling), dando agli esempi sottocampionati un peso maggiore nell'addestramento (upweighting).</p> <p>Facciamo un esempio. Scegliendo di mantenere soltanto il \\(10 \\%\\) delle mail non-spam, avremmo circa \\(99\\) campioni. Ci\u00f2 porter\u00e0 il rapporto tra le mail di spam e quelle non di spam a circa il \\(5 \\%\\), passando da una situazione di sbilanciamento estremo ad una di sbilanciamento moderato.</p> <p>A valle di questa operazione, dovremo dare maggior peso ai campioni delle mail non-spam, usando un fattore tendenzialmente pari a quello che abbiamo usato in fase di downsampling. Nella pratica, ogni mail non-spam avr\u00e0 un peso dieci volte superiore a quello che avrebbe avuto se si fosse utilizzato il dataset iniziale.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#trasformazione-dei-dati","title":"Trasformazione dei dati","text":"<p>Il passo successivo nella preparazione dei dati \u00e8 quello di trasformare un insieme (o la totalit\u00e0) dei valori. Ci\u00f2 avviene principalmente per due motivi.</p> <p>Il primo \u00e8 legato alla necessit\u00e0 di trasformazioni obbligatorie volte a garantire la compatibilit\u00e0 dei dati. Alcuni esempi:</p> <ul> <li>conversione di feature non numeriche in numeriche: in pratica, non possiamo effettuare operazioni sensate tra interi e stringhe, per cui dovremmo trovarci ad individuare un modo per permettere tale confronto;</li> <li>ridimensionamento degli input ad una dimensione fissa: alcuni modelli, come ad esempio le reti neurali, prevedono un numero fisso di nodi di input, per cui i dati in ingresso devono avere sempre la stessa dimensione.</li> </ul> <p>Il secondo motivo \u00e8 legato a delle trasformazioni opzionali che ottimizzino i risultati ottenuti durante l'addestramento. Ad esempio, potremmo dover portarli tutti i valori numerici all'interno di una stessa scala di valori (rescaling), oppure trasformarli in modo tale che ricordino una distribuzione normale (normalizzazione).</p> <p>Per comprendere al meglio il motivo alla base di questa necessit\u00e0, immaginiamo di avere un dataset che comprende due feature: l'et\u00e0 (che assume valori da \\(0\\) a \\(100\\)) e reddito annuo (che supponiamo assuma valori da \\(10.000\\) a \\(100.000\\) \u20ac). Dando in pasto queste feature ad algoritmi che le combinano in qualche modo, l'et\u00e0 diventer\u00e0 presto trascurabile rispetto al reddito, dato che quest'ultimo risulta essere di due o tre ordini di grandezza superiore. Di conseguenza, il modello trascurer\u00e0 l'et\u00e0 in fase di analisi, utilizzando esclusivamente lo stipendio.</p> <p>Da questa considerazione appare evidente come sia necessario portare tutti i dati ad una \"base comune\" prima di effettuare un addestramento. Per farlo, abbiamo a disposizione quattro tipi di trasformazione.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#scaling","title":"Scaling","text":"<p>Lo scaling prevede la conversione dei valori assunti da una feature in un range che va di solito tra \\([0, 1]\\) o \\([-1, 1]\\). La formula dello scaling \u00e8 la seguente:</p> \\[ y = \\frac{(x - x_{min})}{(x_{max} - x_{min})} \\]"},{"location":"material/03_ml/01_intro/04_data_prep/#clipping","title":"Clipping","text":"<p>La tecnica del clipping viene usata quando il dataset contiene degli outlier, ovvero alcuni campioni che divergono notevolmente dalle caratteristiche statistiche del resto dei dati. In questo caso, potremmo limitarci a rimuovere completamente tali valori mediante soglie statistiche, come i range interquartili in caso di distribuzione parametrica, o i classici \\(3 \\sigma\\) in caso di distribuzione normale.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#trasformazione-logaritmica","title":"Trasformazione logaritmica","text":"<p>Un'altra possibilit\u00e0 \u00e8 quella di convertire i nostri valori in scala logaritmica, comprimendo un range ampio in uno pi\u00f9 piccolo usando la funzione logaritmo:</p> \\[ y = Log(x) \\]"},{"location":"material/03_ml/01_intro/04_data_prep/#z-score","title":"Z-score","text":"<p>Un ultimo tipo di trasformazione prevede l'uso dello z-score, che prevede una riformulazione dei valori assunti dalla feature per fare in modo che questi aderiscano ad una distribuzione a media nulla e deviazione tandard unitaria. Per calcolarlo, si usa la seguente formula:</p> \\[ y = \\frac{x - \\mu}{\\sigma} \\] <p>dove \\(\\mu\\) \u00e8 la media della distribuzione dei nostri dati, mentre \\(\\sigma\\) ne \u00e8 chiaramente la varianza.</p>"},{"location":"material/03_ml/01_intro/04_data_prep/#trasformazione-dei-dati-categorici","title":"Trasformazione dei dati categorici","text":"<p>Alcune delle nostre feature possono assumere esclusivamente valori discreti. Ad esempio, la feature \"localit\u00e0\" associato alle precipitazioni potrebbe riportare il CAP, mentre la feature \"spam\" nel nostro dataset di email potrebbe avere al suo interno un booleano. Queste feature sono dette categoriche, ed i valori ad esse associate possono essere sia stringhe sia numeri.</p> <p>Le feature categoriche di tipo numerico</p> <p>Spesso, dobbiamo trattare feature categoriche che contengono valori numerici. Consideriamo ad esempio il CAP: rappresentandolo come un tipo numerico, il nostro modello potrebbe interpretare la distanza tra Bari (\\(70126\\)) e Taranto (\\(74121\\)) come pari a \\(3.995\\), il che non avrebbe ovviamente senso.</p> <p>Le feature categoriche devono per\u00f2 essere convertite in valori numerici, mantenendo contestualmente il riferimento al significato originario. Immaginiamo ad esempio di avere a che fare con una feature categorica che descrive il giorno della settimana. Il modo pi\u00f9 semplice per passare da una rappresentazione puramente categorica ad una numerica \u00e8 quella di usare un numero:</p> Giorno Rappresentazione numerica Luned\u00ec 1 Marted\u00ec 2 Mercoled\u00ec 3 Gioved\u00ec 4 Venerd\u00ec 5 Sabato 6 Domenica 7 <p>In questa maniera creeremo un dizionario, nel quale potremo accedere ad una chiave (la rappresentazione numerica) che rappresenter\u00e0 un determinato valore (il giorno).</p> <p>Sulle feature categoriche trasformate</p> <p>A valle di questa trasformazione, la differenza aritmetica tra domenica e sabato continua ad avere un senso alquanto limitato, e comunque relativo ad un generico concetto di distanza.</p> <p>Un altro modo di rappresentare le feature categoriche \u00e8 mediante una rappresentazione sparsa, detta anche one-hot encoding, nella quale ogni valore \u00e8 rappresentato da un vettore \\(V\\) di lunghezza \\(m\\), con \\(m\\) numero di categorie possibili. In questo caso, tutti i valori di \\(V\\) saranno pari a \\(0\\), tranne quello rappresentativo del valore attualmente assunto dalla feature, che sar\u00e0 pari ad \\(1\\). Ad esempio, la rappresentazione sparsa del luned\u00ec \u00e8 data da:</p> <pre><code>lunedi = np.array([1 0 0 0 0 0 0])\n</code></pre> <p>mentre quella del gioved\u00ec:</p> <pre><code>giovedi = np.array([0 0 0 1 0 0 0])\n</code></pre>"},{"location":"material/03_ml/01_intro/04_data_prep/#dati-di-training-test-e-validazione","title":"Dati di training, test e validazione","text":"<p>L'ultimo passo nella preparazione del dataset \u00e8 quello della suddivisione dei dati. In particolare, si destinano un certo quantitativo di dati per l'addestramento del modello, delegando la restante parte alla validazione dei risultati ottenuti; ci\u00f2 \u00e8 legato alla volont\u00e0 di verificare la capacit\u00e0 di generalizzazione del modello, ovvero a quanto \u00e8 in grado di \"funzionare\" il nostro algoritmo in caso di analisi di dati su cui non \u00e8 stato addestrato.</p> <p>Un rapporto molto usato in tal senso \u00e8 quello che prevede che il \\(60\\%\\) dei dati sia usato per l'addestramento, un altro \\(20\\%\\) per il test del modello, ed il restante \\(20\\%\\) per la validazione dei risultati ottenuti.</p>"},{"location":"material/03_ml/01_intro/05_base/","title":"3.1.4 - Task, predizione ed esperienza","text":"<p>Abbiamo visto come un algoritmo di machine learning sia in grado di apprendere a partire da un insieme di dati. Per comprendere al meglio cosa questo significhi, possiamo rifarci direttamente alla definizione fornita da Tom Mitchell nel suo libro \"Machine Learning\" del 1997:</p> <p>Definizione di apprendimento</p> <p>A computer program is said to learn from experience \\(E\\) with respect to some class of tasks \\(T\\) and performance measure \\(P\\), if its performance at tasks in\\(T\\), as measured by \\(P\\), improves with experience \\(E\\).</p> <p>Cerchiamo quindi di dare una definizione intuitiva per \\(T\\), \\(E\\) e \\(P\\).</p>"},{"location":"material/03_ml/01_intro/05_base/#il-task","title":"Il task","text":"<p>Il task \u00e8 il compito affidato all'algoritmo di machine learning, che di solito \u00e8 troppo complesso da risolvere per un essere umano, o anche per un algoritmo puramente deterministico. Appare quindi ovvio come il processo di apprendimento non coincida con il task: l'apprendimento \u00e8 infatti il mezzo che permette al nostro modello di imparare ad effettuare il compito assegnato. Ad esempio, qualora volessimo creare un robot in grado di camminare (e, quindi, di soddisfare il task del camminare), dovremmo istruirlo su come fare.</p> <p>Per un algoritmo di machine learning, risolvere un task significa effettuare una qualche operazione su di un certo campione, inteso come un insieme di valori misurati in qualche modo. Tipicamente, un campione pu\u00f2 essere rappresentato come un vettore \\(x \\in \\mathbb{R}^n\\), con ogni valore \\(x_i\\) feature dello stesso. Ad esempio, le feature associate ad un'immagine possono essere, a basso livello, i valori dei pixel presenti nella stessa.</p> <p>Esistono diverse tipologie di task che possono essere trattate con algoritmi di machine learning. Oltre ai problemi di classificazione e regressione trattati nella lezione introduttiva, alcuni task che possono essere affidati ad un algoritmo di machine learning sono:</p> <ul> <li> <p>trascrizione: sono i task in cui il sistema osserva una rappresentazione di un dato non strutturato, trascrivendone l'informazione sotto forma testuale. Ad esempio, nell'OCR il programma visualizza un'immagine che contiene del testo, restituendo lo stesso sotto forma di una sequenza di caratteri, mentre nel riconoscimento vocale l'algoritmo ha il compito di scrivere una sequenza di caratteri che rispecchino le parole pronunciate in una sorgente audio;</p> </li> <li> <p>machine translation: sono i task in cui il sistema traduce una sequenza di simboli in un certo linguaggio in una sequenza in un altro linguaggio. Un esempio di task di questo tipo \u00e8 la traduzione di un testo dalla lingua inglese a quella italiana;</p> </li> <li> <p>anomaly detection: in un task di anomaly detection l'algoritmo valuta un insieme di eventi o oggetti, alla ricerca di campioni che risultino essere atipici. Un software di questo tipo potrebbe ad esempio controllare delle transazioni anomale sulla carta di credito.</p> </li> </ul>"},{"location":"material/03_ml/01_intro/05_base/#la-performance","title":"La performance","text":"<p>La capacit\u00e0 di un algoritmo di machine learning pu\u00f2 essere valutata mediante una misura quantitativa specifica per il task svolto.</p> <p>Per i task di classificazione, una delle misure pi\u00f9 utilizzate \u00e8 l'accuracy, che non \u00e8 altro se non la proporzione di campioni per la quale il modello \u00e8 in grado di produrre l'output corretto. Nella pratica, possiamo ottenere un'informazione equivalente considerando l'error rate, ovvero la proporzione di campioni per la quale il modello produce un output non corretto. I task di regressione invece prevedono l'utilizzo di una metrica differente che dia al modello un punteggio numerico valutato su ogni campione.</p> <p>E' importante sottolineare come la performance vada misurata su dati che l'algoritmo non ha visto in precedenza, il che ci permette di \"simularne\" le performance nel mondo reale. Queste misure sono quindi valutate su un insieme di test separato dai dati usati per l'addestramento, che vanno per l'appunto a formare l'insieme di training.</p> <p>La scelta della misura di performance pu\u00f2 sembrare diretta ed oggettiva, ma spesso pu\u00f2 essere difficile sceglierne una in grado di definire al meglio il comportamento desiderato del sistema. In alcuni casi, questo \u00e8 legato alle difficolt\u00e0 nello stabilire ci\u00f2 che dovrebbe essere misurato. Ad esempio, in un task di trascrizione, sarebbe meglio usare l'accuracy del sistema nel trascrivere intere sequenze o una misura pi\u00f9 granulare, che fa in modo che specifici elementi della sequenza siano corretti? Ancora, in un task di regressione, vanno maggiormente penalizzati tanti errori di piccola dimensione o pochi errori di grande dimensione? Queste scelte di design dipendono prevalentemente dall'applicazione.</p>"},{"location":"material/03_ml/01_intro/05_base/#lesperienza","title":"L'esperienza","text":"<p>L'esperienza viene fornita ad un algoritmo mediante un intero dataset, ovvero un insieme di molti campioni. Un esempio di dataset molto utilizzato per il test di algoritmi di machine learning \u00e8 Iris, composto da un insieme di misure fatte su sepali e petali di \\(150\\) piante di iris, ciascuna appartenente ad una tra tre differenti classi. In particolare, ogni singola pianta \u00e8 un campione del dataset, la quale \u00e8 a sua volta caratterizzata da quattro feature, ovvero lunghezza ed ampiezza sia di sepalo, sia di petalo.</p> <p>In base al tipo di esperienza fornita dal dataset, l'algoritmo pu\u00f2 essere supervisionato o non supervisionato; come abbiamo gi\u00e0 visto, gli algoritmi di learning non supervisionato apprendono propriet\u00e0 intrinseche della struttura del dataset, mentre quelli di learning supervisionato imparano a relazionare tali propriet\u00e0 con una label o un target. In altri termini:</p> <ul> <li>l'apprendimento non supervisionato prevede l'osservazione di molti esempi di un vettore casuale \\(x\\) a partire dai quali si apprende la distribuzione di probabilit\u00e0 \\(p(x)\\), o delle propriet\u00e0 interessanti della stessa;</li> <li>l'apprendimento supervisionato prevede l'osservazione di molti esempi di un vettore casuale \\(x\\) a cui \u00e8 associato un vettore \\(y\\) a partire dai quali si apprende la distribuzione di probabilit\u00e0 \\(p(y|x)\\).</li> </ul> <p>Il termine supervisionato si origina quindi dal fatto che il target \\(y\\) viene fornito da un \"istruttore\" che mostra al sistema di machine learning la \"verit\u00e0\", figura che non \u00e8 presente nell'apprendimento non supervisionato.</p> <p>Reinforcement learning</p> <p>Gli algoritmi non fanno esperienza su di un dataset, ma apprendono ad interagire con un ambiente mediante un apposito loop di feedback.</p>"},{"location":"material/03_ml/01_intro/05_base/#descrivere-un-dataset","title":"Descrivere un dataset","text":"<p>Il dataset \u00e8 descritto in termini della cosiddetta design matrix, che contiene tante righe quanti sono i campioni disponibili, e tante colonne quante sono le feature. Ad esempio, il dataset Iris sar\u00e0 caratterizzato da una design matrix del tipo \\(X \\in \\mathbb{R}^{150 \\times 4}\\), dove \\(X(i, 1)\\) \u00e8 la lunghezza del sepalo della pianta \\(i\\), \\(X(i, 2)\\) \u00e8 l'ampiezza del sepalo della stessa pianta, e via dicendo. La design matrix definisce quindi ogni campione come un vettore della stessa lunghezza. Ad esempio, una design matrix non pu\u00f2 contenere delle immagini con un numero diverso di pixel.</p> <p>Nel caso dell'apprendimento supervisionato, alla design matrix si associa un vettore di label \\(y\\), con \\(y_i\\) che fornisce la label per il campione associato alla riga \\(i\\)-ma.</p>"},{"location":"material/03_ml/01_intro/06_generalization/","title":"3.1.5 - Generalizzazione","text":"<p>L'obiettivo principale di un algoritmo di machine learning \u00e8 quello di avere buone performance anche su input non visti in precedenza. In altre parole, l'algoritmo deve essere in grado di generalizzare la conoscenza acquisita sul dataset su cui \u00e8 stato addestrato.</p> <p>Tipicamente, un algoritmo viene addestrato su un set di training, sulla quale calcoliamo una serie di metriche che, a seconda del tipo di problema, vogliamo massimizzare o minimizzare. I lettori pi\u00f9 attenti noteranno che questo altro non \u00e8 se non un problema di ottimizzazione: tuttavia, quello che separa nei fatti il machine learning da un problema di questo tipo \u00e8 che nel machine learning si desidera che l'errore di generalizzazione sia quanto pi\u00f9 possibile ridotto. Questo errore \u00e8 formalmente definito come il valore atteso dell'errore su tutti gli input non visti prima dal modello.</p> <p>Tipicamente, l'errore di generalizzazione \u00e8 stimato misurando le performance dell'algoritmo addestrato su di un insieme di esempi di test, chiamato per l'appunto test set, collezionati separatamente dai dati di addestramento. In tal senso, ci viene in aiuto il campo della statistica, che ci dice che nel caso in cui sia possibile fare delle ipotesi su come i set di training e di test sono stati collezionati possiamo mettere in relazione le performance sul test set anche se possiamo soltanto osservare il training set.</p> <p>In particolare, i dati di training e di test vengono generati da un'unica distribuzione di probabilit\u00e0 chiamato meccanismo di generazione dei dati. Tipicamente, facciamo un insieme di ipotesi chiamate collettivamente ipotesi i.i.d., ovvero:</p> <ul> <li>i campioni sono distribuiti indipendentemente gli uni dagli altri;</li> <li>gli insieme di training e di test sono identicamente distribuiti, ovvero estratti ciascuno dalla stessa distribuzione di probabilit\u00e0.</li> </ul> <p>Grazie all'ipotesi i.i.d., possiamo descrivere il processo di generazione dei dati mediante un'unica distribuzione, che possiamo indicare con \\(p_{data}\\). Se questa ipotesi \u00e8 vera, possiamo aspettarci che, dato un modello, il valore atteso dell'errore su campioni selezionati casualmente dal dataset di training \u00e8 uguale a quello su campioni selezionati casualmente dal dataset di test.</p> <p>Naturalmente, quando usiamo un algoritmo di machine learning, il modello non viene fissato a priori, ma a seguito del campionamento del dataset di training. Una volta acquisito detto dataset, i parametri del modello saranno addestrati su di esso, per cui l'errore atteso sul test set sar\u00e0 maggiore di quello atteso sul training set. Di conseguenza, gli obiettivi dell'addestramento sono due:</p> <ol> <li>minimizzare l'errore di training;</li> <li>minimizzare il gap tra l'errore di training e quello di test.</li> </ol> <p>Ci\u00f2 comporta due sfide: l'underfitting e l'overfitting. L'underfitting avviene quando il modello non \u00e8 in grado di ottenere un errore sufficientemente basso sul training set; l'overfitting avviene quando il gap tra l'errore di training e quello di test \u00e8 troppo elevato.</p> <p>Possiamo controllare se un modello \u00e8 pi\u00f9 prono ad overfitting o underfitting modificandone la capacit\u00e0, ovvero l'abilit\u00e0 di adatatrsi ad un range pi\u00f9 o meno elevato di funzioni e possibilit\u00e0. In altre parole, un modello a basso capacit\u00e0 potrebbe avere problemi di underfitting, mentre un modello ad alta capacit\u00e0 \u00e8 pi\u00f9 prono all'overfitting, dato che memorizza delle propriet\u00e0 del training set che non serve utilizzare sul test set.</p> <p>Un modo per controllare la capacit\u00e0 di un modello \u00e8 quello di scegliere il suo spazio delle ipotesi, ovvero l'insieme di funzioni che l'algoritmo di apprendimento pu\u00f2 selezionare come possibile soluzione. Ad esempio, l'algoritmo di regressione lineare pu\u00f2 scegliere una qualsiasi funzione lineare, e pu\u00f2 essere generalizzato includendo dei polinomi nello spazio delle ipotesi, aumentando la capacit\u00e0 del modello.</p>"},{"location":"material/03_ml/01_intro/07_hyperparameters/","title":"3.1.6 - Iperparametri","text":"<p>Molti algoritmi di machine learning hanno degli iperparametri, ovvero delle \"impostazioni\" che possiamo utilizzare per controllare il comportamento del modello. A differenza dei parametri, gli iperparametri non sono automaticamente adattati durante l'addestramento.</p> <p>Apprendimento degli iperparametri</p> <p>Vedremo come esistono algoritmi che permettono di definire la miglior combinazione possibile per gli iperparametri di un modello dato un certo dataset.</p> <p>Alle volte, un iperparametro \u00e8 definito come tale perch\u00e9 \u00e8 difficile da ottimizzare; tuttavia, capita pi\u00f9 spesso che detti valori non devono essere appresi soltanto sul training set, ma devono avere valenza generale, allo scopo di evitare underfitting ed overfitting. Ci\u00f2 ci riconduce al concetto di generalizzazione visto nella lezione precedente.</p> <p>Abbiamo visto come la stima dell'errore di generalizzazione una volta completato l'addestramento avvenga su un test set: in tal senso, \u00e8 importante che i campioni del test set non siano in alcun modo usati per condizionare l'addestramento del modello. Di conseguenza, andremo a selezionare un set di validazione a partire dal training set: questo sar\u00e0 usato per stimare l'errore di generalizzazione durante o immediatamente dopo l'addestramento, permettendo l'addestramento degli iperparametri. Normalmente, il dataset iniziale viene suddiviso in percentuali pari all'\\(80\\%\\) per i dati di training ed il \\(20\\%\\) per i dati di validazione.</p> <p>Errore sul set di validazione</p> <p>Dato che il set di validazione \u00e8 preso dal set di training, l'errore di generalizzazione calcolato su di esso sar\u00e0 inferiore rispetto a quello calcolato sul set di test.</p> <p> </p> Figura 1 - Un esempio del processo di training con minimizzazione di errore di training e test, oltre che ottimizzazione degli iperparametri sult est di validazione."},{"location":"material/03_ml/01_intro/08_regularization/","title":"3.1.7 - Regolarizzazione","text":""},{"location":"material/03_ml/01_intro/08_regularization/#il-no-free-lunch-theorem","title":"Il No Free Lunch Theorem","text":"<p>La teoria matematica alla base del machine learning afferma che un algoritmo sia in grado di generalizzare usando un dataset di addestramento di cardinalit\u00e0 finita. Questo sembra contraddire alcuni principi logici di base: infatti, l'intuizione ci porta a dire che per inferire una legge che descriva ogni campione in un insieme occorre avere delle informazioni su ogni membro dello stesso.</p> <p>Il machine learning \"aggira\" questo problema usando delle regole di tipo probabilistico. In altre parole, un modello di machine learning individua regole che sono probabilmente corrette sulla maggiro parte dei membri dell'insieme di dati.</p> <p>Tuttavia, questo non risolve realmente il problema. Infatti, il no free lunch theorem (NFL) afferma che, considerando tutte le distribuzioni possibili per la generazione dei dati, ogni algoritmo mostra lo stesso errore di generalizzazione. In altre parole, non esiste un algoritmo di machine learning universalmente migliore degli altri.</p> <p>Fortunatamente, l'NFL \u00e8 valido soltanto considerando tutte le possibili distribuzioni di dati. Nella realt\u00e0, possiamo progettare degli algoritmi che si comportino bene sul nostro specifico problema. Questo significa che l'obiettivo del machine learning non \u00e8 quello di ricercare un algoritmo universale, ma di comprendere che tipi di distribuzioni sono rilevanti per caratterizzare il nostro problema, e quali algoritmi sono in grado di avere un errore di generalizzazione basso su questo tipo di distribuzioni generatrici.</p>"},{"location":"material/03_ml/01_intro/08_regularization/#regolarizzazione","title":"Regolarizzazione","text":"<p>Il NFL richiede di progettare un algoritmo di machine learning che lavori bene su un task specifico, creando un insieme di preferenze nell'algoritmo di apprendimendo allineate con il problema da risolvere.</p> <p>Finora, l'unico modo che abbiamo discusso per modificare un algoritmo \u00e8 stato quello di cambiarne la capacit\u00e0 modificandone gli iperparametri. Tuttavia, procedere esclusivamente in questo modo \u00e8 semplicistico: infatti, il comportamento di un modello \u00e8 influenzato anche dall'identit\u00e0 specifica delle funzioni contenute nello spazio di ricerca. Ad esempio, la regressione lineare ha uno spazio delle ipotesi che consiste nell'insieme delle funzioni lineari dei suoi ingressi. Queste funzioni lineari possono essere utili per problemi dove la relazione tra gli input e gli output \u00e8 quasi lineare, ma sono inadatte a problemi non lineari. </p> <p>Problemi non lineari</p> <p>Per fare un esempio, una regressione lineare non pu\u00f2 caratterizzare il rapporto tra \\(x\\) e \\(sen(x)\\).</p> <p>Ci\u00f2 ci suggerisce che si possa controllare il comportamento del modello scegliendo il tipo di funzioni da includere nello spazio di ricerca. Un modo per farlo \u00e8 dare una certa preferenza per una soluzione rispetto ad un'altra: ci\u00f2 implica che entrambe le funzioni potranno essere utilizzate, ma una delle due avr\u00e0 un peso maggiore rispetto all'altra, che sar\u00e0 scelta soltanto se si adatta ai dati di training in maniera significativamente migliore della soluzione pi\u00f9 \"pesante\".</p> <p>Ad esempio, possiamo modificare il criterio di addestramento per la regressione lineare per fare in modo che includa il decadimento dei pesi. Per farlo, dobbiamo minimizzare una funzione \\(L(w)\\) che comprende sia l'errore quadratico medio sui dati di training, sia un criterio che esprima una preferenza per i pesi che abbiano una norma \\(L^2\\) minore. Nello specifico:</p> \\[ L(w) = MSE_{train} + \\lambda w^t w \\] <p>dove \\(\\lambda\\) \u00e8 un valore scelto a priori che controlla la forza della nostra preferenza per pesi pi\u00f9 piccoli. In pratica, se \\(\\lambda \\rightarrow 0\\) non imponiamo alcuna preferenza di minimizzazione dei pesi, metnre se \\(\\lambda \\rightarrow 1\\) i pesi \\(w\\) devono diventare piccoli. Minimizzare \\(L(w)\\) fa in modo che i pesi scelti siano un compromesso tra il fitting dei dati di training ed il loro valore assoluto; in altri termini, avremo soluzioni a pendenza inferiore, o che danno peso ad un numero minore dif eature.</p> <p>In generale, ad un modello che apprende una funzione \\(f(x; \\theta)\\) viene regolarizzato mediante una funzione di penalit\u00e0 aggiunta alla funzione di costo. Nel caso del decadimento dei pesi, la funzione di regolarizzazione \u00e8 data da \\(\\Omega(w) = w^T w\\).</p> <p>Esprimere una \"preferenza\" per una funzione rispetto ad un'altra permette quindi di controllare la capacit\u00e0 del modello includendo o escludendo membri dallo spazio delle ipotesi. In altre parole, includere una funzione dallo spazio delle ipotesi, escludendone un'altra, significa esprimere una preferenza infinitamente forte per la prima. Nell'esempio del decadimento dei pesi, abbiamo esplicitamente espresso la nostra preferenza per funzioni lineari con pesi piccoli; tuttavia, esistono molti altri modi per esprimere preferenze per diverse soluzioni; insieme, questi approcci sono conosciuti come regolarizzazioni, ovvero tutte quelle modifiche fatte ad un algoritmo di learning intese a ridurre l'errore di generalizzazione preservando quello di training. In tal senso, \u00e8 opportuno seguire i dettami del NFL, andando a scegliere una forma di regolarizzazione specifica per il nostro problema, e che quindi si adatti a modello e dati analizzati.</p>"},{"location":"material/03_ml/02_lin_reg/lecture.en/","title":"Lecture.en","text":"<p>coming soon...</p>"},{"location":"material/03_ml/02_lin_reg/lecture/","title":"3.2 - La regressione lineare","text":"<p>Nelle lezioni precedenti abbiamo visto come esistano fondamentalmente due tipi di tecniche di apprendimento supervisionato, ovvero regressione e classificazione. In questa lezione, approfondiremo il caso pi\u00f9 semplice di regressione, ovvero quello lineare.</p>"},{"location":"material/03_ml/02_lin_reg/lecture/#cosa-e-la-regressione","title":"Cosa \u00e8 la regressione?","text":"<p>Non occorre essere esperti di meteorologia per sapere che i millimetri di pioggia caduti durante una precipitazione sono correlata a fattori quali temperatura, venti, umidit\u00e0, posizione geografica, ed altri ancora. Immaginiamo quindi di avere un insieme di dati che, al loro interno, descrivano la condizione meteorologica ad un dato istante di tempo (supponiamo, per semplicit\u00e0, giornaliero), oltre che i millimetri di pioggia caduti nello stesso periodo. Supponiamo di effettuare un'analisi esplorativa dei dati, i cui risultati sono (parzialmente) riportati in figura 1.</p> <p> </p> Figura 1 - Relazione tra millimetri di pioggia (sulle ordinate) e temperatura in gradi (sulle ascisse). <p>La nostra analisi esplorativa ci porta subito ad intuire l'esistenza di una relazione tra la quantit\u00e0 di pioggia caduta e la temperatura: in particolare, a valor di temperatura pi\u00f9 elevati corrisponde una minore quantit\u00e0 di pioggia, e viceversa.</p> <p>Proviamo adesso a tracciare la retta ai minimi quadrati, ovvero quella che minimizza la somma quadratica dello scarto tra il valore \\((\\hat{x}_i, \\hat{y}_i)\\) da essa \"attraversato\" ed il valore \\((x_i, y_i)\\) \"vero\" associato ai dati, per ciascun punto \\(i\\) nell'insieme considerato. Il risultato \u00e8 mostrato in figura 2.</p> <p> </p> Figura 2 - Retta ai minimi quadrati relativa al rapporto tra precipitazioni e gradi."},{"location":"material/03_ml/02_lin_reg/lecture/#rappresentazione-analitica-del-modello","title":"Rappresentazione analitica del modello","text":"<p>Come gi\u00e0 detto, notiamo come i millimetri di pioggia attesi diminuiscano all'aumentare della temperatura, secondo una relazione (pi\u00f9 o meno) lineare, riconducibile ad una forma del tipo:</p> \\[ y = mx + b \\] <p>dove:</p> <ul> <li>\\(y\\) sono i millimetri di pioggia medi caduti nell'arco di tutte le giornate con un dato valore medio di temperatura;</li> <li>\\(x\\) \u00e8 il valore medio di temperatura;</li> <li>\\(m\\) \u00e8 il coefficiente angolare della retta di regressione;</li> <li>\\(b\\) \u00e8 l'incercetta della retta di regressione.</li> </ul> <p>Nota</p> <p>Ovviamente, la retta di regressione non tocca tutti i punti, ma approssima l'andamento di \\(x\\) in funzione di \\(y\\).</p> <p>Possiamo riscrivere l'equazione precedente come segue:</p> \\[ \\hat{y} = b + w_1 x_1 \\] <p>dove:</p> <ul> <li>\\(\\hat{y}\\) \u00e8 l'output predetto dal modello, detto anche variabile dipendente;</li> <li>\\(b\\) \u00e8 il bias, equivalente al concetto analitico di intercetta;</li> <li>\\(w_1\\) \u00e8 il peso della prima feature, equivalente al concetto analitico di coefficiente angolare;</li> <li>\\(x_1\\) \u00e8 il valore di ingresso assunto dalla prima ed unica feature considerata, detta anche variabile indipendente.</li> </ul> <p>Per inferire un nuovo valore di \\(\\hat{y}\\) ci baster\u00e0 quindi cambiare il valore assunto da \\(x_1\\). In pratica, proprio come accade per una retta, se poniamo \\(x_1 = 8\\) (ovvero assumendo un valore di temperatura di 8 gradi), avremo un corrispondente valore per le precipitazioni pari ad \\(\\hat{y} = 25\\) mm, mentre se \\(x_1 = 32\\) il valore predetto per i millimetri di pioggia sar\u00e0 \\(\\hat{y}=0\\).</p> <p>Regressione multivariata</p> <p>In questo caso, abbiamo presupposto che vi sia un'unica variabile indipendente a determinare il valore di un'unica variabile dipendente. Esistono ovviamente casi pi\u00f9 complessi, con diverse variabili dipendenti il cui valore \u00e8 determinato da pi\u00f9 feature secondo formule del tipo \\(b + w_1 x_1 + \\ldots + w_n x_n\\).</p>"},{"location":"material/03_ml/02_lin_reg/lecture/#addestramento-e-funzione-di-costo","title":"Addestramento e funzione di costo","text":"<p>Addestrare un modello significa determinarne i valori ottimali per pesi e bias a partire dai dati che, nel caso della regressione, sono etichettati. Per far questo, i modelli ad apprendimento supervisionato esaminano iterativamente tutti i campioni presenti nel set di addestramento alla ricerca di un modo per minimizzare un costo (o, in inglese, loss) direttamente proporzionale alla differenza intercorrente tra la predizione del modello ed il valore vero. In altri termini, la loss \u00e8 una misura dell'errore delle predizioni effettuate dal modello.</p>"},{"location":"material/03_ml/02_lin_reg/lecture/#interpretazione-grafica-della-funzione-di-costo","title":"Interpretazione grafica della funzione di costo","text":"<p>Nel caso ideale, la loss sarebbe pari a \\(0\\). Ovviamente, nel caso reale, la loss \u00e8 sempre maggiore di questo valore, e sar\u00e0 tanto maggiore quanto pi\u00f9 il valore predetto diverger\u00e0 da quello atteso. Proviamo ad interpretare al meglio questo concetto aiutandoci con la figura 3.</p> <p> </p> Figura 3 - Interpretazione della funzione di costo <p>In particolare, le frecce schematizzano l'entit\u00e0 della differenza intercorrente tra il valore vero \\(y\\) ed il valore predetto \\(\\hat{y}\\). Come evidente, la loss complessiva \u00e8 maggiore nella situazione a sinistra; ci\u00f2 significa che l'addestramento dovr\u00e0 necessariamente prediligere una retta di regressione come quella mostrata a destra.</p>"},{"location":"material/03_ml/02_lin_reg/lecture/#calcolo-della-funzione-di-costo","title":"Calcolo della funzione di costo","text":"<p>Per calcolare la loss complessiva del modello a partire da un dato insieme di campioni dobbiamo utilizzare una funzione di costo, o loss function. Ne esistono molteplici esempi, alcuni dei quali saranno approfonditi nel prosieguo. Tuttavia, uno dei pi\u00f9 semplici \u00e8 dato dall'errore quadratico medio, o mean squared error (MSE), rappresentabile secondo la seguente formula:</p> \\[ MSE = \\frac{1}{N} \\sum_{(x, y) \\in D} (y - \\hat{y})^2 \\] <p>In particolare:</p> <ul> <li>\\((x, y)\\) rappresenta l'insieme di feature \\(x\\) con la corrispondente label \\(y\\);</li> <li>\\(\\hat{y}\\) \u00e8 il valore predetto della label a partire dall'applicazione del modello;</li> <li>\\(D\\) \u00e8 il nostro dataset etichettato;</li> <li>\\(N\\) \u00e8 il numero di campioni prensenti in \\(D\\).</li> </ul> <p>In pratica, l'MSE \u00e8 dato dalla media delle differenze tra il valore predetto dal modello e quello vero calcolata sull'intero dataset. Come prevedibile, l'errore sar\u00e0 tanto pi\u00f9 alto quanto maggiore \u00e8 la distanza complessiva tra le label e le predizioni; nell'esempio visto in figura 3, appare chiaro come l'MSE sia maggiore per la prima approssimazione rispetto alla seconda.</p> <p>Perch\u00e9 quadratico?</p> <p>I pi\u00f9 attenti avranno notato che stiamo utilizzando il quadrato dell'errore. Se non lo facessimo, gli errori \"positivi\" annullerebbero i \"negativi\", il che ovviamente non \u00e8 il nostro obiettivo, in quanto ci interessa soltanto il modulo dello scarto quadratico.</p>"},{"location":"material/03_ml/02_lin_reg/lecture/#addestramento-iterativo","title":"Addestramento iterativo","text":"<p>Gli algoritmi di machine learning sono normalmente addestrati seguendo un approccio iterativo che prevede, al termine di ogni singola iterazione, l'aggiornamento del valore dei pesi, allo scopo di ridurre il costo complessivo associato alle predizioni dell'algoritmo. Possiamo riassumere questo comportamento mediante il seguente schema:</p> <pre><code>flowchart TB\n    A[Dataset] --&gt; B[Feature] &amp; C[Label];\n    B &amp; C --&gt; D[Prediction];\n    D --&gt; E[Loss evaluation];\n    E --&gt; F[Parameters update];\n    F --&gt; D;</code></pre> <p>Nella pratica, alla prima iterazione, il modello stabilisce in maniera casuale i valori dei parametri da utilizzare per la predizione; nel caso della regressione lineare, abbiamo visto come questi siano \\(w_1\\) e \\(b\\). A questo punto, viene calcolato il valore predetto \\(\\hat{y}\\)</p> <p>Nella pratica, ad ogni iterazione, il modello effettua una predizione a partire dai campioni a disposizione. Il risultato viene comparato con la label, ed il costo complessivo calcolato secondo la funzione scelta. I pesi sono quindi aggiornati seguendo una certa regola di ottimizzazione, ed il ciclo si ripete.</p> <p>Quante iterazioni?</p> <p>Le iterazioni non sono infinite: normalmente, si imposta un numero preciso di epoche di training, oppure si aspetta che l'algoritmo arrivi ad una sorta di \"convergenza\", nella quale il valore della loss non decresce ulteriormente.</p> <p>Quanti dati?</p> <p>Come vedremo anche nel seguito, il calcolo della loss non avviene considerando il dataset nella sua interezza, ma soltanto dei sottoinsiemi di dati per ogni iterazione.</p>"},{"location":"material/03_ml/02_lin_reg/lecture/#ottimizzazione-della-funzione-di-costo","title":"Ottimizzazione della funzione di costo","text":"<p>Come gi\u00e0 accennato, l'aggiornamento dei pesi segue una ben precisa regola di ottimizzazione, volta a minimizzare il valore complessivo assunto dal costo. In altre parole, durante l'addestramento, l'algoritmo cerca di trovare quella combinazione di pesi per la quale il costo legato all'approssimazione dei valori predetti ai valori veri \u00e8 minimo.</p> <p>Esistono diversi tipi di algoritmi di ottimizzazione, ma la maggior parte si rif\u00e0 al concetto di discesa del gradiente, schematizzto in figura 4.</p> <p> </p> Figura 4 - La discesa di gradiente <p>In questa sede, daremo un'interpretazione puramente qualitativa dei concetti alla base dell'algoritmo, che andremo poi ad approfondire in un'altra lezione. Per farlo, osserviamo brevemente cosa accade in figura 4, guardando da sinistra verso destra.</p> <p>Per prima cosa, dobbiamo immaginare che la funzione che modella la nostra loss sia una sorta di paraboloide, dotato di un valore minimo (sull'asse delle ordinate) che viene raggiunto in corrispondenza di una determinata combinazione dei pesi (mostrata sull'asse delle ascisse). Nel nostro esempio, presupponiamo un unico peso; tuttavia, nei casi reali, il numero dei pesi \u00e8 molto pi\u00f9 elevato, per cui lo spazio considerato sar\u00e0 \\(n\\) dimensionale, con \\(n\\) numero dei pesi da ottimizzare.</p> <p>L'immagine pi\u00f9 a sinistra rappresenta la situazione iniziale: abbiamo dei pesi nel ramo sinistro del paraboloide e, conseguentemente, l'obiettivo sar\u00e0 quello di spostarci verso il minimo globale assunto dalla funzione (ovvero, verso destra). Per farlo, possiamo utilizzare la derivata o, nel caso di funzioni \\(n\\)-dimensionali, il gradiente della funzione di costo, aggiornando i pesi in maniera che questo assuma, all'iterazione successiva, un valore inferiore. Questo ci porta alla figura centrale, in cui notiamo che il gradiente si muove dal punto rosso a quello blu; a seguito di questo spostamento, dovremo ancora aumentare il valore dei pesi, allo scopo di far diminuire la loss, portandoci quindi nella situazione raffigurata nella figura a destra.</p> <p>In quest'ultima situazione, vedremo che il segno del gradiente sar\u00e0 diventato positivo, in quanto ci troveremo nella parte ascendente del paraboloide. Di conseguenza, la convergenza dell'algoritmo si otterr\u00e0 diminuendo il valore assunto dai pesi.</p> <p>Learning rate</p> <p>Il \"quantitativo\" di cui sono aggiornati i pesi \u00e8 spesso denotato come learning rate. Un learning rate troppo basso porta ad una convergenza molto lenta dell'algoritmo, che potrebbe \"esaurire\" le iterazioni prima di arrivare al minimo della funzione di costo. Un learning rate eccessivamente altro potrebbe invece fare in modo che l'algoritmo \"salti\" da una parte all'altra del minimo, non arrivando neanche in questo caso a convergenza.</p> <p>Minimi locali</p> <p>Il nostro banale esempio presuppone che la funzione di costo non abbia alcun minimo locale. Ci\u00f2 non \u00e8 ovviamente vero, e delle scelte sbagliate in termini di punto di partenza o learning rate potrebbero farci finire all'interno di un minimo locale, impedendoci di arrivare a convergenza.</p>"},{"location":"material/03_ml/02_lin_reg/lecture/#overfitting-e-regolarizzazione","title":"Overfitting e regolarizzazione","text":"<p>Alle volte, accade che il nostro modello sia in grado di arrivare ad una loss estremamente bassa sui dati di training, ma che tuttavia inizia ad aumentare sui dati di validazione, un po' come mostrato in figura 5:</p> <p> </p> Figura 5 - Andamento della funzione di costo in training e validazione <p>Ci\u00f2 pu\u00f2 accadere per diversi motivi, come errori nei parametri di addestramento o dati non ben bilanciati. Ad ogni modo, questo fenomeno prende il nome di overfitting, e comporta che il modello, che si comporta molto bene sui dati di training, non riesca a generalizzare, comportandosi in maniera meno egregia su quelli di validazione. L'overfitting si manifesta all'aumentare delle epoche di training, quando il nostro modello diventa sempre pi\u00f9 \"complesso\", ed apprende sempre meglio a caratterizzare relazioni di complessit\u00e0 crescente intercorrenti tra feature e label.</p> <p>Per arginare il fenomeno dell'overfitting, oltre ad agire sui dati e sui parametri del modello, si inserisce spesso un termine di regolarizzazione, che tende a penalizzare un modello in grado di caratterizzare relazioni eccessivamente complesse. Il termine di regolarizzazione interviene direttamente sul valore trattato dall'ottimizzatore, che non avr\u00e0 pi\u00f9 come unico obiettivo quello di minimizzare la loss, ma quello di minimizzare congiuntamente la loss e la complessit\u00e0 del modello ottenuto.</p> <p>Una funzione di regolarizzazione molto usata \u00e8 la regolarizzazione \\(L_2\\), definita come la somma dei quadrati dei pesi associati alle feature:</p> \\[ L_2 = ||w||_2^2 = w_1^2 + w_2^2 + \\ldots + w_n^2 \\] <p>Minimizzare questo termine significa dare meno \"importanza\" ad alcuni pesi che inficiano la complessit\u00e0 totale del modello. Se, ad esempio, avessimo i seguenti pesi:</p> \\[ \\begin{aligned} &amp; w_1 = 0.1 \\\\ &amp; w_2 = 0.025 \\\\ &amp; w_3 = 0.4 \\\\ &amp; w_4 = 10 \\end{aligned} \\] <p>il termine di regolarizzazione \\(L_2\\) diverrebbe pari a:</p> \\[ L_2 = 0.01 + 0,000625 + 0.16 + 100 \\sim 100.17 \\] <p>E' evidente come la maggior parte del contributo sia data dal quarto peso, per cui risulta essere necessario diminuirne l'influenza nel modello allo scopo di bilanciare l'overfitting.</p>"},{"location":"material/03_ml/03_log_reg/lecture/","title":"3.3 - La regressione logistica","text":"<p>Nella lezione precedente abbiamo introdotto l'algoritmo di regressione lineare, il cui comito \u00e8 quello di \"tracciare\" la relazione intercorrente tra una serie di variabili indipendenti (le feature) ed una variabile dipendente che, come abbiamo visto, \u00e8 continua e di tipo numerico. La regressione logistica, invece, ed a discapito del nome, \u00e8 il pi\u00f9 semplice dei classificatori, e viene usata quando abbiamo a che fare con variabili di tipo categorico.</p> <p>A scopo di esempio, supponiamo di creare un modello che predica la probabilit\u00e0 che una mail ricevuta da un mittente a noi sconosciuto rappresenti uno spam. Indicheremo questa probabilit\u00e0 come \\(p(mail|unknown)\\).</p> <p>In pratica, se il modello afferma che \\(p(mail|unknown) = 0.05\\), allora, in media, su \\(100\\) mail ricevute da indirizzi sconosciuti, \\(5\\) saranno di spam:</p> \\[ \\begin{align} spam &amp;= p(mail|unknown) \\cdot mail_{rec} \\\\ &amp;= 0.05 * 100 \\\\ &amp;= 5 \\end{align} \\] <p>Questo \u00e8 un esempio di utilizzo della probabilit\u00e0 as is. In molti casi, tuttavia, mapperemo l'output della soluzione su un problema di classificazione binario, nel quale l'obiettivo \u00e8 predire correttamente uno di due possibili label (in questo caso, spam o non spam).</p>"},{"location":"material/03_ml/03_log_reg/lecture/#la-funzione-sigmoidale","title":"La funzione sigmoidale","text":"<p>Ci si potrebbe chiedere come un modello per la regressione logistica sia in grado di asicurarsi che l'uscita ricada sempre nell'intervallo tra \\(0\\) ed \\(1\\). In tal senso, questo \u00e8 assicurato dall'uso della funzione sigmoidale, definita come segue:</p> \\[ y = \\frac{1}{1+e^{-z}} \\] <p>la cui formulazione grafica \u00e8 la seguente:</p> <p> </p> Figura 1 - Interprete Python <p>Nell'espressione precedente, notiamo che:</p> <ul> <li>\\(y\\) \u00e8 l'uscita della regressione logistica;</li> <li>\\(z\\) \u00e8 pari, per un generico modello lineare, a \\(b + w_1 x_1 + \\ldots + w_N z_N\\).</li> </ul>"},{"location":"material/03_ml/03_log_reg/lecture/#funzione-di-costo","title":"Funzione di costo","text":"<p>La funzione di costo per la funzione logistica \u00e8 chiamata log loss, ed \u00e8 espressa come:</p> \\[ LogLoss = \\sum_{(x, y) \\in D} -y log(y') - (1 - y) log (1 - y') \\] <p>dove:</p> <ul> <li>\\((x, y)\\) sono le coppie date da feature e label nel dataset \\(D\\);</li> <li>\\(y\\) \u00e8 la label vera per un dato insieme di feature;</li> <li>\\(y'\\) \u00e8 il valore predetto.</li> </ul>"},{"location":"material/03_ml/03_log_reg/lecture/#soglia-di-decisione","title":"Soglia di decisione","text":"<p>La regressione logistica restituisce quindi una probabilit\u00e0, la quale dovr\u00e0 essere in qualche modo \"convertita\" in un valore relativo ad una classe.</p> <p>Tornando al nostro esempio, un modello che restituisce una probabilit\u00e0 \\(p\\) pari a \\(0.999\\) ci dice che, molto probabilmente, il messaggio elaborato \u00e8 di spam. Di converso, nel caso il modello restituisca una probabilit\u00e0 \\(p\\) pari a \\(0.003\\), allora \u00e8 molto probabile che il messaggio non sia spam. Cosa accade, per\u00f2, quando \\(p\\) \u00e8 pari a \\(0.495\\)?</p> <p>Essendo il nostro modello deterministico, \u00e8 necessario passare da un valore puramente numerico (ovvero, la probabilit\u00e0) ad una classe che, nel caso specifico, pu\u00f2 essere positiva (messaggio di spam, con \\(p\\) genericamente \"alto\") o negativa (messaggio non di spam, con \\(p\\) genericamente \"basso\"). Di conseguenza, dovremo definire una soglia decisionale, in base alla quale decideremo che i messaggi con probabilit\u00e0 sopra-soglia sono spam, mentre quelli con probabilit\u00e0 sotto-soglia sono legittimi. Di default, questo valore viene impostato a \\(\\rho=0.5\\); tuttavia, potrebbe essere necessario ridefinirlo sulla base dello specifico problema sotto esame. Ne parleremo pi\u00f9 approfonditamente nella lezione dedicata alle metriche.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/","title":"3.4.1 - Gli alberi decisionali","text":"<p>Gli alberi decisionali sono una tra le famiglie di modelli maggiormente utilizzate per l'apprendimento supervisionato, e sono in grado di risolvere sia problemi di classificazione, sia di regressione. In particolare, gli alberi decisionali offrono alcuni benefici rispetto ad altri tipi di modello, tra cui:</p> <ul> <li>semplicit\u00e0 di configurazione, grazie alla presenza di un numero limitato di parametri, la cui modifica influenza in maniera abbastanza limitata il risultato finale;</li> <li>gestione di feature di diverso tipo, ovvero sia numeriche, sia categoriche, con l'ovvia conseguenza di richiedere un preprocessing limitato rispetto ad altri modelli;</li> <li>efficienza con dataset di piccole dimensioni.</li> </ul> <p>In pratica, gli alberi decisionali sono spesso in grado di fornire una buona accuratezza, sono semplici da configurare, robusti a rumore e feature mancanti, non richiedono preprocessing, ed i risultati ottenuti sono facilmente interpretabili. </p> <p>Gli alberi decisionali sono molto efficaci quando si usano dei dataset di tipo tabellare, normalmente contenuti in file CSV o tabelle di database, come ad esempio il dataset Titanic. Tuttavia, risultano inadeguati quando il tipo di dato utilizzato non \u00e8 strutturato: in pratica, non possiamo efficacemente utilizzarli su immagini o testo.</p> <p>Un altro punto di forza \u00e8 che un albero decisionale \u00e8 efficace anche su dataset di dimensioni ridotte, ovverosia quelli nei quali il rapporto tra feature e numero di campioni \u00e8 di poco superiore ad uno; si parla, in questo caso, di sample efficiency.</p> <p>Alberi e dati</p> <p>Anche se gli alberi decisionali sono sample efficient, il loro funzionamento risulta comunque migliore nel caso di disponibilit\u00e0 di grosse quantit\u00e0 di dati.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#funzionamento-degli-alberi-decisionali","title":"Funzionamento degli alberi decisionali","text":"<p>Un albero decisionale \u00e8 un modello creato a partire da una serie di condizioni, che potremmo interpretare come delle \"domande\", organizzate in maniera gerarchica a ricalcare, per l'appunto, un albero. Ognuna delle foglie dell'albero contiene il valore predetto, che pu\u00f2 essere sia una classe, sia un valore numerico; i restanti nodi, invece, descrivono una certa condizione. Un semplice esempio \u00e8 mostrato nel seguente schema.</p> <pre><code>flowchart TB\n  A[\"Zampe &gt; 2\"] --&gt;|S\u00ec| B[\"Occhi &gt; 2\"]\n  A --&gt;|No| C(Gallina)\n  B --&gt;|S\u00ec| D(Ragno)\n  B --&gt;|No| E(Cane)</code></pre> <p>L'abero decisionale determina il valore predetto seguendo il percorso che va dalla radice fino ad una delle foglie, a seconda dei valori assunti dalle diverse feature. Questo cammino \u00e8 detto percorso di inferenza. Nel nostro caso, alla radice dell'albero viene valutata la feature Zampe. In particolare, se questo valore \u00e8 maggiore di \\(2\\) andremo a valutare il valore della feature Occhi; in caso contrario, il percorso di inferenza ci porta direttamente a predire come Gallina l'animale caratterizzato.</p> <p>Per quello che riguarda il nostro esempio, immaginiamo di avere un campione con i seguenti valori per le feature:</p> <ul> <li>Zampe: 4</li> <li>Occhi: 2</li> </ul> <p>Dato che le zampe sono \\(4\\), alla prima domanda si risponder\u00e0 ovviamente con un No. Andando quindi a valutare la seconda condizione (il numero di occhi), avremo in questo caso un valore pari a \\(2\\), per cui il risultato della predizione sar\u00e0 un cane.</p> <pre><code>flowchart TB\n  A[\"Zampe &gt; 2\"] --&gt;|S\u00ec| B[\"Occhi &gt; 2\"]\n  A --&gt;|No| C(Gallina)\n  B --&gt;|S\u00ec| D(Ragno)\n  B --&gt;|No| E(Cane)\n  linkStyle 0,3 stroke-width:2px,fill:none,stroke:red;</code></pre> <p>Nell'esempio appena visto, il valore predetto \u00e8 categorico, e le feature numeriche. Facciamo un esempio per dimostrare la versatilit\u00e0 degli alberi decisionali, predicendo la tenerezza di un animale a partire dalle categorizzazioni delle feature Pelosit\u00e0 e Carineria.</p> <pre><code>flowchart TB\n  A[\"Pelosit\u00e0 &gt; medio\"] --&gt;|Si| B[\"Carineria &gt; medio\"]\n  A --&gt;|No| C(1)\n  B --&gt;|S\u00ec| D(3)\n  B --&gt;|No| E(2)</code></pre> <p>Questo problema \u00e8 caratterizzabile come un problema di regressione: infatti, a partire da feature di tipo categorico, il risultato predetto \u00e8 numerico.</p> <p>Risultati della regressione</p> <p>Dall'esempio precedente, \u00e8 evidente come cani, gatti e pulcini siano gli animali che ispirano pi\u00f9 tenerezza nell'essere umano. D'altro canto, aracnidi, insetti e serpenti appaiono spesso molto meno teneri ed affettuosi.</p> <p>Per andare avanti nella nostra discussione, dobbiamo adesso distinguere tra i diversi tipi di condizione descrivibili da un albero decisionale.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#condizioni-in-un-albero-decisionale","title":"Condizioni in un albero decisionale","text":"<p>Esistono due categorizzazioni possibili per le condizioni in un albero decisionale.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#axis-aligned-vs-oblique","title":"Axis-aligned vs. oblique","text":"<p>La prima categorizzazione possibile per le condizioni in un albero decisionale riguarda il fatto che queste interessino una o pi\u00f9 feature. In particolare, una condizione che interessa un'unica feature \u00e8 detta axis-aligned, mentre una condizione oblique riguarda diverse feature.</p> <p>Tornando all'esempio precedente, la condizione \\(Zampe &gt; 2\\) \u00e8 chiaramente axis-aligned, in quanto prevede la valutazione esclusiva del numero di zampe dell'animale. Se per qualche motivo la condizione fosse del tipo \\(Zampe &lt; Occhi\\), allora avremmo a che fare con una condizione oblique.</p> <p>Cosa accade nella pratica?</p> <p>Spesso gli alberi decisionali contengono soltanto condizioni axis-aligned, soprattutto a causa del fatto che pu\u00f2 essere saggio eliminare feature interdipendenti prima di effettuare l'addestramento. Le condizioni oblique restano comunque potenzialmente molto potenti, in quanto in grado di modellare relazioni complesse, anche se, nella pratica, l'uso (ed abuso) di queste condizioni non garantisce necessariamente migliori performance.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#binarie-vs-non-binarie","title":"Binarie vs. non binarie","text":"<p>La seconda distinzione che \u00e8 possibile fare tra diversi tipi di condizione riguarda quelle binarie, ovvero quelle che hanno soltanto due possibili esiti, e non binarie che, prevedibilmente, hanno pi\u00f9 di due possibili esiti. Prevedibilmente, un albero decisionale contenente esclusivamente delle condizioni binarie \u00e8 detto binario, mentre uno contenente anche condizioni non binarie \u00e8 detto non binario.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#addestramento-degli-alberi-decisionali","title":"Addestramento degli alberi decisionali","text":"<p>Come per tutti i modelli di apprendimento supervisionato, gli alberi decisionali sono addestrati a spiegare al meglio un insieme di esempi di training. Gli algoritmi utilizzati nell'addestramento usano spesso un approccio di tipo divide-et-impera. Infatti, l'algoritmo inizia creando un singolo nodo, ovvero la radice, e quindi va ad aumentare le dimensioni dell'albero in maniera ricorsiva usando un approccio greedy.</p> <p>Approccio greedy</p> <p>Per approccio greedy si intende il metodo secondo il quale l'algoritmo va a scegliere la migliore opzione possibile al momento, senza considerare quello che accadr\u00e0 negli istanti successivi.</p> <p>In particolare, l'addestramento valuta, per ogni nodo, tutte le possibili condizioni, scegliendo quella con il punteggio pi\u00f9 alto, in modo da massimizzare il valore della metrica.</p> <p>Ad esempio, nel nostro dataset, tutti i cani ed i ragni hanno un numero di zampe maggiore di quattro, mentre tutte le galline ne hanno due. Di conseguenza, la condizione \\(Zampe &gt; 2\\) ci permette di arrivare a determinare in maniera affidabile le galline, ma non riesce a discernere tra ragni e cani. Di conseguenza, al primo step, l'albero sar\u00e0 in questa forma:</p> <pre><code>flowchart TB\n  A[\"Zampe &gt; 2\"] --&gt;|S\u00ec| B[\"Ragno o Cane\"]\n  A --&gt;|No| C[Gallina]</code></pre> <p>Successivamente, l'albero andr\u00e0 ad esplorare i nodi Ragno e Cane, cercando una maniera per caratterizzarli sulla base delle feature disponibili. Se non viene individuata una condizione soddisfacente, il nodo diventa una foglia: in altre parole, senza il numero di occhi, l'albero non riuscirebbe a distinguere tra ragni e cani.</p> <p>Ragni, cani, e zampe</p> <p>Nella realt\u00e0, nel caso precedente, l'algoritmo andrebbe a specializzare ulteriormente il valore considerato per le zampe.</p> <p>Vediamo adesso pi\u00f9 nel dettaglio i passi necessari a creare il precedente albero decisionale.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#step-1-creazione-del-nodo-radice","title":"Step 1: Creazione del nodo radice","text":"<p>Al primo step l'algoritmo si occupa di creare un nodo radice. Nel nostro caso, il nodo radice si occuper\u00e0 di valutare il numero di zampe.</p> <pre><code>flowchart TD\n\nA[\"Zampe\"]</code></pre>"},{"location":"material/03_ml/04_trees/01_decision_trees/#step-2-accrescimento-del-nodo-radice","title":"Step 2: Accrescimento del nodo radice","text":"<p>Al secondo step, accresceremo il nodo radice. L'algoritmo verificher\u00e0, in base ai dati a sua disposizione, che tutti i campioni etichettati come \"Ragno\" o \"Cane\" hanno pi\u00f9 di due zampe, mentre quelli etichettati con \"Gallina\" ne hanno soltanto due. Di conseguenza, andremo a creare due nodi figli:</p> <pre><code>flowchart TD\n\n  A[\"Zampe &gt; 2\"] --&gt; |S\u00ec| B[\"Ragno o Cane\"]\n  A --&gt; |No| C[Gallina]</code></pre>"},{"location":"material/03_ml/04_trees/01_decision_trees/#step-3-accrescimento-dei-nodi-figli","title":"Step 3: Accrescimento dei nodi figli","text":"<p>Proviamo in primis ad accrescere il nodo \"Gallina\". Ovviamente, dato che l'algoritmo non \u00e8 in grado di suddividere tra loro i dati etichettati in questo modo, non saranno effettuate ulteriori suddivisioni, per cui il nodo diverr\u00e0 una foglia.</p> <p>Provando adesso ad accrescere il nodo \"Ragno o Cane\". In questo caso, l'algoritmo trover\u00e0 la regola per la quale se il numero di occhi \u00e8 pari a 2, allora siamo in presenza di un \"Cane\"; altrimenti, qualora il numero di occhi \u00e8 superiore a 2, saremo in presenza di un Ragno. L'albero diventer\u00e0 il seguente:</p> <pre><code>flowchart TD\n\n  A[\"Zampe &gt; 2\"] --&gt; |S\u00ec| B[\"Occhi &gt; 2\"]\n  A --&gt;|No| C[Gallina]\n  B --&gt; |S\u00ec| D[Ragno]\n  B --&gt; |No| E[Cane]</code></pre>"},{"location":"material/03_ml/04_trees/01_decision_trees/#funzione-di-suddivisione-di-un-nodo","title":"Funzione di suddivisione di un nodo","text":"<p>Ogni albero decisionale si basa sulla routine chiamata splitter per individuare il miglior valore possibile per la soglia \\(t\\) sulla feature \\(f_i\\). </p> <p>L'efficienza dello splitter rappresenta il collo di bottiglia di un albero decisionale: a seconda del numero e del tipo di feature in input, il numero totale di possibili condizioni da verificare per un dato nodo pu\u00f2 essere molto grande. In tal senso, \u00e8 facile verificare che se la regola di split di un nodo \u00e8 data da \\(f_i \\geq t\\), con \\(f_i\\) \\(i\\)-ma feature e \\(t \\in \\mathbb{R}\\), l'insieme dei possibili valori di soglia \u00e8 infinito.</p> <p>Nella pratica, lo splitter non fa altro che massimizzare il valore di una funzione obiettivo, usando criteri come l'information gain, la Gini impurity, o anche l'RMSE.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#information-gain","title":"Information gain","text":"<p>Vediamo adesso un semplice esempio di come lavora uno splitter. Per farlo, ipotizziamo un classificatore binario, operante con feature puramente numeriche, e senza alcun valore mancante. Formalmente, ipotizziamo di avere un insieme di \\(n\\) campioni caratterizzati da una feature numerica ed un label binario, il quale potr\u00e0 assumere valore arancio o blu. Possiamo descrivere il dataset come:</p> \\[ D={(x_i, y_i)}_{i \\in[1, n]} \\] <p>dove:</p> <ul> <li>\\(x_i\\) \u00e8 il valore associato alla feature numerica, contenuto ovviamente in \\(\\mathbb{R}\\);</li> <li>\\(y_i\\) \u00e8 una valore per la label di classificazione binaria, che pu\u00f2 valere arancio o blu.</li> </ul> <p>Dovremo quindi trovare un valore di soglia \\(t\\) che ci permetta di suddividere l'insieme \\(D\\) ottenendo due gruppi \\(T\\) e \\(F\\) il quanto pi\u00f9 possibile \"simili\" all'assegnazione stabilita dalle label.</p> <p>Un modo per valutare questa similitudine \u00e8 utilizzare l'entropia di Shannon. Questa \u00e8 una misura del \"disordine\" presente nei nostri dati, e sar\u00e0:</p> <ul> <li>massima quando un gruppo \u00e8 perfettamente bilanciato (ovvero, il \\(50\\%\\) dei campioni \u00e8 arancione, ed il \\(50\\%\\) blu);</li> <li>minima quando un gruppo \u00e8 puro (ovvero, contiene solo campioni blu o arancioni).</li> </ul> <p>Ne consegue che l'obiettivo sar\u00e0 trovare una soglia \\(t\\) che diminuisca la sommatoria pesata dell'entropia delle distribuzioni delle label nei gruppi \\(T\\) ed \\(F\\). Questo valore \u00e8 chiamato information gain, e rappresenta la differenza tra l'entropia complessiva di \\(D\\), e quella data dall'insieme \\({T, F}\\). Per fare un esempio, osserviamo la figura 1. A sinistra, lo splitter seleziona una suddivisione ad alta entropia, che comporta un basso guadagno informativo. A destra, invece, lo splitter sceglie una suddivisione migliore, aumentando di conseguenza l'information gain.</p> <p> </p> Figura 1 - Splitting di $D$ <p>Formalmente:</p> \\[ \\begin{align} &amp;T = {(x_i, y_i)|(x_i, y_i) \\in D con x_i \\geq t} \\\\ &amp;F = {(x_i, y_i)|(x_i, y_i) \\in D con x_i &lt; t} \\\\ &amp;R(X) = \\frac{|{x|x \\in X, x=pos}|}{|X|} \\\\ &amp;H(X) = -p log p - (1-p) log(1-p) con p = R(X) \\\\ &amp;IG(D, T, F) = H(D) - \\frac{|T|}{|D|} H(T) - \\frac{|F|}{|D|}H(F) \\end{align} \\] <p>con:</p> <ul> <li>\\(IG(D,T,F)\\) guadagno infomrativo legato alla suddivisione di \\(D\\) in \\(T\\) ed \\(F\\).</li> <li>\\(H(X)\\) \u00e8 l'entropia dell'insieme di campioni \\(X\\).</li> <li>\\(|X|\\) \u00e8 il numero di elementi nell'insieme \\(|X|\\).</li> <li>\\(t\\) \u00e8 il valore di soglia.</li> <li>\\(pos\\) \u00e8 il valore della label positivo, ad esempio, blue nell'esempio prec edfentre. Scegelier una diversa label come positiva non cambia il valore dell'entropia o dell'information gain.</li> <li>\\(R(X)\\) \u00e8 il rapporto dei valori delle label positive nei campioni \\(X\\).</li> <li>\\(D\\) \u00e8 il dataset.</li> </ul> <p>Abbiamo gi\u00e0 sottolineato come vi siano infiniti valori possibili per la soglia \\(t\\). Tuttavia, dato un numero finito di campioni, possiamo stimare un numero finito di divisioni di \\(D\\) in \\(T\\) ed \\(F\\), per cui si pu\u00f2 tranquillamente dire che ha senso testare solo un numero finito di valori di \\(t\\).</p> <p>In tal senso, un approccio possibile \u00e8 ordinare i valori \\(x_j\\) in ordine crescente \\(x_{s(i)}\\), in modo che:</p> \\[ x_{s(i)} \\leq x_{s(i+1)} \\] <p>A questo punto, possiamo testare \\(t\\) per ogni valore a met\u00e0 tra due valori consecutivi di \\(x_{s(i)}\\). Se, per esempio, supponessimo che i primi due valori possibili per una feature numerica siano \\(3.5\\) e \\(3.7\\), avrebbe senso testare un primo valore di soglia pari a \\(t_1 = 8.6\\).</p> <p>Formalmente, potremo considerare l'insieme dei valori \\(X\\) candidati per \\(t\\):</p> \\[ X = \\{\\frac{x_{s(i)}+x_{s(i+1)}}{2}|x_{s(i)}  x_{s(i+1)}\\} \\] <p>Ci\u00f2 comporta che la complessit\u00e0 di questo algoritmo nel tempo sia un \\(O(n \\cdot log(n))\\), con \\(n\\) il numero di campioni. Quando applicato ad un albero decisionale, l'algoritmo di splitting viene applicato ad ogni nodo ed ogni feature. Supponendo che ogni nodo analizzi circa la met\u00e0 dei campioni analizzati dal nodo padre, potremo stimare la complessit\u00e0 per l'addestramento di un albero decisionale ricorrendo al teorema dell'esperto:</p> \\[ O(m \\cdot n \\cdot log(n)^2) \\] <p>con \\(m\\) numero di feature.</p> <p>Sottolineamo come l'algoritmo sia indipendente dalla scala o dalla distribuzione dei valori delle feature: infatti, soltanto l'ordine dei valori da queste assumibili \u00e8 da ritenersi importante. Per questa ragione, non \u00e8 necessario normalizzare o scalare le feature numeriche prima dell'addestramento di un albero decisionale.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#overfitting-e-pruning","title":"Overfitting e pruning","text":"<p>L'algoritmo descritto nel paragrafo precedente ci permette di addestrare un albero che classifichi perfettamente i campioni del dataset, a patto che questi siano separabili. Tuttavia, nel caso il dataset contenga del rumore, l'albero potrebbe andare in overfitting, mostrando scarse capacit\u00e0 di generalizzazione.</p> <p>Per limitare l'overfitting di un albero decisionale, quindi, andranno applicati dei criteri di regolarizzazione. In particolare, alcuni dei pi\u00f9 diffusi sono:</p> <ul> <li>impostare una profondit\u00e0 massima, ovvero fare in modo che l'albero decisionale non vada oltre un certo numero di livelli gerarchici (generalmente \\(10\\));</li> <li>impostare un numero minimo di campioni per ciascuna foglia, non considerando come valido un nuovo nodo che caratterizzi meno campioni di quelli impostati in questo valore di soglia.</li> </ul> <p>Un altro modo di ridurre l'overfitting \u00e8 rimuovere selettivamente alcuni rami mediante una procedura di pruning, che \"converte\" determinati nodi non-foglia in nodi foglia. </p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#interpretabilita-dellalbero-decisionale","title":"Interpretabilit\u00e0 dell'albero decisionale","text":"<p>Gli alberi decisionali sono molto semplici da interpretare. Per comprendere il perch\u00e9, rifacciamoci all'esempio mostrato in figura 2:</p> <p> </p> Figura 2 - Le regole stabilite da un albero decisionale <p>Le regole di questo albero decisionale, addestrato chiaramente sul dataset Tips, ci mostrano chiaramente che il primo nodo stabilisce come criterio di splitting una prima soglia sull'entit\u00e0 del conto, che va in due nodi figli, il primo del quale valuta come suddivisione la mancia, ed il secondo di nuovo il conto, stavolta con una soglia differente. Componendo le diverse regole, \u00e8 possibile arrivare ad una regola pi\u00f9 o meno complessa, ma in grado di trovare le relazioni di interesse all'interno dei nostri dati.</p>"},{"location":"material/03_ml/04_trees/01_decision_trees/#feature-importance","title":"Feature importance","text":"<p>Possiamo valutare numericamente l'impatto di ciascuna variabile sulla predizione data dall'albero mediante il concetto di feature importance, che associa un punteggio ad ogni feature, indicando quanto questa sia importante ai fini predittivi. Ad esempio, se per un modello con due feature \\(f_1, f_2\\) si quantifica che la loro importanza \u00e8 pari, rispettivamente, a \\(5.8\\) e \\(2.5\\), allora la feature \\(f_1\\) risulter\u00e0 essere pi\u00f9 discriminativa della feature \\(f_2\\).</p> <p>Per stabilire l'importanza delle diverse feature, possiamo usare metodi agnostici come il permutation importance, o anche criteri specificamente calcolati dall'albero, come il numero di nodi con una certa feature, o la profondit\u00e0 media dell'occorrenza di una determinata feature in tutti i percorsi dell'aalbeor.</p> <p>Ad esempio, il numero di nodi contenenti una certa feature quantifica il modo in cui l'albero considera la stessa, il che pu\u00f2 essere un indice della sua importanza: in pratica, il modello tende ad utilizzare molto le feature pi\u00f9 importanti, sfruttandole per le loro capacit\u00e0 di generalizzazione. Idealmente, comunque, a nessuna feature dovrebbe essere data una maggiore importanza rispetto ad altre, e tutte dovrebbero influenzare egualmente la predizione.</p>"},{"location":"material/03_ml/04_trees/02_decision_forests/","title":"3.4.2 - Foreste decisionali","text":"<p>Introduciamo le foreste decisionali con un aneddoto.</p> <p>Nel 1906, si svolse in Inghilterra una competizione in cui i partecipanti dovevano indovinare il peso di un bue, ovviamente senza l'uso di una bilancia. Al termine della competizione, l'errore mediano calcolato sulle predizioni di tutti i partecipanti fu di circa \\(17\\) kg; tuttavia, la mediana delle predizioni era errata di circa quattro kg. Questo combacia perfettamente con il principio della saggezza della folla: in determinate situazioni, il giudizio fornito dall'opinione collettiva risulta essere (inaspettatamente) molto preciso. Dal punto di vista statistico, questo principio segue il teorema centrale del limite, che dice che lo scarto quadratico medio tra il valore vero \\(x\\) e quello \\(\\hat{x}\\) derivante dalla media di \\(N\\) stime (rumorose) di \\(x\\) tende a zero con un fattore pari ad \\(\\frac{1}{n}\\). In pratica, quanti pi\u00f9 \"giudizi\" abbiamo, tanto pi\u00f9 sar\u00e0 preciso il \"giudizio collettivo\".</p> <p>Basandoci su queste considerazioni, definiamo come foresta decisionale un modello composto da un insieme di alberi decisionali, organizzati per dare un \"giudizio collettivo\" a riguardo di un dato problema. Il modo in cui i diversi alberi si aggregano dipende dallo specifico problema: ad esempio, nel caso di un problema multiclasse, ogni albero potrebbe dare il suo \"voto\" ad una certa classe, e la predizione globale sarebbe la classe pi\u00f9 votata. In un'altra implementazione, invece, ogni albero potrebbe dare in output un valore compreso tra zero ed uno, ed il valore predetto sarebbe quello dato dalla somma normalizzata dei valori predetti da ciascun albero. Tuttavia, l'approccio probabilmente pi\u00f9 noto ed utilizzato alle foreste decisionali \u00e8 quello chiamato random forest, per parlare delle quali dobbiamo introdurre il concetto di metodo ensemble.</p>"},{"location":"material/03_ml/04_trees/02_decision_forests/#metodi-ensemble","title":"Metodi ensemble","text":"<p>Un metodo ensemble \u00e8 un algoritmo in cui le predizioni di un insieme di modelli sono tra loro mediate. Il vantaggio principale di un ensemble \u00e8 che, posto che i singoli modelli risultino essere adeguatamente performanti, la qualit\u00e0 complessiva dell'ensemble \u00e8 generalmente pi\u00f9 alta della qualit\u00e0 dei modelli individuali; ovviamente, le maggiori performance comportano un maggior costo computazionale per l'addestramento e l'inferenza del modello.</p> <p>Ovviamente, affinch\u00e9 un metodo ensemble garantisca le performance migliori, i singoli modelli devono avere un certo grado di indipendenza reciproca. Infatti, se da un lato un ensemble composto da dieci modelli identici non potr\u00e0 mai avere performance migliori del singolo modello, dall'altro avere dieci modelli forzosamente indipendenti pu\u00f2 peggiorarne le performance. Di conseguenza, un ensembling efficace richiede di individuare un giusto bilanciamento tra indipendenza e qualit\u00e0 dei singoli modelli.</p>"},{"location":"material/03_ml/04_trees/02_decision_forests/#random-forest","title":"Random forest","text":"<p>Dopo aver brevemente introdotto il concetto di ensemble, possiamo introdurre quello di random forest. In particolare, le random forest sono un ensemble di alberi decisionali, nel quale ciascun albero \u00e8 addestrato introducendo un certo quantitativo di rumore casuale.</p> <p>Esistono diverse tecniche per creare in maniera efficace una random forest; vediamone alcune.</p>"},{"location":"material/03_ml/04_trees/02_decision_forests/#bagging","title":"Bagging","text":"<p>La tecnica del bagging (bootstrap aggregating) prevede che ogni albero decisionale sia addestrato su un sottoinsieme casuale dei campioni presenti nell'insieme di addestramento.</p> <p>In particolare, ogni albero decisionale viene addestrato su un numero di campioni pari a quello del dataset iniziale mediante la tecnica chiamata replacement training, che prevede che i valori utilizzati per l'addestramento del singolo albero siano il \\(67\\%\\) di quelli originari, scelti ovviamente in maniera casuale. Per esempio, se il dataset originario contiene \\(60\\) campioni, ogni albero sar\u00e0 addestrato usando \\(40\\) valori, scelti in maniera casuale. Ovviamente, alcuni dei campioni scelti dovranno essere replicati per rispettare il vincolo che vi siano comunque \\(60\\) dati di training.</p> <p>Facciamo un esempio con sei campioni e tre alberi decisionali, mostrandolo nella tabella successiva.</p> Campioni di training 1 2 3 4 5 6 D 1 1 1 1 1 1 AD1 1 0 2 0 2 1 AD2 2 2 0 1 0 1 AD3 2 1 2 0 0 1 <p>In particolare, nella tabella precedente:</p> <ul> <li>\\(D\\) \u00e8 il dataset iniziale, composto da sei diversi campioni;</li> <li>sulle colonne, abbiamo il numero di volte in cui ogni campione \u00e8 usato per addestrare ogni albero;</li> <li>sulle righe, abbiamo i diversi alberi decisionali (\\(AD_1\\), \\(AD_2\\), ed \\(AD_3\\)).</li> </ul> <p>Notiamo che:</p> <ul> <li>ogni albero decisionale \u00e8 addestrato su un totale di sei campioni;</li> <li>l'insieme di addestramento contiene sempre il \\(67\\%\\) dei possibili valori originari (ovvero quattro), alcuni dei quali vengono riutilizzati pi\u00f9 volte.</li> </ul> <p>Questa scelta, motivata nel paper originale, fa in modo che le performance ottenute siano le migliori possibili. Tuttavia, \u00e8 anche possibile utilizzare una tecnica di training senza replacement: in altri termini, ogni campione non pu\u00f2 essere presente pi\u00f9 di una volta nel set di training di un singolo albero, per cui ciascuno dei modelli sarebbe addestrato su un sottoinsieme casuale di quattro diversi campioni.</p>"},{"location":"material/03_ml/04_trees/02_decision_forests/#attribute-sampling","title":"Attribute sampling","text":"<p>A differenza del bagging, che prevede che l'albero sia addestrato su un sottoinsieme di campioni, l'attribute sampling fa in modo che ogni albero sia addestrato su un sottoinsieme casuale di feature. Un esempio di attribute sampling \u00e8 mostrato in figura 1.</p> <p> </p> Figura 1 - Un esempio di attribute sampling <p>Nella figura precedente, l'albero viene addestrato su \\(4\\) diverse feature: quelle in blu sono quelle testate nello specifico nodo, mentre quelle in bianco non sono testate. La regola dell'albero viene costruita a partire dalla feature maggiormente discriminativa, rappresentata mediante un contorno rosso.</p> <p>Il rapporto tra feature testate e feature totali \u00e8 un iperparametro molto importante a scopo di regolarizzazione. Nel nostro esempio, stiamo usando un rapporto del \\(50\\%\\); tuttavia, in molte implementazioni, questo rapporto \u00e8 pari ad \\(\\frac{F}{3}\\) in caso di regressione, e di \\(\\sqrt{F}\\) in caso di classificazione, con \\(F\\) numero di feature.</p>"},{"location":"material/03_ml/04_trees/02_decision_forests/#assenza-di-pruning","title":"Assenza di pruning","text":"<p>Gli alberi in una random forest sono addestrati senza usare la tecnica di pruning. Ci\u00f2 aumenta significativamente la varianza delle predizioni, riducendo il bias legato al singolo albero. In altri termini, l'assenza di pruning fa s\u00ec che il singolo possa andare in overfitting, ma la presenza di numerosi alberi fa in modo che il loro ensemble non sia affetto da questo fenomeno.</p> <p>Considerata l'assenza di pruning, \u00e8 possibile addestrare un random forest senza un dataset di validazione.</p>"},{"location":"material/03_ml/04_trees/02_decision_forests/#setting-degli-iperparametri","title":"Setting degli iperparametri","text":"<p>Teoricamente, un random forest pu\u00f2 essere addestrato senza impostare una profondit\u00e0 massima o un numero minimo di osservazioni per foglia. Nella pratica, tuttavia, impostare questi iperparametri comporta un beneficio, proprio come nel caso degli alberi decisionali. Due valori che sono spesso usati sono \\(16\\) per la massima profondit\u00e0, e \\(5\\) osservazioni minime per foglia.</p> <p>Un altro iperparametro che \u00e8 possibile impostare \u00e8 il numero di alberi decisionali presenti nell'ensemble. Per quello che abbiamo visto in precedenza, un numero pi\u00f9 elevato di alberi non fa altro se non \"rafforzare\" il voto raggiunto dalla \"massa\", per cui la qualit\u00e0 delle predizioni \u00e8 direttamente proporzionale all'entit\u00e0 di questo parametro. Va tuttavia sottolineato come, oltre un certo numero di alberi, dipendente dal problema sotto analisi, non vi siano miglioramenti significativi.</p>"},{"location":"material/03_ml/04_trees/02_decision_forests/#out-of-bag-evaluation","title":"Out-of-bag evaluation","text":"<p>Abbiamo in precedenza accennato al fatto che il random forest non richieda un dataset per la validazione. In tal senso, molte implementazioni dell'algoritmo utilizzano una valutazione chiamata out-of-bag (OOB) per la valutazione delle performance, che prevede che il training set sia trattato come il test set in caso di cross-validazione.</p> <p>Come gi\u00e0 detto, il singolo albero in un random forest \u00e8 tipicamente addestrato su circa il \\(67\\%\\) dei campioni disponibili; di conseguenza, il singolo albero non \"vede\" circa il \\(33\\%\\) dei restanti esempi. L'idea alla base della valutazione OOB \u00e8, dato un random forest addestrato sull'intero dataset, valutare il singolo albero soltanto sui campioni non visti in fase di addestramento.</p> <p>Ad esempio, nel caso illustrato nella tabella precedente:</p> Campioni di training Campioni per la valutazione OOB 1 2 3 4 5 6 D 1 1 1 1 1 1 AD1 1 0 2 0 2 1 2, 4 AD2 2 2 0 1 0 1 3, 5 AD3 2 1 2 0 0 1 4, 5 <p>Ad esempio:</p> <ul> <li>i campioni \\(2\\) e \\(4\\) saranno usati per validare l'albero \\(AD_1\\);</li> <li>i campioni \\(3\\) e \\(5\\) saranno usati per validare l'albero \\(AD_2\\);</li> <li>i campioni \\(4\\) e \\(5\\) saranno usati per validare l'albero \\(AD_3\\).</li> </ul> <p>Dimensioni del dataset ed OOB</p> <p>In questo caso, la dimensione del dataset limita fortemente l'efficacia della valutazione OOB. Nei casi reali, dato un dataset con dimensioni ragionevoli, tutti gli alberi avranno almeno un insieme di predizioni OOB.</p>"},{"location":"material/03_ml/04_trees/02_decision_forests/#interpretabilita-del-random-forest","title":"Interpretabilit\u00e0 del random forest","text":"<p>Data la loro natura di insieme di modelli, un random forest \u00e8 pi\u00f9 complesso da interpretare rispetto ad un singolo albero decisionale. Tuttavia, ci\u00f2 \u00e8 comunque possibile usando metodi indipendenti dal singolo algoritmo, come SHAP.</p>"},{"location":"material/03_ml/04_trees/03_gbt/","title":"3.4.3 Gradient boosted trees","text":"<p>Il gradient boosting \u00e8 un altro approccio che pu\u00f2 essere utilizzato per l'addestramento di alberi decisionali. Informalmente, il gradient boosting coinvolge due modelli:</p> <ul> <li>il primo \u00e8 un modello debole, tipicamente un albero decisionale;</li> <li>il secondo \u00e8 un modello forte, composto da pi\u00f9 modelli deboli.</li> </ul> <p>Il gradient boosting lavora in maniera iterativa. Ad ogni step, un nuovo modello debole viene addestrato per caratterizzare l'errore, o pseudo riposta, del modello forte, che assumiamo essere la differenza tra il valore vero \\(y\\) ed il valore predetto \\(\\hat{y}\\). Conseguentemente, il modello debole sar\u00e0 \"sottratto\" al modello forte, in modo tale da ridurre l'entit\u00e0 dell'errore complessivo. Analiticamente, ad ogni iterazione avviene la seguente:</p> \\[ F_{i+1} = F_i - f_i \\] <p>dove:</p> <ul> <li>\\(F_i\\) \u00e8 il modello forte all'\\(i\\)-mo step;</li> <li>\\(f_i\\) \u00e8 il modello debole all'\\(i\\)-mo step.</li> </ul> <p>Le iterazioni sono ripetute sino al raggiungimento di un determinato criterio di arresto, come ad esempio un numero massimo di iterazioni, o anche il fatto che il modello forte inizia ad andare in overfitting. Ovviamente, se i modelli deboli utilizzati sono degli alberi decisionali, siamo di fronte a dei gradient boosting decision trees (GBT).</p> <p>Esistono diverse tecniche per migliorare le performance del gradient boosting, evitando che il modello vada in overfitting. Un esempio di queste tecniche \u00e8 lo shrinkage.</p>"},{"location":"material/03_ml/04_trees/03_gbt/#shrinkage","title":"Shrinkage","text":"<p>Lo shrinkage prevede che il modello debole \\(f_i\\) sia moltiplicato per un \\(\\epsilon\\) molto piccolo, chiamato appunto shrinkage, prima di essere aggiunto al modello forte. In altre parole, il gradient boosting diventa:</p> \\[ F_{i+1} = F_i - \\epsilon \\cdot f_i \\] <p>Lo shrinkage permette di controllare la \"forza\" dell'apprendimento del modello forte, limitando quindi l'entit\u00e0 dell'overfitting.</p> <p>Shrinkage e learning rate</p> <p>Lo shrinkage \u00e8 concettualmente analogo al learning rate nelle reti neurali.</p>"},{"location":"material/03_ml/04_trees/03_gbt/#vantaggi-e-svantaggi","title":"Vantaggi e svantaggi","text":"<p>I GBT hanno diversi vantaggi:</p> <ul> <li>come gli alberi decisionali, supportano nativamente feature numeriche e categoriche e spesso non hanno bisogno di preprocessing delle feature;</li> <li>i valori di default degli iperparametri possono essere utilizzati dando ottimi risultati;</li> <li>sono poco costosi sia dal punto di vista spaziale, sia da quello temporale.</li> </ul> <p>Tuttavia, presentano anche alcuni limiti:</p> <ul> <li>a differenza delle random forest, i singoli modelli deboli devono essere addestrati in maniera sequenziale, il che pu\u00f2 rallentare la velocit\u00e0 di esecuzione dell'algoritmo;</li> <li>ogni albero deve essere addestrato da zero sull'intero dataset. Specialmente in caso di dati non strutturati, ci\u00f2 fa in modo che i GBT abbiano performance peggiori rispetto ad altri metodi.</li> </ul>"},{"location":"material/03_ml/05_clustering/lecture/","title":"3.5 - Algoritmi di clustering","text":"<p>Gli algoritmi di clustering hanno come obiettivo la suddivisione non supervisionata dei campioni presenti in un dataset. Di conseguenza, questo tipo di algoritmi opera senza una label determinata a priori, ed agendo quindi direttamente sui dati.</p> <p>Facciamo un esempio. Immaginiamo di voler fare un po' di pulizia nei nostri album musicali, lasciando soltanto quelli che sono pi\u00f9 vicini ai gusti che abbiamo maturato nel corso degli anni, e vendendo gli altri. La suddivisione degli album non avverr\u00e0 quindi in base ad una determinata label come, ad esempio, anno di produzione, artista, o genere musicale; sar\u00e0 invece seguito un criterimo molto pi\u00f9 empirico, legato alla vicinanza delle canzoni contenute nel disco ai nostri gusti musicali. Una volta suddivisi gli album, dovremo assegnare ad ogni gruppo, o cluster, un identificativo che, in qualche modo, riassuma le informazioni associate al cluster: banalmente, in questo caso potremo usare gli identificativi buono e non buono.</p> <p>Gli algoritmi di clustering sono usati in numerosi ambiti ed applicazioni. Per fare un paio di esempi, in un'analisi di mercato, potremmo suddividere i potenziali clienti sulla base di profili \"vicini\", oppure ancora raggruppare zone di un'immagine di colore simile. Diamone quindi una descrizione pi\u00f9 dettagliata.</p>"},{"location":"material/03_ml/05_clustering/lecture/#tipologie-di-clustering","title":"Tipologie di clustering","text":"<p>Un algoritmo di clustering deve essere scelto sulla base di diversi fattori, tra cui uno dei maggiormente influenti riguarda la complessit\u00e0 e, di conseguenza, la scalabilit\u00e0 dell'approccio. Ad esempio, alcuni algoritmi confrontano ogni possibile coppia di dati, il che porta ad una complessit\u00e0 \\(O(n^2)\\), con \\(n\\) numero complessivo di campioni; altri invece, come il K-Means, effettuano un numero molto pi\u00f9 limitato di operazioni, con una complessit\u00e0 nell'ordine di un \\(O(n)\\). Va da s\u00e8 che l'effetto legato alla diversa complessit\u00e0 computazionale \u00e8 visibile soprattutto in caso \\(n\\) sia molto maggiore di \\(1\\). Ovviamente, oltre alla complessit\u00e0, ogni algoritmo presenta tutta una serie di vantaggi e svantaggi, che potranno orientare la nostra scelta.</p> <p>In generale, comunque, esistono quattro diverse categorie di algoritmo.</p> <ol> <li>Nel centroid-based clustering, i dati sono organizzati secondo la loro distanza dai centroidi, ovvero dei campioni considerati come \"base\" per ciascun cluster. Questo tipo di algoritmi risulta essere mediamente efficace, ma \u00e8 sensibile alle condizioni iniziali ed alla presenza di eventuali outliers.</li> <li>Nel density-based clustering, i dati sono organizzati in aree ad alta densit\u00e0. Ci\u00f2 permette la connessione di cluster di forma arbitraria, e facilita inoltre l'individuazione di outlier, che per definizione sono nelle zone a minore densit\u00e0 di campioni. Possono per\u00f2 essere sensibili a dataset con densit\u00e0 variabile ed alta dimensionalit\u00e0.</li> <li>Nel distribution-based clustering si suppone che i dati abbiano distribuzione gaussiana, e siano quindi suddivisibili come tali. Questo tipo di algoritmi non \u00e8 efficiente se non si conosce a priori il tipo di distribuzione dei dati.</li> <li>Nello hierarchical clustering viene creato un albero a partire dai dati. Questo tipo di clustering \u00e8 particolarmente efficace nel caso si trattino certi tipi di dati, come ad esempio le tassonomie, e prevede che possa essere selezionato un numero ridotto di cluster tagliando l'albero al giusto livello.</li> </ol>"},{"location":"material/03_ml/05_clustering/lecture/#funzionamento-generale-di-un-algoritmo-di-clustering","title":"Funzionamento generale di un algoritmo di clustering","text":"<p>Ogni algoritmo di clustering prevede tre step fondamentali.</p> <ol> <li>Il primo step prevede la preparazione dei dati mediante opportune operazioni di normalizzazione e riduzione della dimensionalit\u00e0.</li> <li>Il secondo step prevede l'individuazione di una serie di feature cui applicare una metrica di distanza, determinando la \"vicinanza\" di una coppia di campioni.</li> <li>Il terzo step preveder\u00e0 l'esecuzione vera e propria dell'algoritmo.</li> </ol>"},{"location":"material/03_ml/05_clustering/lecture/#feature-e-metrica-di-distanza","title":"Feature e metrica di distanza","text":"<p>Concentriamoci brevemente sul secondo step. Definire la similarit\u00e0 di due campioni implica stabilire un modo per valutare quanto questi risultino essere tra affini sulla base delle loro caratteristiche. Possiamo quindi operare in due modi:</p> <ul> <li>la similarit\u00e0 pu\u00f2 essere calcolata a partire da feature scelte manualmente da un esperto di dominio;</li> <li>in alternativa, le feature possono essere scelte in maniera automatica partendo da un embedding, ovvero da una rappresentazione a dimensionalit\u00e0 ridotta del dato iniziale.</li> </ul> <p>Nel primo caso, la scelta avviene basandosi su una valutazione preliminare dei dati fatta da un esperto di dominio. Tornando agli album musicali, potremmo considerare come feature anno di produzione e genere di appartenenza, ed usare una metrica di distanza, come ad esempio la distanza euclidea, per rappresentare la \"quantit\u00e0 di spazio\" che separa due campioni. </p> <p>Tuttavia, questo approccio non \u00e8 efficace nel caso in cui ci sia un elevato numero di dimensioni. Questo \u00e8 legato essenzialmente ad un fattore, che approfondiamo di seguito.</p>"},{"location":"material/03_ml/05_clustering/lecture/#distanza-euclidea-e-dimensionalita-elevata","title":"Distanza euclidea e dimensionalit\u00e0 elevata","text":"<p>In caso di spazio con un elevato numero di feature (e, quindi, a dimensionalit\u00e0 elevata), assistiamo ad una serie di fenomeni controintuitivi, magistralmente descritti da Pedro Domingos della University of Washington in questo articolo.</p> <p>In particolare, Domingos afferma che \u00e8 proprio il nostro modo di percepire il mondo a limitare la nostra comprensione del funzionamento della distanza euclidea. Consideriamo ad esempio una distribuzione gaussiana in uno spazio a due o tre dimensioni: la nostra esperienza ci dice che la maggior parte dei dati \u00e8 concentrata attorno alla media, e questo ci porta a pensare che ci\u00f2 valga in ogni caso. Ci\u00f2 per\u00f2 non \u00e8 necessariamente vero nel caso ci sia un numero elevato di dimensioni: ad esempio, si pu\u00f2 verificare che la maggior parte del volume di un'arancia ad elevata dimensionalit\u00e0 non sia concentrata nella polpa, ma sulla sua buccia e, in altre parole, non valgono le regole di approssimazione e distanza a cui siamo abituati a sottostare negli spazi a noi \"comprensibili\".</p> <p>Tornando alla distanza euclidea, nelle alte dimensionalit\u00e0 si assiste ad un fenomeno abbastanza peculiare: in pratica, i punti diventano praticamente equidistanti l'uno dall'altro. Questo fenomeno, magistralmente descritto in questo articolo, comporta l'inutilizzabilit\u00e0 delle metriche di distanza tradizionali, con particolare riferimento a quella Euclidea, negli spazi ad elevata dimensionalit\u00e0.</p> <p>Di conseguenza, nel momento in cui si considerano dei campioni con un numero elevato di feature, \u00e8 necessario estrarre rappresentazioni compatte degli stessi, utilizzando tecniche di dimensionality reduction o feature selection, che tratteremo in una delle prossime lezioni.</p>"},{"location":"material/03_ml/05_clustering/lecture/#applicazione-di-un-algoritmo-di-clustering-il-k-means","title":"Applicazione di un algoritmo di clustering: il K-Means","text":"<p>Concentriamoci adesso sui casi ppi\u00f9 semplici, e vediamo come usare il pi\u00f9 noto tra gli algoritmi di clustering, ovvero il K-Means.</p> <p>Il K-Means \u00e8 un algoritmo centroid-based che distribuisce i campioni tra \\(k\\) diversi cluster in base alla distanza tra il campione ed i centroidi dei diversi cluster. Le ipotesi alla base dell'algoritmo sono diverse, tra cui la pi\u00f9 restrittiva \u00e8 quella legata alla definizione del numero di cluster \\(k\\), che \u00e8 immutabile per l'intera esecuzione dell'algoritmo.</p> <p>Una volta fissato il valore di \\(k\\), il K-Means opera in tre diversi step successivi:</p> <ol> <li>al primo step, l'algoritmo sceglie casualmente \\(k\\) centroidi tra i diversi dati a disposizione;</li> <li>al secondo step, l'algoritmo assegna ogni punto al centroide pi\u00f9 vicino, definendo i \\(k\\) cluster iniziali;</li> <li>al terzo step, l'algoritmo ricalcola il centroide considerando il valore medio di tutti i punti del cluster, e ritorna allo step 2.</li> </ol> <p>Si noti che, in generale, i centroidi non sono campioni presenti nel dataset, se non al punto 1. Le iterazioni proseguono fino a che i cluster calcolati al punto 2 si stabilizzano o, in alternativa, fino a che non sar\u00e0 raggiunto il massimo numero di iterazioni ammesse. In figura 1 possiamo osservare una spiegazione visiva del funzionamento dell'algoritmo.</p> <p> </p> Figura 1 - Step dell'algoritmo K-Means <p>Formalmente, il \\(K-Means\\) divide un insieme di \\(N\\) campioni \\(X\\) in \\(K\\) cluster \\(C\\), ognuno descritto dalla media \\(\\mu_{j}\\) dei campioni nel cluster (ovvero, i centroidi). In particolare, i centroidi sono scelti in modo da minimizzare il criterio di inerzia, definito come:</p> \\[ \\sum_{i=0}^n \\min_{\\mu_j \\in C} (|| x_i - \\mu_j||^2) \\] <p>Il criterio di inerzia definisce quindi un indice di \"coesione\" interna dei cluster. Tuttavia, la scelta di questo criterio impone diversi limiti al K-Means:</p> <ul> <li>in primis, il criterio di inerzia presuppone che i cluster siano isotropi, ad egual varianza, e contengano un numero comparabile di campioni, il che non \u00e8 sempre vero nella realt\u00e0;</li> <li>inoltre, l'inerzia \u00e8 basata sulla distanza euclidea che, come abbiamo visto in precedenza, tende a non essere efficace negli spazi ad elevata dimensionalit\u00e0. </li> </ul>"},{"location":"material/03_ml/05_clustering/lecture/#scelta-del-numero-di-cluster","title":"Scelta del numero di cluster","text":"<p>La scelta del valore ottimale di \\(k\\) \u00e8 un procedimento emnpirico, in quanto non abbiamo a disposizione delle vere e proprie label per la verifica dell'uscita dell'algoritmo. In tal senso, abbiamo a disposizione sia delle metriche, che vedremo in seguito, sia degli approcci pi\u00f9 qualitativi, che dipendono dai concetti di cardinalit\u00e0 e magnitudine del clustering.</p> <p>In particolare, per cardinalit\u00e0 si intende il numero di campioni per ogni cluster, mentre per magnitudine la somma delle distanze di tutti i campioni in un cluster dal centroide. Immaginiamo di essere in un caso come quello descritto in figura 2.</p> <p> </p> Figura 2 - Rapporto tra cardinalit\u00e0 e magnitudine dei cluster <p>Prevedibilmente, il rapporto tra cardinalit\u00e0 e magnitudine dovrebbe essere all'incirca lineare. Quindi, come si pu\u00f2 vedere dalla figura precedente, ci potrebbe essere qualcosa che non va con il cluster \\(4\\).</p> <p>A questo punto, avendo valutato empiricamente la possibile presenza di un problema qualitativo con il clustering, possiamo provare ad eseguire l'algoritmo per un valore crescente di \\(k\\). Proviamo a plottare questo valore in rapporto alla somma delle magnitudini del risultato, che diminuir\u00e0 all'aumentare di \\(k\\); un valore ottimale per \\(k\\) \u00e8 quello che si ottiene quando questo grafico tende a stabilizzarsi, ad esempio considerando il valore per cui la derivata diventa maggiore di -1 (e quindi l'angolo della funzione dei \\(k\\) \u00e8 maggiore di \\(135\u00b0\\)).</p> <p> </p> Figura 3 - Rapporto tra il numero dei cluster e la magnitudine"},{"location":"material/03_ml/05_clustering/lecture/#un-altro-algoritmo-il-dbscan","title":"Un altro algoritmo: il DBSCAN","text":"<p>Il DBSCAN \u00e8 un algoritmo di clustering di tipo agglomerativo density-based che opera considerando due parametri principali:</p> <ul> <li>la distanza massima \\(\\epsilon\\) per considerare due punti come appartenenti allo stesso cluster;</li> <li>il numero minimo di campioni \\(m\\) per il quale \u00e8 possibile definire un cluster.</li> </ul> <p>Nella pratica, il DBSCAN seleziona un campione casuale tra quelli non visitati, e valuta se ci sono \\(m\\) campioni all'interno della distanza \\(\\epsilon\\), nel qual caso si ha un core point. In alternativa, se il numero di campioni presenti in \\(\\epsilon\\) \u00e8 minore di \\(m\\), ma comunque maggiore di 0, i campioni si dicono \\(density reachable\\) e, se connessi ad un core point, appartengono allo stesso cluster. Infine, se non vi sono campioni presenti in \\(\\epsilon\\), allora il punto \u00e8 isolato, ed \u00e8 interpretato come un outlier. Un'interpretazione visiva \u00e8 quella proposta in figura; in particolare, i punti in rosso definiscono diversi core points, i punti in giallo sono density reachable, e quindi fanno parte dello stesso cluster dei core points, mentre \\(N\\) \u00e8 un outlier.</p> <p> </p> Figura 20.4 - Algoritmo DBSCAN.  Di Chire - Opera propria, CC BY-SA 3.0, Wikipedia"},{"location":"material/03_ml/06_metrics/01_classification/","title":"3.6.1 - Metriche di classificazione","text":"<p>Nella lezione sulla regressione logistica abbiamo visto come l'algoritmo restituisca un valore di probabilit\u00e0 poi convertito in classe mediante una soglia \\(\\mu\\), che di default vale \\(0.5\\). Tuttavia, abbiamo sottolineato come questo valore non sia ideale in ogni situazione: chiariamo questo concetto con un semplice esempio.</p>"},{"location":"material/03_ml/06_metrics/01_classification/#precisione-recall-ed-accuracy","title":"Precisione, recall ed accuracy","text":"<p>Partiamo ricordando che, per il nostro esempio, la classe positiva \u00e8 rappresentata da tutte le mail classificate come spam, mentre quella negativa \u00e8 data dalle mail legittime. Ciascuna predizione fatta da questo tipo di classificatore, che \u00e8 indicato come binario, in quanto deve scegliere tra due possibili classi, pu\u00f2 essere di uno tra i seguenti tipi:</p> <ul> <li>nel caso in cui il modello classifichi correttamente una mail di spam, parleremo di true positive (TP);</li> <li>nel caso in cui il modello classifichi correttamente una mail legittima, parleremo di true negative (TN);</li> <li>nel caso in cui il modello classifichi erroneamente una mail di spam come legittima, si parler\u00e0 di false negative (FN);</li> <li>nel caso in cui il modello classifichi erroneamente una mail legittima come spam, si parler\u00e0 di false positive (FP).</li> </ul> <p>Generalizzando, abbiamo un true positive (o negative) quando il modello predice correttamente la classe positiva (o negativa), ed un false positive (o negative) quando il modello predice erroneamente la classe negativa (o positiva).</p> <p>Il rapporto tra questi quattro valori permette di definire delle metriche volte a valutare le performance del nostro classificatore. Vediamone in breve alcune.</p>"},{"location":"material/03_ml/06_metrics/01_classification/#metrica-1-accuracy","title":"Metrica 1: Accuracy","text":"<p>L'accuracy \u00e8 la principale metrica utilizzata per valutare le performance dei modelli di classificazione. Informalmente, rappresenta la percentuale complessiva di predizioni corrette effettuate dal nostro modello, e pu\u00f2 essere definita secondo la seguente formula:</p> \\[ A = \\frac{C}{T} \\] <p>dove \\(C\\) \u00e8 il numero totale di predizioni corrette, mentre \\(T\\) \u00e8 il numero totale di predizioni. Nel caso della classificazione binaria, la precedente diventa:</p> \\[ A = \\frac{TP + TN}{TP + TN + FP + FN} \\] <p>Facciamo un esempio numerico. Immaginiamo di aver ricevuto \\(100\\) email, tra cui dieci di spam. Il nostro spam detector individua correttamente cinque messaggi di spam su dieci, per cui \\(TP=5\\), e \\(FN=5\\). Inoltre, dieci messaggi legittimi sono erroneamente indicati come spam, per cui \\(FP=10\\), e \\(TN=80\\). Allora:</p> \\[ A = \\frac{TP+TN}{TP+TN+FP+FN}=\\frac{5+80}{5+80+10+5}=\\frac{85}{100} \\] <p>L'accuracy del modello \u00e8 quindi pari a \\(0.85\\): in altri termini, il modello sembra essere in grado di gestire correttamente l'\\(85\\%\\) delle mail ricevute.</p> <p>In realt\u00e0, questo risultato \u00e8 soltanto apparentemente buono. Per capire il motivo di questa affermazione, pensiamo al fatto che le mail ricevute non sono egualmente distribuite tra spam e legittime: in grande maggioranza, infatti, abbiamo ricevuto mail legittime (il \\(90\\%\\)), e soltanto una sparuta minoranza di spam. Ci\u00f2 implica che il modello \u00e8 stato in realt\u00e0 in grado di individuare soltanto il \\(50\\%\\) dello spam ricevuto, classificando inoltre circa il \\(7\\%\\) delle mail legittime come spam.</p> <p>Vista sotto questa luce, di conseguenza, l'accuracy assume un aspetto completamente differente, e non \u00e8 sempre efficace nel valutare l'efficacia di un modello. Ci\u00f2 \u00e8 soprattutto evidente quando lavoriamo su un dataset molto sbilanciato, nel quale vi \u00e8 una disparit\u00e0 significativa tra il numero di campioni disponibili per ciascuna classe.</p>"},{"location":"material/03_ml/06_metrics/01_classification/#metrica-2-precisione","title":"Metrica 2: Precisione","text":"<p>Per risolvere alcuni dei problemi dell'accuracy \u00e8 possibile utilizzare diverse altre metriche. Una di queste \u00e8 la precisione, che valuta la proporzione di campioni positivi identificati correttamente.</p> <p>Analiticamente, la precisione \u00e8 espressa come:</p> \\[ P = \\frac{TP}{TP+FP} \\] <p>Tornando all'esempio precedente, la precisione sar\u00e0 data dal rapporto tra le mail di spam riconosciute come tali ed il totale di mail riconosciute come spam, includendo anche gli errori fatti sulle mail legittime. Nella pratica:</p> \\[ P = \\frac{5}{5+5} = 0.5 \\] <p>Di conseguenza, il nostro modello ha una precisione pari al \\(50\\%\\). La precisione ci permette quindi di quantificare l'affidabilit\u00e0 del nostro sistema: un alto valore di \\(P\\), infatti, ci assicura che una mail identificata come spam sar\u00e0 effettivamente tale. Tuttavia, ci manca ancora un fattore: infatti, come quantificare la capacit\u00e0 del modello di caratterizzare tutto lo spam ricevuto? Per farlo, ci viene in aiuto la terza metrica.</p>"},{"location":"material/03_ml/06_metrics/01_classification/#metrica-3-recall","title":"Metrica 3: Recall","text":"<p>Il recall \u00e8 espresso dalla seguente relazione:</p> \\[ R = \\frac{TP}{TP+FN} \\] <p>Nel nostro caso, il recall esprime il rapporto tra le mail correttamente indicate come spam e tutte le mail effettivamente di spam. Numericamente:</p> \\[ R = \\frac{5}{5+5} \\] <p>Cos\u00ec come la precisione, il recall \u00e8 del \\(50\\%\\).</p>"},{"location":"material/03_ml/06_metrics/01_classification/#metrica-4-f1-score","title":"Metrica 4: F1 score","text":"<p>Come abbiamo potuto vedere, precisione e recall permettono di caratterizzare in maneira adeguata l'affidabilit\u00e0 del nostro modello. Tuttavia, i due valori sono in contrapposizione: come vedremo a breve, migliorare la precisione riduce generalmente il recall, e viceversa. Di conseguenza, \u00e8 opportuno utilizzare una metrica che sintetizzi le due in un unico valore; questa \u00e8 chiamata F1-score, ed \u00e8 espressa come segue:</p> \\[ F1 = 2 \\frac{P \\cdot R}{P + R} \\] <p>Nel nostro caso:</p> \\[ F1 = 2 \\frac{0.5 \\cdot 0.5}{0.5 + 0.5} = 0.5 \\]"},{"location":"material/03_ml/06_metrics/01_classification/#tuning-della-soglia-di-decisione","title":"Tuning della soglia di decisione","text":"<p>Abbiamo detto in precedenza che i valori di precisione  e recall sono inversamente proporzionali: aumentando l'uno, diminuisce l'altro, e viceversa. Proviamo a comprendere empiricamente questo concetto facendo un esempio.</p> <p>Immaginiamo di avere un dataset per lo spam limitato a \\(13\\) email. Addestriamo il classificatore, ed impostiamo la soglia di decisione \\(\\mu\\) a \\(0.6\\). I risultati del primo addestramento sono mostrati in figura 1.</p> <p> </p> Figura 1 - Performance del modello con $\\mu=0.6$. <p>I risultati in termini di veri e falsi sono:</p> \\[ \\begin{align} &amp; TP = 4 \\\\ &amp; TN = 6 \\\\ &amp; FP = 1 \\\\ &amp; FN = 2 \\\\ \\end{align} \\] <p>Di conseguenza:</p> \\[ P = \\frac{TP}{TP+FP}=\\frac{4}{4+1} = 0.8 \\\\ R = \\frac{TP}{TP+FN}=\\frac{4}{4+2} = 0.66 \\] <p>Proviamo adesso ad aumentare il valore di \\(\\mu\\), portandolo a \\(0.75\\). I risultati sono mostrati in figura 2.</p> <p> </p> Figura 2 - Performance del modello con $\\mu=0.75$. <p>In questo caso:</p> \\[ \\begin{align} &amp; TP = 3 \\\\ &amp; TN = 7 \\\\ &amp; FP = 0 \\\\ &amp; FN = 3 \\\\ \\end{align} \\] <p>per cui</p> \\[ P = \\frac{TP}{TP+FP}=\\frac{3}{3} = 1 \\\\ R = \\frac{TP}{TP+FN}=\\frac{3}{3+3} = 0.5 \\] <p>Vediamo quindi che la precisione aumenta, portandosi al \\(100\\%\\), mentre il recall diminuisce, arrivando al \\(50\\%\\).</p> <p>Proviamo infine a diminuire la soglia di decisione, portandola al \\(50%\\), come mostrato in figura 3.</p> <p> </p> Figura 3 - Performance del modello con $\\mu=0.50$. <p>In questo caso:</p> \\[ \\begin{align} &amp; TP = 4 \\\\ &amp; TN = 4 \\\\ &amp; FP = 3 \\\\ &amp; FN = 2 \\\\ \\end{align} \\] <p>per cui:</p> \\[ P = \\frac{TP}{TP+FP}=\\frac{4}{4+3} \\approx 0.57 \\\\ R = \\frac{TP}{TP+FN}=\\frac{4}{4+2} = 0.66 \\] <p>Il recall torna ad aumentare, mentre la precisione diminuisce notevolmente.</p> <p>Abbiamo quindi visto come la variazione di \\(\\mu\\) agisca su \\(P\\) ed \\(R\\), il cui andamento \u00e8 quasi sempre inversamente proporzionale, a meno che non si abbia a disposizione un modello accurato nel \\(100\\%\\) dei casi. Di conseguenza, la detection threshold va scelta a seconda dell'applicazione specifica: nel nostro caso, se non abbiamo paura di perdere mail legittime, potremo tranquillamente abbassare il valore di \\(\\mu\\), aumentando il recall; viceversa, se tolleriamo un po' di spam, possiamo aumentare la precisione alzando la soglia decisionale.</p>"},{"location":"material/03_ml/06_metrics/02_regression/","title":"3.6.2 - Metriche di regressione","text":"<p>La valutazione di un modello di regressione avviene utilizzando delle metriche differenti rispetto a quelle usate per un modello di classificazione. Vediamone brevemente alcune.</p>"},{"location":"material/03_ml/06_metrics/02_regression/#errore-di-regressione","title":"Errore di regressione","text":""},{"location":"material/03_ml/06_metrics/02_regression/#mean-squared-error","title":"Mean Squared Error","text":"<p>L'errore quadratico medio, o mean squared error (MSE), \u00e8 una metrica comunemente utilizzata per la valutazione delle performance di regressione, ed \u00e8 definito come:</p> \\[ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y}_i)^2 \\] <p>L'MSE permette di eliminare l'influenza del segno dell'errore, valutandone esclusivamente il modulo. Tuttavia, \u00e8 estremamente sensibile all'entit\u00e0 dello stesso: infatti, un errore dell'\\(1\\%\\) su un valore \\(y=100\\) sar\u00e0 pi\u00f9 influente di un errore del \\(50\\%\\) su un valore \\(y=1\\).</p> <p>Ovviamente, tanto minore \u00e8 l'MSE, tanto \u00e8 migliore il modello considerato.</p>"},{"location":"material/03_ml/06_metrics/02_regression/#mean-absolute-error","title":"Mean Absolute Error","text":"<p>L'errore assoluto medio, o mean absolute error (MAE) rappresenta il valore atteso dell'errore assoluto o, equivalentemente, della norma \\(l1\\).</p> <p>Se \\(\\hat{y}_i\\) \u00e8 il valore predtto per l'\\(i\\)-mo campione, ed \\(y_i\\) \u00e8 il corrispondente valore \"vero\", allora il MAE calcolato su \\(N\\) campioni \u00e8 dato da:</p> \\[ MAE(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=0}^{N-1} | y_i - \\hat{y}_i | \\]"},{"location":"material/03_ml/06_metrics/02_regression/#mean-absolute-percentage-error","title":"Mean Absolute Percentage Error","text":"<p>Il mean absolute percentage error (MAPE) viene calcolato a partire dal rapporto tra il valore assoluto della differenza tra i valori veri e quelli predetti dal regressore e i valori veri stessi. Tale rapporto viene quindi mediato sull'insieme dei campioni, e ne viene dedotta la percentuale. La formula \u00e8 la seguente:</p> \\[ MAPE = \\frac{1}{n} \\sum_{i=1}^n \\frac{|y_i - \\hat{y}_i|}{max (\\epsilon, y_i)} \\% \\] <p>Anche nel caso del MAPE, l'utilizzo del valore assoluto elimina gli annullamenti non desiderati derivanti da errori di segno opposto. Inoltre, la presenza del valore vero a denominatore fa in modo che la metrica sia sensibile all'entit\u00e0 relativa dell'errore.</p> <p>Anche in questo caso, un valore di MAPE basso indica un'ottima approssimazione.</p>"},{"location":"material/03_ml/06_metrics/02_regression/#metriche-statistiche","title":"Metriche statistiche","text":""},{"location":"material/03_ml/06_metrics/02_regression/#coefficiente-di-determinazione","title":"Coefficiente di determinazione","text":"<p>Il valore \\(R^2\\) determina la proporzione della varianza del valore vero che viene spiegata dal modello. In pratica, ci permette di definire quanta della variabilit\u00e0 del fenomeno (ovvero, del modo in cui il fenomeno combina le \\(n\\) variabili indipendenti per ottenere le \\(m\\) variabili dipendenti) viene correttamente caratterizzata attraverso il modello considerato.</p> <p>Il valore di \\(R^2\\) pu\u00f2 oscillare tra \\(1\\) e \\(- \\infty\\), ovvero tra la modellazione completa dell'intera variabilit\u00e0 del fenomeno ed un modello totalmente incorrelato allo stesso. Analiticamente, \\(R^2\\) \u00e8 definito come:</p> \\[ R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i-avg(y_i))} \\] <p>con:</p> \\[ avg(y_i) = \\frac{1}{n} \\sum_{i=1}^n y_i \\]"},{"location":"material/03_ml/06_metrics/02_regression/#varianza-spiegata","title":"Varianza spiegata","text":"<p>La varianza spiegata \u00e8 definita come segue:</p> \\[ EV(y, \\hat{y}) = 1 - \\frac{\\sigma \\{y - \\hat{y}\\}}{\\sigma \\{y\\}} \\] <p>dove \\(\\hat{y}\\) \u00e8 il valore predetto per la variabile indipendente, \\(y\\) \u00e8 il valore vero, e \\(\\sigma\\) \u00e8 la varianza.</p>"},{"location":"material/03_ml/06_metrics/02_regression/#differenze-tra-r2-e-varianza-spiegata","title":"Differenze tra \\(R^2\\) e varianza spiegata","text":"<p>La differenza tra \\(R^2\\) e la varianza spiegata sta nel fatto che quest'ultima non tiene conto di errori sistematici presenti nelle predizioni; di conseguenza, \u00e8 preferibile usare \\(R^2\\) ove possibile. Inoltre, nel caso in cui la variabile indipendente sia costante, il valore numerico della varianza spiegata non \u00e8 un valore reale: pu\u00f2 essere infatti <code>NaN</code>, in caso di predizioni perfette, o \\(- \\infty\\), in caso di predizioni imperfette.</p> <p>Varianza spiegata in Scikit-Learn</p> <p>A causa dell'ultimo fattore, in Scikit-Learn la varianza spiegata assume valore compreso tra \\(1\\) e \\(0\\) proprio per evitare l'insorgenza di problematiche durante la validazione delle performance.</p>"},{"location":"material/03_ml/06_metrics/03_clustering/","title":"3.6.3 - Metriche di clustering","text":"<p>Cos\u00ec come per la regressione e la classificazione, esistono delle metriche appositamente progettate per valutare la qualit\u00e0 dei risultati ottenuti da un algoritmo di clustering. Nello specifico, valuteremo l'adjusted rand index ed il silhouette score.</p>"},{"location":"material/03_ml/06_metrics/03_clustering/#adjusted-rand-index","title":"Adjusted Rand Index","text":"<p>Sia \\(C\\) l'insieme dei cluster \"veri\" assegnati ad un certo dataset, e \\(K\\) l'insieme dei cluster assegnati a valle dell'applicazione di un algoritmo di clustering. Allora definiamo l'indice di Rand come:</p> \\[ RI = \\frac{a + b}{C_2^n} \\] <p>dove:</p> <ul> <li>\\(a\\) \u00e8 il numero di coppie di campioni che appartengono allo stesso cluster sia in \\(C\\) sia a valle dell'assegnazione \\(K\\);</li> <li>\\(b\\) \u00e8 il numero di coppie di campioni che non appartengono allo stesso cluster sia in \\(C\\) sia a valle dell'assegnazione \\(K\\);</li> <li>\\(C_2^n\\) \u00e8 il numero totale di coppie di campioni presenti nel dataset.</li> </ul> <p>In pratica, se:</p> \\[ s = [s_1, s_2, s_3, s_4, s_5] \\\\ C = [s_1, s_2], [s_3, s_4, s_5] \\\\ K = [s_1, s_2, s_3], [s_4, s_5] \\\\ \\] <p>allora:</p> \\[ a = |(s_1, s_2), (s_4, s_5)| = 2 \\\\ b = |(s_1, s_4), (s_1, s_5), (s_4, s_2), (s_5, s_2)| = 4 \\\\ C_2^n = \\frac{|s|*|s-1|}{2} = 5 * 2 = 10 \\] <p>Di conseguenza, \\(RI=\\frac{6}{10}=0.6\\).</p> <p>Si pu\u00f2 dimostrare non \u00e8 garantito che l'indice di Rand assuma valore vicino allo zero a seguito di un'assegnazione completamente casuale dei cluster da parte dell'algoritmo.</p> <p>Possiamo quindi tenere conto dell'aspettazione \\(E[RI]\\) di ottenere un'assegnazione casuale mediante l'indice di Rand modificato:</p> \\[ ARI = \\frac{RI - E[RI]}{max(RI) - E[RI]} \\] <p>In Scikit-Learn, l'indice di Rand modificato \u00e8 ottenuto usando la funzione <code>adjusted_rand_score()</code> del package <code>metrics</code>.</p> <p>Il valore ottimale dell'ARI \u00e8 pari proprio ad 1, caso in cui il clustering \u00e8 riuscito a predire correttamente tutte le classi dei singoli campioni. Valori prossimi allo zero o negativi (fino a -1) contraddistinguono invece labeling non corretti.</p> <p>Una metrica di questo tipo ha l'ovvio vantaggio di essere facilmente interpretabile, oltre che di non essere collegata ad uno specifico algoritmo di clustering. Tuttavia, vi \u00e8 una criticit\u00e0 indotta dalla necessit\u00e0 di conoscere a priori il labeling esatto dei campioni (il che, quindi, potrebbe farci propendere per l'uso di un algoritmo di classificazione).</p>"},{"location":"material/03_ml/06_metrics/03_clustering/#silhouette-score","title":"Silhouette Score","text":"<p>A differenza dell'ARI, il silhouette score non richiede la conoscenza aprioristica delle label vere; per valutare la qualit\u00e0 del clustering, invece, questa metrica si affida a valutazioni sulla separazione dei cluster, ottenendo un valore tanto pi\u00f9 alto quanto questi sono tra di loro ben separati e definiti.</p> <p>In particolare, il silhouete score per un singolo campione \u00e8 definito come:</p> \\[ s = \\frac{b-a}{max(a, b)} \\] <p>dove:</p> <ul> <li>\\(a\\) \u00e8 la distanza media tra un campione e tutti gli altri campioni appartenenti allo stesso cluster;</li> <li>\\(b\\) \u00e8 la distanza media tra un campione e tutti gli altri campioni appartenenti al cluster pi\u00f9 vicino.</li> </ul> <p>Questa metrica, implementata grazie alla funzione <code>silhouette_score()</code> del package <code>metrics</code>, \u00e8 anch'essa di facile interpretazione, in quanto pu\u00f2 assumere valori compresi nell'intervallo \\([-1, 1]\\), con:</p> <ul> <li>valori prossimi a \\(-1\\) che indicano un clustering non corretto;</li> <li>valori prossimi allo \\(0\\) che indicano cluster sovrapposti;</li> <li>valori prossimi a \\(+1\\) che indicano cluster densi e ben suddivisi.</li> </ul> <p>Uno svantaggio del silhouette score \u00e8 che, in generale, pu\u00f2 variare in base all'algoritmo utilizzato.</p>"},{"location":"material/03_ml/07_challenges/01_curse/","title":"3.7 - Sfide aperte nel machine learning","text":"<p>Lo sviluppo e le innovazioni introdotte da machine learning prima, e deep learning dopo, sono in parte motivate dal fallimento degli algoritmi tradizionali nella generalizzazione su diversi task di intelligenza artificiale. In particolare, la generalizzazione diventa sempre pi\u00f9 difficile man mano che si lavora con dati ad elevata dimensionalit\u00e0, dove i meccanismi tradizionali diventano insufficienti ad apprendere funzioni complesse ed adeguate al problema. Questi spazi, inoltre, spesso impongono l'uso di algoritmi ad alto costo computazionale: in tal senso, i recenti avanzamenti nel deep learning sono stati esplicitamente progettati per superare, tra gli altri, questi ostacoli. Vediamone alcuni</p>"},{"location":"material/03_ml/07_challenges/01_curse/#curse-of-dimensionality","title":"Curse of dimensionality","text":"<p>Come abbiamo accennato, i problemi di machine learning sono tanto pi\u00f9 difficili quanto maggiore \u00e8 la dimensionalit\u00e0 dei dati. Questo fenomeno \u00e8 conosciuto come curse of dimensionality, ed \u00e8 legato al fatto che il numero di possibili configurazioni distinte dei dati caratterizzati da un insieme di feature aumenta esponenzialmente man mano che il numero di queste variabili aumenta.</p> <p>La sfida posta dalla curse of dimensionalit\u00e0 \u00e8 di tipo squisitamente statistico. Nella pratica, il numero di possibili configurazioni di un dato \\(x_i\\) diventa sempre pi\u00f9 grande all'aumentare del numero di feature dello stesso, il che richiede un numero di campioni di training sempre pi\u00f9 elevato. Consideriamo ad esempio la situazione mostrata in figura 1, nel quale si mostra uno spazio a due dimensioni, ognuna delle quali rappresenta una possibile feature categorica a tre diversi valori. Come \u00e8 possibile notare, lo spazio \u00e8 interamente caratterizzabile da \\(9\\) campioni, per cui un ipotetico modello addestrato su questo dataset avrebbe bisogno di un numero abbastanza ridotto di esempi per avere delle buone performance di generalizzazione.</p> <p> </p> Figura 1 - Possibili combinazioni in un dataset a due dimensioni. <p>Cosa accade aggiungendo un'altra feature, come illustrato in figura 2, con quattro possibili valori? In questo caso, il numero di possibili configurazioni diventa \\(9 \\cdot 4\\), ovvero \\(36\\), per cui avremo bisogno di un numero molto pi\u00f9 elevato di campioni per caratterizzare completamente il nostro spazio delle feature. Trasportando questo problema in \\(n\\) dimensioni, appare chiaro come in questi casi avremo bisogno di un numero estremamente elevato, e potenzialmente non ottenibile, di campioni, il che ci porter\u00e0 ad addestrare il nostro dataset su una rappresentazione parziale e, giocoforza, limitata dello spazio dei dati.</p> <p> </p> Figura 2 - Possibili combinazioni in un dataset a tre dimensioni. <p>Per arginare questo problema, vedremo come sia possibile usare delle tecniche di feature selection o dimensionality reduction.</p>"},{"location":"material/04_sklearn/01_intro/lecture/","title":"4.1 - Una breve introduzione a Scikit-Learn","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>Scikit-Learn \u00e8 una delle librerie per il machine learning tra le pi\u00f9 utilizzate in Python. Questo avviene principalmente a causa di tre fattori:</p> <ul> <li>un esteso supporto ad una grande variet\u00e0 di algoritmi di machine learning;</li> <li>la semplicit\u00e0 di utilizzo della libreria;</li> <li>la perfetta integrazione con NumPy e Pandas.</li> </ul> <p>Per iniziare, quindi, diamo una panoramica ad ampio spettro sulle potenzialit\u00e0 della libreria.</p> <p>Installazione di Scikit-Learn</p> <p>Ovviamente, prima di inizizare, installiamo la libraria:</p> <pre><code>pip install scikit-learn\n</code></pre>"},{"location":"material/04_sklearn/01_intro/lecture/#stimatori-e-transformer","title":"Stimatori e transformer","text":"<p>Scikit-Learn si basa su due concetti fondamentali, ovvero quelli di stimatore (estimator) e transformer.</p> <p>In particolare, uno stimatore \u00e8 un oggetto che implementa uno specifico algoritmo di machine learning, mentre un trasformer permette di effettuare delle trasformazioni sui dati. Per esempio, un'istanza della classe <code>RandomForestClassifier</code> \u00e8 uno stimatore, mentre un'istanza della classe <code>StandardScaler</code> \u00e8 un transformer.</p> <p>Gli stimatori ed i transformer offrono un'interfaccia comune, la quale offre (nella maggior parte dei casi) i metodi <code>fit</code> e <code>transform</code> per, rispettivamente, addestrare l'algoritmo (<code>fit</code>) ed effettuare le predizioni (<code>transform</code>). Tuttavia, \u00e8 importante notare come ogni stimatore e transformer abbiano parametri specifici e dipendenti dalla natura dell'algoritmo utilizzato; ogni algoritmo, inoltre, andr\u00e0 verificato secondo delle opportune metriche, che permettono di definire, in termini percentuali o assoluti, l'accuratezza dell'algoritmo utilizzato.</p> <p>OOP in NumPy</p> <p>Gli stimatori derivano tutti da una classe base comune, chiamata <code>BaseEstimator</code>. Questa scelta garantisce l'interfaccia comune accennata in precedenza e, conseguentemente, il rispetto dei principi di incapsulamento e polimorfismo. Anche i transformer offrono un'interfaccia comune, basata tuttavia sull'uso del <code>TransformerMixin</code>, ovvero di un particolare tipo di classe (detta, per l'appunto, mixin) che permette di implementare meccanismi di ereditariet\u00e0 multipla. </p>"},{"location":"material/04_sklearn/01_intro/lecture/#preprocessing","title":"Preprocessing","text":"<p>Quando abbiamo introdotto i concetti alla base del machine learning, abbiamo visto come sia spesso necessario effettuare una serie di operazioni di preprocessing sui dati. Scikit-Learn offre un gran numero di strumenti per farlo; tra questi, vale la pena ricordarne tre in particolare, ovvero:</p> <ul> <li>la funzione <code>train_test_split</code>, utile a suddividere il dataset in un insieme di training ed uno di test;</li> <li>gli imputer come <code>SimpleImputer()</code> transformer che ci permettono di assegnare eventuali valori mancanti all'interno del dataset;</li> <li>i transformer, come il gi\u00e0 citato <code>StandardScaler()</code>, che permettono di categorizzare e normalizzare i dati.</li> </ul>"},{"location":"material/04_sklearn/01_intro/lecture/#convenzioni","title":"Convenzioni","text":"<p>Gli stimatori in Scikit-Learn seguono alcune regole; elenchiamone alcune tra le pi\u00f9 rilevanti.</p>"},{"location":"material/04_sklearn/01_intro/lecture/#type-casting","title":"Type casting","text":"<p>Quando possibile, Scikit-Learn fa in modo che i dati di ingresso mantengano il loro tipo; in caso contrario, saranno convertiti in <code>float64</code>. Ad esempio:</p> <pre><code>import numpy as np\nfrom sklearn import kernel_approximation\n\nrng = np.random.RandomState(0)\nX = rng.rand(0, 100)\nX = np.array(X, dtype='float32')\n\ntransformer = kernel_approximation.RBFSampler()\nX_new = transformer.fit_transform(X)\nX_new.dtype\n\nprint(X.dtype)              # Il risultato sar\u00e0 dtype('float32')\nprint(X_new.dtype)          # Anche qui, il risultato sar\u00e0 dtype('float32')\n</code></pre> <p>In questo esempio, il formato di <code>X</code> \u00e8 <code>float32</code>, e possiamo verificare come non cambi dopo la chiamata a <code>fit_transform(X)</code>.</p> <p>Tipo di dati e prestazioni</p> <p>L'uso di dati <code>float32</code> \u00e8 spesso pi\u00f9 efficiente rispetto all'uso del formato <code>float64</code>, in quanto permette di ridurre i requisiti sia spaziali, sia temporali. Tuttavia, ci potrebbero essere degli errori di troncamento che causando problemi di stabilit\u00e0 numerica.</p> <p>Nota</p> <p>Esistono anche degli stimatori (soprattutto regressori) che lavorano esclusivamente con dati in formato <code>float64</code>.</p>"},{"location":"material/04_sklearn/01_intro/lecture/#refitting-ed-aggiornamento-dei-parametri","title":"Refitting ed aggiornamento dei parametri","text":"<p>I parametri di uno stimatore vengono fissati passando gli opportuni argomenti al costruttore. Tuttavia, questi possono essere successivamente modificati utilizzando il metodo <code>set_params()</code>. Tuttavia, per vedere gli effetti dei nuovi valori dei parametri, dovremo provvedere a ri-addestrare il modello. Ad esempio:</p> <pre><code>X, y = load_iris(return_X_y=True)\nclf = SVC()\nclf.set_params(kernel='linear').fit(X, y)\nclf.predict(X[:10])\nclf.set_params(kernel='rbf').fit(X, y)\nclf.predict(X[:10])\n</code></pre> <p>Nel codice precedente:</p> <ul> <li>alla riga 1, carichiamo il dataset Iris;</li> <li>alla riga 2, creiamo uno stimatore di classe <code>SVC</code>;</li> <li>alla riga 3, impostiamo il parametro <code>kernel</code> del nostro stimatore a <code>linear</code>, e lo addestriamo sui dati a nostra disposizione;</li> <li>alla riga 4, effettuiamo la predizione sui primi dieci campioni con i parametri impostati in precedenza;</li> <li>alla riga 5, modifichiamo il parametro <code>kernel</code>, riaddestrando il modello con il nuovo valore impostato;</li> <li>alla riga 6, infine, effettuiamo un'altra predizione, usando il modello appena riaddestrato.</li> </ul>"},{"location":"material/04_sklearn/01_intro/lecture/#problemi-multiclasse-vs-problemi-multilabel","title":"Problemi multiclasse vs. problemi multilabel","text":"<p>Un problema si definisce multiclasse quando ad ogni campione \u00e8 possibile associare una ed una sola classe tra molte disponibili. Un problema \u00e8 invece definito come multilabel quando ad ogni campione \u00e8 possibile associare pi\u00f9 di una tra le classi disponibili.</p> <p>Ad esempio, classificare un film in base al numero di stelle ricevute pu\u00f2 essere interpretato come un problema multiclasse:</p> Film Numero di stelle Jumanji 5 Il Grinch 2 La fabbrica di cioccolato 4 <p>Se invece considerassimo il genere a cui ciascun film appartiene, potremmo trovarci di fronte ad una situazione di questo tipo:</p> Film Avventura Commedia Fantasy Crime Per bambini Jumanji Vero Falso Vero Falso Vero Il Grinch Falso Vero Vero Falso Vero La fabbrica di cioccolato Vero Falso Vero Falso Vero <p>In questo caso, avremmo diverse etichette \"vere\" per ciascun campione, per cui il problema pu\u00f2 essere impostato come un multilabel.</p> <p>Nel caso si voglia affrontare un problema multiclasse, l'apprendimento e la predizione dipendono dal formato dei dati di output:</p> <p>Quando usiamo un classificatore multiclasse, il task di learning e predizione che viene effettuato \u00e8 dipendente dal formato dei dati target. Proviamo con un array monodimensionale:</p> <pre><code>X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]\ny = [0, 0, 1, 1, 2]\n\nclf = OneVsRestClassifier(estimator=SVC(random_state=0))\nclf.fit(X, y).predict(X)\n</code></pre> <p>Il risultato sar\u00e0:</p> <pre><code>array([0, 0, 1, 1, 2])\n</code></pre> <p>Notiamo quindi come il metodo <code>predict()</code> fornisca un array a singola dimensione, nel quale l'\\(i\\)-mo elemento \u00e8 associato alla classe predetta per l'\\(i\\)-mo campione. Se invece volessimo effettuare il fitting su un array bidimensionale, dovremmo trasformare <code>y</code>, ad esempio mediante un'operazione di one-hot encoding:</p> <pre><code>y = LabelBinarizer().fit_transform(y)\nclf.fit(X, y).predict(X)\n</code></pre> <p>In questo caso, il risultato sar\u00e0:</p> <pre><code>array([[1, 0, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 0],\n       [0, 0, 0]])\n</code></pre> <p>Notiamo quindi come <code>predict()</code> restituisca un array bidimensionale, nel quale ad ogni riga \u00e8 associato un campione, con il valore <code>1</code> assegnato alla classe predetta, e <code>0</code> altrimenti. In altre parole, per il primo campione, il predittore associer\u00e0 la classe \\(0\\), per il secondo sempre la classe \\(0\\), per il terzo la classe \\(1\\), e via dicendo.</p> <p>In caso di problema multilabel dovremo usare un <code>MultiLabelBinarizer</code>:</p> <p>Notiamo che la quarta e quinta istanza restituiscono tutti zero, il che indica che non combaciano con nessuna delle tre label su cui sono state addestrate. Con gli output multilable, \u00e8 simile per un'istanza ad avere label multiple:</p> <pre><code>y = [[0, 1], [0, 2], [1, 3], [0, 2, 3], [2, 4]]\ny = MultiLabelBinarizer().fit_transform(y)\nclf.fit(X, y).predict(X)\n</code></pre> <p>Il risultato sar\u00e0:</p> <pre><code>array([[1, 1, 0, 0, 0],\n       [1, 0, 1, 0, 0],\n       [0, 1, 0, 1, 0],\n       [1, 0, 1, 0, 0],\n       [1, 0, 1, 0, 0]])\n</code></pre> <p>Come \u00e8 possibile vedere, l'array bidimensionale restituito ha diverse label per ogni singola istanza.</p>"},{"location":"material/04_sklearn/02_linear_models/01_metrics/","title":"4.2.1 - Metriche di regressione","text":"<p>Il modulo <code>metrics</code> offre numerose metriche per la valutazione delle performance di regressione, alcune delle quali possono gestire la regressione multivariata.</p> <p>In particolare, le metriche che offrono il parametro <code>multioutput</code> permettono di specificare il modo in cui mediare i contributi associati alle diverse variabili dipendenti. Il valore di default di questo parametro \u00e8 <code>uniform_average</code>, che fa in modo che sia utilizzata una media pesata. In alternativa, \u00e8 possibile passare un array di pesi, oppure utilizzare il valore <code>raw_values</code>, che fa in modo che nel computo complessivo della metrica siano utilizzati i punteggi non mediati.</p>"},{"location":"material/04_sklearn/02_linear_models/01_metrics/#errore-di-regressione","title":"Errore di regressione","text":""},{"location":"material/04_sklearn/02_linear_models/01_metrics/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>Il valore di MSE \u00e8 ottenuto usando la funzione <code>mean_squared_error</code>. Ad esempio:</p> <p>Il loro utilizzo \u00e8 sempre del tipo:</p> <pre><code>from sklearn.metrics import mean_squared_error\n\n# ...\n\nmse = mean_squared_error(y_true, y_pred)\n</code></pre>"},{"location":"material/04_sklearn/02_linear_models/01_metrics/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>Il MAE pu\u00f2 essere ottenuto usando la funzione <code>mean_absolute_error</code>:</p> <pre><code>from sklearn.metrics import mean_absolute_error\n\n# ...\n\nmae = mean_absolute_error(y, y_pred)\n</code></pre>"},{"location":"material/04_sklearn/02_linear_models/01_metrics/#mean-absolute-percentage-error-mape","title":"Mean Absolute Percentage Error (MAPE)","text":"<p>Il valore del MAPE pu\u00f2 essere ottenuto usando la funzione <code>mean_absolute_percentage_error</code>:</p> <pre><code>from sklearn.metrics import mean_absolute_error\n\n# ...\n\nmape = mean_absolute_error(y, y_pred)\n</code></pre>"},{"location":"material/04_sklearn/02_linear_models/01_metrics/#metriche-statistiche","title":"Metriche statistiche","text":""},{"location":"material/04_sklearn/02_linear_models/01_metrics/#coefficiente-di-determinazione-r2","title":"Coefficiente di determinazione \\(R^2\\)","text":"<p>Abbiamo visto che una delle metriche pi\u00f9 utilizzate \u00e8 il coefficiente di determinazione \\(R^2\\), che pu\u00f2 variare tra \\(- \\infty\\), in quanto il modello pu\u00f2 commettere errori potenzialmente \"infiniti\", ed \\(1\\), ottenuto quando il modello aderisce perfettamente ai dati.</p> <p>Per quantificare il coefficiente di determinazione, Scikit-Learn offre la funzione <code>r2_score</code>. Questa funzione \u00e8 utilizzabile come segue:</p> <pre><code>from sklearn.metrics import r2_score\n\n# Caricamento di modello e dati...\nrgr.fit(X_true, y_true)\ny_pred = rgr.predict(X_true)\n\nr2 = r2_score(y_true, y_pred)\n</code></pre> <p>Regressione multivariata</p> <p>La funzione <code>r2_score</code> pu\u00f2 essere usata anche in caso di regressione multivariata. In questo caso, oltre ai possibili valori indicati in precedenza, il parametro <code>multioutput</code> accetta anche il valore <code>variance_weighted</code>, che pesa ciascun punteggio per la varianza della corrispondente variabile target. In pratica, utilizzare questa modalit\u00e0 fa s\u00ec che le variabili a maggiore varianza abbiamo maggiore importanza nella quantificazione della metrica.</p>"},{"location":"material/04_sklearn/02_linear_models/01_metrics/#varianza-spiegata","title":"Varianza spiegata","text":"<p>In modo del tutto analogo alle altre metriche, la varianza spiegata \u00e8 calcolata usando la funzione <code>explained_variance_score</code>:</p> <pre><code>from sklearn.metrics import explained_variance_score\n\n# ...\nev = explained_variance_score(y_true, y_pred)\n</code></pre>"},{"location":"material/04_sklearn/02_linear_models/02_regression/","title":"4.2.2 - Regressione in Scikit-Learn","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>La forma pi\u00f9 semplice di regressione in Scikit-Learn \u00e8 implementata grazie ad una serie di stimatori contenuti nel package <code>linear_model</code> della libreria.</p> <p>Esaminiamone brevemente alcuni, ovvero quello lineare, robusto e polinomiale.</p>"},{"location":"material/04_sklearn/02_linear_models/02_regression/#regressione-lineare","title":"Regressione lineare","text":"<p>Gli stimatori di classe <code>LinearRegression()</code> permettono di effettuare una regressione lineare basandosi sul metodo dei minimi quadrati.</p> <p>Il funzionamento base di un oggetto di tipo <code>LinearRegression</code> \u00e8 il seguente:</p> <pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()\nX = np.array([[0, 1, 2]])\ny = np.array([[0, 1, 2]])\nreg.fit(X, y)\n</code></pre> <p>In particolare:</p> <ul> <li>alla riga 4, creiamo un oggetto di classe <code>LinearRegression()</code>;</li> <li>alle righe 5-6, creiamo due array chiamati <code>X</code> ed <code>y</code> che saranno usati rispettivamente come variabili indipendente e dipendente;</li> <li>alla riga 7 addestriamo il regressore sugli array creati in precedenza.</li> </ul> <p>Una volta addestrato, il regressore sar\u00e0 pronto ad effettuare le opportune predizioni mediante il metodo <code>predict()</code>:</p> <pre><code>reg.predict([[5]])\n</code></pre>"},{"location":"material/04_sklearn/02_linear_models/02_regression/#accesso-ai-parametri-dello-stimatore","title":"Accesso ai parametri dello stimatore","text":"<p>Una volta addestrato, un regressore lineare ci offre l'accesso a due parametri, ovvero coefficiente angolare ed intercetta. Questi sono accessibili mediante gli attributi <code>coef_</code> ed <code>intercept_</code>; ad esempio:</p> <pre><code>print(f'Coefficiente: {reg.coef_}')\nprint(f'Intercetta: {reg.incercept_}')\n</code></pre> <p>Dimensionalit\u00e0 di coefficiente ed intercetta</p> <p>Come ovvio, la dimensionalit\u00e0 di coefficiente ed intercetta dipende dal numero di variabili indipendenti considerate. In altre parole, la regressione lineare non restituisce necessariamente una retta, ma piuttosto un piano interpolante ad \\(n\\) dimensioni, con \\(n\\) numero di variabili indipendenti.</p>"},{"location":"material/04_sklearn/02_linear_models/02_regression/#coefficiente-di-regressione","title":"Coefficiente di regressione","text":"<p>La classe <code>LinearRegression()</code> ci mette a disposizione anche il metodo <code>score()</code>, che ci permette di ottenere il coefficiente \\(R^2\\) ottenuto dal modello di regressione. Questo \u00e8 pari a:</p> \\[ R^2 = (1 - \\frac{u}{v}) \\] <p>dove:</p> <ul> <li>\\(u\\) \u00e8 pari alla sommatoria dei quadrati dei residui, ovvero \\(\\sum (y - y')^2\\);</li> <li>\\(v\\) \u00e8 pari alla sommatoria della differenza tra i valori veri ed il valor medio, ovvero \\(\\sum (y - \\mu(y))^2\\).</li> </ul> <p>Conoscere il valore di \\(R^2\\) \u00e8 importante per avere un'idea della bont\u00e0 del modello. Nel caso ideale, infatti, questo valore \u00e8 \\(1\\), mentre valori inferiori (o addirittura negativi) rappresentano delle possibili criticit\u00e0 del modello.</p> <p>Intervalli di confidenza</p> <p>Scikit-Learn non fornisce un intervallo di confidenza per le predizioni ottenute; pi\u00f9 informazioni su questa scelta di design qui. Tuttavia, \u00e8 possibile implementare questa funzionalit\u00e0 usando NumPy, come descritto qui, o in alternativa usare il package Statsmodels.</p>"},{"location":"material/04_sklearn/02_linear_models/02_regression/#regressione-robusta","title":"Regressione robusta","text":"<p>Nel caso in cui i nostri dati siano \"sporchi\", pu\u00f2 essere utile utilizzare un regressore in grado di minimizzare l'influenza di questi effetti. In particolare, questi algoritmi ci permettono di isolare gli inlier dagli outlier: i primi, infatti, sono i campioni che afferiscono alla distribuzione \"vera\" del dato, mentre i secondi sono campioni \"esterni\" alla stessa.</p> <p>Esempio pratico</p> <p>Facciamo un banale esempio pratico: in un dataset di cani pastore, la presenza di un chihuahua sar\u00e0 da considerarsi come un outlier.</p> <p>Scikit-Learn offre tre tipi di regressore, ovvero il RANSAC, il Theil Sen, e l'HuberRegressor.</p> <p>Tra questi, il pi\u00f9 utilizzato \u00e8 certamente il RANSAC, crasi di RANdom SAmple Consensus. Vediamolo in breve.</p>"},{"location":"material/04_sklearn/02_linear_models/02_regression/#ransac","title":"RANSAC","text":"<p>L'algoritmo RANSAC prevede che il modello sia calcolato a partire da un insieme di inlier casualmente estratti a partire dal dataset iniziale. Il risultato sar\u00e0, per l'appunto, un modello lineare che si adatta esclusivamente agli inlier, i quali possono essere soggetti a rumore bianco, scartando gli outlier, che si suppone provengano da distribuzioni dei dati differenti.</p> <p>A differenza di regressori pi\u00f9 semplici, come quello lineare, il RANSAC produce risultati non deterministici: in altre parole, il modello garantisce soltanto una certa \"percentuale\" di efficacia, strettamente dipendente dal numero di iterazioni considerate.</p> <p>Il RANSAC \u00e8 un algoritmo puramente iterativo, in cui ogni ciclo si articola in quattro step:</p> <ol> <li>Al primo step della \\(i\\)-ma iterazione, viene selezionato un certo numero di campioni casuali dal dataset originario, chiamato \\(S_i\\). In Scikit-Learn, questo numero \u00e8 determinato con il parametro <code>min_samples</code>.</li> <li>Al secondo step, si addestra un modello lineare \\(M_i\\) sui dati \\(S_i\\).</li> <li>Al terzo step, si utilizza \\(M_i\\) per calcolare le predizioni su tutti i campioni \\(S_i\\). Se il residuo su ciascuna predizione (ovvero, la differenza tra dato predetto e dato vero) \u00e8 al di sotto di un certo valore di soglia, il campione \u00e8 classificato come inlier. In Scikit-Learn, il valore di soglia \u00e8 determinato dal parametro <code>residual_threshold</code>.</li> <li>Il modello \\(M_i\\) viene considerato come \"miglior modello\" se ha un numero di inlier superiore a quello del modello migliore ottenuto fino a quel momento, che viene quindi aggiornato con \\(M_i\\).</li> </ol> <p>Il numero di iterazioni massime ammesse sul RANSAC \u00e8 determinato dal parametro <code>max_trials</code>. Tuttavia, \u00e8 possibile anche iterare l'algoritmo soltanto fino a quando non viene rispettato un certo criterio, come il numero di inliers considerati (<code>stop_n_inliers</code>) o il punteggio \\(R^2\\) ottenuto (<code>stop_score</code>).</p>"},{"location":"material/04_sklearn/02_linear_models/03_logistic/","title":"4.2.3 - Regressione logistica in Scikit-Learn","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p> <p>La regressione logistica in Scikit-Learn \u00e8 implementata mediante la classe <code>LogisticRegression()</code>, e pu\u00f2 essere utilizzata sia in caso di classificazione binaria, sia in caso di classificazione multiclasse.</p>"},{"location":"material/04_sklearn/02_linear_models/03_logistic/#caso-binario","title":"Caso binario","text":"<p>Nel caso binario, un oggetto di classe <code>LogisticRegression()</code> addestrato su un insieme di dati \\(X\\) restituisce un valore di probabilit\u00e0 \\(\\hat{y}_i\\) compreso nell'insieme \\([0, 1]\\). In particolare, il metodo <code>predict_proba</code> predirr\u00e0 la probabilit\u00e0 della classe positiva \\(P(\\hat{y}_i = 1 | X_i)\\) come:</p> \\[ \\hat{p}(X_i) = \\frac{1}{1 + e^{-X_i w - w_0}} \\] <p>La funzione obiettivo da minimizzare \u00e8 data da:</p> \\[ \\min_w C \\sum_{i=1}^n (-y_i \\log(\\hat{p}X_i)) - (1-y_i) log (1-\\hat{p}(X_i)) + r(w) \\] <p>con $r(w) termine di regolarizzazione.</p>"},{"location":"material/04_sklearn/02_linear_models/03_logistic/#caso-multiclasse","title":"Caso multiclasse","text":"<p>Nel caso multiclasse, invece, il metodo <code>predict_proba</code> predice la probabilit\u00e0 per la classe \\(k\\) \\(P(\\hat{y}_i = k | X_i)\\) come:</p> \\[ \\hat{p}_k(X_i) = \\frac{e^{X_i W_k + W_{0,k}}}{\\sum_{l=0}^{K-1} e^{X_iW_l + W_{0, l}}} \\] <p>In questo caso, la funzione obiettivo da minimizzare assume la seguente forma:</p> \\[ \\min_W -C \\sum_{i=1}^n \\sum_{k=0}^{K-1} [y_i = k] \\log(\\hat{p}_k (X_i)) + r(W) \\]"},{"location":"material/04_sklearn/02_linear_models/03_logistic/#il-parametro-penalty","title":"Il parametro <code>penalty</code>","text":"<p>Il coefficiente di penalizzazione \\(r(w)\\) pu\u00f2 essere uno tra quattro diversi valori specificati al parametro <code>penalty</code>, riassunti nella seguente tabella.</p> <code>penalty</code> Descrizione Valore per caso binario Valore per caso multiclasse <code>None</code> Non assegna alcuna penalizzazione \\(0\\) \\(0\\) <code>l1_ratio</code> Regolarizzazione mediante la norma \\(l_1\\) \\(\\|w\\|_1\\) \\(\\sum_{i=1}^n \\sum_{j=1}^K \\| W_{i, j}\\|\\) <code>l2_ratio</code> Regolarizzazione mediante la norma \\(l_2\\) \\(\\frac{1}{2}\\|w\\|_2^2 = \\frac{1}{2}w^Tw\\) \\(\\frac{1}{2} \\|W\\|_F^2 = \\frac{1}{2} \\sum_{i=1}^n\\sum_{j=1}^K W_{i,j}^2\\) <code>ElasticNet</code> Regolarizzazione mediante una combinazione di norma \\(l_1\\) e norma \\(l_2\\) \\(\\frac{1-\\rho}{2} w^Tw + \\rho\\|w\\|_1\\) $\\frac{1-\\rho}{2}|W|F^2 + \\rho |W| <p>In particolare, il termine \\(\\rho\\) in ElasticNet controlla la forza relativa della regolarizzazione \\(l_1\\) rispetto alla regolarizzazione \\(l_2\\). In pratica, se \\(\\rho=1\\), ElasticNet \u00e8 equivalente ad \\(l_1\\), mentre se \\(\\rho\\) \u00e8 uguale a \\(0\\) allora \u00e8 equivalente ad \\(l_2\\).</p>"},{"location":"material/04_sklearn/02_linear_models/03_logistic/#esempio-di-utilizzo","title":"Esempio di utilizzo","text":"<p>Per utilizzare la regressione logistica dovremo creare uno stimatore di tipo <code>LogisticRegression()</code> ed addestrarlo come segue:</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\nlr = LogisticRegression()\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nlr.fit(X_train)\ny_pred = lr.predict(X_test)\n</code></pre> <p>In particolare:</p> <ul> <li>alla riga 5, importiamo il dataset Iris;</li> <li>alla riga 6, creiamo il nostro stimatore;</li> <li>alla riga 7, suddividiamo i dati in insiemi di training e test;</li> <li>alla riga 8, addestriamo il nostro stimatore sull'insieme di training;</li> <li>alla riga 9, usiamo lo stimatore addestrato per effettuare le predizioni sui dati di test.</li> </ul>"},{"location":"material/04_sklearn/02_linear_models/exercises/","title":"Esercitazione 4.2","text":"<p>Soluzioni</p> <p>Per questi esercizi esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p>"},{"location":"material/04_sklearn/02_linear_models/exercises/#esercizio-421","title":"Esercizio 4.2.1","text":"<p>Proviamo ad operare sul dataset Tips di Seaborn, effettuando una regressione lineare che riguardi le mance ed il conto totale. Per farlo, usiamo un oggetto di classe <code>LinearRegression()</code> messo a disposizione dal package <code>linear_model</code> di Scikit-Learn.</p> <p>Valutiamo lo score \\(R^2\\) ottenuto, e mostriamo a schermo i risultati dell'interpolazione, assieme al coefficiente angolare ed all'intercetta ottenuti.</p>"},{"location":"material/04_sklearn/02_linear_models/exercises/#esercizio-422","title":"Esercizio 4.2.2","text":"<p>Proviamo ad effettuare poi un'interpolazione mediante un oggetto di classe <code>RANSACRegression()</code>, e confrontiamo i risultati ottenuti in precedenza in tre modi:</p> <ul> <li>tramite un plot;</li> <li>valutando lo score;</li> <li>valutando i valori di coefficiente ed intercetta del modello usato.</li> </ul> <p>Proviamo infine ad eseguire due volte il RANSAC, e verifichiamo che i risultati ottenuti siano differenti.</p>"},{"location":"material/04_sklearn/02_linear_models/exercises/#esercizio-423","title":"Esercizio 4.2.3","text":"<p>Continuiamo ad operare sul dataset Tips di Seaborn. In particolare, scegliamo come label il giorno, e come feature sulle quali operare il conto totale, la mancia e la dimensione del tavolo. Addestriamo un classificatore a determinare qual \u00e8 il giorno pi\u00f9 probabile sulla base delle feature selezionate.</p>"},{"location":"material/04_sklearn/02_linear_models/exercises/#esercizio-424","title":"Esercizio 4.2.4","text":"<p>Proviamo adesso a verificare come variano i valori di accuratezza, precisione e recall per diversi valori della soglia di decisione. In tal senso:</p> <ul> <li>semplifichiamo il problema riducendolo ad una classificazione binaria, e quindi considerando come label la colonna <code>time</code>;</li> <li>utilizziamo il metodo <code>predict_proba(X)</code> del <code>LogisticRegressor()</code>.</li> </ul>"},{"location":"material/04_sklearn/02_linear_models/exercises/#esercizio-425","title":"Esercizio 4.2.5","text":"<p>Consideriamo il regressore lineare usato nell'esercizio 4.2.1. Valutiamo i risultati ottenuti in termini di MSE, MAPE ed \\(R^2\\).</p>"},{"location":"material/04_sklearn/03_estimator/01_metrics/","title":"4.3.1 - Metriche di classificazione","text":"<p>Cos\u00ec come per le metriche di regressione e di clustering Scikit-Learn mette a disposizione numerose metriche per la valutazione dei risultati di classificazione. Vediamole brevemente.</p>"},{"location":"material/04_sklearn/03_estimator/01_metrics/#metriche-binarie-e-multiclasse","title":"Metriche binarie e multiclasse","text":"<p>In modo simile alle metriche di regressione, alcune delle metriche di classificazione, come ad esempio quelle che riguardano il calcolo di precisione, recall ed F1-score, sono esplicitamente pensate per la classificazione binaria, e presuppongono che la classe positiva sia etichettata con un <code>1</code>.  </p> <p>Di conseguenza, quando si affronta un problema multiclasse, Scikit-Learn tratta \"internamente\" il problema come una serie di problemi binari del tipo one-vs-all, uno per ciascuna classe. Vi \u00e8 quindi la necessit\u00e0 di specificare il modo con cui il valore complessivo della metrica dovr\u00e0 essere calcolato mediante il parametri <code>average</code>, che pu\u00f2 assumere uno tra i seguenti valori:</p> Valore del parametro <code>average</code> Breve spiegazione <code>macro</code> In questa modalit\u00e0, viene calcolata la media delle singole metriche, dando egual peso a ciascuna classe <code>weighted</code> Tiene conto di eventuali sbilanciamenti del dataset, usando un peso proporzionale alla presenza nel dataset iniziale per ciascun contributo <code>micro</code> Ad ogni coppia campione-classe viene dato egual contributo alla metrica complessiva <code>samples</code> <p>Metriche per le singole classi</p> <p>Selezionando <code>average=None</code> avremo un array di metriche, una per ogni singola classe.</p>"},{"location":"material/04_sklearn/03_estimator/01_metrics/#accuracy-precisione-recall-f1-score","title":"Accuracy, precisione, recall, F1 score","text":""},{"location":"material/04_sklearn/03_estimator/01_metrics/#accuracy","title":"Accuracy","text":"<p>L'accuratezza delle predizioni effettuate da un classificatore \u00e8 ottenuta in Scikit-Learn utilizzando il metodo <code>accuracy_score()</code>.</p> <p>Ad esempio:</p> <pre><code>from sklearn.metrics import accuracy_score\n\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\naccuracy_score(y_test, y_pred)\n</code></pre> <p>Top-k accuracy</p> <p>La funzione <code>top_k_accuracy_score</code> \u00e8 una generalizzazione dell'<code>accuracy_score</code> che considera la predizione corretta se viene individuata nei primi <code>k</code> punteggi restituiti dall'algoritmo.</p>"},{"location":"material/04_sklearn/03_estimator/01_metrics/#precisione","title":"Precisione","text":"<p>La precisione delle predizioni effettuate da un classificatore \u00e8 ottenuta in Scikit-Learn utilizzando il metodo <code>precision_score()</code>.</p> <p>Ad esempio:</p> <pre><code>from sklearn.metrics import precision_score\n\nprecision_score(y_test, y_pred)\n</code></pre> <p>Da notare che, nel caso di problemi multiclasse, sar\u00e0 necessario specificare il parametro <code>average</code>, che ci consente di assegnare a ciascuna classe un \"peso\" complessivio all'interno del calcolo della precisione globale.</p>"},{"location":"material/04_sklearn/03_estimator/01_metrics/#recall","title":"Recall","text":"<p>Ovviamente, anche il recall ha una rappresentazione in Scikit-Learn mediante la funzione <code>recall_score()</code>:</p> <pre><code>from sklearn.metrics import recall_score\n\nrecall_score(y_test, y_pred)\n</code></pre> <p>Anche in questo caso, i problemi multiclasse prevederanno l'utilizzo del parametro <code>average</code>.</p>"},{"location":"material/04_sklearn/03_estimator/01_metrics/#f1-score","title":"F1 Score","text":"<p>Finiamo la carrellata con la funzione <code>f1_score</code> che, prevedibilmente, permette di calcolare l'F1 del modello.</p> <pre><code>from sklearn.metrics import f1_score\n\nf1_score(y_test, y_pred)\n</code></pre> <p>Cos\u00ec come per precisione e recall, nei problemi multiclasse dovremo usare il parametro <code>average</code>.</p>"},{"location":"material/04_sklearn/03_estimator/01_metrics/#classification-report","title":"Classification report","text":"<p>E' possibile condensare le metriche precedenti utilizzando un unico comando chiamato <code>classification_report</code>:</p> <pre><code>from sklearn.metrics import classification_report\n\nclassification_report(y_test, y_pred)\n</code></pre>"},{"location":"material/04_sklearn/03_estimator/01_metrics/#matrice-di-confusione","title":"Matrice di confusione","text":"<p>Ricordiamo che una matrice di confusione \u00e8 una matrice che, dati i valori \"veri\" sulle righe, e quelli predetti sulle colonne, associa all'elemento in posizione \\((i, j)\\) il numero di volte in cui il modello ha associato la classe \\(j\\) all'elemento \\(i\\).</p> <p>Matrice di confusione ideale</p> <p>Idealmente, una matrice di confusione dovrebbe essere diagonale. Nella realt\u00e0, ci accontentiamo di matrici \"per lo pi\u00f9\" diagonali.</p> <p>Scikit-Learn ci permette di calcolare la matrice di confusione come metrica a s\u00e8 stante mediante il metodo <code>confusion_matrix</code>, che restituisce un array rappresentativo della matrice di confusione:</p> <pre><code>from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)\n</code></pre>"},{"location":"material/04_sklearn/03_estimator/01_metrics/#visualizzazione-della-matrice-di-confusione","title":"Visualizzazione della matrice di confusione","text":"<p>Per visualizzare la matrice di confusione abbiamo due strade:</p> <ol> <li>passare l'array restituito da <code>confusion_matrix</code> ad una heatmap di Seaborn;</li> <li>utilizzare un oggetto di classe <code>ConfusionMatrixDisplay</code>.</li> </ol> <p>Nel secondo caso, potremo sia passare la matrice di confusione come parametro all'inizializzatore dell'oggetto di classe <code>ConfusionMatrixDisplay</code>, sia utilizzare i metodi <code>from_estimator</code>, che calcola la matrice di confusione a partire da stimatore e dataset, e <code>from_predictions</code>, che accetta come parametri <code>y_true</code> ed <code>y_pred</code>.</p>"},{"location":"material/04_sklearn/03_estimator/02_clf_rgr/","title":"4.3.2 - Classificatori e regressori","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p>"},{"location":"material/04_sklearn/03_estimator/02_clf_rgr/#alberi-decisionali","title":"Alberi decisionali","text":"<p>Scikit-Learn implementa due versioni degli alberi decisionali: la prima \u00e8 dedicata alla classificazione, ed \u00e8 chiamata <code>DecisionTreeClassifier()</code>, mentre la seconda \u00e8 orientata alla regressione ed \u00e8 chiamata <code>DecisionTreeRegressor()</code>.</p> <p>In entrambi i casi, l'interfaccia per utilizzare l'albero decisionale \u00e8 quella comunemente offerta dagli stimatori di Scikit-Learn:</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_train)\n</code></pre> <p>Da notare che, una volta addestrato, \u00e8 possibile mostrare a schermo l'albero decisionale mediante la funzione <code>plot_tree</code>:</p> <pre><code>from sklearn.tree import plot_tree\n\nplot_tree(dt)\n</code></pre> <p>Da notare che Scikit-Learn offre supporto nativo ai problemi multi-output.</p>"},{"location":"material/04_sklearn/03_estimator/02_clf_rgr/#consigli-pratici-sullutilizzo-degli-alberi-decisionali","title":"Consigli pratici sull'utilizzo degli alberi decisionali","text":"<ol> <li>Gli alberi decisionali tendono all'overfitting in presenza di un elevato numero di feature. In questo caso, pu\u00f2 essere utile utilizzare tecniche di riduzione della dimensionalit\u00e0 o di feature selection.</li> <li>Visualizzare e comprendere la struttura del nostro albero ci pu\u00f2 dare dei suggerimenti su come l'albero sta effettuando le predizioni.</li> <li>Dato che il numero di campioni richiesti per popolare l'albero raddoppia ad ogni livello, \u00e8 necessario impostare opportunamente il parametro <code>max_depth</code> per prevenire l'overfitting.</li> <li>I parametri <code>min_samples_leaf</code> e <code>min_samples_split</code> controllano quali percorsi saranno considerati dall'albero. In particolare, va scelto un numero adeguato, in quanto un valore troppo piccolo potr\u00e0 condurre verso l'overfitting, mentre un valore troppo elevato potrebbe impedire all'albero di apprendere correttamente.</li> <li>Fornire un dataset bilanciato pu\u00f2 condurre verso risultati migliori.</li> </ol>"},{"location":"material/04_sklearn/03_estimator/02_clf_rgr/#random-forest","title":"Random forest","text":"<p>I random forest sono dei metodi ensemble basati su alberi decisionali. Un metodo ensemble (letteralmente \"insieme\") permette di combinare i risultati provenienti da diversi algoritmi, ottenendo in generale risultati migliori.</p> <p>In particolare, il random forest sfrutta un insieme di alberi decisionali, ognuno dei quali modellato su un sottoinsieme di dati e feature presenti nel set di training; i risultati provenienti da ciascuno degli alberi saranno poi mediati e combinati. La presenza di queste due componenti di casualit\u00e0 permette di raggiungere un obiettivo ben preciso, ovvero diminuire l'overfitting proprio di un singolo albero decisionale, ottenendo un modello generalmente migliore.</p> <p>Anche per il random forest esistono due versioni, ovvero quella dedicata alla regressione (RandomForestRegressor()) e quella dedicata alla classificazione (RandomForestClassifier()).</p> <p>L'utilizzo di un random forest \u00e8 analogo a quello degli altri stimatori:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_train)\n</code></pre>"},{"location":"material/04_sklearn/03_estimator/02_clf_rgr/#extremely-randomized-trees","title":"Extremely Randomized Trees","text":"<p>Scikit-Learn offre un'ulteriore variante di random forest, chiamata Extremely Randomized Trees. In questo tipo di foresta decisionale, viene inserito un ulteriore elemento di casualit\u00e0 legato a come vengono calcolate le regole per lo split dei dati: infatti, viene scelto un insieme casuale di soglie, a partire dal quale viene individuata quella che garantisce la maggior discriminativit\u00e0.</p> <p>Questo tipo di foresta \u00e8 utilizzabile usando le classi <code>ExtraTreesClassifier()</code> ed <code>ExtraTreesRegressor()</code>; l'utilizzo \u00e8 analogo al classico random forest.</p> <pre><code>from sklearn.ensemble import ExtraTreesClassifier\n\net = ExtraTreesClassifier()\net.fit(X_train, y_train)\ny_pred = et.predict(X_train)\n</code></pre>"},{"location":"material/04_sklearn/03_estimator/02_clf_rgr/#parametri-di-un-random-forest","title":"Parametri di un random forest","text":"<p>I principali parametri da impostare per le foreste casuali sono <code>n_estimators</code> e <code>max_features</code>. Come suggerisce il nome, il primo rappresenta il numero di stimatori nella foresta che, idealmente, dovrebbe essere quanto pi\u00f9 elevato possibile, ovviamente al costo di una maggiore complessit\u00e0 computazionale. Il parametro <code>max_features</code>, invece, rappresenta il sottoinsieme casuale di feature da considerare quando si suddivide un nodo: considerarne un numero limitato permette di ridurre la varianza dei risultati, ma introduce anche un bias legato alle feature selezionate. Valori sperimentalmente buoni per <code>max_features</code> sono:</p> <ul> <li><code>None</code> per i problemi di regressione, nei quali si considerano quindi tutte le possibili feature;</li> <li><code>sqrt</code> per i problemi di classificazione, nei quali si considerano quindi soltanto la radice quadrata delle possibili feature.</li> </ul>"},{"location":"material/04_sklearn/03_estimator/02_clf_rgr/#multilayer-perceptron","title":"Multilayer perceptron","text":"<p>Un multilayer perceptron \u00e8 il pi\u00f9 semplice modello di rete neurale che \u00e8 possibile concepire. Nella pratica, \u00e8 un algoritmo che considera una relazione del tipo:</p> \\[ f:\\mathbb{R}^m \\rightarrow \\mathbb{R}^o \\] <p>dove \\(m\\) \u00e8 il numero di input ed \\(o\\) \u00e8 il numero di dimensioni per l'output. Ad esempio, se avessimo un insieme di feature \\(X=x_1, x_2, \\ldots, x_m\\) ed un'output \\(y\\), sia esso una classe o un valore di regressione, il multilayer perceptron apprender\u00e0 una funzione \\(f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^1\\).</p> <p>Una rappresentazione del multilayer perceptron \u00e8 mostrata nella seguente figura.</p> <p> </p> Figura 1 - Schema di un multi-layer perceptron. <p>Nella pratica, il layer di input (a sinistra) consiste di un insieme di neuroni, uno per ogni feature. Ogni neurone nello strato nascosto trasforma i valori del layer precedente con una sommatoria pesata \\(w_1 x_1 + \\ldots + w_m x_m\\) seguita da una funzione di attivazione non lineare del tipo \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\).</p> <p>La funzione di attivazione</p> <p>Le funzioni di attivazioni pi\u00f9 usate sono state per lungo tempo le sigmoidali e le loro varianti. Vedremo in seguito come negli ultimi anni quelle maggiormente gettonate siano diventate le ReLU.</p> <p>Nell'ultimo layer, infine, i valori ricevuti dal layer nascosto sono sommati e combinati nell'output.</p> <p>Ovviamente, Scikit-Learn offre due varianti dell'algoritmo, quella per la classificazione (<code>MLPClassifier()</code>) e quella per la regressione (<code>MLPRegressor()</code>), utilizzabili in modo analogo a tutti gli altri stimatori.</p> <pre><code>from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier()\nmlp.fit(X_train, y_train)\ny_pred = mlp.predict(X_train)\n</code></pre>"},{"location":"material/04_sklearn/03_estimator/02_clf_rgr/#suggerimenti-pratici","title":"Suggerimenti pratici","text":"<p>A differenza degli alberi decisionali, il multilayer perceptron richiede diversi accorgimenti per essere usato nel migliore dei modi. Infatti:</p> <ul> <li>\u00e8 fortemente consigliata una procedura di normalizzazione delle feature mediante uno <code>StandardScaler</code>;</li> <li>\u00e8 consigliato effettuare una ricerca empirica del parametro di regolarizzazione \\(\\alpha\\) della funzione di costo usata per l'addestramento del percettrone, con un valore scelto nell'intervallo \\([10^{-1}, 10^{-7}]\\);</li> <li>\u00e8 necessario scegliere l'algoritmo di ottimizzazione in base alle proprie esigenze. In particolare, il consiglio \u00e8 quello di usare <code>L-BFGS</code> su piccoli dataset, <code>Adam</code> su grossi dataset, ed <code>SGD</code> nei casi pi\u00f9 generali, specie quando \u00e8 possibile impostare in maniera adeguata il learning rate del percettrone.</li> </ul>"},{"location":"material/04_sklearn/03_estimator/exercises/","title":"Esercitazione 4.3","text":"<p>Soluzioni</p> <p>Le soluzioni a questi esercizi sono mostrate nel notebook reperibile a questo indirizzo.</p>"},{"location":"material/04_sklearn/03_estimator/exercises/#esercizio-431","title":"Esercizio 4.3.1","text":"<p>Operiamo sul problema visto nell'esercizio 4.2.1 usando un albero decisionale, un random forest ed un multilayer perceptron. Compariamo i risultati in termini di errore quadratico medio usando la funzione <code>mean_squared_error</code> del package <code>sklearn.metrics</code>.</p>"},{"location":"material/04_sklearn/03_estimator/exercises/#esercizio-432","title":"Esercizio 4.3.2","text":"<p>Operiamo sul problema visto nell'esercizio 4.2.3 usando un albero decisionale, un random forest ed un multilayer perceptron. Compariamo i risultati usando un <code>classification_report</code>.</p>"},{"location":"material/04_sklearn/03_estimator/exercises/#esercizio-433","title":"Esercizio 4.3.3","text":"<p>Esploriamo i risultati ottenuti dall'albero decisionale nell'esercizio 4.3.2. Per farlo, usiamo il metodo <code>plot_tree</code> del package <code>sklearn.tree</code>.</p>"},{"location":"material/04_sklearn/03_estimator/exercises/#esercizio-434","title":"Esercizio 4.3.4","text":"<p>Proviamo a variare leggermente alcuni parametri per i classificatori ed i regressori usati negli esercizi precedenti. Confrontiamo i risultati ottenuti nei termini delle metriche viste in precedenza.</p>"},{"location":"material/04_sklearn/04_clustering/01_metrics/","title":"4.4.1 - Metriche di clustering","text":"<p>Diamo un breve cenno a due tra le metriche utilizzate per la valutazione degli algoritmi di clustering, ovvero il silhouette score e l'adjusted rand index.</p>"},{"location":"material/04_sklearn/04_clustering/01_metrics/#adjusted-rand-index","title":"Adjusted Rand Index","text":"<p>L'ARI pu\u00f2 essere calcolato con la funzione <code>adjusted_rand_score()</code>, che funziona esattamente come le metriche usate per la classificazione e la regressione:</p> <pre><code>from sklearn.metrics import adjusted_rand_score\n\nadusted_rand_score(labels_pred, labels_true)\n</code></pre> <p>Da notare che, nonostante venga usata per valutare un algoritmo non supervisionato, l'ARI richieda, nei fatti, la conoscenza delle label associate ai campioni utilizzati.</p>"},{"location":"material/04_sklearn/04_clustering/01_metrics/#silhouette-score","title":"Silhouette score","text":"<p>Il silhouette score \u00e8 calcolato con la funzione <code>silhouette_score()</code>, che determina il silhouette coefficient di tutti i campioni presenti nel dataset. Da notare come questa funzione non richieda la conoscenza delle label \"vere\", permettendo quindi di stimare lo score a partire dai dati iniziali e dalle label predette dall'algoritmo di clustering:</p> <pre><code>from sklearn.metrics import silhouette_score\n\nsilhouette_score(X, labels_pred)\n</code></pre> <p>La funzione <code>silhouette_samples</code></p> <p>Scikit-Learn ci offre anche la funzione <code>silhouette_samples</code>, che permette di calcolare il silhouette coefficient per ogni singolo campione presente nel dataset iniziale.</p>"},{"location":"material/04_sklearn/04_clustering/02_clustering/","title":"4.4.2 - Clustering in Scikit-Learn","text":"<p>Notebook di accompagnamento</p> <p>Per questa lezione esiste un notebook di accompagnamento, reperibile a questo indirizzo.</p>"},{"location":"material/04_sklearn/04_clustering/02_clustering/#k-means","title":"K-Means","text":"<p>Il pi\u00f9 semplice algoritmo di clustering disponibile in Scikit-Learn \u00e8 il K-Means, implementato mediante l'omonima classe <code>KMeans()</code>. Essendo degli stimatori, gli oggetti di questa classe usano i soliti metodi <code>fit()</code> e <code>predict()</code>:</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs()\nclusterer = KMeans()\nclusterer.fit(X)\ny_pred = clusterer.predict(X)\n</code></pre> <p>E' possibile settare diversi parametri per il K-Means, tuttavia il pi\u00f9 importante \u00e8 sicuramente <code>n_clusters</code>, che indica il valore di \\(k\\) da considerare:</p> <pre><code>clusterer = KMeans(n_clusters=8)\n</code></pre> <p>Uno degli attributi pi\u00f9 interessanti \u00e8 sicuramente <code>labels_</code>, che restituisce un array indicativo delle label di ciascun campione.</p> <pre><code>clusterer.labels_\n</code></pre> <p>Mini-batch K-Means</p> <p>Scikit-Learn offre una variante del K-Means che opera su piccoli insiemi di dati, implementata nella classe <code>MiniBatchKMeans</code>. Il funzionamento di un oggetto di questa classe \u00e8 concettualmente analogo a quello di oggetti di classe <code>KMeans()</code>; tuttavia, in alcuni casi, l'uso dei mini batch pu\u00f2 essere utile a ridurre la complessit\u00e0 computazionale, a patto di accettare un lieve calo in termini di performance.</p>"},{"location":"material/04_sklearn/04_clustering/02_clustering/#clustering-gerarchico","title":"Clustering gerarchico","text":"<p>In una delle lezioni precedenti abbiamo visto che gli algoritmi di clustering gerarchico vanno a definire un \"albero\" di cluster annidati, suddividendo cluster all'\\(i\\)-mo livello in pi\u00f9 cluster al livello \\(i+1\\)-mo o, al contrario, agglomerando pi\u00f9 cluster all'\\(i+1\\)-mo livello in un singolo cluster al livello \\(i\\)-mo.</p> <p>Il clustering gerarchico \u00e8 implementato in Scikit-Learn mediante la classe <code>AgglomerativeClustering</code>. Il funzionamento \u00e8 concettualmente identico a quello degli altri stimatori; tuttavia, ci sono alcuni parametri di interesse, ovvero:</p> <ul> <li><code>n_clusters</code>: in questo caso, il parametro <code>n_clusters</code> indica il numero di cluster che desideriamo che l'algoritmo individui.</li> <li><code>compute_full_tree</code>: questo parametro indica se calcolare o meno l'intera gerarchia di cluster.</li> <li><code>linkage</code>: indica la strategia utilizzata per definire se due campioni appartengono ad uno stesso cluster.</li> </ul> <p>In particolare, il criterio di linkage pu\u00f2 essere uno tra i seguenti:</p> <ul> <li><code>ward</code>: questo criterio minimizza la somma dei quadrati delle distanze nei campioni del cluster, in modo simile al criterio di inerzia del K-Means;</li> <li><code>complete</code>: questo criterio, chiamato complete linkage, definisce un cluster minimizzando la distanza massima tra le osservazioni che lo compongono;</li> <li><code>average</code>: questo criterio, chiamato average linkage, definisce un cluster minimizzando la distanza media tra le osservazioni che lo compongono;</li> <li><code>single</code>: questo criterio, chiamato single linkage, definisce un cluster minimizzando la distanza tra le osservazioni pi\u00f9 vicine in un cluster.</li> </ul> <p>Costo computazionale</p> <p>Il clustering gerarchico \u00e8 particolarmente costoso nel caso non vi siano dei particolari vincoli di connettivit\u00e0 tra campioni, in quanto deve considerare, ad ogni step, tutte le diverse combinazioni di cluster. Per ridurre questo problema, \u00e8 possibile usare la variante <code>FeatureAgglomeration</code>, che raggruppa tra loro feature di valore simile, riducendo il costo computazionale complessivo.</p> <p>Visualizzazione della gerarchia di cluster</p> <p>E' possibile visualizzare la gerarchia dei cluster definiti da un metodo agglomerativo usando la funzione <code>dendrogram</code> del package SciPy.</p>"},{"location":"material/04_sklearn/04_clustering/02_clustering/#dbscan","title":"DBSCAN","text":"<p>Anche il DBSCAN ha ovviamente un'implementazione in Scikit-Learn grazie alla classe <code>DBSCAN</code>. Da notare come l'algoritmo restituisce anche label di valore <code>-1</code>, che contraddistinguono i campioni contrassegnati come rumorosi o, per meglio dire, outlier.</p>"},{"location":"material/04_sklearn/04_clustering/exercises/","title":"Esercitazione 4.4","text":"<p>Soluzioni</p> <p>Le soluzioni a questi esercizi sono mostrate nel notebook reperibile a questo indirizzo.</p>"},{"location":"material/04_sklearn/04_clustering/exercises/#esercizio-441","title":"Esercizio 4.4.1","text":"<p>Il dataset Iris contiene i dati riguardanti lunghezza ed ampiezza di steli e petali per tre classi di fiori, ed \u00e8 uno dei dataset \"standard\" per l'analisi dei dati nel machine learning. In tal senso, usiamo il metodo <code>load_iris</code> del package <code>datasets</code> di Scikit-Learn per caricarlo. Una volta caricato in memoria, proviamo ad effettuare un primo clustering usando l'algoritmo k-means con 3 cluster.</p>"},{"location":"material/04_sklearn/04_clustering/exercises/#esercizio-442","title":"Esercizio 4.4.2","text":"<p>Verificare il valore di magnitudine e cardinalit\u00e0 per i cluster identificati nell'esercizio precedente.</p>"},{"location":"material/04_sklearn/04_clustering/exercises/#esercizio-443","title":"Esercizio 4.4.3","text":"<p>Valutiamo il valore migliore per il numero di cluster da utilizzare per il K-means utilizzando il dataset Iris e l'approccio empirico discusso a lezione. Usiamo valori per il clustering compresi tra 2 e 4.</p>"},{"location":"material/04_sklearn/04_clustering/exercises/#esercizio-444","title":"Esercizio 4.4.4","text":"<p>Ricreiamo le condizioni sperimentali degli esempi visti nel notebook che accompagna la lezione. Stavolta, per\u00f2, valutiamo le performance di ogni algoritmo utilizzando l'ARI ed il silhouette score.</p> <p>Inoltre, proviamo a vedere cosa accade per i seguenti parametri:</p> <ul> <li>per il K-Means, facciamo variare il numero di cluster tra <code>2</code> e <code>5</code>;</li> <li>per il DBSCAN, assegnamo ad \\(\\epsilon\\) i valori <code>0.5</code> o <code>1.0</code>, ed a <code>min_samples</code> i valori <code>5</code> e <code>10</code>.</li> </ul> <p>Per ognuno dei due algoritmi, infine, riportiamo a schermo solo i valori dei parametri per i quali le metriche assumono valore massimo.</p>"},{"location":"material/04_sklearn/05_advanced/01_pipeline/","title":"4.5.1 - Processing pipeline","text":"<p>Finora abbiamo spesso visto i singoli algoritmi di machine learning come blocchi monolitici ed a s\u00e8 stanti. Tuttavia, questo non \u00e8 quello che accade nella realt\u00e0.</p> <p>Immaginiamo, ad esempio, di dover effettuare un clustering usando il K-Means: come abbiamo avuto modo di vedere, l'algoritmo ha dei forti prerequisiti, tra cui la gaussianit\u00e0 dei dati, che ne pregiudicano il funzionamento qualora non rispettati. Potremmo voler quindi utilizzare uno <code>StandardScaler</code> per effettuare un'operazione di normalizzazione del nostro dataset, riportandolo a varianza unitaria e media nulla, ed applicando il K-Means ai dati gi\u00e0 trasformati.</p> <p>Portando questo concetto alle sue naturali conseguenze, \u00e8 abbastanza semplice vedere come non si parla pi\u00f9 di algoritmo, ma di pipeline di processing, intesa come insieme di transformer e stimatori da applicare in maniera sequenziale sui dati per ottenere il risultato atteso. In tal senso, Scikit-Learn ci mette a disposizione un'apposita classe chiamata <code>Pipeline()</code>, la quale ci permette per l'appunto di concatenere pi\u00f9 algoritmi, senza dover gestire manualmente i risultati intermedi.</p> <p>Facciamo un esempio. Riprendiamo gli algoritmi di clustering trattati in precedenza, con particolare riferimento al K-Means, per il quale abbiamo illustrato le difficolt\u00e0 nell'utilizzo in caso di dati non normalizzati. Volendo quindi applicare una standardizzazione ai dati sotto analisi, possiamo usare in cascata un trasformer di classe <code>StandardScaler</code> ed un oggetto di tipo <code>KMeans</code>:</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = make_blobs()\nscaler = StandardScaler()\nX_new = scaler.fit_transform(X)\nkmeans = KMeans()\nkmeans.fit_predict(X_new)\n</code></pre> <p>In particolare:</p> <ul> <li>alla riga 6, l'oggetto <code>scaler</code> creato alla riga 5 viene utilizzato per trasformare <code>X</code> in <code>X_new</code>;</li> <li>alla riga 8, l'oggetto <code>kmeans</code> viene addestrato su <code>X_new</code>.</li> </ul> <p>Ovviamente, questa sintassi \u00e8 molto prolissa, e pu\u00f2 essere facilmente suscettibile ad errori. Usando un oggetto di tipo <code>Pipeline</code>, invece, il codice precedente diventa:</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nX, y = make_blobs()\n\npipe = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('kmeans', KMeans())\n])\npipe.fit_predict(X)\n</code></pre> <p>Alla riga 8 viene quindi creato un oggetto di tipo <code>Pipeline</code> cui vengono passati i singoli algoritmi che lo compongono sotto forma di tuple, ognuna delle quali \u00e8 a sua volta composta da un identificativo univoco (ad esempio, <code>scaler</code> o <code>kmeans</code>), oltre che dallo stimatore o trasformer stesso. Conseguentemente, alla riga 12, vediamo un esempio di come \u00e8 possibile usare una pipeline: molto semplicemente, basta invocare i metodi opportuni (che sono una combinazione tra <code>fit</code>, <code>predict</code> e <code>trasform</code>) sui dati originari.</p>"},{"location":"material/04_sklearn/05_advanced/01_pipeline/#accesso-e-modifica-dei-parametri-degli-stimatori","title":"Accesso e modifica dei parametri degli stimatori","text":"<p>Sottolineamo come sia possibile accedere ai parametri di ciascuno degli stimatori e dei trasformer presenti all'interno della pipeline al momento della sua creazione. Ad esempio:</p> <pre><code>pipe = Pipeline(steps=[\n    ('scaler', MinMaxScaler(range=(0, 2))),\n    ('kmeans', KMeans(n_clusters=3))\n])\n</code></pre> <p>In questo caso, alla riga 2 stiamo creando un <code>MinMaxScaler</code> il cui range varia tra \\(0\\) e \\(2\\), mentre alla riga 3 creiamo uno stimatore per il K-Means con <code>3</code> cluster.</p> <p>Possiamo anche modificare successivamente tali parametri a patto di conoscere l'identificativo assegnato all'elemento della pipeline, ed utilizzando congiuntamente il comando <code>set_params</code> e la notazione <code>elemento__parametro</code>. Ad esempio, per portare il numero di cluster a <code>4</code>:</p> <pre><code>pipe.set_params(kmeans__n_clusters=4)\n</code></pre> <p>Infine, notiamo che possiamo anche accedere al singolo elemento di una pipeline trattandola come se fosse un dizionario:</p> <pre><code>pipe['kmeans']\n</code></pre> <p>e modificarne di conseguenza i parametri.</p>"},{"location":"material/04_sklearn/05_advanced/02_col_tran/","title":"4.5.2 - Preprocessing sui dataframe","text":"<p>Abbiamo visto come i dataset non siano sempre uniformi: infatti, alle volte, avremo feature numeriche accompagnate da altre feature di tipo categorico. Ci\u00f2 comporta la necessit\u00e0 di applicare trasformazioni ad hoc, ciascuna tarata su una specifica feature.</p> <p>Per farlo, un'idea potrebbe essere quella di applicare un trasformer su ogni feature che abbia bisogno di essere trasformata. Tuttavia, appare evidente come questo modo di procedere sia estremamente inefficiente nel momento in cui abbiamo a che fare con un numero di feature pi\u00f9 o meno alto. Per aiutarci in questo compito, quindi, Scikit-Learn ci mette a disposizione un particolare trasformer, chiamato <code>ColumnTransformer()</code>, che permette di semplificare il processing di un numero arbitrario di feature.</p>"},{"location":"material/04_sklearn/05_advanced/02_col_tran/#un-esempio","title":"Un esempio","text":"<p>Immaginiamo di dover trasformare tutte le colonne del dataset tips, standardizzando quelle numeriche, e codificando quelle categoriche. Per farlo, possiamo usare un <code>ColumnTransformer()</code> come segue:</p> <pre><code>import seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\ntips = sns.load_dataset('tips')\nct = ColumnTransformer(\n    [('scaler', StandardScaler(), ['total_bill', 'tip']),\n     ('encoder', OrdinalEncoder(), ['sex', 'smoker', 'day', 'time'])],\n    remainder='passthrough'\n)\n\nct.fit(tips)\nct.transform(tips)\n</code></pre> <p>Nota</p> <p>Se non fosse chiaro, i <code>ColumnTransformer()</code> lavorano sui dataframe.</p>"},{"location":"material/04_sklearn/05_advanced/02_col_tran/#columntransformer-e-pipeline","title":"<code>ColumnTransformer</code> e <code>Pipeline</code>","text":"<p>Possiamo anche combinare i <code>ColumnTransformer()</code> con le <code>Pipeline()</code>, usando al posto di un singolo transformer una pipeline di transformer. Ad esempio, se volessimo assegnare i dati mancanti usando un <code>SimpleImputer()</code> prima dello scaling, potremmo usare una pipeline da passare al transformer:</p> <pre><code>from sklearn.impute import SimpleImputer\n\nnumerical_transformer = Pipeline(\n    [('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())]\n)\n\nct = ColumnTransformer(\n    [('scaler', numerical_transformer, ['total_bill', 'tip']),\n     ('encoder', OrdinalEncoder(), ['sex', 'smoker', 'day', 'time'])],\n    remainder='passthrough'\n)\n</code></pre> <p>Nota</p> <p>Ovviamente, anche un <code>ColumnTransformer()</code> pu\u00f2 essere usato in una pipeline!</p>"},{"location":"material/04_sklearn/05_advanced/04_feat_sel/","title":"4.5.4 - Feature selection","text":"<p>Le tecniche di feature selection ci permettono di \"isolare\" le feature maggiormente significative tra quelle presenti all'interno del nostro dataset, scartando contestualmente le altre.</p> <p>Il motivo principale per il quale \u00e8 necessario operare la feature selection \u00e8 quello di ridurre la dimensionalit\u00e0 del dataset, evitando cos\u00ec l'insorgenza di fenomeni indesiderati come quelli correlati alla curse of dimensionality.</p> <p>Scikit-Learn offre diverse tecniche per la feature selection, contenute nel modulo omologo. Vediamone alcune.</p>"},{"location":"material/04_sklearn/05_advanced/04_feat_sel/#scartare-le-feature-a-bassa-varianza","title":"Scartare le feature a bassa varianza","text":"<p>Una prima, semplice tecnica per la feature selection prevede la rimozione delle feature che hanno una bassa varianza. Il motivo di questa scelta \u00e8 abbastanza facile da intuire: infatti, una feature che assume un valore costante ed uguale per ogni campione non dar\u00e0 alcun contributo ad un modello che provi a discriminare tra una coppia degli stessi.</p> <p>A tal scopo, Scikit-Learn ci offre i trasformer di classe <code>VarianceThreshold</code>, che rimuovono tutte le feature la cui varianza \u00e8 al di sotto di un valore di soglia determinato dal parametro <code>threshold</code>:</p> <pre><code>from sklearn.feature_selection import VarianceThreshold\n\nsel = VarianceThreshold(threshold=.2)\nsel.fit_transform(X)\n</code></pre> <p>In questo caso, il trasformer eliminer\u00e0 eventuali feature (colonne) la cui varianza \u00e8 inferiore alla soglia specificata.</p>"},{"location":"material/04_sklearn/05_advanced/04_feat_sel/#feature-selection-univariata","title":"Feature selection univariata","text":"<p>Le tecniche di feature selection univariata offerte da Scikit-Learn permettono di scegliere le feature da mantenere sulla base di un determinato test statistico. Un esempio di questo trasformer \u00e8 <code>SelectKBest</code> che, sulla base di un determinato test statistico, restituisce soltanto le \\(k\\) migliori feature.</p> <p>Ad esempio:</p> <pre><code>from sklearn.feature_selection import SelectKBest, f_classif\n\nselect = SelectKBest(f_classif, k=10)\nselect.fit_transform(X)\n</code></pre> <p>In poche parole, il trasformer inizializzato alla riga 3 accetta come test statistico <code>f_classif</code> (che, in pratica, restituisce l'F-value calcolato secondo un ANOVA), andando a restituire soltanto le migliori \\(10\\) feature.</p> <p>Esistono anche altri trasformer che implementano feature selection basata su test univariati: un esempio \u00e8 <code>SelectPercentile</code>, che seleziona soltanto le feature con le performance migliori in base al valore minimo di percentile scelto dall'utente, o classi come <code>SelectFpr</code>, <code>SelectFdr</code> e <code>SelectFwe</code>. Il funzionamento di ognuno di questi test \u00e8 praticamente sempre lo stesso; ci\u00f2 che cambia \u00e8 il modo in cui vengono selezionate le migliori feature.</p>"},{"location":"material/04_sklearn/05_advanced/04_feat_sel/#recursive-feature-elimination","title":"Recursive feature elimination","text":"<p>La tecnica chiamata recursive feature elimination (RFE) permette la selezione ricorsiva delle feature pi\u00f9 importanti. In pratica, dato uno stimatore che assegna un determinato peso alle sue feature (come ad esempio la regressione logistica o il percettrone), la RFE prevede che lo stesso venga addestrato sull'intero insieme delle feature. Una volta completato l'addestramento, l'importanza di ciascuna feature viene stimata secondo un determinato criterio, e quella a pi\u00f9 bassa importanza viene rimossa. A questo punto, lo stimatore viene addestrato sul dataset \"ridotto\", e la procedura viene reiterata fino a che non si raggiunge il numero desiderato di feature da mantenere.</p> <p>Scikit-Learn ci offre la classe <code>RFE</code>, che accetta come argomenti l'istanza di uno stimatore, oltre che il numero di feature da selezionare, ed il criterio con il quale valutarne l'importanza. Ad esempio:</p> <pre><code>from sklearn.feature_selection import RFE\n\nselect = RFE(\n    estimator,\n    n_features_to_select=10,\n    importance_getter='auto')\n</code></pre> <p>In particolare, l'attributo <code>n_features_to_select</code> definisce il numero di feature da mantenere, mentre <code>importance_getter</code> specifica la modalit\u00e0 con cui valutare l'importanza delle singole feature. Se impostato ad <code>auto</code>, il trasformer prover\u00e0 ad inferire automaticamente questo metodo, usando eventuali attributi <code>coef_</code> o <code>feature_importances_</code> dello stimatore; in alternativa, potr\u00e0 essere una funzione specificata dall'utente, oppure una stringa che specifica il nome dell'attributo da utilizzare.</p> <p><code>RFE</code> e <code>Pipeline</code></p> <p>Se usato per effettuare la RFE su di una pipeline, l'attributo <code>importance_getter</code> pu\u00f2 essere una stringa nella forma <code>pipeline.clf.feature_importances_</code>, con <code>pipeline</code> nome della pipeline, e <code>clf</code> nome dello step di cui recuperare l'attributo <code>feature_importances_</code>.</p>"},{"location":"material/04_sklearn/05_advanced/05_hyperpars/","title":"4.5.5 - Ottimizzazione degli iperparametri","text":"<p>Ai pi\u00f9 attenti non sar\u00e0 sfuggito che tutto ci\u00f2 che abbiamo visto finora \u00e8 inficiato da una sfida molto complesssa, legata alla natura degli algoritmi utilizzati, per i quali pu\u00f2 essere necessario valutare pi\u00f9 combinazioni di \"parametri\" da passare al modello, spesso seguendo un approccio di tipo empirico.</p> <p>Ci\u00f2 deriva dal fatto che questi \"parametri\", che nel gergo sono chiamati iperparametri, non sono appresi dal modello a partire dai dati forniti. Di conseguenza, sono state sviluppate nel corso del tempo delle tecniche di ricerca ed ottimizzazione degli iperparametri, che ci permettono di selezionarli sulla base del miglior punteggio ottenuto da una specifica combinazione in fase di cross-validazione.</p> <p>Scikit-Learn, ovviamente, ci offre delle soluzioni gi\u00e0 pronte a ci\u00f2, che andremo brevemente a discutere nel seguito.</p>"},{"location":"material/04_sklearn/05_advanced/05_hyperpars/#ricerca-ed-ottimizzazione-degli-iperparametri-in-scikit-learn","title":"Ricerca ed ottimizzazione degli iperparametri in Scikit-Learn","text":"<p>In generale, la ricerca ed ottimizzazione degli iperparametri ha bisogno di cinque fattori, ovvero:</p> <ul> <li>uno stimatore da ottimizzare;</li> <li>uno spazio dei parametri da ricercare;</li> <li>una tecnica per la scelta dei valori da ricercare;</li> <li>uno schema di cross validazione;</li> <li>una metrica.</li> </ul> <p>Scikit-Learn offre due approcci generici all'ottimizzazione degli iperparametri, ovvero la grid search e la random search. Vediamoli brevemente.</p>"},{"location":"material/04_sklearn/05_advanced/05_hyperpars/#grid-search","title":"Grid search","text":"<p>La grid search effettua una ricerca esaustiva di tutti i valori che gli iperparametri possono assumere. In altre parole, vengono specificati degli intervalli discreti per i valori assumibili dagli \\(n\\) iperparametri da ricercare, ed a partire da questi viene creata una griglia in un iperspazio ad \\(n\\) dimensioni. Tale griglia sar\u00e0 quindi esplorata in maniera esaustiva, alla ricerca del valore massimo (o minimo) associato alla metrica di scoring selezionata. Questo principio di funzionamento \u00e8 brevemente riassunto per \\(n=2\\) in figura 1.</p> <p> </p> Figura 1 - Schema della grid search. <p>Notiamo come all'interno della griglia i valori della metrica scelta (ad esempio, l'accuratezza) si vanno a disporre secondo dei picchi (indicati dalle isoipse rosse) e delle valli (indicati dalle isoipse blu). L'obiettivo sar\u00e0 quinid andare a trovare il massimo globale dell'accuratezza, sulla base dei valori degli iperparametri che abbiamo individuato. Per farlo, la grid search andr\u00e0 ad eseguire una cross-validazione del modello sui dati di training per ciascuna combinazione dei parametri possibile, fino a trovare la migliore.</p> <p>Per la grid search, Scikit-Learn offre la classe <code>GridSearchCV()</code> che, dato uno stimatore (o, equivalentemente, una pipeline di processing), restituisce i parametri migliori individuati dopo la \\(k\\)-fold cross-validation. Ad esempio, possiamo provare a valutare il funzionamento di un <code>MLPClassifier</code> con diversi algoritmi di ottimizzazione e parametri di regolarizzazione:</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\nparameters = {\n    'solver': ['lbfgs', 'adam', 'sgd'],\n    'alpha': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n}\nclf = MLPClassifier()\nsearch = GridSearchCV(clf, parameters, scoring='accuracy_score')\nsearch.fit(X_train, y_train)\n</code></pre> <p>In particolare, alla riga 8 creeremo un oggetto di tipo <code>GridSearchCV</code>, specificando lo stimatore (il nostro <code>MLPClassifier</code>), la griglia di parametri, ed infine l'accuracy come metrica da ottimizzare. </p> <p>Se volessimo usare una pipeline di processing:</p> <pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\n\npipe = Pipeline([\n    ('pca', PCA()),\n    ('clf', MLPClassifier())\n])\n\nparam_grid = {\n    'pca__n_components': [2, 3],\n    'clf__solver': ['lbfgs', 'adam', 'sgd'],\n    'clf__alpha': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]}\n\nsearch = GridSearchCV(pipe, param_grid, scoring='accuracy_score')\nsearch.fit(X_train, y_train)\n</code></pre> <p>Appare evidente come il funzionamento sia concettualmente analogo al caso in cui si usi un singolo stimatore, grazie soprattutto all'equivalenza tra pipeline e stimatori. Tuttavia, \u00e8 interessante sottolineare che, come evidenziato nelle righe 10, 11 e 12, \u00e8 necessario usare una notazione simile al name mangling per specificare la combinazione tra l'elemento della pipeline ed il valore associato agli iperparametri da ricercare.</p>"},{"location":"material/04_sklearn/05_advanced/05_hyperpars/#random-search","title":"Random search","text":"<p>Prevedibilmente, utilizzare la grid search porter\u00e0 sempre all'individuazione del picco (o della valle) all'interno delle possibili combinazioni di iperparametri selezionate. Questo \u00e8 legato al fatto che l'approccio usato dalla grid search \u00e8 a forza bruta: in altre parole, sono testate tutte le possibili combinazioni di parametri fino a che non si trova la migliore.</p> <p>Tuttavia, appare evidente come il costo computazionale legato ad una ricerca talmente estesa non sia alle volte giustificabile alla luce di miglioramenti tutto sommato marginali nel valore assunto dalla metrica di scoring. Di conseguenza, sono stati sviluppati degli approcci a minor costo, tra cui il pi\u00f9 semplice \u00e8 quello proposto dalla random search che, come suggerisce il nome stesso, effettua una selezione casuale dei valori associati agli iperparametri a partire da una determinata distribuzione. Questa scelta ha due vantaggi rispetto alla grid search:</p> <ul> <li>il primo \u00e8 che \u00e8 possibile imporre un valore massimo di ricerche, associando un budget alla procedura di ottimizzazione;</li> <li>il secondo \u00e8 che \u00e8 possibile limitare l'impatto in fase di ricerca di iperparametri che non influenzano la metrica di scoring selezionata.</li> </ul> <p>Per implementare la random search, Scikit-Learn offre la classe <code>RandomizedSearchCV()</code>, il cui funzionamento \u00e8 in linea di principio analogo alla <code>GridSearchCV()</code>, con una notevole differenza: infatti, va specificata una distribuzione di parametri, o a partire da una distribuzione presente nel package <code>scipy.stats.distribution</code>, oppure da una lista di valori discreti fornita dall'utente. Nel primo caso, sar\u00e0 scelto un metodo di campionamento scelto a partire dal parametro <code>rvs</code> associato alla distribuzione, mentre nel secondo sar\u00e0 usato un campionamento uniforme.</p>"},{"location":"material/04_sklearn/05_advanced/exercises/","title":"Esercitazione 4.5","text":"<p>Soluzioni</p> <p>Le soluzioni a questi esercizi sono mostrate nel notebook reperibile a questo indirizzo.</p>"},{"location":"material/04_sklearn/05_advanced/exercises/#esercizio-451","title":"Esercizio 4.5.1","text":"<p>Creiamo una pipeline di processing che, dati i dati relativi a conto e mance del dataset <code>tips</code>, e come label il giorno dello stesso, calcoli l'ARI a valle dell'applicazione di un'operazione di scaling prima dell'algoritmo di clustering. Successivamente, provare a modificare il numero di cluster, ricalcolando l'ARI.</p>"},{"location":"material/04_sklearn/05_advanced/exercises/#esercizio-452","title":"Esercizio 4.5.2","text":"<p>Usiamo un column transformer per filtrare e pre-elaborare i dati contenuti nel dataset Tips. In particolare:</p> <ul> <li>assegnamo i dati mancanti;</li> <li>scaliamo i dati numerici nel range \\([0, 1]\\);</li> <li>codifichiamo i dati categorici mediante un <code>OrdinalEncoder()</code>.</li> </ul> <p>Usiamo il trasformer come step di ingresso ad un multi-layer perceptron, limitandoci a chiamare il metodo <code>fit()</code>, e non valutando quindi i risultati.</p>"},{"location":"material/04_sklearn/05_advanced/exercises/#esercizio-453","title":"Esercizio 4.5.3","text":"<p>Usiamo la grid search per ottimizzare i seguenti parametri del percettrone usato nell'esempio precedente:</p> <ul> <li><code>solver</code>, tra tutti quelli disponibili;</li> <li><code>learning_rate_init</code>, tra \\(10^{-1}\\) e \\(10^{-5}\\), ad ordini di grandezza decrescenti;</li> <li><code>max_iter</code>, tra \\(1000\\) e \\(2000\\).</li> </ul> <p>Stampiamo a schermo il miglior punteggio ottenuto, oltre che i parametri del migliore stimatore.</p>"},{"location":"material/04_sklearn/05_advanced/exercises/#esercizio-454","title":"Esercizio 4.5.4","text":"<p>Integriamo nella pipeline una semplice procedura di feature selection mediante la <code>VarianceThreshold()</code>.</p>"},{"location":"material/04_sklearn/05_advanced/exercises/#esercizio-455","title":"Esercizio 4.5.5","text":"<p>Dato il dataset Diabetes:</p> <ol> <li>caricarlo in memoria;</li> <li>visualizzarne e studiarne la struttura mediante un'analisi esplorativa;</li> <li>effettuare un'analisi di correlazione;</li> <li>isolare le \\(k\\) feature pi\u00f9 importanti;</li> <li>comparare i risultati in termini di regressione usando un regressore lineare ed un albero decisionale;</li> <li>eseguire una <code>GridSearchCV</code> su un albero decisionale, cercando di migliorare le performance in termini di MAPE.</li> </ol>"},{"location":"material/04_sklearn/05_advanced/exercises/#esercizio-456","title":"Esercizio 4.5.6","text":"<p>Dato il dataset Iris:</p> <ol> <li>caricarlo in memoria;</li> <li>visualizzarne e studiarne la struttura mediante un'analisi esplorativa;</li> <li>effettuare un'analisi di correlazione;</li> <li>isolare le \\(k\\) feature pi\u00f9 importanti;</li> <li>comparare i risultati in termini di regressione usando un regressore logistico ed un albero decisionale;</li> <li>eseguire una <code>GridSearchCV</code> su un albero decisionale, cercando di migliorare le performance in termini di accuracy.</li> </ol>"},{"location":"material/04_sklearn/05_advanced/exercises/#esercizio-457","title":"Esercizio 4.5.7","text":"<p>Comparare, per mezzo di due pipeline, i risultati ottenuti effettuando il clustering mediante algoritmo KMeans sui dati di <code>make_blobs()</code> con e senza riduzione della dimensionalit\u00e0.</p>"},{"location":"material/04_sklearn/05_advanced/03_cross_val/01_intro/","title":"4.5.3.1 - Crossvalidazione","text":"<p>Abbiamo visto come sia necessario suddividere il nostro dataset in almeno due insiemi, ovvero quelli di training e di validazione, allo scopo di assicurarci che il modello, addestrato sui dati di training, sia in grado di generalizzare le sue predizioni a casi che non ha mai visto durante l'addestramento.</p> <p>Tuttavia nel tempo \u00e8 emerso come questa procedura spesso non sia sufficiente: infatti, pur scegliendo i dati di training e testing in maniera completamente casuale, rimane una possibilit\u00e0 tutt'altro che remota che esistano dei particolari meccanismi di generazione dati specifici per quel determinato sottoinsieme di dati. In altre parole, permane il rischio che l'algoritmo vada in overfitting.</p> <p>Per ovviare a questa evenienza, esiste una procedura specifica chiamata \\(k\\)-fold corss validation. In questa procedura, l'insieme di dati \u00e8 suddiviso in \\(k\\) diverse porzioni, ognuna delle quali sar\u00e0 usata come test set ad una specifica iterazione, con la restante parte usata come set di training. Dopo \\(k\\) iterazioni, i risultati saranno quindi mediati tra loro; questo far\u00e0 in modo che il contributo di ogni singolo sottoinsieme di dati di training sia meno rilevante e, di conseguenza, il modlelo complessivo maggiormente robusto rispetto a fenomeni di overfitting. Graficamente, possiamo descrivere la \\(k\\)-fold cross validation con la figura 1, presa direttamente dalla documentazione di Scikit-Learn.</p> <p> </p> Figura 1 - Principio di funzionamento della k-fold cross validation <p>Strategie di cross validation</p> <p>Esistono diverse strategie di cross-validation, le quali differiscono principalmente nel modo in cui sono assegnati i valori al test set.</p>"},{"location":"material/04_sklearn/05_advanced/03_cross_val/01_intro/#cross-validation-in-scikit-learn","title":"Cross validation in Scikit-Learn","text":""},{"location":"material/04_sklearn/05_advanced/03_cross_val/01_intro/#la-funzione-cross_val_score","title":"La funzione <code>cross_val_score()</code>","text":"<p>Il modo pi\u00f9 semplice di effettuare la cross-validazione in Scikit-Learn \u00e8 usare la funzione <code>cross_val_score()</code>, che accetta come parametri uno stimatore, i dati su cui si vuole che operi, e la metrica da analizzare. Ad esempio:</p> <pre><code>from sklearn.model_selection import cross_val_score\n\nkmeans = KMeans()\nscores = cross_val_score(kmeans, X, y, scoring='adjusted_rand_score', cv=10)\n</code></pre> <p>In questo caso, alla riga 4 calcoleremo l'adjusted rand index per un K-Means sui dati <code>(X, y)</code> con \\(k\\)=10.</p> <p>Da notare che <code>scores</code> avr\u00e0 al suo interno la media (<code>scores.mean()</code>) e la varianza (<code>scores.std()</code>) assunta dallo score durante le iterazioni di addestramento.</p> <p>Da notare che <code>cross_val_score()</code> pu\u00f2 anche essere usata con una <code>Pipeline</code>:</p> <pre><code>clf = Pipeline(\n    [('scaler', StandardScaler()),\n    ('kmeans', KMeans())])\ncross_val_score(clf, X, y)\n</code></pre>"},{"location":"material/04_sklearn/05_advanced/03_cross_val/01_intro/#la-funzione-cross_validate","title":"La funzione <code>cross_validate()</code>","text":"<p>Una funzione leggermente pi\u00f9 complessa rispetto a <code>cross_val_score()</code> \u00e8 <code>cross_validate()</code>, che differisce dalla prima per due aspetti:</p> <ul> <li>permette di valutare diverse metriche contemporaneamente;</li> <li>restituisce un dizionario che contiene, oltre agli score sul test set, anche i tempi impiegati per ottenere lo score e per addestrare l'algoritmo. Ad esempio:</li> </ul> <pre><code>from sklearn.model_selection import cross_validate\n\nkmeans = KMeans()\ncross_validate(kmeans, X, y, scoring=[\n    'adjusted_rand_score',\n    'adjusted_mutual_info_score'])\n</code></pre>"},{"location":"material/04_sklearn/05_advanced/03_cross_val/01_intro/#la-funzione-cross_val_predict","title":"La funzione <code>cross_val_predict()</code>","text":"<p>La funzione <code>cross_val_predict()</code> viene utilizzata per ottenere i valori predetti per ciascun elemento dell'input quando l'elemento stesso faceva parte del test set.</p> <pre><code>from sklearn.model_selection import cross_val_predict\n\ncross_val_predict(kmeans, X, y)\n</code></pre> <p>L'utilizzo della <code>cross_val_predict()</code> \u00e8 consigliato in due casi, ovvero:</p> <ul> <li>quando vogliamo visualizzare le predizioni ottenute da diversi modelli;</li> <li>quando le predizioni di uno stimatore supervisionato sono usate per addestrare un altro stimatore in un modello ensemble (model blending).</li> </ul> <p>Utilizzare la <code>cross_val_predict()</code></p> <p>A causa della sua natura, la <code>cross_val_predict()</code> pu\u00f2 essere usata soltanto con strategie di cross validazione che assegnano tutti gli elementi del dataset ad un sottoinsieme di test esattamente una volta.</p>"},{"location":"material/04_sklearn/05_advanced/03_cross_val/02_strategies/","title":"4.5.3.2 - Strategie di cross-validazione","text":"<p>Abbiamo accennato al fatto che esistono diverse strategie di cross-validazione. In tal senso, Scikit-Learn offre delle utility per ciascuna di queste strategie; vediamole brevemente.</p>"},{"location":"material/04_sklearn/05_advanced/03_cross_val/02_strategies/#k-fold","title":"K-Fold","text":"<p>La K-Fold cross-validation suddivide i campioni in \\(k\\) gruppi di dimensione uguale (laddove possibile). Il modello viene quindi addestrato su \\(k-1\\) gruppi, ed il restante \u00e8 usato come test. Scikit-Learn usa la classe <code>KFold()</code> per questa strategia di cross-validazione:</p> <pre><code>from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=2)\nkf.split(X)\n</code></pre>"},{"location":"material/04_sklearn/05_advanced/03_cross_val/02_strategies/#repeated-k-fold","title":"Repeated K-Fold","text":"<p>Come suggerisce il nome stesso, questa strategia ripete la K-Fold classica per un certo numero di volte, con diverse suddivisioni ad ogni ripetizione. In tal senso, Scikit-Learn offre la classe <code>RepeatedKFold</code>:</p> <pre><code>from sklearn.model_selection import RepeatedKFold\n\nrkf = RepeatedKFold(n_splits=2, n_repeats=2)\nrkf.split(X)\n</code></pre>"},{"location":"material/04_sklearn/05_advanced/03_cross_val/02_strategies/#stratified-k-fold","title":"Stratified K-Fold","text":"<p>La K-Fold classica opera in maniera indipendente dalle classi: in pratica, non tiene conto della distribuzione delle label nel dataset iniziale. La stratified K-Fold, invece, restituisce degli insiemi nei quali \u00e8 contenuta approssimativamente la stessa percentuale di campioni di ogni classe target del set completo. Anche in questo caso, viene offerta una funzione apposita da Scikit-Learn, chiamata <code>StratifiedKFold</code>:</p> <pre><code>from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=2)\nskf.split(X)\n</code></pre>"},{"location":"material/04_sklearn/05_advanced/03_cross_val/03_permutations/","title":"4.5.3.3 - Permutation test score","text":"<p>La cross-validazione non \u00e8 l'unico modo di valutare le performance di un classificatore. In particolare, esiste anche il permutation test score, implementato in Scikit-Learn mediante la funzione <code>permutation_test_score</code>, che restituisce un valore rappresentante il p-value legato alla probabilit\u00e0 che le performance ottenute dal classificatore siano legate al caso.</p> <p>In particolare, l'ipotesi nulla del test di permutazione \u00e8 che il classificatore non sia in grado di sfruttare alcuna dipendenza statistica tra feature e label per ottenere una predizione corretta sui dati di validazione. Il test viene svolto calcolando \\(n\\) diverse permutazioni casuali dei dati, rimuovendo le dipendenze tra le feature e le label; l'output \u00e8 la frazione di permutazioni per la quale il punteggio di cross-validazione medio ottenuto dal modello \u00e8 maggiore del punteggio ottenuto dallo stesso usando i dati originari. In tal senso, per ottenere risultati significativi, spesso si sceglie \\(n=100\\), con \\(k\\) compreso tra \\(3\\) e \\(10\\).</p> <p>Un valore basso del \\(p\\)-value fornisce la prova statistica che il dataset contiene una vera dipendenza tra le feature e le label, e che il classificatore \u00e8 stato in grado di utilizzarla per ottenere buoni risultati. Un valore alto di \\(p\\)-value, invece, pu\u00f2 essere causato da una mancanza di dipendenza tra le feature e le label, o perch\u00e9 il classificatore non \u00e8 stato in grado di usare le dipendenze presenti nei dati.</p>"},{"location":"material/04_sklearn/exercises/exercises/","title":"Esercitazione 4 - Scikit Learn","text":""},{"location":"material/04_sklearn/exercises/exercises/#esercizio-1","title":"Esercizio 1","text":"<p>Usiamo Scikit Learn per una prima analisi del dataset Tips. In particolare, proviamo a:</p> <ul> <li>classificare i clienti in base al fatto che siano fumatori o meno;</li> <li>prevedere il conto di un tavolo in base alle sue caratteristiche.</li> </ul>"},{"location":"material/05_dl/01_nn/lecture/","title":"5.1 - Anatomia di una rete neurale","text":"<p>Le reti neurali sono ormai sulla bocca di tutti: chiunque le usa per risolvere con (apparente) successo ogni tipo di problema. Tuttavia, prima di usarle, \u00e8 opportuno comprendere effettivamente a cosa servano.</p> <p>Per farlo, non possiamo non partire dall'introdurre il tipo di problemi per risolvere i quali le reti neurali sono state progettate, ovvero i problemi non lineari.</p>"},{"location":"material/05_dl/01_nn/lecture/#problemi-non-lineari","title":"Problemi non lineari","text":"<p>Diamo uno sguardo al dataset rappresentato in figura 1.</p> <p> </p> Figura 1 - Dataset non lineare <p>Un dataset di questo tipo \u00e8, evidentemente, non lineare. In altre parole, questo significa che non esistono algoritmi in grado di trovare una funzione lineare (ovvero, una retta) che separi i punti appartenenti alle due classi. In termini analitici, non esiste un modello nella forma:</p> \\[ y = ax_1 + bx_2 + c \\] <p>che permetta di determinare \\(y\\) a partire dai valori \\(x_1\\) ed \\(x_2\\).</p> <p>Numero di feature</p> <p>Ovviamente, abbiamo considerato due feature per semplicit\u00e0 rappresentativa. Nel caso il numero di feature fosse pi\u00f9 alto, il modello lineare dovrebbe considerare un numero pi\u00f9 elevato di variabili indipendenti.</p> <p>Potremmo pensare a questo punto di utilizzare un'approssimazione lineare a tratti che, nel caso precedente, sarebbe di grande aiuto nella risoluzione del problema. Tuttavia, consideriamo per un attimo la situazione illustrata in figura 2.</p> <p> </p> Figura 2 - Dataset estremamente non lineare <p>In quest'ultimo caso \u00e8 evidente come i dati non siano in alcun modo linearmente separabili. E' quindi necessario utilizzare un approccio adeguato.</p>"},{"location":"material/05_dl/01_nn/lecture/#reti-neurali-e-problemi-non-lineari","title":"Reti neurali e problemi non lineari","text":"<p>Per capire come le reti neurali ci aiutino a risolvere un problema non lineare, partiamo da un semplice sommatore pesato, mostrato in figura 3.</p> <p> </p> Figura 3 - Un sommatore lineare <p>Il sommatore lineare ha tre ingressi, che vengono combinati secondo una qualche terna di pesi \\((w_1, w_2, w_3)\\) per ottenere un singolo output. In pratica, l'uscita sar\u00e0 data dalla seguetne relazione:</p> \\[ y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b \\] <p>Da notare come sia stato aggiunto un contributo \\(b\\) legato ad un termine opzionale di bias. Proviamo adesso ad aggiungere uno strato intermedio (o nascosto) tra l'ingresso e l'uscita, ottenendo lo schema mostrato in figura 2.</p> <p> </p> Figura 4 - Un sommatore con uno strato nascosto <p>Lo strato nascosto (hidden layer) rappresenta una serie di valori intermedi considerati dal sommatore nel calcolo dell'output. In pratica:</p> <ul> <li>ogni nodo dello strato nascosto sar\u00e0 dato da una somma pesata dei nodi dell'input;</li> <li>l'output sar\u00e0 dato da una somma pesata dei nodi appartenenti allo strato nascosto.</li> </ul> <p>E' importante sottolineare come ogni \"freccia\", o per meglio dire connessione, abbia uno specifico peso associato.</p> <p>Nonostante queste modifiche, il modello rimane comunque lineare: in altre parole, potremo aggiungere un numero arbitrario di nodi e strati nascosti, ma l'uscita sar\u00e0 sempre una somma pesata (pi\u00f9 o meno complessa) dell'ingresso. Ci\u00f2 per\u00f2 cambia nel momento in cui andiamo a considerare una o pi\u00f9 funzioni di attivazione.</p>"},{"location":"material/05_dl/01_nn/lecture/#funzioni-di-attivazione","title":"Funzioni di attivazione","text":"<p>La modellazione di un problema non lineare prevede l'introduzione di non linearit\u00e0 all'interno del modello: per farlo, dovremo inserire delle funzioni (per l'appunto) non lineari, chiamate funzioni di attivazione. Uno schema \u00e8 mostrato in figura 5.</p> <p> </p> Figura 5 - Rete neurale con funzioni di attivazione <p>Ovviamente, con un maggior numero di strati nascosti l'impatto delle non-linearit\u00e0 diventa maggiore. In questo modo, saremo in grado di inferire delle relazioni anche molto complesse tra gli input e gli output.</p> <p>Le funzioni di attivazione pi\u00f9 utilizzate in passato erano di tipo sigmoidale (simili, per intenderci, alla funzione che abbiamo visto in uscita alla regressione logistica). Attualmente, le funzioni pi\u00f9 usate sono le rectified linear unit, o ReLU, che hanno risultati comparabili in termini di accuratezza del modello alla sigmoidale, ma risultano essere significativamente meno complesse dal punto di vista computazionale.</p> <p>Le ReLU sono espresse dalla seguente funzione:</p> \\[ y = max(0, x) \\] <p>che graficamente si traduce in una forma espressa come:</p> <p> </p> Figura 6 - Schema della funzione di attivazione ReLU <p>ReLU nella pratica</p> <p>In pratica, una ReLU \"fa passare\" soltanto i valori positivi, portando a zero tutti quelli negativi.</p> <p>Riassumendo:</p> <ul> <li>una rete neurale \u00e8 data da un insieme di nodi, o neuroni, organizzati in uno o pi\u00f9 strati;</li> <li>ogni neurone \u00e8 connesso a quelli dello strato successivo mediante dei pesi, che rappresentano la \"forza\" della connessione;</li> <li>esiste una funzione di attivazione che trasforma l'uscita di ogni neurone verso lo strato successivo inserendo delle non linearit\u00e0.</li> </ul>"},{"location":"material/05_dl/01_nn/lecture/#formulazione-matematica","title":"Formulazione matematica","text":""},{"location":"material/05_dl/01_nn/lecture/#funzionamento-di-una-rete-neurale","title":"Funzionamento di una rete neurale","text":"<p>Dato un insieme di coppie campione/label nella forma \\((x_1, y_1), (x_2, y_2), \\ldots (x_n, y_n)\\), con \\(x_i \\in \\mathbb{R}^n, y_i \\in {0, 1}\\), una rete neurale con un singolo neurone in un singolo strato nascosto apprende la funzione \\(f(x)=W_2 g(W_1^T x + b_1)+b_2\\), dove \\(W_1 \\in \\mathbb{R}^m\\) e \\(W_2, b_1, b_2 \\in \\mathbb{R}\\) sono i parametri del modello.</p> <p>In particolare, \\(W_1\\) e \\(W_2\\) rappresentano i pesi associati ai neuroni nel layer di input ed in quello nascosto, mentre \\(b_1\\) e \\(b_2\\) rappresentano i bias aggiunti dallo strato di ingresso e quello nascosto; \\(g(): \\mathbb{R} \\rightarrow \\mathbb{R}\\) \u00e8 la funzione di attivazione.</p> <p>Per un problema di classificazione binaria, viene utilizzata nell'ultimo strato la funzione logistica:</p> \\[ g(z) = \\frac{1}{(1+e^{-z})} \\] <p>In questo modo, si ottiene un valore d'uscita compresto tra \\(0\\) ed \\(1\\). Per discriminare tra le due possibili classi, viene usata una soglia \\(\\rho=0.5\\).</p> <p>Nel caso di un problema multiclasse, \\(f(x)\\) sar\u00e0 un vettore ad \\(n\\) dimensioni, con \\(n\\) numero di classi. In questo caso, non viene utilizzata la funzione logistica per l'attivazione, bens\u00ec una softmax:</p> \\[ softmax(z)_i = \\frac{e^{z_i}}{\\sum_{l=1}^k e^{z_l}} \\] <p>dove \\(z_i\\) rappresenta l'\\(i\\)-mo elemento dell'ingresso alla softmax, che corrisponde quindi alla classe \\(i\\), e \\(K\\) \u00e8 il numero totale di classi. Il risultato sar\u00e0 un vettore contenente le probabilit\u00e0 che il campione \\(x\\) appartenga a ciascuna classe; conseguentemente, l'output sar\u00e0 la classe con il valore di probabilit\u00e0 pi\u00f9 alto.</p> <p>Nel caso si affronti un problema di regressione, l'output \u00e8 proprio \\(f(x)\\), per cui la funzione di attivazione diventa semplicemente la funzione identit\u00e0.</p>"},{"location":"material/05_dl/01_nn/lecture/#funzione-di-costo","title":"Funzione di costo","text":"<p>Cos\u00ec come tutti i classificatori e regressori, anche una rete neurale valuta le proprie performance mediante una funzione di costo. In particolare, nel caso di classificazione, la funzione di costo \u00e8 data dalla cross-entropia, che nel caso binario \u00e8 data da:</p> \\[ L(\\hat{y}, y, W) = -\\frac{1}{n}\\sum_{i=0}^n(y_i \\ln \\hat{y}_i + (1-y_i) \\ln (1-\\hat{y}_i)) + \\frac{\\alpha}{2n} \\|W\\|_2^2 \\] <p>dove \\(\\|W\\|_2^2\\) \u00e8 un termine di regolarizzazione L2, ed \\(\\alpha\\) parametro che controlla il peso di tale termine.</p> <p>Nel caso della regressione, viene utilizzato l'errore quadratico medio:</p> \\[ L(\\hat{y}, y, W) = \\frac{1}{2n} \\sum_{i=0}^n \\|\\hat{y}_i - y_i \\|_2^2 + \\frac{\\alpha}{2n}\\|W\\|_2^2 \\]"},{"location":"material/05_dl/01_nn/lecture/#aggiornamento-dei-pesi","title":"Aggiornamento dei pesi","text":"<p>Le reti neurali partono con pesi assegnati in maniera casuale, ed hanno l'obiettivo di minimizzare la funzione di costo aggiornando i valori assegnati a tali pesi. Di conseguenza, dopo che il valore della funzione di costo \u00e8 stato calcolato, viene effettuato il cosiddetto backward pass, allo scopo di aggiornare i valori di pesi e bias in modo da ridurre il valore complessivo della funzione di costo.</p> <p>Dato che gli algoritmi utilizzati sono tutti basati su varianti a discesa di gradiente, sar\u00e0 il gradiente \\(\\nabla L_W\\) ad essere calcolato e sottratto a \\(W\\). In pratica:</p> \\[ W^{i+1}=W^i - \\epsilon \\nabla L_W^i \\] <p>dove \\(i\\) \u00e8 l'epoca di training ed \\(\\epsilon\\) \u00e8 un parametro chiamato learning rate.</p> <p>L'addestramento si interrompe per un certo valore di \\(i\\) o, nei casi pi\u00f9 sofisticati, quando il miglioramento in termini di funzione di costo scende al di sotto di una certa soglia.</p>"},{"location":"material/05_dl/01_nn/lecture/#tipi-di-reti-neurale","title":"Tipi di reti neurale","text":"<p>Esistono molti tipi di rete neurale. Quello che abbiamo descritto in questa lezione \u00e8 il pi\u00f9 generico, e viene comunemente indicato come rete neurale feed-forward. Ci\u00f2 \u00e8 legato al fatto che in questo tipo di rete l'input \"fluisce\" verso l'output, a meno del backward pass legato all'aggiornamento dei parametri.</p> <p>Nelle prossime lezioni vedremo altri tre tipi di rete, ovvero gli autoencoder, le recurrent neural network, e le convolutional neural network, ognuna delle quali ha uno specifico dominio applicativo.</p>"},{"location":"material/05_dl/02_autoencoder/01_intro/","title":"5.2.1 - Introduzione agli autoencoder","text":"<p>Un autoencoder \u00e8 un tipo di rete neurale usato per la codifica non supervisionata di un insieme di dati in ingresso.</p> <p>In pratica, un autoencoder ha l'obiettivo di apprendere una rappresentazione a bassa dimensionalit\u00e0 di un dato ad alta dimensionalit\u00e0 effettuando, nei fatti, una procedura di riduzione della dimensionalit\u00e0 non lineare detta anche encoding. Per farlo, l'autoencoder viene addestrato a ricostruire l'input originario a partire dalla rappresentazione a bassa dimensionalit\u00e0, minimizzando contestualmente l'errore di ricostruzione.</p>"},{"location":"material/05_dl/02_autoencoder/01_intro/#architettura-di-un-autoencoder","title":"Architettura di un autoencoder","text":"<p>L'architettura di un generico autoencoder \u00e8 mostrata in figura 1.</p> <p> </p> Figura 1 - Architettura di un generico autoencoder <p>E' quindi evidente come vi siano tre diverse parti, chiamate rispettivamente encoder, bottleneck e decoder. Vediamole pi\u00f9 nel dettaglio.</p>"},{"location":"material/05_dl/02_autoencoder/01_intro/#encoder","title":"Encoder","text":"<p>La prima parte di un autoencoder \u00e8 chiamata encoder, ricorda un'architettura standard di rete neurale. In particolare, l'encoder \u00e8 costituito da un insieme di neuroni il cui compito \u00e8 quello di estrarre una rappresentazione compatta del dato in ingresso \\(x\\).</p> <p>Layer di un encoder</p> <p>Sottolineamo come l'encoder non sia necessariamente composto da un solo layer: molto spesso, infatti, la parte di codifica di un autoencoder \u00e8 composta da un numero pi\u00f9 o meno elevato di strati.</p>"},{"location":"material/05_dl/02_autoencoder/01_intro/#bottleneck","title":"Bottleneck","text":"<p>Il bottleneck si colloca a valle dell'encoder, ed \u00e8 la parte dell'autoencoder che contiene la rappresentazione codificata in maniera non lineare dell'input nel cosiddetto spazio latente delle feature.</p> <p>Perch\u00e9 non lineare?</p> <p>Il motivo per il quale abbiamo sottolineato la non linearit\u00e0 della rappresentazione contenuta nel bottleneck sar\u00e0 pi\u00f9 chiaro nel seguito; tuttavia, per adesso, ci basti pensare che, dato che l'encoder \u00e8 una vera e propria rete neurale feedforward, conterr\u00e0 al suo interno delle funzioni di attivazione intrinsecamente non lineari.</p> <p>La dimensione del bottleneck influenza la possibilit\u00e0 che la rete vada o meno in overfitting. In pratica, un bottleneck di piccole dimensioni riduce il rischio di overfitting, in quanto la rete pu\u00f2 memorizzare un minor numero di informazioni riguardo uno specifico input; tuttavia, ci\u00f2 comporta il rishcio che informazioni importanti vengano comunque omesse in fase di decodifica.</p>"},{"location":"material/05_dl/02_autoencoder/01_intro/#decoder","title":"Decoder","text":"<p>Il bottleneck \u00e8 seguito dal decoder, che consite in una serie di layer e neuroni speculare all'encoder, il cui compito \u00e8 quello di riportare la rappresentazione compressa nello spazio latente nello spazio originale delle feature. Nel caso standard, l'uscita del decoder \\(\\hat{x}\\) deve essere identica all'input a meno di un contributo stocastico legato all'errore di ricostruzione.</p>"},{"location":"material/05_dl/02_autoencoder/01_intro/#addestramento-di-un-autoencoder","title":"Addestramento di un autoencoder","text":"<p>L'addestramento di un autoencoder avviene considerando quattro diversi iperparametri:</p> <ol> <li>dimensione del bottleneck: questo iperparametro decide l'entit\u00e0 della compressione dei dati;</li> <li>numero di layer: una profondit\u00e0 maggiore per encoder e decoder aumenta la complessit\u00e0 del modello, che quindi pu\u00f2 meglio caratterizzare relazioni complesse e non lineari; tuttavia, come per tutte le reti neurali, a profondit\u00e0 maggiori corrisponde un costo computazionale pi\u00f9 elevato;</li> <li>numero di nodi per layer: anche il numero di layer influisce sia sulla complessit\u00e0 sia sulla capacit\u00e0 del modello di caratterizzare relazioni complesse. Di solito, il numero di neuroni per ciascun layer si dimezza ad ogni strato successivo nell'encoder, raddoppiando contestualmente nel decoder;</li> <li>reconstruction loss: la funzione di costo usata per addestrare l'autoencoder dipende da input ed output, e va ad influenzare la tipologia di autoencoder costruito.</li> </ol> <p>Con particolare riferimento all'ultimo punto, vedremo quattro diversi tipi di autoencoder, ovvero undercomplete autoencoder, sparse autoencoders, denoising autoencoder, e variational autoencoder.</p>"},{"location":"material/05_dl/02_autoencoder/02_undercomplete/","title":"5.2.2 Undercomplete autoencoder","text":"<p>Gli undercomplete autoencoder (UAE) sono i pi\u00f9 semplici tra gli autoencoder. In particolare, gli UAE accettano provano a dare in uscita un output quanto pi\u00f9 fedele possibile all'input preso in ingresso; in altre parole, dato un certo input \\(x\\), creer\u00e0 una rappresentazione nello spazio latente a partire dalla quale ricostruir\u00e0 un output \\(\\hat{x}\\) minimizzando l'errore di ricostruzione \\(\\hat{x}-x\\).</p> <p>UAE ed approccio non supervisionato</p> <p>Per come \u00e8 stato descritto, un UAE \u00e8 un tipo di rete neurale non supervisionata, visto e considerato che non richiede la presenza di una label per ricostruire l'input.</p> <p>L'uso primario di un UAE \u00e8 quindi generare una rappresentazione compressa \\(h\\) dell'input \\(x\\) nello spazio latente definito dal bottleneck. In quest'ottica, tale operazione pu\u00f2 essere interpretata come una dimensionality reduction non lineare.</p> <p>UAE e dimensionality reduction</p> <p>Ricordiamo che le tecniche di dimensionality reduction prevedono che i dati originari, definiti in uno spazio ad alta dimensionalit\u00e0, siano riproiettati su un iperpiano a dimensionalit\u00e0 inferiore, cercando di conservare quanta pi\u00f9 informazione possibile sullo spazio originario. In tal senso, la tecnica che ci viene subito in mente \u00e8 la PCA, che tuttavia sfrutta proiezioni di tipo esclusivamente lineare. Di contro, un UAE \u00e8 in grado di apprendere delle relazioni non lineari tra le componenti dell'ingresso proprio grazie alla presenza delle funzioni di attivazione, il che gli permette di lavorare su un insieme pi\u00f9 ampio di dati. Di fatto, qualora rimuovessimo tutte le non linearit\u00e0 introdotte dalle funzioni di attivazione di un UAE, avremmo un'elaborazione puramente lineare.</p> <p>La funzione di costo usata per addestrare un UAE \u00e8 chiamata reconstruction loss alla luce del fatto che valuta quanto l'output del decoder \\(\\hat{x}\\) \u00e8 simile al ground truth \\(x\\). Per farlo, possiamo usare la norma \\(L_1\\):</p> \\[ L_{UAE} = |x-\\hat{x}| \\] <p>Regolarizzazione</p> <p>I pi\u00f9 attenti avranno notato come la funzione di costo non abbia un termine di regolarizzazione esplicito. Di conseguenza, l'unico modo per assicurarsi che l'autoencoder non si limiti a memorizzare i dati in ingresso \u00e8 quello di impostare adeguatamente gli iperparametri del modello, quali le dimensioni del bottleneck e dell'encoder.</p>"},{"location":"material/05_dl/02_autoencoder/03_sparse/","title":"5.2.3 - Sparse autoencoder","text":"<p>Gli sparse autoencoder (SAE) sono simili agli UAE con un'unica, importantissiam differenza: infatti, gli SAE introducono una regolarizzazione mediante un termine di sparsity penalty \\(\\Omega(h)\\), per cui la loss assume la forma:</p> \\[ L_{SAE} = L_{UAE} + \\Omega(h) \\] <p>In termini pratici, il termine di regolarizzazione introduce una penalit\u00e0 sul numero di nodi attivi in ogni layer nascosto dell'encoder e del decoder, facendo in modo che lo SAE eviti di attivare pi\u00f9 neuroni di quelli strettamente necessari.</p> <p>Uno SAE \u00e8 quindi regolarizzato per essere sparso e rispondere alle feature statistiche specifiche per il dataset su cui sono addestrati, invece di agire come una sorta di funzione di identit\u00e0. In altre parole, la sparse penalty fa in modo che la rete attivi esclusivamente i nodi dedicati alle feature rilevanti per i dati utilizzati per il training.</p>"},{"location":"material/05_dl/02_autoencoder/04_contractive/","title":"04 contractive","text":""},{"location":"material/05_dl/02_autoencoder/04_contractive/#contractive-autoencoders","title":"Contractive autoencoders","text":"<p>I contractive autoencoder introducono un termine di regolarizzazione sulla rappresentazione intermedia \\(h=f(x)\\), facendo in modo che le derivate di \\(f\\) siano quanto pi\u00f9 piccole possibile:</p> \\[ \\Omega(h) = \\lambda \\| \\frac{\\partial f(x)}{\\partial x} \\|_F^2 \\] <p>La penalit\u00e0 \\(\\Omega(h)\\) \u00e8 il quadrato della nofrma di Frobenius della matrice Jacobiana delle derivate parziali associate alla funzione di encoding.</p> <p>Il nome contractive deriva dal modo in cui questo tipo di autoencoder modifica lo spazio delle feature. Nello specifico, i contractive autoencoder sono addestrati per resitstere a perturbazioni sugli input, incoraggiando a mappare un intorno di punti di input in un intorno di putni di output pi\u00f9 piccolo. </p>"},{"location":"material/05_dl/02_autoencoder/04_denoising/","title":"5.2.4 - Denoising autoencoders","text":"<p>I denoising autoencoders (DAE) sono utilizzati per rimuovere il rumore che affligge i dati originari.</p> <p>Per farlo, viene utilizzata una funzione di regolarizzazione \\(\\Omega\\) alla funzione di costo. In particolare, la funzione di costo assume una forma del tipo:</p> \\[ L_{DAE}(x, g(f(\\tilde{x}))) \\] <p>dove \\(\\tilde{x}\\) \u00e8 una versione di \\(x\\) corrotta da una qualche forma di rumore. I DAE provano quindi a rimuovere questo rumore, invece di limitarsi a riprodurre l'ingresso.</p>"},{"location":"material/05_dl/02_autoencoder/05_denoising/","title":"5.2.4 - Denoising autoencoders","text":"<p>Piuttosto che aggiungere una penalit\u00e0 \\(\\Omega\\) alla funzione di costo, possiamo ottenere un autoencoder che apprende qualcosa di utile cambianod il termine dell'errore di ricostruzione nella funzione di costo.</p> <p>Abbiamo visto che gli autoencoder minimizzano una funzione nella forma:</p> \\[ L(x, g(f(x))) \\] <p>dove \\(L\\) \u00e8 una funzione di costo che penalizza \\(g(f(x))\\) se non simile ad \\(x\\), come ad esempio la norma \\(L^2\\) della loro differenza. Questo fa in modo che la funzione composta \\(g \\circ f\\) apprenda una funzione identit\u00e0.</p> <p>In un denoising autoencoder viene invece minimizzata la funzione:</p> \\[ L(x, g(f(\\hat{x}))) \\] <p>dove \\(\\hat{x}\\) \u00e8 una copia di \\(x\\) che \u00e8 stata corrotta da una qualche forma di rumore. I denoising autoencoder devono quindi rimuovere questo rumore invece di limitarsi a copiare l'ingresso.</p>"},{"location":"material/05_dl/02_autoencoder/05_variational/","title":"5.2.5 - Variational autoencoder","text":"<p>Abbiamo visto come gli autoencoder classici come gli UAE comprimono l'input \\(x\\) in una rappresentazione compressa \\(h\\) a partire dal quale \u00e8 possibile ricostruire \\(x\\) stesso. Normalmente, la rappresentazione compressa \\(h\\) \u00e8 di tipo deterministico, e permette una ricostruzione simile a quella mostrata in figura 1.</p> <p> </p> Figura 1 - Rappresentazione deterministica appresa da un UAE <p>Una rappresentazione \\(h\\) di tipo deterministico permette di ricostruire in maniera adeguata l'input \\(x\\). Tuttavia, potrebbe essere interessante valutare come si distribuiscano i parametri caratterizzanti \\(h\\) all'interno dello spazio della rappresentazione latente: in questo modo, potremmo ottenere delle distribuzioni di probabilit\u00e0 da utilizzare per generare dei dati simili ad \\(x\\). I variational autoencoder (VAE) si occupano proprio di far questo, generando gli attributi latenti in un modo simile a quello mostrato in figura 2:</p> <p> </p> Figura 2 - Rappresentazione probabilistica appresa da un VAE <p>Il motivo per cui gli attributi latenti possono essere espressi sotto forma di distribuzioni di probabilit\u00e0 pu\u00f2 essere spiegato come segue. Proviamo ad esempio ad identificare le caratteristiche del vettore latente \\(h\\) che ricostruisce l'output a partire da un certo input \\(x\\), modellando la distribuzione \\(x[p(z|x)]\\).</p> <p>La stima esatta di questa distribuzione pu\u00f2 essere molto costosa dal punto di vista computazionale; in tal senso, un'opzione molto pi\u00f9 semplice \u00e8 costruire un modello parametrizzato \\(q\\) minimizzando una metrica di distanza, come la divergenza di Kullback-Leibler, tra la distribuzione \"vera\" e quella parametrizzata. Grazie al modello parametrizzato \\(q\\) potremo inferire gli attributi latenti ed usarli per ricostruire il dato iniziale. Per quello che riguarda i parametri da utilizzare, questi variano a seconda delle ipotesi fatte su \\(h\\): infatti, nel caso semplicistico di una distribuzione normale, potremo utilizzare la media e la varianza.</p> <p>La funzione di costo di un VAE prevede quindi due contributi. Il primo \u00e8 quello legato alla reconstruction loss che, focalizzandosi soltanto sulla minimizzazione dell'errore di ricostruzione, fa in modo che la rete provi ad apprendere delle distribuzioni estremamente centrate sui parametri che servono a soddisfare questa condizione. L'altro contributo \u00e8 dato dalla divergenza di Kullback-Leibler, che viene utilizzata per fare in modo che la distribuzione dei parametri sia il quanto pi\u00f9 vicina possibile a quella considerata come reale. Di conseguenza, la funzione di costo complessiva pu\u00f2 essere espressa come:</p> \\[ L_{VAE} = |x - \\hat{x}|+\\beta \\sum_i KL(q_j(z|x)\\|N(0,1)) \\] <p>dove \\(N\\) indica la distribuzione normale a media nulla e varianza unitaria, e \\(\\beta\\) \u00e8 il fattore di peso associato al contributo della divergenza di Kullback-Leibler.</p>"},{"location":"material/05_dl/02_autoencoder/06_variational/","title":"5.2.5 - Variational autoencoder","text":"<p>Gli standard ed i variational autoencoder apprendono a rappresentare l'input in una forma compressa chiamata latent space o bottleneck. Quindi, lo spazio latente formato dopo l'addestramento del modello non \u00e8 necessariamente continuo e, in effetti, potrebbe non essere facile da interpolare.</p> <p>Ad esempio, questo \u00e8 quello che un variational autoencoder apprenderebbe dall'input:</p> <p>FOTO: IMMAGINE A SINISTRA CON ATTRIBUTI SIMLE; SKIN TONE; GENDER, BEARD, GLASSES, HAIR COLOR</p> <p>Anche se questi attributi spiegano l'immagine e possoono essere usati per ricostruire l'immagine dallo spazio latente, non permettono agli attributi latenti di essere espressi in maniera probabilistica</p> <p>I vairational autonecoder si occupano di spe ific argomenti, ed esprimono i loro attributi latenti come una distribuzione di probabilit\u00e0, il che porta alla fomrazione di uno spazio continuo latente che pu\u00f2 essere facilmente campionato ed interpolato.</p> <p>Quando viene dato lo stesso input, un variational autoencoder costruir\u00e0 gli attributi latenti nel seguente modo:</p> <p>Gli attributi latenti saranno quindi campionati dalla distribuzione latente cos\u00ec formata ed inviati al decoder, ricostruendo l'input.</p> <p>il motivo per cui gli attributi latenti possono essere espressi come una distribuzione di probabilit\u00e0 pu\u00f2 essere facilmente compreso mediante espressioni di tipo statistico.</p> <p>IOn tal senso, proviamo ad identificare le caratteristiche del vettore latente \\(z\\) che ricostruisce l'output dato un certo input.- In pratica, vogliamo studiare le carateristriche del vettore latente dato un certo output \\(x[p(z|x)]\\).</p> <p>Anche se una stima esatta della distribuzione pu\u00f2 essere molto costosa dal punto di vista analitico, un'opzione molto pi\u00f9 semplice da seguire \u00e8 quella di costruire un modello parametrizzato che psosa stimare la distribuzione per noi. Possiamo farlo minimizzando la divergenza KL tra la distribuzione originale e la nostra distribuzione parametrizzata.</p> <p>Esprimendo la distribuzione parmaetrizzata come \\(q\\), possiamo inferire gli attributi latenti usati nella ricostruzione dell'immagine.</p> <p>Assumendo che la distribuzione a priori \\(z\\) sia un modello gaussiano multivariato, possiamo costruire una distribuzione parametrizzata come una contentente due parametri, la media e la varianza. La corrispondente distribuzione viene quindi campionata ed inviata al decoder, che quindi procedere a ricostruire l'input dai punti campionati.</p> <p>Tuttavia, nonostante questo appaia essere semplice in teoria, diventa impossibile da implementare poer che la backpropagation non pu\u00f2 essere definita per un processo di campionamento casuale effettuato prima di inviare i dati al decoder. Di conseguenza, usiamo un trucco chiamato reparametrization - un modo per bipassare il processo di campionamento fatto dalla rete neurale.</p> <p>Di che si tratta? Nella reparametrization, campioniamo casualmente un valore \\(\\epsilon\\) da una gaussian unitaria, e quindi lo scaliamo per la varianza della distribuione latente \\(\\sigma\\) e lo shiftiamo per la media \\(\\mu\\) dlela stessa.</p> <p>Adesso, abbiamo lasciato dietro il processo di campionamento come qualcosa fatto al di fuori della pipeline di backpropagation, ed il valore dcampionato \\(\\epsilon\\) funziona come un altro input al modello, che viene inviato al bottleneck.</p> <p>Una vista sotto forma di diagramma \u00e8 mostrata in figura.</p> <p>TODO: REPARAMETRIZATION</p> <p>Per addestrare un VAR, usiamo due funzioni di costo, ovvero quella di ricostruzione e la divergenza KL.</p> <p>La loss di reconstruction permette alla distribuzione di descrivere in maneir corretta l'input, focalizzandoci soltnanto sulla minimizzazione dell'errore di ricostruzione. Di conseguenza, la rete apprende delle distribuzioni molto strette. La divergenza KL fa in modo che ci\u00f2 non accada, e prova a fare in modo che la distribuzione sia pi\u00f9 vicina ad una gaussiana a media nulla e varianza unitaria.</p> <p>La loss complessiva pu\u00f2 essere espressa come:</p> \\[ L = |x - \\hat{x}|+\\beta \\sum_i KL(q_j(z|x)\\|N(0,1)) \\] <p>dove \\(N\\) indica la distribuzione normale a media nulla e varianza unitaria, e \\(\\beta\\) \u00e8 un certo fattore di peswo.</p> <p>L'uso primario dei variational autoencoder pu\u00f2 essere visto nei modelli generativi.</p> <p>Campionare dalla distribuzione latente addestrata ed inviare i risultati al decoder pu\u00f2 condurre ai dati generati in un autoencoder.</p>"},{"location":"material/05_dl/03_rnn/01_rnn/","title":"01 rnn","text":"<p>When it comes to sequential or time series data, traditional feedforward networks cannot be used for learning and prediction. A mechanism is required to retain past or historical information to forecast future values. Recurrent neural networks, or RNNs for short, are a variant of the conventional feedforward artificial neural networks that can deal with sequential data and can be trained to hold knowledge about the past.</p> <p>Prerequisites This tutorial assumes that you are already familiar with artificial neural networks and the backpropagation algorithm. If not, you can go through this very nice tutorial, Calculus in Action: Neural Networks, by Stefania Cristina. The tutorial also explains how a gradient-based backpropagation algorithm is used to train a neural network.</p> <p>What Is a Recurrent Neural Network A recurrent neural network (RNN) is a special type of artificial neural network adapted to work for time series data or data that involves sequences. Ordinary feedforward neural networks are only meant for data points that are independent of each other. However, if we have data in a sequence such that one data point depends upon the previous data point, we need to modify the neural network to incorporate the dependencies between these data points. RNNs have the concept of \u201cmemory\u201d that helps them store the states or information of previous inputs to generate the next output of the sequence.</p>"},{"location":"material/05_dl/03_rnn/01_rnn/#srotolare-la-rnn","title":"Srotolare la RNN","text":"<p>Una semplice RNN ha un loop di feedback, come mostrato nel primo diagramma della figura precedente. Il feedback loop mostrato nel rettangolo grigio pu\u00f2 essere UNROLLED in tre step temporali per produrre la seconda rete della figura precednete... Ovviamente, possiamo variare l'architettura in modo che la rete si srotoli in \\(k\\) step temporali. nella figura, utilizziamo la seguente notazione:</p> <ul> <li>\\(x_t \\in R\\) \u00e8 l'input allo step temporale \\(t\\). Per mantenere le cose semplici</li> </ul>"},{"location":"material/05_dl/04_cnn/01_cnn/","title":"5.4.1 - Convolutional neural networks","text":"<p>Le convolutional neural networks (CNN) sono un tipo di rete neurale specializzato nell'elaborazione di dati che hanno una topologia riconducibile ad una griglia temporale. Un tipico esempio di applicazione delle CNN sono le immagini, che nell'ambito informatico sono rappresentate come griglie bidimensionali (o tridimensionali) di pixel.</p> <p>Il termine convolutional nel nome suggerisce inoltre che la rete si basa su una specifica operazione matematica di tipo lineare, chiamata convoluzione, che viene usata al posto della generica moltiplicazione matriciale in uno o pi\u00f9 strati della rete; vediamola pi\u00f9 nel dettaglio.</p>"},{"location":"material/05_dl/04_cnn/01_cnn/#la-convoluzione","title":"La convoluzione","text":"<p>Per comprendere i principi sottostanti l'operazione di convoluzione \u00e8 utile fare un breve esempio. Supponiamo di voler tracciare la posizione di una navicella spaziale mediante un sensore laser, il quale restituir\u00e0 un output \\(x(t), x \\in \\mathbb{R}\\) rappresentativo della posizione della navicella al tempo \\(t, t \\in \\mathbb{R}\\). In altre parole, il fatto che sia \\(x\\) sia \\(t\\) definite nel dominio dei valori reali far\u00e0 s\u00ec che potremo avere una diversa lettura del sensore laser ad ogni istante di tempo.</p> <p>Supponiamo adesso che l'uscita del sensore sia affetta da rumore; in tal caso, potremmo voler effettuare diverse misurazioni della posizione della navetta allo scopo di ottenere una stima pi\u00f9 precisa della sua posizione. Ovviamente, per\u00f2, dovremo dare maggiore rilevanza alle misure pi\u00f9 recenti, per cui dovremo trovare un modo per farlo dando un peso opportuno a ciascun valore considerato, magari usando una funzione di peso \\(w(a)\\), con \\(a\\) un parametro di \"et\u00e0\" associato alla misura. Applicando questa media pesata per ciascun istante \\(t\\), avremo una nuova funzione composta \\(s\\) che fornir\u00e0 la seguente stima della posizione della navicella:</p> \\[ s(t) = \\int x(a) w(t-a) \\delta a \\] <p>Questa operazione, chiamata per l'appunto convoluzione, \u00e8 tipicamente denotata utilizzando la seguente notazione:</p> \\[ s(t)  = (x * w)(t) \\] <p>Ritorno al futuro</p> <p>Notiamo che, limitatamente a questo specifico caso, se \\(t-a &lt; 0\\), allora \\(w = 0\\), onde fare in modo che la funzione non guardi al futuro. In generale, comunque, la convoluzione \u00e8 definita per qualsiasi funzione per la quale \u00e8 possibile definire le relazioni precedenti.</p>"},{"location":"material/05_dl/04_cnn/01_cnn/#convoluzione-discreta","title":"Convoluzione discreta","text":"<p>Nella terminologia delle CNN, il primo argomento (ovvero la funzione \\(x\\)) \u00e8 chiamato input, mentre il secondo argomento (la funzione \\(w\\)) \u00e8 chiamato kernel.</p> <p>Ovviamente, l'idea che un sensore laser possa fornire la misura ad ogni istante temporale non \u00e8 realistica. Normalmente, infatti, lavoriamo con tempi discreti, con un sensore che fornisce dati ad intervalli all'incirca regolari; realisticamente quindi potremo presumere che il nostro laser fornisca una misura (ad esempio) una volta al secondo. L'indice temporale \\(t\\) potr\u00e0 quindi considerare soltanto valori interi e discreti. Di consequenza, potremo definire la convoluzione discreta come:</p> \\[ s(t) = (x * w)(t) = \\sum_{a=-\\infty}^{\\infty} x(a) w(t-a) \\] <p>Nelle CNN, l'input \u00e8 di solito un array di dati multidimensionali, mentre il kernel \u00e8 un array di parametri multidimensionale adattato all'algoritmo di apprendimento. Dato che ogni elemento sia dell'input sia del kernel deve essere salvato all'interno della memoria del calcolatore, possiamo ipotizzare che queste funzioni siano zero ovunque, tranne che nell'insieme finito di punti per i quali stiamo effettuando l'osservazione.</p>"},{"location":"material/05_dl/04_cnn/01_cnn/#convoluzione-multidimensioanle","title":"Convoluzione multidimensioanle","text":"<p>Nell'esempio precedente abbiamo applicato una convoluzione su di un unico asse. Tuttavia, \u00e8 possibile applicare </p> <p>Infine, possiamo usare le convoluzioni su pi\u00f9 di un asse per volta. Per esempio, se usiamo un'imamgine bidimensionale \\(I\\) come input,. probabilmente vorremo usare un kernel bidimensionale \\(K\\):</p> \\[ S(i,j) = (I * K)(i, j) = \\sum_m \\sum_n I(m,n)K(i-m, j-n) \\] <p>La convoluzione \u00e8 commutativa, il che significa che possiamo scrivere in maneir aequivalente:</p> \\[ S(i,j) = (K*I)(i,j) = \\sum_m \\sum_n I(i-m, j-n) K(m,n) \\] <p>La propriet\u00e0\u00f2 commutativa della convoluzione nasce dal fatto che abbiamo inveritop il kernel rispetto all'iunput,. nel senso che man mano che \\(m\\) aumenta, l'indice nell'ingresso aumenta, ma l'indice nel kernel decrementa. Mentre la proprrioet\u00e0 commutativa \u00e8 utile per scirvere teorimi, non \u00e8 normalmente una propriet\u00e0 importante dell'0implementazione di una rete neurale. Invece, molte librerie di reti neurali implementano una funzioen correlata chiamata *cross-correlazione*\u00e9, che \u00e8 identica alla convoluzione ma senza rovesciare il kernel:</p> \\[ S(i,j) = (K * I)(i,j) = \\sum_m \\sum_n I(i+m, j+n) K(m,n) \\] <p>Molte libreire di machine learning implemnentano la cross-correlazione chiamadnoal convoluzoione.</p>"},{"location":"material/05_dl/04_cnn/02_feats/","title":"5.4.2 - Principi alla base delle CNN","text":"<p>Allo scopo di massimizzare le performance in termini di classificazione, le CNN sfruttano tre importanti principi, ovvero le interazioni sparse, la condivisione (sharing) dei parametri e la rappresentazione equivariante. Vediamoli nel dettaglio</p>"},{"location":"material/05_dl/04_cnn/02_feats/#interazioni-sparse","title":"Interazioni sparse","text":"<p>Le classiche reti neurali feedforward sfruttano la moltiplicazione (matriciale) per una matrice di parametri nella quale ogni parametro descrive l'interazione tra una singola unit\u00e0 di input ed una singola unit\u00e0 di output; se la matrice dei parametri non ha alcun valore pari a zero, ci\u00f2 significa che ogni unit\u00e0 di input interagisce con ogni unit\u00e0 di output, e viceversa.</p> <p>Le CNN, invece, sono caratterizzate da delle interazioni sparse, anche conosciute come connettivit\u00e0 sparsa (sparse connectivity): in altre parole, non tutte le unit\u00e0 di input interagiscono con ogni unit\u00e0 di output, e viceversa. Per ottenere questo scopo, basta utilizzare in fase di convoluzione un kernel pi\u00f9 piccolo dell'input. Immaginiamo ad esempio di elaborare un'immagine: questa pu\u00f2 avere migliaia, o anche milioni, di pixel; tuttavia, per individuare le feature di interesse, si possono utilizzare dei kernel di piccola dimensione, attorno alle decine di pixel; ci\u00f2 permette di memorizzare un numero inferiore di parametri, il che da un lato migliora l'efficienza statistica del modello, riducendo al contempo i requisiti in termini di memoria e tempo necessario per l'addestramento.</p> <p>In particolare, il miglioramento in termini di efficienza apportato dalla connettivit\u00e0 sparsa \u00e8 decisamente rilevante. Supponendo che vi siano \\(m\\) input ed \\(n\\) output ad un dato layer, una moltiplicazione matriciale completa richiederebbe \\(m \\times n\\) parametri, con una complessit\u00e0 pari ad \\(O(m \\times n)\\) per singola osservazione. Limitando invece le possibili connessioni di ciascun output a \\(k\\), avremo la necessit\u00e0 di utilizzare soltanto \\(k \\times n\\) parametri, con un \\(O(k \\times n)\\).</p> <p>Il valore di \\(k\\)</p> <p>Per la maggior parte delle applicazioni, il valore di \\(k\\) pu\u00f2 essere scelto di diversi ordini di grandezza inferiore ad \\(n\\), mantenendo comunque delle buone performance.</p>"},{"location":"material/05_dl/04_cnn/02_feats/#parameters-sharing","title":"Parameters sharing","text":"<p>In una classica rete feedforward, il calcolo dell'uscita di un layer prevede che ogni elemento della matrice dei parametri venga usato solo e soltanto una volta. Ci\u00f2 non \u00e8 tuttavia vero in una CNN, in quanto il paradigma del parameters sharing prevede che un singolo parametro sia usato da pi\u00f9 di una funzione. In altre parole, una CNN ha dei parametri strettamente correlati (strictly tied parameters), dato che il peso applicato a due o pi\u00f9 input pu\u00f2 essere lo stesso; ci\u00f2 avviene perch\u00e9 nelle CNN il kernel viene fatto scorrere sull'interezza dell'input (a possibile eccezione di alcuni pixel di contorno).</p> <p>Il paradigma del parameter sharing fa quindi in modo che, invece di apprendere un insieme di parametri specifico per ogni posizione dell'input, la rete apprenda un parametro per ciascun intorno. Questo non influenza la complessit\u00e0 della rete, che rimane sempre \\(O(k \\times n)\\); tuttavia, i requisiti di memoria del modello a \\(k\\) parametri risultano essere ulteriormente ridotti.</p> <p>Il parameter sharing usato dall'operazione di convoluzoien indica che piuttosto che apprendere un insieme di parameteri separato per oigni posizione, apprendiamo soltanto un insieme. Questo non influenza il runtime della forward propagation (\u00e8 sempre \\(O(k \\times n)\\)), ma riduce ulteriormente i requisiti di memoria dle modello a \\(k\\) parametri. Ricordiamo che \\(k\\) \u00e8 normalemnte diversi ordini di grandezza pi\u00f9 piccola di \\(M\\). Dal momento che \\(m\\) ed \\(n\\) soino normalmente della stesso rordine, \\(k\\) \u00e8 praticamente trascurabile se comparato ad \\(m \\times n\\). La convoluzione \u00e8 qunidion molto pi\u00f9 efficiente della moltiplicazione matriciale densa in termini dei requisiti di memoria e dell'efficienza statistica.</p>"},{"location":"material/05_dl/04_cnn/02_feats/#rappresentazione-equivariante","title":"Rappresentazione equivariante","text":"<p>Il parameter sharing di una CNN ha come consequenza l'equivarianza di un layer convoluzionale alla transazione; in altri termini, l'output del layer cambier\u00e0 nello stesso modo in cui viene modificato l'input.</p> <p>Formalmente, uan funzione \\(f(x)\\) \u00e8 equivariante ad una funzione \\(g(x)\\) se \\(f(g(x)) = g(f(x))\\). Immaginiamo ad esempio che la funzione \\(I(\\cdot)\\) sia quella che restituisce la luminosit\u00e0 di un'immagine. Sia \\(g\\) una funzione che trasli l'ouptut di un pixel a destra in modo che:</p> \\[ I^{'} = g(I) \\rightarrow g(x, y) = I^{'}(x, y) = I(x-1, y) \\] <p>Applicando la trasformazione \\(g(I)\\), e successivamente convolvendo il risultato, avremo lo stesso effetto dell'applicazione diretta della convoluzione ad \\(I^{'}\\), applicando di conseguenza la trasformazione \\(g\\) all'output di convoluzione.</p> <p>Ci\u00f2 implica l'insorgenza di un fenomeno interessante. Applicare la convoluzione a dati monodimensionali ci restituisce una sorta di \"timeline\", la quale ci mostra quando appaiono specifiche feature nell'input. Grazie alla propriet\u00e0 di equivarianza, qualora spostassimo un determinato evento nell'input, avremmo che la rappresentazione di tale evento nell'output traslerebbe in maniera equivalente. Lo stesso avviene per una rappresentazione bidimensionale: la convoluzione crea una feature map che rappresenta la posizione in cui appaiono alcune feature all'interno dell'immagine di ingresso. Se questa viene manipolata spostando un oggetto, la rapprentazione a valle della convoluzione sar\u00e0 contestualmente spostata allo stesso modo.</p> <p>Equivarianza</p> <p>La convoluzione \u00e8 equivariante esclusivamente alla traslazione. Nel caso di altre trasformazioni, come ad esempio rescaling o rotazioni, l'equivarianza non \u00e8 garantita.</p>"},{"location":"material/05_dl/04_cnn/03_pooling/","title":"03 pooling","text":""},{"location":"material/05_dl/04_cnn/03_pooling/#pooling","title":"Pooling","text":"<p>Un tlayer classico di una rete convoluzionale consiste di tre stage, come mostrato in figura 1. Nel primo stage, il layer efefttua diverse convoluzioni in parallelo, per produrre un insieme di attivazioni lineari. Nel secondo stage, ogni attivazione lineare passa attraverso una funzione di attivzione non lineare, come la funzione ReLU. Questo step \u00e8 alle volte chiamato detector stage. Nel terzo stage, usiamo una funzione di pooling per modificare l'uscita del layer ulteriormente.</p> <p>Una funzione di pooling rimpiazza l'uscita della rete ad una certa posizione con una statistica sommaria degli output circostanti. Ad esempio, la funzione di max pooling restituisce l'output massimo alll'interno di un intorno rettangolare. Altre funzionidi pooling popolari includono la media di un intorno rettangolare, la norma \\(L^2\\) dello stesos, o una media pesata basata sulla distanza dal pixel centrale.</p> <p>TODO FIGURA NN</p> <p>In tutti i casi, il pooling ci aiuta a rendere la rappresentazione approssimativamente invariante a piccole traslazioni dell'input. L'invarianza alla traslazione implica che se trasliamo l'input di una piccola quantita\u00e0, il valore della maggior aprte degli output del pooling non cambia. L'invarianza alle traslazioni locali pu\u00f2 essere una propriet\u00e0 utilse se ci importa pi\u00f9 del fatto che una feature sia presente che della sua posizione esatta. Ad esempio, quando determiniamo se un'immagine contiene un volto, non dobbiamo conoscere la posizione degli occhi con accuratezza al pixel, ma semplicemente se vi \u00e8 un occhio sulla parte sinistra del volto ed uno sulla parte destra. In altri contesti, \u00e8 pi\u00f9 importante preservare la poszione di una feature. PEr esempio, se vogliamo trovare un angolo definitoda due bordi che si incontrano ad uno specifico orientamento, dobbiamo preservare la posizione dei bordin abbastanza da testare se si incontrano.</p> <p>Dato che il pooling \"riassume\" le risposte in un intorno, \u00e8 p\u00e8ossibile usare meno unit\u00e0 di pooling che di detector, in modo da creare una statistica \"riassuntiva\" su regioni spaziate a \\(k\\) pixel di distanza piuttosto che ad 1 pixel di distanza. Questo migliora l'efficienza computazionale della rete perch\u00e9 il layer successivo ha approssimativamente \\(k\\) volte meno input da elaborare. Quando il numero di parametri nel layer successivo \u00e8 una funzione della sua dimensione dell'input (cos\u00ec come quando il layer successivo \u00e8 completamente connesso e basato sulla moltiplicazione matriciale), questa riduzione nella dimensione dell'input pu\u00f2 anche portare una efficienza statistica migliorata e ridurre i requisiti in termini  di memoria per memorizzare i parametri.</p> <p>Per molti task, il pooling \u00e8 essenziale per gestire gli input di diversa dimensione. Per esempio, se vogliamo classificare immagini di dimensioni variabili, l'input al layer di classificazione deve avere uan dimensione fissa. Questo si ottiene nromalmente variando la dimensione di un offset tra el regioni di pooling, in modo che il layer di classificazione rcieva sempre lo stesso unermo di statistiche riassunte indipendentemnete dalla dimensione dell'input. Per esempio, il layer di pooling finale della rete potrebbe essere definito per mandare in uscita quattro insiemi di statistiche riassuntive, una per ogni quadrante di un'immagine, indipendentemnete dalla dimensione dlla stessa.</p>"},{"location":"material/05_dl/05_obj_det/01_intro/","title":"5.5.1 - Introduzione alla object detection","text":"<p>Quello della object detection \u00e8 un campo applicativo della computer vision il cui duplice obiettivo \u00e8 quello di identificare e localizzare determinate classi di oggetti  all'interno di immagini o video. In particolare, la object detection ci permette di identificare la posizione degli oggetti mediante una bounding box, che altro non \u00e8 se non un rettangolo che sar\u00e0 \"disegnato\" attorno all'oggetto di interesse. </p> <p> </p> Figura 1 - Un esempio di object detection. A bounding box di colore differente corrispondono oggetti di tipo differente; in particolare, nelle diverse figure riconosciamo pomodori, nodi e fiori. <p>Tradizionalmente, il task di object detection veniva effettuato sfruttando tecniche di template matching: un esempio sono infatti gli approcci usati fino ai primi anni '00 per il riconoscimento facciale, come il classificatore di Viola - Jones, il quale prevedeva l'uso di particolari maschere, dette per l'appunto template, che venivano fatte \"scorrere\" sull'immagine alla ricerca di zone che rispettassero il pattern individuato.</p> <p>A partire dalla seconda met\u00e0 degli anni '10, tuttavia, si sono andati via via diffondendo gli approcci basati sulle deep neural network. In particolare, esistono due tipologie di algoritmo:</p> <ul> <li>nei two-stages object detector l'immagine viene dapprima passata attraverso una Region Proposal Network, che ha lo scopo di individuare le regioni \"candidate\" ad \"ospitare\" un determinato oggetto. A valle di questo primo stage, per ogni zona \u00e8 determinato un punteggio che va a determinare se sia presente o meno una determinata tipologia di oggetto;</li> <li>nei single-stage object detector i due passaggi sono \"condensati\", e localizzazione e valutazione del tipo di oggetto sono effettuati in un unico passo.</li> </ul> <p>Vediamo quindi nel dettaglio questi ultimi algoritmi, che sono anche i pi\u00f9 utilizzati, soprattutto grazie alle performance che sono in grado di offrire.</p>"},{"location":"material/05_dl/05_obj_det/02_ssd/","title":"02 ssd","text":""},{"location":"material/05_dl/05_obj_det/02_ssd/#yolo","title":"YOLO","text":"<p>YOLO, acronimo di You Only Look Once, \u00e8 attualmente l'algoritmo (o, per meglio dire, la famiglia di algoritmi) per la object detection a singolo stage pi\u00f9 utilizzato. Introdotto da Joseph Redmon nel 2015, YOLO si basa su un'architettura composta da 24 layer convoluzionali, quattro layer di max pooling, e due layer completamente connessi, come mostrato in figura 2.</p> <p> </p> Figura 2 - L'architettura alla base della prima versione di YOLO. <p>Notiamo che:</p> <ul> <li>le immagini sono ridimensionate in un input di dimensione \\(448 \\times 448\\);</li> <li>sono utilizzate convoluzioni \\(1 \\times 1\\), che permettono di ridurre la dimensionalit\u00e0 dei tensori che attraversano la rete;</li> <li>viene utilizzata una funzione di attivazione ReLU (a meno del layer finale);</li> <li>sono sfruttate diverse tecniche di regolarizzazione, come batch normalization e dropout, per ridurre l'overfitting del modello.</li> </ul> <p>Detto questo, possiamo schematizzare il funzionamento di YOLO in quattro punti fondamentali.</p>"},{"location":"material/05_dl/05_obj_det/02_ssd/#step-1-suddivisione-in-celle-dellimmagine","title":"Step 1: suddivisione in celle dell'immagine","text":"<p>YOLO suddivide l'immagine originaria in una griglia di \\(N \\times N\\) celle di forma e dimensioni uguali. Ciascuna cella sar\u00e0 dedicata alla localizzazione e predizione degli oggetti al suo interno, a cui assegner\u00e0 un certo punteggio (confidence score).</p> <p> </p> Figura 3 - Suddivisione dell'immagine originaria in griglia."},{"location":"material/05_dl/05_obj_det/02_ssd/#step-2-determinazione-delle-bounding-box","title":"Step 2: determinazione delle bounding box","text":"<p>Il secondo step consiste nella determinazione delle bounding box. In particolare, YOLO caratterizza una bounding box come segue:</p> \\[ Y = [p_c, b_x, b_y, b_h, b_w, c_1, c_2] \\] <p>In particolare:</p> <ul> <li>\\(Y\\) \u00e8 la bounding box;</li> <li>\\(p_c\\) corrisponde al confidence score associato al fatto che la griglia contenga o meno un oggetto. Ad esempio, tutte le griglie mostrate in rosso in figura 4 hanno un punteggio pi\u00f9 alto di zero, mentre quelle in verde hanno un punteggio pari a zero.</li> <li>\\(b_x\\) e \\(b_y\\) sono le coordinate \\((x, y)\\) del centro della bounding box rispetto alla cella che racchiude l'oggetto;</li> <li>\\(b_h, b_w\\) sono invece l'altezza e l'ampiezza della bounding box che contiene l'oggetto di interesse;</li> <li>\\(c_1, c_2\\) corrispondono alle due classi (giocatore e palla). Ovviamente, possiamo avere tante classi quante richieste dal caso d'uso specifico.</li> </ul> <p> </p> Figura 4 - Assegnazione del punteggio di confidenza alle celle della griglia."},{"location":"material/05_dl/05_obj_det/02_ssd/#step-3-intersection-over-union-iou","title":"Step 3: Intersection Over Union (IoU)","text":"<p>Nella quasi totalit\u00e0 dei casi, YOLO determiner\u00e0 pi\u00f9 \"candidati\" sulle stesse celle. Ovviamente, non tutti saranno rilevanti o corretti: per questo, nel terzo step si valuta la Intersection Over Union, ovvero una metrica data dal rapporto tra l'intersezione tra il ground truth e la bounding box rilevata, e l'unione delle stesse aree. Il valore della IoU \u00e8 sempre compreso tra \\(0\\) ed \\(1\\), ed \u00e8 tanto pi\u00f9 alto quanto pi\u00f9 la bounding box si sovrappone al ground truth. L'obiettivo \u00e8 quindi quello di scartare le bounding box non rilevanti, a favore di quelle pi\u00f9 significative. Nella pratica:</p> <ul> <li>viene definito (anche dall'utente) un valore di soglia minimo per la IoU (di solito, \\(0.5\\));</li> <li>l'algoritmo calcola la IoU per ogni bounding box individuata;</li> <li>le box con una IoU minore alla soglia sono scartate.</li> </ul> <p>Un esempio \u00e8 mostrato in figura 5, nella quale la bounding box verde \u00e8 il ground truth, quella arancione ha una IoU minore di \\(0.5\\), mentre quella blu maggiore di detto valore.</p> <p> </p> Figura 5 - Valutazione delle bounding box in base alla IoU."},{"location":"material/05_dl/05_obj_det/02_ssd/#step-4-non-max-suppression-nms","title":"Step 4: Non-Max Suppression (NMS)","text":"<p>L'ultimo passo \u00e8 detto Non-Max Suppression, e prevede l'eliminazione di tutte le bounding box sovrapponibili a meno di quella con il valore di IoU massimo.</p>"},{"location":"material/05_dl/05_obj_det/anchor/","title":"Anchor","text":"<p>Anchor Boxes for Object Detection Object detection using deep learning neural networks can provide a fast and accurate means to predict the location and size of an object in an image. Ideally, the network returns valid objects in a timely manner, regardless of the scale of the objects. The use of anchor boxes improves the speed and efficiency for the detection portion of a deep learning neural network framework.</p> <p>What Is an Anchor Box? Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training datasets. During detection, the predefined anchor boxes are tiled across the image. The network predicts the probability and other attributes, such as background, intersection over union (IoU) and offsets for every tiled anchor box. The predictions are used to refine each individual anchor box. You can define several anchor boxes, each for a different object size. Anchor boxes are fixed initial boundary box guesses.</p> <p>The network does not directly predict bounding boxes, but rather predicts the probabilities and refinements that correspond to the tiled anchor boxes. The network returns a unique set of predictions for every anchor box defined. The final feature map represents object detections for each class. The use of anchor boxes enables a network to detect multiple objects, objects of different scales, and overlapping objects.</p>"},{"location":"material/05_dl/06_xai/02_learned_feature/","title":"Learne feature","text":"<p>Le CNN apprendono feature astratte e concetti dai pixel raw dell'immagine. Le tecniche di feature visualization visualizzano le feature apprese mediante la massimizzazione delle attivazioni. Invece, le tecniche di network dissection etichettano le unit\u00e0 delle reti neurali con dei concetti umani.</p> <p>Le deep neural netowrk apprendono feature ad alto livello nei layer nascosti. Questa \u00e8 una delle pi\u00f9 grandi forze, e riduce la richiesta per l'ingegnerizzazione delle feature. Assumendo che si voglia cosruire un classificatore per immagini con una SVM. Le matrici di pixel grezzi non sono il miglior input per l'addestramento della SVM, per cui possiamo creare nuove feature basate sul colore, dominio di frequenza, edge detecto, e via. Con le NCN, l'immagine viene mandata nella rete nella sua forma grezza (pixel). La rete trasfrma l'immagine diverse volte. Per prima cosa, l'immagine va in molti layer convoluzoinali. In questi layer convoluzionali, la rete apprende nuove, ed incrementalmente complesse, feature. Qunid l'informazione trasformata dell'immagine va attraverso i layer completamente connessi, e viene trasformata in una classificazione opredizione.</p> <p>Di soito:</p> <ul> <li>i primi layer convoluzionali apprendono delle feature come edge e semplici texture</li> <li>i layer convoluzionali successivi apprendono feature pi\u00f9 complesse come texture e pattern</li> <li>gli ultimi layer convoluzionali apprendono feature come oggetti o parti di oggetti</li> <li>il layer completamente connesso apprede  a connettere le attivazioni dalle feature ad alto livello alle classi individuali.</li> </ul>"},{"location":"material/05_dl/06_xai/02_learned_feature/#feature-visualization","title":"Feature visualization","text":"<p>L'approccio di rendere le feature apprese esplicite \u00e8 chiamatl Feature Visualization. La feature visualization per un'unit\u00e0 di una rete neruale \u00e8 svolta individuando l'input che massimizza l'attivazione di quell'unit\u00e0.</p> <p>PEr \"unit\u00e0\" intendiamo i singoli neuroni, canali (chiamati anche feature map), interi livelli o la probabiit\u00e0 di classe ottenuta dalla classificazione.</p> <p>I singoli neuroni sono unit\u00e0 atomiche della rete, per cui avremo la maggior parte dell'informazione creando delle visualizzazioni delle feature per ciascun neurone. Ma c'\u00e8 un problema: le reti neurali spesso contengono milioni di neuroni. Guardare la feature visualization di ciascun neurone richiede molto tempo. I canali (chiamate anche activation map\u00e8s) sono una buona scelta come unit\u00e0 per la feature visualization. Possiamo andare un passo oltre, e visualizzare un intero layer convoluzionale.</p> <p>TODO: COPIARE FIGURA 10.2</p>"},{"location":"material/05_dl/06_xai/02_learned_feature/#feature-visualiztion-through-optimization","title":"Feature visualiztion through optimization","text":"<p>In termini matematici, la feature visualization \u00e8 un problema di ottimizzazione. Supponiamo che i pesi della rete neurale siano fissati, il che significa che la rete \u00e8 addestrata. Stiamo cercando una nuova immagine che massimizzi l'attivazione media di un'unit\u00e0, qui un singolo neurone:</p> \\[ img^* = \\argmax_{img} h_{n, x, y, z}(img) \\] <p>dove \\(h\\) \u00e8 l'attivazione del neurone, \\(img\\) l'ingresso della rete, \\(x\\) ed \\(y\\) descrivono la posizione spaziale del neurone, \\(n\\) specifica il layer, e \\(z\\) \u00e8 l'indice del canale. Per l'attivazione</p> <p>https://christophm.github.io/interpretable-ml-book/cnn-features.html#feature-visualization</p>"},{"location":"material/05_dl/06_xai/intro/","title":"Intro","text":"<p>la storia di successo delle deep neural netowrk inziia approssimativamnete nel 2012, con la vittoria della ImageNet classification challenge da parte di AlexNet.</p> <p>Per effettuare le predizioni con una rete neurlae, gli input passano attraverso diversi layer moltipliacativi, con i pesi appresi ed attraverso trasformazioni on lineari. Una singoal predizione pu\u00f2 coinvolgere milioni di operazioni matematiche a seconda dell'architettura della rete neurale. Non c'\u00e8 modo per un umano di seguire il mapping esatto dai dati alla predziione. Dovremmo considerare milioni di pesi che interagiscono in modo complesso per comprendere la predizione da una rete neurale. Per interpretare il comportamenot e le predizioni di una rete neurale, abbiamo bisongo di metodi specifici.</p> <p>Per prima cosa, le reti neruali apprendono delle feature e dei concetti negli strati nascosti, ed abbiamo bisogno di strumenti specifici per scoprirli. Seconda cosa, il gradiente pu\u00f2 essere utlilzato per implmentare dei metodi interpretativi che siano pi\u00f9 efficienti dal punto di vista computazionale dei meotdi model-agnostic che guardano al modello \"da fuori\".</p>"},{"location":"material/05_dl/07_gans/lecture/","title":"XX - Le Generative Adversarial Networks","text":"<p>Le Generative Adversarial Networks (GAN) rappresentano una delle pi\u00f9 interessanti tra le recenti innovazioni in ambito deep learning. Le GAN sono un modello di tipo generativo: creano, infatti, nuovi dati che \"ricordano\" quelli usati per l'addestramento della rete. Ad esempio, una GAN pu\u00f2 essere addestrata a creare dei volti che, in realt\u00e0, non appartengono ad una persona reale.</p> <p>TODO: ESEMPIO VOLTI GAN</p> <p>Una GAN \u00e8 in grado di ottenere questo livello di realismo facendo \"lavorare in parallelo\" un generatore, che impara a produrre gli output richiesti, ed un discriminatore, il cui compito \u00e8 discernere i dati \"veri\" da quelli mandati in output dal generatore. La GAN sar\u00e0 ritenuta addestrata con successo quando il generatore sar\u00e0 in grado di ingannare il discriminatore.</p>"},{"location":"material/05_dl/07_gans/lecture/#xx1-i-modelli-generativi","title":"XX.1 - I modelli generativi","text":"<p>Facciamo un passo indietro. Cosa si intende per modello generativo?</p> <p>In breve, un modello generativo \u00e8 un modello statistico in grado di generare nuove istanze appartenenti ad un certo dataset. Di contro, i modelli discriminativi (che abbiamo usato finora) permettono di discriminare tra diverse classi di dati.</p> <p>Cani e gatti</p> <p>Immaginiamo un dataset fatto di foto di cani e gatti. Un modello discriminativo ci permette di capire se la foto rappresenta un cane o un gatto; un modello generativo ci permette di generare la foto di un cane o, in alternativa, quella di un gatto.</p> <p>Pi\u00f9 formalmente, supponiamo di avere una coppia di insiemi \\((X, Y)\\), dove \\(X\\) sono le istanze del nostro dataset, ed \\(Y\\) le label ad esse assegnate. Allora:</p> <p>Modelli generativi</p> <p>I modelli generativi descrivono la probabilit\u00e0 congiunta \\(P(X, Y)\\) o, nel caso di dati non etichettati, la probabilit\u00e0 \\(P(X)\\).</p> <p>Modelli discriminativi</p> <p>I modelli discriminativi descrivono la probabilit\u00e0 condizionale \\(P(Y|X)\\).</p> <p>Appare chiaro come i modelli generativi descrivano la distribuzione dei dati, e ci dicano quanto sia probabile un dato esempio. Ad esempio, un modello che predice la parola successiva in una certa frase \u00e8 tipicamente generativo, perch\u00e9 assegna una probabilit\u00e0 ad una certa sequenza di parole. Di converso, un modello discriminativo ci dice solo quanto \u00e8 probabile che una certa label sia applicata ad una data istanza.</p> <p>Un modello discriminativo ignora la questione di se una data istanza \u00e8 probabile, e ci dice solo quanto probabile \u00e8 che una label sia applicata ad un'istanza.</p> <p>Notiamo che questo \u00e8 una definizione molto generale. Ci sono molti tipi diversi di modelli generativi. Le GAN sono soltanto uno.</p>"},{"location":"material/05_dl/07_gans/lecture/#modellare-le-probabilita","title":"Modellare le probabilit\u00e0","text":"<p>Non tutti i tipi di modelli devono restituire un numero che rappresenta una probabilit\u00e0. Possiamo modellare la distribuzione dei dati che immita quella distribuzione.</p> <p>Ad esempio, un classificatore discriminativo come un albero decisionale pu\u00f2 etichettare un'istanza senza assegnare una probabilit\u00e0 a quella label. Un classificatore di questo tipo sarebbe sempre un modello perch\u00e9 la distribuzione di tutti i label predetti modellerebbero la distribuzione reale delle label nei dati.</p> <p>In modo simile, un modello generativo pu\u00f2 modellare una distribuzione producendo dei falsi convincenti che sembrano essere estratti da quella stessa distribuzione.</p>"},{"location":"material/05_dl/07_gans/lecture/#i-modelli-generativi-sono-difficili","title":"I modelli generativi sono difficili","text":"<p>I modelli generativi affrontano un task pi\u00f9 difficile di quello gestito dai modelli discriminativi analoghi. I modelli generativi devono modellare infatti pi\u00f9 dati.</p> <p>Un modello generativo per le immagini pu\u00f2 catturare le correlazioni come come \"cose che sembrano navi dovrebbero probabilmente apparire vicino a cose che sembrano acqua\" e \"gli occhi probabilmente non saranno sulla fronte\". Queste sono distribuzioni complicate.</p> <p>In contrasto, un modello discriminativo pu\u00f2 apprendere la differenza tra \"nave\" e \"non nave\" guardando soltanto pochi pattern. Pu\u00f2 ignorare molte delle correlazioni che il modello generativo deve invece tenere in conto.</p> <p>I modelli discriminativi provano a disegnare dei confini nello spazio dei dati, mentre i modelli generativi provano a modellare come i dati sono disposti nello spazio. Ad esempio, il seguente diagramma mostra i modell discriminativi e generativi per le cifre scritte a mano:</p> <p>TODO: DIAGRAMMA</p> <p>Il modello discriminativo prova ad individuare la diferenza tra gli 0 e gli 1 scritti a mano tracciando una riga nello spazio dei dati. Se fa le giuste assunzioni sulla linea, pu\u00f2 distinguere gli 0 dagli 1 senza doiver modellare dove le istanze sono piazzate nello spazio dei dati su ogni lato della linea.</p> <p>In contrasto,il modello generativo prova a produrre degli 1 e degli 0 convinceenti generando dei numeri che cadono vicino alle controparti vene nello spazio dei dati. Deve modellare la distribuzione attraverso lo spazio dei dati.</p> <p>Le GAN offrono un modo efficace di addestrare questi modelli ricchi che ricordano una distribuzione reale. Per capire comue funzionano dobbiamo capire la struttura base di una GAN.</p>"},{"location":"material/05_dl/07_gans/lecture/#test","title":"Test","text":"<p>Abbiamo il test della QI per 1000 persone. Modelliamo la distribuzione dei punteggi dei QI mediante la seguente procedura:</p> <ol> <li>lanciamo tre dadi a sei facce * moltiplichiamo il risultato per una costante w</li> <li>ripetiamo 100 volte e prendiamo la media dei risultati</li> </ol> <p>Proviamo diversi valori per w fino a che il risultato della procedura \u00e8 uguale alla media dei punteggi QI reali. Il modello \u00e8 genrativo o discriminativo?</p> <p>Not enough information to tell. This model does indeed fit the definition of one of our two kinds of models. Try again. Discriminative model Incorrect: an analogous discriminative model would try to discriminate between different kinds of IQ scores. For example, a discriminative model might try to classify an IQ as fake or real. Try again. Generative model Correct: with every roll you are effectively generating the IQ of an imaginary person. Furthermore, your generative model captures the fact that IQ scores are distributed normally (that is, on a bell curve). Correct answer.</p> <p>Un modello restituisce una probabilit\u00e0 quando gli diamo un'istanza dei dati. E' generativo o discriminativo?</p> <p>Not enough information to tell. Both generative and discriminative models can estimate probabilities (but they don't have to). Correct answer. Generative model A generative model can estimate the probability of the instance, and also the probability of a class label. Try again. Discriminative model A discriminative model can estimate the probability that an instance belongs to a class. Try again.</p>"},{"location":"material/05_dl/07_gans/lecture/#anatomia-di-una-gan","title":"Anatomia di una GAN","text":"<p>Una GAN ha due parti:</p> <ul> <li>il generatore, che apprende a generare dati plausibili. Le istanze generate divetano degli esempi negativi per il discriminatore</li> <li>il discriminatore che apprende a distinguere i dati falsi del genratore dai dati reali. il discriminatore penalizza il genaeratore che produce risultati poco plausibili.</li> </ul> <p>Quando inizia il training, il generatore produce ovviamente dati falsi, ed il discriminatore impara rapidamente a dire che \u00e8 falso:</p> <p>TODO: ESEMPIO</p> <p>Man mano che l'addestramento procede, il generatore produce degli oiutput che possono man mano sempre pi\u00f9 ingannare il discriminatore:</p> <p>TODO IMMAGINE</p> <p>Infine, se l'addestramento del generatore va a buon fine, il discriminatore non riesce a dire la differenza tra reale e falso. Inizia a classificare i dati falsi come reali, e l'accuracy diminuisce.</p> <p>TODO IMMAGINE</p> <p>Ecco uno schema dell'intero sistema</p> <p>https://developers.google.com/machine-learning/gan/gan_structure?hl=en</p> <p>Sia il generatore sia il discriminaotre sono reti neurali. L'output del generatore \u00e8 connesso direttamente all'input del discriminatore. Attraverso la backpropagation, la classificazione del discriminatore fornisce un segnale che il generatore usa per aggiornare i suoi pesi.</p>"},{"location":"material/05_dl/07_gans/lecture/#il-discriminatore","title":"Il discriminatore","text":"<p>Il discriminatore in una GAN \u00e8 semplicemente un classificatore. Prova a distinguere i dati reali dai dati creati dal generatore. Pu\u00f2 usare una qualsiasi archiettura di rete appropriata al tipo di dati da classificare.</p>"},{"location":"material/05_dl/07_gans/lecture/#dati-di-training-per-il-discriminatore","title":"Dati di training per il discriminatore","text":"<p>I dati di training per il discriminatore provengono da due sorgenti:</p> <ul> <li>istanze di dati reali, come vere immagini di persone. Il discriminatore usa queste istanze come campioni positivi durante il training.</li> <li>istanze di dati falsi creati dal generatore. Il discriminatore usa queste istazne come campioni negativi durante il training.</li> </ul> <p>Nella figura precedente, le due box \"Samples\" rappresentano quesete due sorgenti dati chee vengono inviate al discriminatore. Durante l'addestramento del discriminatore, il generatore non viene addestrato. I suoi pesi rimangono costanti, mentre produce i campioni su cui il discrimniatore verr\u00e0 addestrato.</p>"},{"location":"material/05_dl/07_gans/lecture/#addestramento-del-discriminatore","title":"Addestramento del discriminatore","text":"<p>Il discriminatore si connette a due funzioni di costo. Durante l'addestreamento del discrimiantore, questo ignora la loss del generatore e usa soltanto la sua. Usiamo la loss del generatore durante l'adestramento del generatore.</p> <p>Durante il trainign del discriminatore:</p> <ol> <li>il discriminaotre classifica sia i dati reali sia quelli falsi dal generatore</li> <li>la loss del discrimiantore penalizza il discriminatore per classificare male un'istanza vera come falsa o una falsa come vera</li> <li>il discriminaotre aggiorna i suoi pesi mediante la backgpropagation dalla loss del discrimiantore attraverso la rete dello stesso</li> </ol>"},{"location":"material/05_dl/07_gans/lecture/#il-generatore","title":"il generatore","text":"<p>la parte del generatore di una GAN apprende a creare dei dati falsi incororando il feedback dal discriminatore. Apprende a fare in modo che il discriminatore classifichi il suo output come reale.</p> <p>L'addestramento del generatore richiede un'integrazione pi\u00f9 strateta tra il genratore ed il discriminatore rispetto a quella raggiunta durante il training del discriminatore. La porzione della GAN che addestra il generatore include:</p> <ul> <li>inpuyt casuale</li> <li>rete generatrice, che trasforma l'input random in un'istanza dei dati</li> <li>rete discriminatorice, che classifica i dati generati</li> <li>output del discriminatore</li> <li>loss del generatore, che penalizza il generatore se questo non riesce ad imbrogliare il discriminatore</li> </ul>"},{"location":"material/05_dl/07_gans/lecture/#input-casuale","title":"input casuale","text":"<p>le reti neurali hanno bisogno di un qualche tipo di input. Normalmente daimo in input dati con i quali vogliamo fare qualcosa, come un'istanza che vogliaomo classificare o su cui vogliamo effettuare una predizione. Ma cosa usiamo come input per una rete che manda in output delle nuove istanze dati?</p> <p>Nella sua forma base, una GAN prende del rumore casuale come input. Il generatore quindi trasforma questo rumore in un output significativo. introduciendo rumore, possiamo fare in modo che la GAN produca un'ampia quantit\u00e0 di dati, campionati da diversi posti nella distribuzione obiettivo.</p> <p>GLi espeirmenti suggeriscono che la distribuzione del rumore non conti molto, per cui possiamo scegliere qualcosa da cui sia facile campionare, come una distribuzione unifoirme. Per convenienza, lo spazio dal quale il rumore viene campionato \u00e8 spesso di dimensioni pi\u00f9 piccole rispetto alla dimensionalit\u00e0 dello spazio in output.</p>"},{"location":"material/05_dl/07_gans/lecture/#usare-il-discriminatore-per-addestrare-il-generatore","title":"usare il discriminatore per addestrare il generatore","text":"<p>Per addestrare una rete neurale, atleriamo i pesi dellar ete per ridurre l'errore o la perdita del suo output. Nella nostra GAN, tuttavia, il generatore non \u00e8 direttamente connesso alla loss che stiamo provando ad influenzare. La loss del generatore penalizza il generatore per produrre unc ampione che la rete discriminativa classifica come falso.</p> <p>Questo pezzo extra di rete deve essere incluso nella backpropagation. La backpropagation modifica ogni peso nella direzione giusta calcolando l'impatto del peso sull'output - come l'output cambierebbe se cambiamo il peso. Ma l'impatto del peso di un generatore dipende dall'impatto dei pesi del discriminatore nel quale va ad inseririsi. Per cui la backpropagation inizia all'usicta e va indietro attraverso il discriminatore nel generatore.</p> <p>Allo stesso tempo, non vogliamo che il discrimiantore cambi durante l'addestramento del generatore. Provare a colpire un target in movimento renderebbe un problema duro ancora pi\u00f9 duro per il generatore.</p> <p>Per cui addestriamo il generatore ocn la seguente procedura:</p> <ol> <li>campioniamo rumore casuale</li> <li>produciamo l'output del generatore da rumore campionato casualmente</li> <li>otteneiamo delle classificazioni reali o false per l'output del generatore</li> <li>calcoliamo la loss dalla classificazione del discriminatore</li> <li>effettuiamo la backpropagation attraverso il discriminatore ed il generatore per ottenere i gradienti</li> <li>usiamo i gardienti per cambiare soltanto i pesi del generatore</li> </ol> <p>Questa \u00e8 un'iterazione del training del generatore. Nella prossima sezione vedremo come unire il training del generatore e del discrimiantore.</p>"},{"location":"material/05_dl/07_gans/lecture/#gan-training","title":"GAN Training","text":"<p>Dato che una GAN contiene due reti addestrate separatamente, i suoi algoritmi di training devono risolvere due complicazioni:</p> <ul> <li>le GAN devono tenere in conto due diversi tipi di training (generatore e discriminatore)</li> <li>la convergenza della GAN \u00e8 difficile da identificare</li> </ul>"},{"location":"material/05_dl/07_gans/lecture/#alternare-i-training","title":"Alternare i training","text":"<p>Il generatore ed il discrimanatore hanno diversi processi di training. Per cui come addestriamo la GAN?</p> <p>L'addestramento della GAN avviene a fasi alterne:</p> <ol> <li>il discriminatore si addestra per una o pi\u00f9 epoche</li> <li>il generaotre si addestra per una o pi\u00f9 epoche</li> <li>ripetiamo gli step 1 e 2 per continuare ad addestrare le reti di generazione e discriminazione</li> </ol> <p>Manteniamo il generatore costante durante la fase di disriminazione. Man manco che il training del discriminatore prova a capire come distinguere i dati reali da quelli falsi, deve apprendere a come riconsocere le \"colpe\" del generatore. Questo \u00e8 un problema differente per un genereatore addestrato piuttosto di uno non addestrato che produce output casuale.</p> <p>In modo simile, dobbiamo mantenere il discrimninatore costante durante la fase di training del generatore. Altrimenti il generatore prover\u00e0 a colpire un bersaglio in movimento, e potrebbe non convergere.</p> <p>E' questo andiriviene che permetete alle GAN di affrontare dei problemi altreimneit intrattabili. Possiamo entrare nel problema complesso di genrazione iniziando con un problema di classificazione molto pi\u00f9 semplice. Di converso, se non possiamo addestrare un classificatore a dire la differenza tra dati reali e generati anche per dati generati casualmente, non possiamo iniziare il training della GAN.</p>"},{"location":"material/05_dl/07_gans/lecture/#convergenza","title":"convergenza","text":"<p>Man mano che il generatore migliora l'addestramento, le performance del discriminatore migliora perch\u00e9 il discrimnaotre non pu\u00f2 facilmente dire la differenza tra reale e falso. Se il generatore ha un successo \"perfetto\"; allora il discriminatreo ha un'accuracy del 50%. Nei fatti, il discriminatore lancia una moneta per fare la sua predizione.</p> <p>Questa progressione pone un problema per la convvergenza dell'intera GAN: il feedback del discriminaotre diventa meno sigificativo nel tempo. Se la GAN continua ad addestrarsioltre il punto dove il discriminaotre sta dando dei feedback completamente casuali, allora il generatore inizia ad addestrarsi su feedback non buoni, e la sua qualit\u00e0 pu\u00f2 collassare.</p> <p>Per una GAN, la convergenza \u00e8 spesso un valore \"mobile\", non stabile.</p>"},{"location":"material/05_dl/07_gans/lecture/#loss-functions","title":"Loss functions","text":"<p>Le GAN provano a replicare una distribuzione di probabilit\u00e0. Dovrebber quindi usare una loss function che riflette la distanza tra la distribuzione dei dati generati dalla GAN e la distribzuione dei dati reali.</p> <p>Come catturare la differnezaa tra due distribuzioni nelle funzioni di costo della GAN? Questa question \u00e8 un'area di ricerca attiva, e molti approcci sono stati proposti. Vedremo due funzioni di costo comuni per le GAN, entrambe le quali sono implementate in TF-GAN:</p> <ul> <li>minimax loss: la funzione di costo usata nel paper che ha introdotto le GAN (https://arxiv.org/abs/1406.2661)</li> <li>loss di Wasserstein: la funzione di costo di defautl per TF-GAN descritta in un paper del 2017 (https://arxiv.org/abs/1701.07875)</li> </ul> <p>TF-GAN implementa molte altre funzioni di costo.</p>"},{"location":"material/05_dl/07_gans/lecture/#una-o-due-funzioni-di-costo","title":"UNa o due funzioni di costo?","text":"<p>Una GAN pu\u00f2 avere due funzioni di costo: una per l'addestramento del generator ed uno per il training del discriminator. Come possono le due funzioni di costo lavorare insieme per riflettere una misura di distanza tra distribuzioni di probabilit\u00e0?</p> <p>Nello schema delle loss che vedremo qui, le loss del generator e del descriminator derivano da una singola misura didistanza tra le distribuzioni di probabilit\u00e0. In entrambi questi schemi, comunque, il generator pu\u00f2 influenzare un solo termine nella misura di distanza: il termine che riflette la distribuzione dei dati falsi. Per cui durante il training  del generator lasciamo l'altro termine, che rfilette la distribuzione dei dati reali.</p> <p>Le loss del generator e del discriminator appaiono differenti alla fine, anche se derivano da una singola formula.</p>"},{"location":"material/05_dl/07_gans/lecture/#minimax-loss","title":"Minimax loss","text":"<p>Nel paper che ha introdtto il GAN, il generatore prova a minimizzare la seguente funzione, mentre il discriminatore prova a massimizzarla:</p> \\[ E_x \\[log(D(x))\\] + E_z [log(1-D(G(z)))] \\] <p>In questa funzione:</p> <ul> <li>\\(D(x)\\) \u00e8 la stima del discriminatore sulla probabilit\u00e0 che l'istanza dei dati reali \\(x\\) sia vera.</li> <li>\\(E_x\\) \u00e8 il valore atteso di tutte le istanze dei dati reali.</li> <li>\\(G(z)\\) \u00e8 l'output del generatore ad un dato rumore \\(z\\).</li> <li>\\(D(G(z))\\) \u00e8 la stima del discriminatore della probabilit\u00e0 che un'istanza falsa sia reale.</li> <li>\\(E_z\\) \u00e8 il valore atteso su tutti gli input casuali del generatore (in effetti, i vaore atteso di tutte le istanze fake generate \\(G(z)\\)).</li> <li>La formula deriva dalla cross-entropia tra le distribuzioni reali e generate.</li> </ul> <p>Il generatore non pu\u00f2 direttamente influenzare ilt ermine \\(log(D(x))\\) nella funzione, per cui, per il genratore, minimizzare la loss \u00e8 equivalente a minimizzare \\(log(1 - D(G(z)))\\).</p>"},{"location":"material/05_dl/07_gans/lecture/#modified-minimax-loss","title":"Modified Minimax Loss","text":"<p>Il paper originale del GAN nota che la funzione di costo precedente pu\u00f2 far s\u00ec che le GAN rimangano ferme nei primi stage dell'addestramento, qunado il compito del discirminatore \u00e8 molto semplice. Il paper quindi suggerisce di modificare la loss del generatore in modo che questi provi a massimizzare \\(log(D(G(z)))\\).</p>"},{"location":"material/05_dl/07_gans/lecture/#wasserstein-loss","title":"Wasserstein Loss","text":"<p>Di default, TF-GAN usa la loss di Wasserstein.</p> <p>Questa funzione di costo dipende da una modifca dello schema della GAN (chiamato \"Wasserstein GAN\" o \"WGAN\") nel quale il discriminatore non classifca in effett delle istanze. Per ogni istanza d\u00e0 invece un numero. Questo numero non deve essere inferiore ad 1 o pi\u00f9 grande di ', per cui non possiamo usare 0.5 come soglia per decidere se un'istanza \u00e8 vera o falsa. Il training del discriminator prova a rendere l'output pi\u00f9 grande per istanze reali che per istanze fake.</p> <p>Siccome non pu\u00f2 realmente discriminare tra real e fake, il discriminatore WGAn \u00e8 in effetti chiamato \"critica\" invece di \"discriminatore\". Questa distinzione ha importanza teorica, ma per gli scopi pratici possiamo trattarlo come un acknowledgement che gli input alla funzione di costo non devono essere delle probabilit\u00e0.</p> <p>La funzione di costo stessa \u00e8 molto semplice:</p> <p>Critic Loss: \\(D(x) - D(G(z))\\)</p> <p>Il discrimnatore prova a massimizzare questa funzione. In altre parole, prova a massimizzare la differenza tra i suoi output su istanze reale ed il suo output su istanze false.</p> <p>Generator Loss: \\(D(G(z))\\)</p> <p>Il generatore prova a massimizzare questa funzione. In altre parole, prova a massimizzare l'output del discrimnatroe per le sue istanze false.</p> <p>In queste funzioni:</p> <ul> <li>\\(D(x)\\) sono l'output della critica per una istanza vera.</li> <li>\\(G(z)\\) \u00e8 l'output del generatore quando c'\u00e8 un dato rumore \\(z\\).</li> <li>\\(D(G(z))\\) \u00e8 l'output della critica per un'istanza fake.</li> <li>L'output della critica \\(D\\) deve NON essere tra 1 e 0.</li> <li>Le formule derivano dalla earth mover distance tra le distribuzioni reali e generate.</li> </ul>"},{"location":"material/05_dl/07_gans/lecture/#requisiti","title":"Requisiti","text":"<p>La giustificazione teorica per la WGAN richiede che i pesi attraverso la GAN possano essere tagliati in modo che rimangano all'interno di un range vincolato.</p>"},{"location":"material/05_dl/07_gans/lecture/#benefit","title":"Benefit","text":"<p>Le WGAN sono meno vulnerabili a rimanere ferme piuttosto che le GAN minimax-based, ed evitare problemi con i vanisghin gradients. L'earth mover distance ha anche il vantaggio di essere una vera metrica: una misura di distanza in uno spazio di distribuzioni di probabilit\u00e0. La cross-entropy non \u00e8 in tal senso una metrica.</p> <p>Earth Mover Distance: https://en.wikipedia.org/wiki/Earth_mover's_distance</p>"},{"location":"material/05_dl/07_gans/lecture/#problemi-comuni","title":"Problemi comuni","text":"<p>Le GAN hanno un certo numero di fallimenti comuni. Tutti questi problemi comuni sono aree di ricerca attiva. Mentre nessuno di questi problemi \u00e8 stato completamente risolto, vedremo alcune cose che le persone hanno gi\u00e0 provato.</p>"},{"location":"material/05_dl/07_gans/lecture/#vanishing-gradients","title":"Vanishing Gradients","text":"<p>La ricerca (https://arxiv.org/pdf/1701.04862.pdf) ha suggerito che se il discriminatore \u00e8 troppo bbuono, allora il training del generatore pu\u00f2 fallire a causa del vanishing gradients (https://wikipedia.org/wiki/Vanishing_gradient_problem). In effetti, un discriminatore ottimale non fornisce abbastanza informazioni per far fare progressi al generatore.</p>"},{"location":"material/05_dl/07_gans/lecture/#prove-a-rimediare","title":"Prove a rimediare","text":"<ul> <li>Wasserstein loss: la Wasserstein loss \u00e8 progettata per prvenire i vanishing gradients anche quando addestriamo il discriminatore all'ottimalit\u00e0.</li> <li>Modified minimax loss: il paper originalke propone una modifica alla minimax loss per affrontare i vanishign gradients.</li> </ul>"},{"location":"material/05_dl/07_gans/lecture/#mode-collapse","title":"MOde Collapse","text":"<p>Normalmente vogliamo che il GAN produca un'ampia variet\u00e0 di output. Vogliamo, ad esempio, un diverso volto per ogni input al nostro generatore di volti.</p> <p>Tuttavia, se un generatore prduce un outptu specialmente plausibile, il generatore pu\u00f2 apprendere a produrre solo quell'output. Infatti, il generatore sta sempre provando a trovare l'output che sembra essere pi\u00f9 plausibile al discriminatore.</p> <p>Se il generaotre inizia a produrre lo stesso output (o un piccolo insieme di output) ancora ed ancora, la miglior strategia del discriminatore \u00e8 apprendere a respingere sempre quell'output. Ma se la prossiam generazione del discriminatore rimane fermo in un minimo locale e non trova la miglior starategia, allora \u00e8 pi\u00f9 facile per l'iterazione successiva del generatore per trovare l'output pi\u00f9 plausibile per il discriminatore attuale.</p> <p>Ogni iterazione del generatore sovra-ottimizza per un certo discriminatore, ed il dsicrimiantore non riesce mai ad apprendere il suo percorso al di fuori della trappola. Come risultato il generatore ruota attraverso un piccolo insieme di tipi di output. Questa forma di fallimento del GAN \u00e8 chiamato mode collapse.</p>"},{"location":"material/05_dl/07_gans/lecture/#tentativi-di-rimediare","title":"Tentativi di rimediare","text":"<p>I seguenti approcci provano a forzare il generatore ad ampliare il suo ambito prevenendo l'ottimizzazione di un singoli discriminatore prefissato:</p> <ul> <li>Wasserstein loss: la Wasserstein loss allevia il mode collapse permettendoci di trainare il discriminatore all'ottimalit\u00e0 senza preoccuparsi dei vanishing gradients. Se il discriminator non si ferma in un minimo locale, apprende a respingere gli outptu su cui si stabilizza il generatore. Per cui il generatore deve imparare qualcosa di nuovo.</li> <li>Unrolled GANs: le unrolled GANs (https://arxiv.org/pdf/1611.02163.pdf) usano una funzione di costo per il generatore che icnorpora non solo l'attuale classificazione del discrimiantor, ma anche l'output delle versioni future del discriminaotre. Per cui il genratore non pu\u00f2 sovraottimizzarsi su un singolo discriminator.</li> </ul>"},{"location":"material/05_dl/07_gans/lecture/#mancata-convergenza","title":"Mancata convergenza","text":"<p>Le GAN spesso onon convergono.</p>"},{"location":"material/05_dl/07_gans/lecture/#prove-a-rimediare_1","title":"PRove a rimediare","text":"<p>I ricercatori hanno provato ad usare varie forme di regolarizzazione per migliroare la convergenza delle GAN, inclusi:</p> <ul> <li>aggiungere rumore agli input del discriminator (https://arxiv.org/pdf/1701.04862.pdf)</li> <li>penalizzare i pesi dei discriminatori (https://arxiv.org/pdf/1705.09367.pdf)</li> </ul>"},{"location":"material/05_dl/07_gans/lecture/#variazioni-tra-gan","title":"Variazioni tra GAN","text":"<p>I ricercatori continuano a trovare delle tecniche GAN migliroate e nuovi usi per le GAN. Ecco un esempio di alcune delle variazioni per darci un'idea delle possibilit\u00e0.</p>"},{"location":"material/05_dl/07_gans/lecture/#progressive-gans","title":"Progressive GANs","text":"<p>In una progressive GAN, i primi layer del genrator producono immagini a risoluzione molto bassa, ed i conseguenti layer aggiungono dei dettagli. Questa tec ica permette alle GAN per addestrare pi\u00f9 velocemente rispetto alle GAN non progressive, e produce delle immagini a risoluzione pi\u00f9 alte. https://arxiv.org/abs/1710.10196</p>"},{"location":"material/05_dl/07_gans/lecture/#conditional-gans","title":"Conditional GANs","text":"<p>Le GAN condizionali addestrate su und ataset etichettato e ci permette di specificare le label per ogni istanza generata. Per esempio, una MNIST GAN non condizionale produrrebbe cifre casuali, mentre una MNIST GAN condizionale ci permetterebbe di specificare quale cifra la GAN deve generare.</p> <p>Invece di modellare la probabilit\u00e0 confiunta \\(P(X,Y)\\) la GAN condizionale modella la probabilit\u00e0 condizionale \\(P(X|Y)\\). https://arxiv.org/abs/1411.1784</p>"},{"location":"material/05_dl/07_gans/lecture/#image-to-image-translation","title":"Image-to-Image Translation","text":"<p>Le GAN che si occupano di Image-to-Image translation prendono un'immagine in input e la mappano ad un'immagine generata in output con propriet\u00e0 difefrenti. Ad esempio, possiamo prendere una maschera con dei blob di colore nella forma di un'auto, e la GAN pu\u00f2 riempire la forma con dei dettagli fotorealistici di un'auto.</p> <p>In modo simile, possiamo addestrare una GAN image-to-image a prendere degli schizzi di bagagli e tradurli in immagini fotorealistiche di bagagli.</p> <p>In questi casi, la loss \u00e8 una combinazione pesata della solita loss basata sul discriminatore e di una loss pixel-wise che penalizza il generatore per discorstarsi dall'immagine sorgente.</p> <p>Isola et al 2016</p>"},{"location":"material/05_dl/07_gans/lecture/#cyclegan","title":"CycleGAn","text":"<p>La CycleGAN apprende a trasformare immagini da un insieme in immagini che possono plausibilmente appartenere ad un altro insieme.</p> <p>I dati di addestramento per una CycleGAN sono semplicemente due insiemi di immagini. Il sistema non richiede alcuna label o corrispondenze a coppie tra immagini. Zhu et al., 2017</p>"},{"location":"material/05_dl/07_gans/lecture/#text-to_image-synthesis","title":"Text-to_image Synthesis","text":"<p>Le Text-to_image GAN prendono il testo come input e producono delle immagini che sono plausibili e descritte dal testo. Zhang et al, 2016.</p>"},{"location":"material/05_dl/07_gans/lecture/#super-resolution","title":"Super-resolution","text":"<p>Le super-resolution GAN aumentano la risoluzione delle immagini, aggiungendo dettagli dove necessario per riempire le aree sfocate. Ledig et al, 2017.</p>"},{"location":"material/05_dl/07_gans/lecture/#face-inpainting","title":"Face Inpainting","text":"<p>Le GAN sono usate per il task di image inpainitng semantico, nel quale parti delle immagini sono oscurate, ed il sistema prova a riempire le parti mancanti. Yeh et al, 2017</p>"},{"location":"material/05_dl/07_gans/lecture/#text-to-speech","title":"Text-to-Speech","text":"<p>Non tutte le GAN producono immagini. Ad esempio, alcuni ricercatori hanno anche usato delle GAN per produrre parlato sintetizzato dall'input testuale. Yang et al., 2017</p>"},{"location":"material/05_dl/07_gans/lecture/#conclusioni","title":"Conclusioni","text":"<p>Dovremmo essere in grado di:</p> <ul> <li>comprendere la differenza tra modelli generativi e discriminativi</li> <li>identificare i problemi che le GAN possono risolvere</li> <li>comprendere il ruolo del generatore e del discriminaotre in una GAN</li> <li>comprendere i vantaggi e svantaggi delle funzioni di costo in comune delle GAN</li> <li>identificare psosibili soluzioni a problemi comuni con l'addestamento delle GAN</li> <li>usare la libreria TF GAN per creare una GAN</li> </ul>"},{"location":"material/06_tflow/01_intro/01_intro/","title":"6.1.1 - Introduzione a TensorFlow","text":"<p>TensorFlow \u00e8 una delle librerie pi\u00f9 utilizzate per l'addestramento di modelli di reti neurali.</p> <p>TensorFlow basa il suo funzionamento sul concetto di tensore, che ricordiamo essere un concetto imputabile ad ogni generico array ad \\(n\\) dimensioni. Visto sotto questo punto di vista, TensorFlow potrebbe apparire simile a NumPy che, come ricordiamo, si occupa proprio di array \\(n\\)-dimensionali. Tuttavia, tra le librerie esistono diverse differenze: ad esempio, TensorFlow \u00e8 pensato esplicitamente per il calcolo differenziale, \u00e8 scalabile su architetture distribuite, ed \u00e8 intrinsecamente orientato al calcolo parallelo, traendo grande beneficio dalla presenza di GPU (Graphic Processing Unit) e TPU (Tensor Processing Unit).</p> <p>TensorFlow, GPU e TPU</p> <p>Allo stato attuale, con la versione 2.12.x di TensorFlow, le schede grafiche supportate (a meno di non utilizzare degli artifici) sono le NVIDIA, che mettono a disposizione i cosiddetti CUDA core e, nelle versioni pi\u00f9 recenti, anche delle TPU specifiche per l'elaborazione dei tensori.</p>"},{"location":"material/06_tflow/01_intro/01_intro/#tensorflow-e-keras","title":"TensorFlow e Keras","text":"<p>TensorFlow pu\u00f2 essere usato per operazioni su tensori di ogni tipo sui tensori: di conseguenza, \u00e8 possibile sfruttarlo anche per creare una rete neurale. Tuttavia, utilizzare direttamente TensorFlow pu\u00f2 risultare abbastanza ostico; di conseguenza, a partire dalla versione 2.0 del framework, \u00e8 stata integrata al suo interno la libreria Keras, il cui scopo \u00e8 quello di fornire un'interfaccia di programmazione (in inglese Application Programming Interface, o API) ad alto livello, che semplifica notevolmente la creazione di architetture per il deep learning. Vediamone alcune tra le pi\u00f9 importanti.</p>"},{"location":"material/06_tflow/01_intro/01_intro/#keras-api-sequential-e-functional","title":"Keras API: Sequential e Functional","text":"<p>Keras ci mette a disposizione due modi per creare un'architettura di rete neurale, definiti rispettivamente dalle API <code>Sequential</code> e <code>Functional</code>.</p> <p>La differenza tra le due API \u00e8 facilmente riassumibile: </p> <ul> <li>l'API <code>Sequential</code> offre un approccio interamente object-oriented alla creazione di architetture puramente sequenziali, il che la rente semplice da usare, ma limitata;</li> <li>l'API <code>Functional</code> offre un approccio puramente funzionale alla creazione dell'architettura, intrinsecamente pi\u00f9 complesso, ma anche in grado di offrire una maggiore flessibilit\u00e0, e non limitato ad architetture puramente sequenziali, in quanto ci permette di creare architetture rappresentabili secondo un grafo pi\u00f9 o meno complesso.</li> </ul> <p>Nel seguente snippet, possiamo vedere le due modalit\u00e0 a confronto.</p> SequentialFunctional <pre><code>model = Sequential()\nmodel.add(Input(shape=(16), dtype=\"float32\"))\nmodel.add(RegLin(32))\nmodel.add(Dropout(0.5))\nmodel.add(RegLin(10))\n</code></pre> <pre><code>inputs = Input(shape=(16,), dtype=\"float32\")\nx = RegLin(32)(inputs)\nx = Dropout(0.5)(x)\noutputs = RegLin(10)(x)\nmodel = Model(inputs, outputs)\n</code></pre> <p>In particolare:</p> <ul> <li>nell'approccio funzionale usiamo un oggetto di tipo <code>Input</code> che crea un tensore per descrivere la forma ed il tipo degli ingressi;</li> <li>nell'approccio sequenziale usiamo invece un <code>InputLayer</code> per aggiungere un layer di ingresso al modello;</li> <li>l'API <code>Sequential</code> utilizza il metodo <code>add()</code> parametrizzato con un layer per concatenare (agendo quindi in modo strettamente sequenziale) uno stack di layer;</li> <li>l'API <code>Functional</code> utilizza invece sfrutta la programmazione funzionale per \"chiamare\" i layer sullo stack precedente: ad esempio, l'istruzione <code>x = Dropout(0.5)(x)</code> \"chiama\" la classe <code>Dropout</code> sullo stack di layer costruito in precedenza;</li> <li>il modello funzionale deve essere definito usando la classe <code>Model</code> e specificandone input ed output.</li> </ul>"},{"location":"material/06_tflow/01_intro/01_intro/#model-training-api","title":"Model training API","text":"<p>L'addestramento di un modello con Keras prevede l'utilizzo di due metodi definiti dalla model training API. Vediamoli insieme.</p>"},{"location":"material/06_tflow/01_intro/01_intro/#il-metodo-compile","title":"Il metodo <code>compile()</code>","text":"<p>Il primo metodo da utilizzare \u00e8 <code>compile()</code>, che configura il modello con i parametri specificati. Ad esempio:</p> <pre><code>model.compile(\n    optimizer=SGD(),\n    loss=BinaryCrossentropy(),\n    metrics=[\n        BinaryAccuracy(),\n        Precision(),\n        Recall()\n    ])\n</code></pre> <p>I tre parametri che \u00e8 importante passare al metodo <code>compile()</code> sono:</p> <ul> <li><code>optimizer</code>, che specifica l'algoritmo di ottimizzazione da utilizzare per l'addestramento del modello;</li> <li><code>loss</code>, che specifica la funzione di costo usata per valutare le performance del modello;</li> <li><code>metrics</code>, che specifica quali metriche saranno mostrate nella valutazione del modello.</li> </ul> <p>Nel caso precedente, utilizziamo l'algoritmo SGD per l'ottimizzazione, la cross-entropy binaria come funzione di costo, e precisione, recall ed accuracy binaria come metriche da mostrare all'utente.</p>"},{"location":"material/06_tflow/01_intro/01_intro/#il-metodo-fit","title":"Il metodo <code>fit()</code>","text":"<p>Il metodo <code>compile()</code> non addestra la rete, limitandosi a configurarla. Per effettuare l'addestramento, dobbiamo usare il metodo <code>fit()</code>. Ad esempio:</p> <pre><code>history = model.fit(\n    x=X,\n    y=y,\n    batch_size=8,\n    epochs=10,\n    validation_split=0.3)\n</code></pre> <p>In particolare, i parametri che stiamo usando sono:</p> <ul> <li><code>X</code> ed <code>y</code>, rappresentativi della matrice di design e del vettore delle label, che possono essere (tra gli altri) degli array NumPy;</li> <li><code>batch_size</code>, che indica quanti campioni verranno elaborati ad ogni singolo passaggio dal modello;</li> <li><code>epochs</code>, che indica quante iterazioni di addestramento dovranno essere effettuate dalla rete;</li> <li><code>validation_split</code>, che indica la percentuale dei dati che saranno usati per la validazione.</li> </ul> <p>Callback e dati</p> <p>Il metodo <code>fit()</code> offre molti altri interessanti parametri, alcuni dei quali approfondiremo nelle lezioni successive.</p>"},{"location":"material/06_tflow/01_intro/02_data/","title":"6.1.2 - Dataset","text":"<p>Nella lezione precedente abbiamo visto come il metodo <code>fit()</code> accetti i dati sotto forma di array NumPy. Tuttavia, nel momento in cui si ha a che fare con dataset di grosse dimensioni, potrebbe essere necessario usare oggetti specifici appartenenti alla classe <code>Dataset</code>.</p> <p>Per farlo, Keras ci mette a disposizione delle tecniche che ci permettono di creare un <code>Dataset</code> a partire dai nostri dati; vediamone alcune. </p>"},{"location":"material/06_tflow/01_intro/02_data/#dataset-di-immagini","title":"Dataset di immagini","text":"<p>Possiamo creare un dataset a partire da una cartella usando la funzione <code>image_dataset_from_directory</code>.</p> <p>Supponiamo per prima cosa che la cartella <code>data_dir</code> sia organizzata come segue:</p> <pre><code>data_dir/\n...class1/\n......1.png\n......2.png\n...class2/\n......1.png\n......2.png\n......3.png\n</code></pre> <p>In pratica, i nostri dati sono organizzati in una cartella \"madre\", all'interno della quale ci sono tante sottocartelle quante sono le classi del nostro problema, ognuna delle quali conterr\u00e0 a sua volta tutte le immagini per quella specifica classe.</p> <p>Usiamo adesso la funzione <code>image_dataset_from_directory</code> per creare il nostro dataset:</p> <pre><code>ds = tf.keras.utils.image_dataset_from_directory(\n    data_dir,\n    image_size=(32, 32),\n    batch_size=32)\n</code></pre> <p>Nel precedente esempio:</p> <ul> <li>alla riga 2, specifichiamo la cartella dove sono contenuti i dati;</li> <li>alla riga 3, specifichiamo il parametro <code>image_size</code> sotto forma di tupla, nella quale il primo elemento \u00e8 l'altezza dell'immagine, mentre il secondo \u00e8 la larghezza (nel nostro caso, siamo di fronte ad una \\(32 \\times 32\\));</li> <li>alla riga 4, specifichiamo il parametro <code>batch_size</code>, utile in fase di addestramento del modello.</li> </ul> <p>I pi\u00f9 attenti si chiederanno se sia possibile caricare il dataset in modo da suddividere automaticamente i dati in insieme di training e di validazione. Ci\u00f2 \u00e8 possibile modificando le precedenti istruzioni come segue:</p> <pre><code>train_ds = tf.keras.utils.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset='training',\n    image_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed=seed)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset='validation',\n    image_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed=seed)\n</code></pre> <p>In particolare:</p> <ul> <li>il parametro <code>validation_split</code>, che deve essere coerente in entrambi i dataset, indica quanti dati usare per la validazione;</li> <li>il parametro <code>subset</code> indica se il dataset \u00e8 indirizzato al training o alla validazione;</li> <li>il parametro <code>seed</code>, che deve essere coerente in entrambi i dataset, fa in modo che i dataset siano generati casualmente in modo consistente.</li> </ul> <p>A questo punto possiamo passare <code>train_ds</code> e <code>val_ds</code> direttamente al metodo <code>fit()</code> del nostro modello usando il parametro <code>validation_data</code>:</p> <pre><code>model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=10)\n</code></pre>"},{"location":"material/06_tflow/01_intro/02_data/#dataset-di-testo","title":"Dataset di testo","text":"<p>Keras offre un metodo simile per creare un dataset a partire da un insieme di file di testo, utilizzando il metodo <code>text_dataset_from_directory</code>.</p> <p>Analogamente al metodo usato per le immagini, <code>text_dataset_from_directory</code> si aspetta una cartella in una certa forma:</p> <pre><code>data_dir/\n...class1/\n......1.txt\n......2.txt\n...class2/\n......1.txt\n......2.txt\n......3.txt\n</code></pre> <p>Per caricare il nostro dataset possiamo usare una forma analoga a quella usata per il dataset di immagini:</p> <pre><code>train = text_dataset_from_direcotry(\n    data_dir,\n    batch_size=batch_size,\n    validation_split=0.2,\n    subset='training',\n    seed=seed)\n\nval = text_dataset_from_directory(\n    data_dir,\n    batch_size=batch_size,\n    validation_split=0.2,\n    subset='validation',\n    seed=seed)\n</code></pre> <p>I parametri sono esattamente gli stessi, a meno dell'assenza del parametro <code>image_size</code>.</p>"},{"location":"material/06_tflow/01_intro/02_data/#preparazione-dei-dati-testuali","title":"Preparazione dei dati testuali","text":"<p>Rispetto alle immagini, i dati testuali richiedono tre ulteriori operazioni, ovvero:</p> <ul> <li>standardizzazione: si tratta di una procedura di preprocessing sul testo, che consiste tipicamente nella rimozione della punteggiatura. Di default, questa operazione converte l'intero testo in minuscolo e rimuove la punteggiatura;</li> <li>tokenizzazione: si tratta di una procedura di suddivisione delle stringhe in token. Ad esempio, si pu\u00f2 suddividere una frase nelle singole parole. Di default, questa operazione suddivide i token in base allo spazio;</li> <li>vettorizzazione: si tratta della procedura di conversione dei token in valori numerici trattabili da un modello di rete neurale. Di default, il metodo di vettorizzazione \u00e8 <code>int</code>.</li> </ul> <p>Questi tre step sono gestiti in automatico da un layer chiamato <code>TextVectorization</code>.</p> <p>Procediamo quindi a creare un layer di TextVectorization utilizzando una vettorizzazione binaria:</p> <pre><code>vectorize_layer = TextVectorization(\n    max_tokens=VOCAB_SIZE,\n    output_mode='binary')\n</code></pre> <p>In particolare, <code>max_tokens</code> permette di stabilire il numero massimo di vocaboli consentiti, mentre l'<code>output_mode</code> indica la modalit\u00e0 con cui sar\u00e0 gestita la sequenza vettorizzat.</p> <p>A questo punto, occorre creare il dataset che sar\u00e0 effettivamente passato al primo layer della rete neurale. In tal senso, dobbiamo tenere conto che i dataset che abbiamo creato mediante <code>text_dataset_from_directory</code> non sono ancora stati vettorizzati, ed inoltre le singole coppie campione/label non sono accessibili mediante le tecniche standard di indicizzazione. Ci\u00f2 \u00e8 legato al fatto che le funzioni <code>*_dataset_from_directory</code> creano un oggetto di tipo <code>BatchDataset</code>, usato da TensorFlow per ottimizzare il caricamento in memoria di dataset di grosse dimensioni.</p> <p>Di conseguenza, dovremo innanzitutto estrarre il testo senza considerare le singole label. Per farlo, possiamo usare la funzione <code>map()</code> del nostro dataset:</p> <pre><code>train_text = train.map(lambda text, labels: text)\n</code></pre> <p>La funzione <code>map()</code> non fa altro che applicare all'intero iterabile la funzione passata come argomento. In tal senso, il parametro passato altro non \u00e8 se non una lambda function, ovvero una funzione anonima che assume una forma sintattica del tipo:</p> <pre><code>lambda args : expression\n</code></pre> <p>e quindi applica l'espressione a valle dei <code>:</code> agli argomenti passati. In questo caso, stiamo semplicemente facendo in modo che tutte le coppie testo/label siano \"mappate\" sul semplice testo.</p> <p>Una volta estratto il testo, dovremo chiamare il metodo <code>adapt</code> del layer di vettorizzazione in modo tale da creare il vocabolario che associ un determinato token numerico a ciascuna stringa.</p> <pre><code>vectorize_layer.adapt(train_text)\n</code></pre> <p>Potremo quindi procedere ad integrare il layer di vettorizzazione all'interno del nostro modello.</p> <p>In tal senso, dovremo assicurarci che il modello abbia un input di forma <code>(1,)</code> e tipo stringa, facendo in modo che la rete abbia un'unica stringa in input per ciascun batch:</p> <pre><code>model.add(\n    keras.Input(shape=(1,),\n    dtype=tf.string))\n</code></pre>"},{"location":"material/06_tflow/01_intro/02_data/#dataset-da-array-numpy","title":"Dataset da array NumPy","text":"<p>Nel caso di un array NumPy, occorre utilizzare il metodo <code>from_tensor_slices</code>:</p> <pre><code>train = from_tensor_slices((x_train, y_train))\nval = from_tensor_slices((x_test, y_test))\n</code></pre>"},{"location":"material/06_tflow/01_intro/03_callbacks/","title":"6.1.3 - Callbacks","text":"<p>Un callback \u00e8 una specifica azione che il modello pu\u00f2 effettuare durante il training. Keras ne offre di numerosi, utilizzati (ad esempio) per monitorare le metriche che abbiamo scelto per la valutazione del modello, o salvare lo stesso su disco.</p> <p>Per usare uno o pi\u00f9 callback, dovremo creare un'apposita lista da passare al parametro omonimo (<code>callbacks</code>) nel metodo <code>fit()</code> del nostro modello.</p>"},{"location":"material/06_tflow/01_intro/03_callbacks/#salvataggio-dei-pesi-del-modello","title":"Salvataggio dei pesi del modello","text":"<p>Come primo esempio, creiamo un callback che salvi i pesi del modello con una certa frequenza. Per farlo, useremo un oggetto di tipo <code>ModelCheckpoint</code>:</p> <pre><code>mc_callback = keras.callbacks.ModelCheckpoint(\n    filepath=path_to_checkpoints,\n    save_weights_only=True,\n    monitor='val_acc',\n    save_best_only=True)\n</code></pre> <p>In particolare:</p> <ul> <li><code>filepath</code> indica il percorso del file nel quale salveremo i checkpoint;</li> <li><code>save_weights_only</code> istruisce Keras a salvare soltanto i pesi del modello, riducendo lo spazio occupato in memoria;</li> <li><code>monitor</code> indica la metrica da monitorare;</li> <li><code>save_best_only</code> istruisce Keras a salvare soltanto il modello \"migliore\", trascurando quelli ottenuti durante le altre iterazioni.</li> </ul>"},{"location":"material/06_tflow/01_intro/03_callbacks/#evitare-loverfitting","title":"Evitare l'overfitting","text":"<p>Un altro callback interessante \u00e8 quello che permette di interrompere l'addestramento della rete quando una o pi\u00f9 metriche non migliorano per un certo intervallo di epoche. Per farlo, useremo un oggetto di tipo <code>EarlyStopping</code>, che ci permette di terminare l'addestramento qualora la metrica monitorata non presenti miglioramenti tra un'epoca e l'altra. Ad esempio:</p> <pre><code>es_callback = keras.callbacks.EarlyStopping(\n    monitor='val_acc',\n    min_delta=0.1,\n    patience=3,\n    restore_best_weights=True)\n</code></pre> <p>In particolare:</p> <ul> <li><code>monitor</code> indica la metrica da monitorare;</li> <li><code>min_delta</code> indica il valore minimo da considerare migliorativo per la metrica;</li> <li><code>patience</code> indica il numero di epoche dopo il quale il training viene interrotto in assenza di miglioramenti;</li> <li><code>restore_best_weights</code> indica se ripristinare i valori migliori ottenuti per i parametri dopo il termine dell'addestramento, o se usare gli ultimi.</li> </ul>"},{"location":"material/06_tflow/01_intro/03_callbacks/#visualizzazione-dei-risultati-delladdestramento","title":"Visualizzazione dei risultati dell'addestramento","text":"<p>Aggiungiamo infine un ultimo callback, da utilizzare per permettere di visualizzare i risultati del nostro training su un tool di visualizzazione chiamato TensorBoard.</p> <pre><code>tb_callback = TensorBoard()\n</code></pre> <p>Per TensorBoard, possiamo lasciare i parametri al loro valore di default. La reference completa \u00e8 comunque disponibile sulla documentazione ufficiale.</p>"},{"location":"material/06_tflow/01_intro/03_callbacks/#mettiamo-i-pezzi-insieme","title":"Mettiamo i pezzi insieme...","text":"<p>Possiamo adesso specificare i callback da utilizzare passando le precedenti variabili sotto forma di lista al metodo <code>fit()</code> del nostro modello.</p> <pre><code>callbacks = [mc_callback, es_callback, tb_callback]\n\nmodel.fit(\n    train_ds,\n    validation_data=val_ds,\n    callbacks=callbacks)\n</code></pre>"},{"location":"material/06_tflow/01_intro/04_tl_ft/","title":"6.1.4 - Transfer learning e fine tuning","text":"<p>Le reti neurali, e soprattutto le CNN, presentano un'interessante caratteristica: infatti, apprendono delle feature generiche nei primi strati, specializzandosi sullo specifico problema man mano che si va in profondit\u00e0 nella rete.</p> <p>Partendo da questa considerazione \u00e8 stata elaborata la tecnica del transfer learning, che consiste nel prendere il modello addestrato su un problema, e riconfigurarlo per risolverne uno nuovo ma, ovviamente, simile (ad esempio, un tool che permette di valutare la razza di un gatto pu\u00f2 essere usato mediante transfer learning per distinguere tra leopardi e tigri). Questa tecnica permette anche di addestrare un numero limitato di parametri, per cui \u00e8 possibile utilizzarla quando si ha a che fare con dataset di dimensioni limitate.</p> <p>In tal senso, il transfer learning segue di solito questi step:</p> <ul> <li>consideriamo i layer ed i pesi di un modello addestrato in precedenza;</li> <li>effettuiamo il freezing (congelamento) di questi layer, fissando i valori dei pesi;</li> <li>creiamo ed inseriamo alcuni layer successivi a quelli congelati per adattarli al nostro problema;</li> <li>addestriamo i nuovi layer sul nostro problema.</li> </ul> <p>Opzionalmente, \u00e8 possibile effettuare un passaggio di fine tuning, \"sbloccando\" il modello ottenuto in precedenza e riaddestrandolo sull'intero dataset con un basso learning rate.</p>"},{"location":"material/06_tflow/01_intro/05_dropout/","title":"6.1.5 - Regolarizzazione in TensorFlow","text":"<p>Abbiamo gi\u00e0 visto i concetti alla base della regolarizzazione in una delle lezioni precedenti, ma pu\u00f2 comunque essere utile richiamarli brevemente.</p> <p>L'uso della regolarizzazione pu\u00f2 essere spiegato con il concetto di rasoio di Occam, per il quale, date due possibili spiegazioni per lo stesso fenomeno, quella che lo descrive meglio \u00e8 molto probabilmente la pi\u00f9 semplice (o, pi\u00f9 formalmente, quella che assume il minor numero di ipotesi).</p> <p>Questo concetto si applica anche ai modelli di machine e deep learning: data (ad esempio) un'architettura di reti neurali, esistono diverse combinazioni di pesi che possono spiegare i dati ma, in linea generale, le combinazioni pi\u00f9 semplici sono meno soggette all'overfitting se comparate a quelle pi\u00f9 complesse.</p> <p>Dalla precedente affermazione discende che un modo comune di mitigare l'overfitting del modello ai dati \u00e8 inserire degli opportuni vincoli sulla complessit\u00e0 della rete, \"forzando\" i pesi ad assumere valori ridotti, e rendendo implicitamente la distribuzione di detti valori maggiormente uniforme. Questo procedimento, chiamato weight regularization, \u00e8 ottenuto aggiungendo alla funzione di costo della rete un termine direttamente proporzionale al valore del peso. Di solito, si utilizzano due tecniche di regolarizzazione:</p> <ul> <li>nella regolarizzazione L1 il costo aggiunto \u00e8 proporzionale al valore assoluto dei coefficienti dei pesi, ovvero alla norma \\(L^1\\);</li> <li>nella regolarizzazione L2 il costo aggiunto \u00e8 proporzionale al quadrato del valore dei coefficienti dei pesi, ovvero alla norma \\(L^2\\). </li> </ul> <p>Nota</p> <p>In generale, la regolarizzazione L1 favorisce la \"sparsit\u00e0\" dei dati, forzando il valore di alcuni pesi a \\(0\\). Ci\u00f2 non avviene con la regolarizzazione L2.</p> <p>In Keras, possiamo aggiungere un parametro di regolarizzazione usando il package <code>regularizers</code> ed il parametro <code>kernel_regularizers</code> del layer da regolarizzare:</p> <pre><code>from keras import regularizers\n\nlayers.Dense(\n    64,\n    activation='relu',\n    kernel_regularizers=regularizers.l2(0.001))\n</code></pre> <p>In questo caso, stiamo usando un valore di regolarizzazione pari a \\(0.001\\), il che significa che ogni peso del layer regolarizzato aggiunger\u00e0 un valore pari a \\(0.001 \\cdot w_i^2\\) al costo totale della rete, con \\(w_i\\) valore del peso dell'\\(i\\)-mo coefficiente.</p>"},{"location":"material/06_tflow/01_intro/05_dropout/#dropout","title":"Dropout","text":"<p>Un'altra tecnica di regolarizzazione molto diffusa nelle reti neurali \u00e8 legata all'uso di un layer di dropout.</p> <p>L'idea alla base del dropout sta nel fatto che ogni nodo della rete deve restituire in output delle feature utili a prescindere da quelle restituite dagli altri nodi. Per far ci\u00f2, si fa in modo che un certo numero di neuroni (scelti in maniera casuale ad ogni iterazione) dello strato precedente venga ignorato dal layer che implementa il dropout.</p> <p>In questo modo, il layer dovr\u00e0 cambiare ad ogni iterazione la sua connettivit\u00e0, ottenendo in un certo senso un diverso \"punto di vista\" sui dati stessi. Si pu\u00f2 quindi dire che il dropout in un certo senso aggiunga del \"rumore\" al processo di apprendimento, forzando le connessioni a modificare la loro importanza a seconda dei nodi scartati, ed evitando quindi situazioni dove i layer di rete tendono ad adattarsi vicendevolmente per \"correggere\" gli errori di predizione. Di conseguenza, dato che ogni neurone isoler\u00e0 delle feature in maniera indipendente dagli altri, il modello acquisisce maggiore capacit\u00e0 di generalizzazione.</p> <p>Per quello che riguarda gli iperparametri usati dal layer di dropout, il pi\u00f9 importante \u00e8 quello che specifica la probabilit\u00e0 con la quale gli output dello strato precedente vengono scartati. Un valore comune in tal senso \u00e8 \\(0.5\\) per gli strati nascosti, e \\(0.8\\) per lo strato di input.</p>"},{"location":"material/06_tflow/01_intro/05_model_api/","title":"6.1.2 - Le subclassing API","text":"<p>Una delle principali astrazioni offerte da Keras \u00e8 la classe <code>Layer</code>. Questa classe incapsula sia uno stato (i pesi del layer) ed una trasformazione dagli input agli output.</p> <p>Un esempio \u00e8 il seguente:</p> <pre><code>class Linear(keras.layers.Layer):\n    def __init__(self, units=32, input_dim=32):\n        super(Linear, self).__init__()\n        w_init = tf.random_normal_initializer()\n        self.w = tf.Variable(\n            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n            trainable=True,\n        )\n        b_init = tf.zeros_initializer()\n        self.b = tf.Variable(\n            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n</code></pre> <p>Useremo un layer chiamandolo su alcuni input di forma tensorliae, come una funzione Python:</p> <pre><code>x = tf.ones((2, 2))\nlinear_layer = Linear(4, 2)\ny = linear_layer(x)\nprint(x)\n</code></pre> <p>Notiamo che i pesi <code>w</code> ed i bias <code>b</code> sono automaticamente tracciati dal layer.</p> <p>Notiamo anche che abbiamo una scorciatoia per aggiungere dei pesi ad un layer mediante il metodo <code>add_weight()</code>:</p> <pre><code>class Linear(keras.layers.Layer):\n    def __init__(self, units=32, input_dim=32):\n        super(Linear, self).__init__()\n        self.w = self.add_weight(\n            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n        )\n        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n\n\nx = tf.ones((2, 2))\nlinear_layer = Linear(4, 2)\ny = linear_layer(x)\nprint(y)\n</code></pre> <p>I layer possono avere anche dei pesi non addestrabili. Qeusti pesi non vengono considerati durante la backgpropagation:</p> <pre><code>class ComputeSum(keras.layers.Layer):\n    def __init__(self, input_dim):\n        super(ComputeSum, self).__init__()\n        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n\n    def call(self, inputs):\n        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n        return self.total\n\n\nx = tf.ones((2, 2))\nmy_sum = ComputeSum(2)\ny = my_sum(x)\nprint(y.numpy())\ny = my_sum(x)\nprint(y.numpy())\n</code></pre> <p>Possiamo anche evitare di specificare in anticipo la dimensione dei nostri ingressi, in quanto potremmo creare dei pesi in maniera lazy quando questo valore viene conosciuto, dopo aver istanziato il layer. Nell'API di Keras, possiamo creare i pesi nel metodo build(self, inputs_shape):</p> <p>class Linear(keras.layers.Layer):     def init(self, units=32):         super(Linear, self).init()         self.units = units</p> <pre><code>def build(self, input_shape):\n    self.w = self.add_weight(\n        shape=(input_shape[-1], self.units),\n        initializer=\"random_normal\",\n        trainable=True,\n    )\n    self.b = self.add_weight(\n        shape=(self.units,), initializer=\"random_normal\", trainable=True\n    )\n\ndef call(self, inputs):\n    return tf.matmul(inputs, self.w) + self.b\n</code></pre> <p>Il metodo cal del layer chiamer\u00e0 in automatico il build la prima volta che viene chiamato. Possiamo avere un lazy layer, che di conseguenza \u00e8 pi\u00f9 semplice da usare.</p> <p>I layer sono cmponibili. Se assegnamo un'istanza di un Layer ad un altro Layer, quello esterno traccer\u00e0 i pesi creati dal layer interno.</p> <p>class MLPBlock(keras.layers.Layer):     def init(self):         super(MLPBlock, self).init()         self.linear1 = Linear(32)         self.linear2 = Linear(32)         self.linear_3 = Linear(1)</p> <pre><code>def call(self, inputs):\n    x = self.linear_1(inputs)\n    x = tf.nn.relu(x)\n    x = self.linear_2(x)\n    x = tf.nn.relu(x)\n    return self.linear_3(x)\n</code></pre> <p>mlp = MLPBlock() y = mlp(tf.ones(shape=(3, 64)))  # The first call to the <code>mlp</code> will create the weights print(\"weights:\", len(mlp.weights)) print(\"trainable weights:\", len(mlp.trainable_weights))</p> <p>Quando scriviamo il metodo call() per un layer possiamo crare un tensore di loss che vogliamo usare dopo, quando scriviamo il nostro loop di training. Per farlo, chiamiamo self.add_loss(value):</p>"},{"location":"material/06_tflow/01_intro/05_model_api/#a-layer-that-creates-an-activity-regularization-loss","title":"A layer that creates an activity regularization loss","text":"<p>class ActivityRegularizationLayer(keras.layers.Layer):     def init(self, rate=1e-2):         super(ActivityRegularizationLayer, self).init()         self.rate = rate</p> <pre><code>def call(self, inputs):\n    self.add_loss(self.rate * tf.reduce_sum(inputs))\n    return inputs\n</code></pre> <p>In modo simile ad addloss(), un layer pu\u00f2 anche avere un metodo addmetric() per tracciare la media mobile di una quantit\u00e0 durante l'addestramento.</p>"},{"location":"material/06_tflow/01_intro/05_model_api/#class-model","title":"class model","text":"<p>In generale, useremo la classe Layer per definrie dei blocchi di calcolo itnerno, ed useremo la classe Model per definire il modello complessivo dir ete, che sar\u00e0 quello che addestreremo. Ad esempio, in un mdoello ResNet50, abbiamo diversi blocchi ResNet che fannod elle sottoclassi di Layer, ed un singolo Model che contiene l'intera archietttura di rete.</p> <p>La classe Model ha la stessa API di Layer, con alcune differenze:</p> <ul> <li>espone i modelli fit(), evaluate() e predict() per i loop di training, valutazione e predizione, rispettivamente</li> <li>espone la lista di layer interni mediante la propriet\u00e0 models.layers</li> <li>espone le API di salvataggio e serializzazione del modello</li> </ul> <p>In efeftti, la classe Layer corrispodne a quello che in letteratura sarebbe un laeyr o blocco (ad esempio, un Inception block). D'altro canto, la classe Modle corrispone a quello che \u00e8 definto come modello o rete.</p>"},{"location":"material/06_tflow/01_intro/06_dropout/","title":"26 - Overfitting e regolarizzazione","text":"<p>Nella lezione precedente abbiamo limitato il training delle reti neurali a 10 epoche, usando come metrica di test esclusivamente l'accuratezza sui dati di training.</p> <p>Tuttavia, se proseguissimo nel training e valutassimo anche l'accuratezza sui dati di validazione, noteremmo che ad una certa epoca questa raggiunger\u00e0 un picco, per poi iniziare a stagnare o, in alcuni casi, diminuire: in altre parole, il nostro modello andr\u00e0 incontro ad overfitting. Gestire correttamente questa situazione \u00e8 molto importante: infatti, ottenere un'elevata accuratezza sui dati di training non \u00e8 importante quanto sviluppare un modello in grado di generalizzare su dati che non ha visto durante l'addestramento.</p> <p>L'opposto dell'overfitting \u00e8, abbastanza prevedibilmente, l'underfitting, situazione che si verifica quando vi \u00e8 ancora la possibilit\u00e0 di migliorare il modello. Di solito, l'underfitting si verifica nel momento in cui un modello non \u00e8 abbastanza descrittivo, oppure quando non \u00e8 stato addestrato per un numero di epoche sufficienti, non permettendo alla rete di caratterizzare i pattern di rilievo presenti nei dati.</p> <p>Ovviamente, trovare i parametri ottimali di addestramento significa trovare un equilibrio tra overfitting ed underfitting. In primis, infatti, \u00e8 necessario scegliere accuratamente il numero di epoche di addestramento, per evitare una scarsa (o al contrario eccessiva) adesione del modello ai dati. Inoltre, \u00e8 necessario verificare che i dati di training utilizzati siano adeguati, seguendo magari le indicazioni date in precedenza; infine, qualora questi passi siano gi\u00e0 stati compiuti, pu\u00f2 essere necessario utilizzare delle tecniche di regolarizzazione.</p>"},{"location":"material/06_tflow/01_intro/06_dropout/#261-strategie-di-regolarizzazione","title":"26.1 - Strategie di regolarizzazione","text":"<p>Abbiamo gi\u00e0 discusso delle strategie di regolarizzazione quando abbiamo visto la regressione logistica.</p> <p>In pratica, per comprendere il motivo per cui si usa la regolarizzazione possiamo usare il concetto di rasoio di Occam, ovvero, date due possibili spiegazioni per lo stesso fenomeno, quella che con tutta probabilit\u00e0 lo descrive in maniera migliore \u00e8 anche la pi\u00f9 semplice o, in altre parole, quella che assume il minor numero di ipotesi.</p> <p>Questo concetto si applica anche ad un modello di rete neurale: data un'architettura, esistono diverse combinazioni di valori di pesi che possono spiegare i dati, ed in generale le combinazioni pi\u00f9 semplici corrono meno il rischio di andare in overfitting se comparati a quelli pi\u00f9 complessi.</p> <p>Dalla precedente affermazione discende che un modo comune di mitigare l'overfitting del modello ai dati \u00e8 inserire degli opportuni vincoli sulla complessit\u00e0 della rete, \"forzando\" i pesi ad assumere valori ridotti, e rendendo implicitamente la distribuzione di detti valori maggiormente uniforme. Questo procedimento, chiamato weight regularization, \u00e8 ottenuto aggiungendo alla funzione di costo della rete un termine direttamente proporzionale al valore del peso. Di solito, si utilizzano due tecniche di regolarizzazione:</p> <ul> <li>nella regolarizzazione L1 il costo aggiunto \u00e8 proporzionale al valore assoluto dei coefficienti dei pesi, ovvero alla norma \\(L^1\\);</li> <li>nella regolarizzazione L2 il costo aggiunto \u00e8 proporzionale al quadrato del valore dei coefficienti dei pesi, ovvero alla norma \\(L^2\\). </li> </ul> <p>Nota</p> <p>In generale, la regolarizzazione L1 favorisce la \"sparsit\u00e0\" dei dati, forzando il valore di alcuni pesi a \\(0\\). Ci\u00f2 non avviene con la regolarizzazione L2.</p> <p>In Keras, possiamo aggiungere un parametro di regolarizzazione usando il package <code>regularizers</code> ed il parametro <code>kernel_regularizers</code> del layer da regolarizzare:</p> <pre><code>from keras import regularizers\n\nlayers.Dense(\n    64,\n    activation='relu',\n    kernel_regularizers=regularizers.l2(0.001))\n</code></pre> <p>In questo caso, stiamo usando un valore di regolarizzazione pari a \\(0.001\\), il che significa che ogni peso del layer regolarizzato aggiunger\u00e0 un valore pari a \\(0.001 \\cdot w_i^2\\) al costo totale della rete, con \\(w_i\\) valore del peso dell'\\(i\\)-mo coefficiente.</p>"},{"location":"material/06_tflow/01_intro/06_dropout/#262-dropout","title":"26.2 - Dropout","text":"<p>Un altro metodo di regolarizzazione molto diffuso \u00e8 dato dall'uso di uno strato di dropout.</p> <p>L'idea alla base del dropout sta nel fatto che ogni nodo della rete deve restituire in output delle feature utili a prescindere da quelle restituite dagli altri nodi. Per ottenere questo risultato, si fa in modo che un certo numero di neuroni (scelti in maniera casuale ad ogni iterazione) dello strato precedente venga ignorato dal layer che implementa il dropout.</p> <p>In questo modo, il layer modifica ad ogni iterazione la sua connettivit\u00e0, ottenendo in un certo senso un diverso \"punto di vista\" sui dati stessi: in tal senso, il dropout aggiunge in maniera artificiosa del rumore sul processo di apprendimento, forzando una maggiore o minore importanza delle connessioni a seconda dei nodi scartati, ed evitando quindi delle situazioni dove i layer di rete tendono ad adattarsi vicendevolmente per \"correggere\" gli errori di predizione. Di conseguenza, il modello acquisisce maggiore capacit\u00e0 di generalizzazione, visto e considerato che ogni neurone isoler\u00e0 delle feature in maniera indipendente dagli altri.</p> <p>Per quello che riguarda gli iperparametri usati dal layer di dropout, il pi\u00f9 importante \u00e8 quello che specifica la probabilit\u00e0 con la quale gli output dello strato precedente vengono scartati. Un valore comune in tal senso \u00e8 \\(0.5\\) per gli strati nascosti, e \\(0.8\\) per lo strato di input.</p>"},{"location":"material/06_tflow/01_intro/06_model_api/","title":"6.1.6 - <code>Layer</code> e <code>Model</code>: un approfondimento","text":""},{"location":"material/06_tflow/01_intro/06_model_api/#la-classe-layer","title":"La classe <code>Layer</code>","text":"<p>L'astrazione fondamentale alla base di Keras \u00e8 la classe <code>Layer</code>, che rappresenta lo stato di un layer di una rete neurale (ovvero, i suoi pesi), oltre che i metodi che permettono di mappare l'input del layer verso l'output, questi ultimi contenuti nel metodo <code>call</code>. Il seguente snippet illustra un esempio di layer Keras:</p> <pre><code>class Linear(tf.keras.layersLayer):\n\n    def __init__(\n        self,\n        units=32,\n        input_dim=32):\n        super().__init__()\n        w_init = tf.random_normal_initializer()\n        self.w = tf.Variable(\n            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n            trainable=True)\n        b_init = tf.zeros_initializer()\n        self.b = tf.Variable(\n            initial_value=b_init(shape=(units,), dtype=\"float32\"),\n            trainable=True)\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n</code></pre> <p>In particolare:</p> <ul> <li>gli attributi di ogni istanza del layer <code>Linear</code> saranno il tensore dei pesi <code>w</code> e quello dei bias <code>b</code>;</li> <li>nel metodo <code>__init__</code>, alla riga 8, usiamo il metodo <code>random_normal_initializer()</code> di TensorFlow per creare un inizializzatore del tensore dei pesi \\(w\\) in modo che detti pesi siano estratti casualmente da una distribuzione normale;</li> <li>i pesi sono inizializzati sotto forma di una <code>Variable</code> TensorFlow usando l'inizializzatore definito in precedenza, ed impostando una dimensione <code>input_dim</code> \\(\\times\\) <code>units</code>, con <code>input_dim</code> dimensione del vettore di ingresso, ed <code>units</code> numero di unit\u00e0. L'attributo <code>trainable</code> assicura infine che sia possibile modificare questi pesi a valle di una procedura di addestramento;</li> <li>in modo simile, alla riga 12 creiamo un inizializzatore del tensore degli offset \\(b\\) sfruttando il metodo <code>zeros_initializer</code>, che verr\u00e0 inizializzato in modo simile al tensore dei pesi \\(w\\);</li> <li>alle righe 17-18, infine, definiamo il metodo <code>call</code>, che descrive il rapporto tra ingresso ed uscita del layer.</li> </ul>"},{"location":"material/06_tflow/01_intro/06_model_api/#aggiunga-dei-pesi-ad-un-layer","title":"Aggiunga dei pesi ad un layer","text":"<p>La procedura vista in precedenza per l'aggiunta dei pesi e del bias al nostro layer pu\u00f2 essere notevolmente semplificata mediante il metodo <code>add_weight()</code>:</p> <pre><code>class Linear(tf.keras.layers.Layer):\n    def __init__(self, units=32, input_dim=32):\n        super(Linear, self).__init__()\n        self.w = self.add_weight(\n            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n        )\n        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n</code></pre>"},{"location":"material/06_tflow/01_intro/06_model_api/#layer-con-pesi-non-addestrabili","title":"Layer con pesi non addestrabili","text":"<p>Un layer pu\u00f2 avere anche dei pesi non addestrabili. In tal senso, si pu\u00f2 impostare a <code>False</code> il parametro <code>trainable</code>.</p> <pre><code>class ComputeSum(tf.keras.layers.Layer):\n    def __init__(self, input_dim):\n        super(ComputeSum, self).__init__()\n        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n\n    def call(self, inputs):\n        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n        return self.total\n</code></pre>"},{"location":"material/06_tflow/01_intro/06_model_api/#omettere-la-dimensione-dellinput","title":"Omettere la dimensione dell'input","text":"<p>Keras ci permette di omettere la dimensione dell'input, creandola in maniera lazy al momento dell'istanziazione del layer nel nsotro modello. Per farlo, Keras ci mette a disposizione il metodo <code>build()</code>:</p> <pre><code>class Linear(tf.keras.layers.Layer):\n    def __init__(self, units=32):\n        super(Linear, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.w = self.add_weight(\n            shape=(input_shape[-1], self.units),\n            initializer=\"random_normal\",\n            trainable=True,\n        )\n        self.b = self.add_weight(\n            shape=(self.units,), initializer=\"random_normal\", trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n</code></pre>"},{"location":"material/06_tflow/01_intro/06_model_api/#stack-di-layer","title":"Stack di layer","text":"<p>I layer sono componibili; ci\u00f2 avviene usando un layer come attributo di un altro:</p> <pre><code>class MLP(keras.layers.Layer):\n    def __init__(self):\n        super(MLPBlock, self).__init__()\n        self.linear_1 = Linear(32)\n        self.linear_2 = Linear(32)\n        self.linear_3 = Linear(1)\n\n    def call(self, inputs):\n        x = self.linear_1(inputs)\n        x = tf.nn.relu(x)\n        x = self.linear_2(x)\n        x = tf.nn.relu(x)\n        return self.linear_3(x)\n</code></pre>"},{"location":"material/06_tflow/01_intro/06_model_api/#la-classe-model","title":"La classe <code>Model</code>","text":"<p>In generale, l'uso della classe <code>Layer</code> \u00e8 destinato alla definizione di nuovi layer della rete neurale o, in alternativa, di stack degli stessi. La classe <code>Model</code>, invece, viene usata per definire il modello complessivo della rete, che sar\u00e0 quello che sar\u00e0 addestrato. Per fare un esempio, in un modello che riproduce una ResNet, avremo diversi blocchi residuali derivanti dalla classe <code>Layer</code>, ed un singolo <code>Model</code> contenente l'intera architettura di rete.</p> <p>La classe <code>Model</code> usa un'interfaccia molto simile a quella di <code>Layer</code> a meno di alcune differenze:</p> <ul> <li>la classe <code>Model</code> espone i metodi <code>compile()</code>, <code>fit()</code>, <code>evaluate()</code> e <code>predict()</code> per addestrare, validare ed usare in inferenza la rete;</li> <li>un oggetto di classe <code>Model</code> espone la lista di layer al suo interno usando l'attributo <code>layers</code>;</li> <li>un oggetto di classe <code>Model</code> ha a sua disposizione le API per il caricamento ed il salvataggio del modello e dei suoi pesi.</li> </ul>"},{"location":"material/06_tflow/01_intro/exercises/","title":"E28 - TensorFlow &amp; Keras: Tips &amp; Tricks","text":""},{"location":"material/06_tflow/01_intro/exercises/#e281","title":"E28.1","text":"<p>Scarichiamo il dataset flowers. Per farlo, usiamo il seguente codice:</p> <pre><code>import pathlib\nfrom tensorflow import keras\n\ndataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\ndata_dir = keras.utils.get_file(origin=dataset_url,\n                            extract=True)\ndata_dir = pathlib.Path(data_dir).parent\ndata_dir = pathlib.Path(data_dir, 'flower_photos')\n</code></pre> <p>Utilizziamo i metodi di Keras per caricare il dataset in memoria ed addestrare un modello per la classificazione di questo tipo:</p> <pre><code>_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n rescaling_1 (Rescaling)     (None, 150, 150, 3)       0         \n\n conv_1 (Conv2D)             (None, 148, 148, 32)      896       \n\n max_pool_1 (MaxPooling2D)   (None, 74, 74, 32)        0         \n\n conv_2 (Conv2D)             (None, 72, 72, 32)        9248      \n\n max_pool_2 (MaxPooling2D)   (None, 36, 36, 32)        0         \n\n dropout (Dropout)           (None, 36, 36, 32)        0         \n\n flatten_1 (Flatten)         (None, 41472)             0         \n\n classification (Dense)      (None, 5)                 207365    \n_________________________________________________________________\n</code></pre> <p>Inferiamo il numero di classi del dataset (ovvero la <code>X</code> nel precedente sommario) usando l'attributo <code>class_names</code> del dataset ottenuto da <code>image_dataset_from_directory</code>.</p> <p>Soluzione</p> <p>La soluzione a questo esercizio \u00e8 contenuta in questo notebook.</p>"},{"location":"material/06_tflow/01_intro/exercises/#e282","title":"E28.2","text":"<p>Utilizziamo gli opportuni callback per terminare l'addestramento del modello visto nell'esercizio 1 dopo 3 epoche nelle quali l'accuracy di validazione non migliora per pi\u00f9 di <code>0.01</code>. Visualizziamo inoltre i risultati ottenuti in TensorBoard.</p> <p>Soluzione</p> <p>La soluzione a questo esercizio \u00e8 contenuta in questo notebook.</p>"},{"location":"material/06_tflow/01_intro/exercises/#e283","title":"E28.3","text":"<p>Scarichiamo il dataset Stack Overflow mediante il seguente codice:</p> <pre><code>import pathlib\nfrom tensorflow import keras\n\ndataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\ndata_dir = keras.utils.get_file(origin=dataset_url,\n                            extract=True,\n                            cache_subdir='datasets/stack_overflow')\n\ndata_dir = pathlib.Path(data_dir).parent\ntrain_dir = pathlib.Path(data_dir, 'train')\ntest_dir = pathlib.Path(data_dir, 'test')\n</code></pre> <p>Utilizziamo i metodi di Keras per caricare il dataset in memoria ed addestrare un modello per la classificazione di questo tipo:</p> <pre><code>_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_1 (Dense)             (None, 64)                64064     \n\n dense_2 (Dense)             (None, 4)                 260       \n\n_________________________________________________________________\n</code></pre> <p>Usiamo gli stessi callback utilizzati in precedenza.</p> <p>Inferiamo il numero di classi del dataset (ovvero la <code>X</code> nel precedente sommario) usando l'attributo <code>class_names</code> del dataset ottenuto da <code>text_dataset_from_directory</code>.</p> <p>Soluzione</p> <p>La soluzione a questo esercizio \u00e8 contenuta in questo notebook.</p>"},{"location":"material/06_tflow/01_intro/exercises/#e26-overfitting-e-regolarizzazione","title":"E26 - Overfitting e regolarizzazione","text":""},{"location":"material/06_tflow/01_intro/exercises/#esercizio-e261","title":"Esercizio E26.1","text":"<p>Proviamo ad utilizzare il dataset IMDB movie da Keras per addestrare una rete neurale con la seguente struttura:</p> <pre><code>_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input (Dense)               (None, 8)                 8008      \n\n dense_1 (Dense)             (None, 8)                 72        \n\n classification (Dense)      (None, 1)                 9         \n\n_________________________________________________________________\n</code></pre> <p>Proviamo ad aggiungere una regolarizzazione ed un dropout sul secondo layer, e compariamo i risultati ottenuti.</p> <p>Soluzione</p> <p>La soluzione a questo esercizio \u00e8 contenuta in questo notebook.</p>"},{"location":"material/06_tflow/02_autoencoder/01_basic/","title":"6.2.1 - Simple autoencoder","text":"<p>Per definire un autoencoder in Keras, possiamo usare due layer di tipo <code>Dense</code>, rappresentativi rispettivamente di un encoder ed un decoder.</p> <p>Per defionire il nostro modello, possiamo usare le API di Keras per creare un model (Keras Model Subclassing API).</p> <p>In</p> <pre><code>class Autoencoder(Model):\n\n    def __init__(self, latent_dim):\n        super(Autoencoder, self).__init__()\n        self.latent_dim = latent_dim\n        self.encoder = tf.keras.Sequential([\n            layers.Flatten(),\n            layers.Dense(latent_dim, activation='relu')\n        ])\n        self.decoder = tf.keras.Sequential([\n            layers.Dense(784, activation='sigmoid'),\n            layers.Reshape((28, 28))\n        ])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n    autoencoder = Autoencoder(latent_dim)\n</code></pre> <p>Chiamiamo poii i metodi fit e compile.</p>"},{"location":"material/06_tflow/02_autoencoder/01_basic/#denoising-autoencoder","title":"Denoising autoencoder","text":""},{"location":"material/06_tflow/02_autoencoder/01_basic/#anomaly-detection","title":"Anomaly detection","text":""},{"location":"material/06_tflow/02_autoencoder/01_basic/#variational-autoencoder","title":"Variational autoencoder","text":""},{"location":"material/06_tflow/04_cnn/exercises/","title":"E27 - Convolutional Neural Networks in TensorFlow e Keras","text":""},{"location":"material/06_tflow/04_cnn/exercises/#esercizio-e271","title":"Esercizio E27.1","text":"<p>Creare un modello di rete neurale composto in questo modo:</p> <pre><code>_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv_1 (Conv2D)             (None, 30, 30, 32)        896       \n\n pooling_1 (MaxPooling2D)    (None, 15, 15, 32)        0         \n\n conv_2 (Conv2D)             (None, 13, 13, 32)        9248      \n\n dropout (Dropout)           (None, 13, 13, 32)        0         \n\n flatten (Flatten)           (None, 5408)              0         \n\n classification (Dense)      (None, 10)                54090     \n\n_________________________________________________________________\n</code></pre> <p>Questo modello deve essere in grado di classificare le immagini presenti nel dataset CIFAR10.</p> <p>Soluzione</p> <p>La soluzione a questo esercizio \u00e8 contenuta in questo notebook.</p>"},{"location":"material/06_tflow/04_cnn/lecture/","title":"27 - Convolutional Neural Networks in TensorFlow e Keras","text":"<p>Una delle applicazioni pi\u00f9 diffuse del deep learning riguarda il riconoscimento degli oggetti presenti all'interno di un'immagine. In tal senso, \u00e8 necessario introdurre due ulteriori layer, che rappresentano la base per le cosiddette convolutional neural network.</p> <p>Queste reti (che chiameremo per brevit\u00e0 CNN) sono specializzate nel lavorare su immagini, ma possono anche essere usate per segnali monodimensionali (come la voce) o tridimensionali (come le nuvole di punti).</p> <p>Le CNN assumono una rilevanza fondamentale nel moderno deep learning: \u00e8 grazie a loro se il campo del deep learning \u00e8 diventato mainstream nel mondo della ricerca, il che ha portato ad un interesse e, conseguentemente, ad avanzamenti impensabili in un ridottissimo lasso di tempo di soli dieci anni. Oggigiorno, le CNN vengono utilizzate in ogni ambito che preveda l'elaborazione di dati bidimensionali, dal riconoscimento facciale all'individuazione e caratterizzazione delle targhe degli autoveicoli in transito; conoscerle, quindi, \u00e8 imprescindibile.</p>"},{"location":"material/06_tflow/04_cnn/lecture/#271-layer-convoluzionale","title":"27.1 - Layer convoluzionale","text":"<p>I layer convoluzionali sono alla base del funzionamento delle CNN.</p> <p>Il concetto alla base di questo tipo di layer \u00e8 la convoluzione che, nel contesto del deep learning, \u00e8 un'operazione lineare che prevede la moltiplicazione di un insieme di pesi, chiamato filtro, con una piccola porzione (o finestra) dell'immagine considerata, seguita ovviamente da una funzione di attivazione. Questo processo \u00e8 analogo a quello che avviene in una tradizionale rete neurale.</p> <p>Il filtro ha dimensioni volutamente inferiori rispetto a quelle dell'immagine da convolvere, tipicamente nell'ordine di \\(3 \\times 3\\) o \\(5 \\times 5\\) pixel; la convoluzione del filtro per ogni finestra dell'immagine \u00e8 inoltre assimilabile ad un prodotto scalare, per cui viene restituito sempre un unico valore per ogni finestra convoluta. In tal senso, il filtro viene \"fatto scorrere\" dall'alto in basso, da sinistra verso destra, anche su finestre sovrapposte, che scorrono quindi a passo di un pixel.</p> <p>Nota</p> <p>Tecnicamente, quindi, l'operazione definita come \"convoluzione\" \u00e8 in realt\u00e0 una cross-correlazione.</p> <p>Applicare sistematicamente lo stesso filtro su tutte le finestre possibili dell'immagine, anche sovrapposte, \u00e8 un'idea alquanto potente: infatti, un filtro viene opportunamente tarato per riconoscere uno specifico tipo di feature, come un bordo o una forma, che potr\u00e0 essere trovata ovunque nell'immagine grazie allo scorrimento, ottenendo la cosiddetta invarianza alla traslazione.</p> <p>Abbiamo accennato al fatto che l'output dell'applicazione di un filtro su di una finestra dell'immagine \u00e8 un prodotto scalare. Di conseguenza, man mano cheil filtro scorre, viene creato un array bidimensionale di valori, che viene indicato come mappa delle feature, o feature map. Sar\u00e0 proprio questa, e non l'immagine iniziale, ad essere passata al layer successivo.</p>"},{"location":"material/06_tflow/04_cnn/lecture/#272-layer-di-pooling","title":"27.2 - Layer di pooling","text":"<p>Abbiamo visto come i layer convoluzionali creino delle feature map che \"sintetizzano\" la presenza di determinate feature all'interno di un input. Un limite di queste mappature sta per\u00f2 nel fatto che registrano la posizione precisa della feature individuata all'interno dell'input: ci\u00f2 significa quindi che anche piccole variazioni nella posizione di una feature risulter\u00e0 in una feature map completamente differente, il che comporta un'estrema sensibilit\u00e0 della rete neurale a piccole trasformazioni dell'immagine di input.</p> <p>Per risolvere questo problema, si utilizza un approccio chiamato sottocampionamento: in pratica, si ricava una versione a pi\u00f9 bassa risoluzione del segnale di ingresso, evidenziando di conseguenza gli elementi pi\u00f9 importanti, e scartando i piccoli dettagli non rilevanti nel task di classificazione. Per far questo, viene utilizzato un layer di pooling, applicato a cascata rispetto a quello convoluzionale.</p> <p>Il layer di pooling non fa altro che applicare un filtro, di solito di dimensioni \\(2 \\times 2\\) e con un passo di \\(2\\) pixel (quindi senza sovrapposizioni), che applica una funzione di sottocampionamento, scegliendo quindi un unico pixel tra quelli presenti nel filtro. Due funzioni di pooling molto comuni sono le seguenti:</p> <ul> <li>average pooling: questo filtro associa ad ogni finestra dell'immagine in input il valore medio presente nella finestra;</li> <li>max pooling: questo filtro associa ad ogni finestra dell'immagine in input il valore massimo presente nella finestra.</li> </ul> <p>Ottenendo quindi una versione \"sottocampionata\" dell'input, si raggiunge la cosiddetta invarianza a traslazioni locali, ovvero una sorta di \"insensibilit\u00e0\" del modello a traslazioni o rotazioni di entit\u00e0 minima.</p>"},{"location":"material/06_tflow/04_cnn/lecture/#273-creazione-di-una-semplice-rete-neurale-per-lelaborazione-delle-immagini","title":"27.3 - Creazione di una semplice rete neurale per l'elaborazione delle immagini","text":"<p>Iniziamo creando una semplice rete neurale per l'elaborazione delle immagini digitali. Per farlo, useremo l'API <code>Sequential</code> di Keras, andando a creare una \"pila\" di layer. Ad esempio:</p> <pre><code>from tensorflow import keras\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=(28, 28, 1)),\n        keras.layers.Conv2D(\n            32,\n            (3, 3),\n            activation='relu',\n            name='first_conv_layer'),\n        keras.layers.Conv2D(\n            32,\n            (3, 3),\n            activation='relu',\n            name='second_conv_layer'),\n        keras.layers.Flatten(name='flatten_layer'),\n        keras.layers.Dense(\n            10,\n            activation='softmax',\n            name='layer_class')\n    ]\n)\n</code></pre> <p>Nel modello precedente:</p> <ul> <li>creiamo un'architettura con due layer convoluzionali bidimensionali istanziando due oggetti di classe <code>Conv2D</code>;</li> <li>ognuno di questi oggetti accetta come attributi:</li> <li>il numero di filtri da usare nel banco convoluzionale (in particolare, 32);</li> <li>la dimensione di ciascun filtro, in pixel (\\(3 \\times 3\\));</li> <li>la funzione di attivazione da usare (<code>activation = 'relu'</code>);</li> <li>opzionalmente, un nome.</li> </ul> <p>Dopo i due layer convoluzionali notiamo la presenza di un layer di vettorizzazione delle feature estratte, ottenuto tramite un oggetto di classe  <code>Flatten</code>, ed un layer completamente connesso con un numero di neuroni pari al numero di classi coinvolte nel nostro problema (in questo caso, dieci) ed attivazione <code>softmax</code>.</p>"},{"location":"material/appendix/01_python_vs_code/lecture/","title":"Appendice A - Configurazione dell'ambiente di sviluppo Python","text":"<p>Versione video</p> <p>Una versione video di questa procedura di installazione \u00e8 in arrivo.</p>"},{"location":"material/appendix/01_python_vs_code/lecture/#installazione-di-python","title":"Installazione di Python","text":"<ol> <li>Andare al seguente indirizzo, e selezionare la versione adatta al proprio sistema operativo.</li> <li>Iniziare la procedura di installazione (ad esempio, in Windows, cliccando sull'eseguibile appena scaricato). E' fortemente consigliato aggiungere Python al proprio PATH spuntando l'opportuna casella durante l'installazione, come mostrato in figura 1.</li> </ol> Figura 1 - Aggiunta di Python al PATH <ol> <li>Una volta completata la procedura di installazione, aprire uno shell (ad esempio, il prompt dei comandi), e digitare <code>python</code>. Se tutto \u00e8 andato per il verso giusto, apparir\u00e0 una schermata simile a quella mostrata in figura 2.</li> </ol> Figura 2 - Interprete Python"},{"location":"material/appendix/01_python_vs_code/lecture/#installazione-di-visual-studio-code","title":"Installazione di Visual Studio Code","text":"<ol> <li>Andare al seguente indirizzo, e selezionare la versione adatta al proprio sistema operativo.</li> <li>Seguire la procedura di installazione mostrata a schermo. E' anche in questo caso consigliata l'aggiunta di Visual Studio Code al path, come mostrato in figura 3.</li> </ol> Figura 3 - Installazione di Visual Studio Code"},{"location":"material/appendix/02_setup_tflow/lecture/","title":"Appendice B - Setup di TensorFlow","text":"<p>Versione video</p> <p>Una versione video di questa procedura di installazione \u00e8 in arrivo.</p> <p>In questa sezione, vedremo come effettuare il setup di TensorFlow su tre diversi sistemi operativi, ovvero Windows, Linux e MacOS.</p> <p>TensorFlow e Windows</p> <p>A partire dalla versione 2.11, TensorFlow non \u00e8 pi\u00f9 supportato su Windows. Di conseguenza, \u00e8 necessario seguire una procedura differente, dettagliata in questa guida.</p> Windows WSL2LinuxmacOS"},{"location":"material/appendix/02_setup_tflow/lecture/#1-requisiti-di-sistema","title":"1. Requisiti di sistema","text":"<p>I requisiti necessari sono:</p> <ul> <li>Windows 10 aggiornato almeno alla versione 21H2;</li> <li>Windows Subsystem on Linux (WSL2), reperibile da qui.</li> </ul>"},{"location":"material/appendix/02_setup_tflow/lecture/#2-opzionale-setup-della-gpu-nvidia-in-wsl","title":"2. (Opzionale) Setup della GPU NVIDIA in WSL","text":"<p>Qualora si disponga di una GPU NVIDIA, si potr\u00e0 effettuare il setup della stessa su WSL2. </p> <p>Supponendo di aver installato Ubuntu sul WSL2, dovremo per prima cosa entrare nel sottosistema Linux scrivendo da terminale l'istruzione:</p> <pre><code>wsl\n</code></pre> <p>Una volta dentro, dovremo eseguire le seguenti istruzioni da riga di comando:</p> <pre><code>sudo apt-key del 7fa2af80\nwget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\nsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/12.0.0/local_installers/cuda-repo-wsl-ubuntu-12-0-local_12.0.0-1_amd64.deb\nsudo dpkg -i cuda-repo-wsl-ubuntu-12-0-local_12.0.0-1_amd64.deb\nsudo cp /var/cuda-repo-wsl-ubuntu-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/\nsudo apt-get update\nsudo apt-get -y install cuda\n</code></pre> <p>Per maggior informazioni, potete consultare la guida completa disponibile a questo indirizzo.</p>"},{"location":"material/appendix/02_setup_tflow/lecture/#3-installazione-di-miniconda","title":"3. Installazione di Miniconda","text":"<p>A partire dalla versione 2.11, il metodo raccomandato per l'utilizzo di TensorFlow \u00e8 quello di utilizzare Miniconda, creando un ambiente virtuale separato all'interno del quale andare ad installare tutte le dipendenze necessarie.</p> <p>Da terminale, scriviamo:</p> <pre><code>curl https://repo.anaconda.com/miniconda Miniconda3-latest-Linux-x86_64.sh -o Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Durante l'installazione, rispondiamo sempre <code>yes</code> alle domande che ci verranno fatte, premendo successivamente <code>Invio</code>.</p>"},{"location":"material/appendix/02_setup_tflow/lecture/#4-creazione-di-un-ambiente-virtuale","title":"4. Creazione di un ambiente virtuale","text":"<p>Una volta terminata l'installazione, riavviamo il sottosistema Linux (per farlo, scriviamo <code>exit</code> e, una volta usciti, <code>wsl</code>). A questo punto potremo creare un nuovo ambiente virtuale basato su Python 3.9 che chiameremo <code>pcs</code>:</p> <pre><code>conda create --name pcs python=3.9\n</code></pre> <p>Possiamo quindi attivare questo ambiente virtuale scrivendo:</p> <pre><code>conda deactivate\nconda activate pcs\n</code></pre>"},{"location":"material/appendix/02_setup_tflow/lecture/#5-opzionale-setup-della-gpu","title":"5. (Opzionale) Setup della GPU","text":"<p>Questo step ci permette di configurare ed utilizzare la GPU nell'ambiente virtuale del nostro sistema Linux. Per prima cosa, verifichiamo che i driver siano installati usando la seguente istruzione:</p> <pre><code>nvidia-smi\n</code></pre> <p>Qualora non siano installati, occorrer\u00e0 farlo prelevandoli dal sito ufficiale.</p> <p>Configuriamo il PATH del sistema:</p> <pre><code>mkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' &gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n</code></pre>"},{"location":"material/appendix/02_setup_tflow/lecture/#6-installazione-di-tensorflow","title":"6. Installazione di TensorFlow","text":"<p>Installiamo TensorFlow usando un package manager a nostra scelta (ad esempio, <code>pip</code>):</p> <pre><code>pip install tensorflow\n</code></pre> <p>Verifichiamo l'installazione per la CPU:</p> <pre><code>python3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n</code></pre> <p>e per la GPU:</p> <pre><code>python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre>"},{"location":"material/appendix/02_setup_tflow/lecture/#1-requisiti-di-sistema_1","title":"1. Requisiti di sistema","text":"<p>I requisiti necessari sono:</p> <ul> <li>Ubuntu aggiornato almeno alla versione 16.04.</li> </ul>"},{"location":"material/appendix/02_setup_tflow/lecture/#2-installazione-di-miniconda","title":"2. Installazione di Miniconda","text":"<p>A partire dalla versione 2.11, il metodo raccomandato per l'utilizzo di TensorFlow \u00e8 quello di utilizzare Miniconda, creando un ambiente virtuale separato all'interno del quale andare ad installare tutte le dipendenze necessarie.</p> <p>Da terminale, scriviamo:</p> <pre><code>curl https://repo.anaconda.com/miniconda Miniconda3-latest-Linux-x86_64.sh -o Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Durante l'installazione, rispondiamo sempre <code>yes</code> alle domande che ci verranno fatte, premendo successivamente <code>Invio</code>.</p>"},{"location":"material/appendix/02_setup_tflow/lecture/#3-creazione-di-un-ambiente-virtuale","title":"3. Creazione di un ambiente virtuale","text":"<p>Una volta terminata l'installazione, riavviamo il sottosistema Linux (per farlo, scriviamo <code>exit</code> e, una volta usciti, <code>wsl</code>). A questo punto potremo creare un nuovo ambiente virtuale basato su Python 3.9 che chiameremo <code>pcs</code>:</p> <pre><code>conda create --name pcs python=3.9\n</code></pre> <p>Possiamo quindi attivare questo ambiente virtuale scrivendo:</p> <pre><code>conda deactivate\nconda activate pcs\n</code></pre>"},{"location":"material/appendix/02_setup_tflow/lecture/#4-opzionale-setup-della-gpu","title":"4. (Opzionale) Setup della GPU","text":"<p>Questo step ci permette di configurare ed utilizzare la GPU nell'ambiente virtuale del nostro sistema Linux. Per prima cosa, verifichiamo che i driver siano installati usando la seguente istruzione:</p> <pre><code>nvidia-smi\n</code></pre> <p>Qualora non siano installati, occorrer\u00e0 farlo prelevandoli dal sito ufficiale.</p> <p>Configuriamo il PATH del sistema:</p> <pre><code>mkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' &gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n</code></pre>"},{"location":"material/appendix/02_setup_tflow/lecture/#5-installazione-di-tensorflow","title":"5. Installazione di TensorFlow","text":"<p>Installiamo TensorFlow usando un package manager a nostra scelta (ad esempio, <code>pip</code>):</p> <pre><code>pip install tensorflow\n</code></pre> <p>Verifichiamo l'installazione per la CPU:</p> <pre><code>python3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n</code></pre> <p>e per la GPU:</p> <pre><code>python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre>"},{"location":"material/appendix/02_setup_tflow/lecture/#errori-in-ubuntu-2204","title":"Errori in Ubuntu 22.04","text":"<p>Nel caso si utilizzi la versione 22.04 di Ubuntu, potremmo trovarci di fronte al seguente errore:</p> <pre><code>Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice.\n...\nCouldn't invoke ptxas --version\n...\nInternalError: libdevice not found at ./libdevice.10.bc [Op:__some_op]\n</code></pre> <p>Per risolverlo, eseguiamo le seguenti istruzioni:</p> <pre><code>conda install -c nvidia cuda-nvcc=11.3.58\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\nprintf 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/\\n' &gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\nsource $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\nmkdir -p $CONDA_PREFIX/lib/nvvm/libdevice\ncp $CONDA_PREFIX/lib/libdevice.10.bc $CONDA_PREFIX/lib/nvvm/libdevice/\n</code></pre>"},{"location":"material/appendix/02_setup_tflow/lecture/#1-requisiti-di-sistema_2","title":"1. Requisiti di sistema","text":"<p>I requisiti necessari sono:</p> <ul> <li>macOS aggiornato almeno alla versione 10.12.6 (Sierra).</li> </ul>"},{"location":"material/appendix/02_setup_tflow/lecture/#2-verifica-della-versione-di-python-installata","title":"2. Verifica della versione di Python installata","text":"<p>Verifichiamo che sia installato Python dalla versione 3.7 alla 3.10:</p> <pre><code>python3 --version\n</code></pre> <p>e pip nella versione superiore alla 20.3:</p> <pre><code>python3 -m pip --version\n</code></pre>"},{"location":"material/appendix/02_setup_tflow/lecture/#2-installazione-di-miniconda_1","title":"2. Installazione di Miniconda","text":"<p>A partire dalla versione 2.11, il metodo raccomandato per l'utilizzo di TensorFlow \u00e8 quello di utilizzare Miniconda, creando un ambiente virtuale separato all'interno del quale andare ad installare tutte le dipendenze necessarie.</p> <p>Da terminale, scriviamo:</p> <pre><code>curl https://repo.anaconda.com/miniconda Miniconda3-latest-Linux-x86_64.sh -o Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Durante l'installazione, rispondiamo sempre <code>yes</code> alle domande che ci verranno fatte, premendo successivamente <code>Invio</code>.</p>"},{"location":"material/appendix/02_setup_tflow/lecture/#3-creazione-di-un-ambiente-virtuale_1","title":"3. Creazione di un ambiente virtuale","text":"<p>Una volta terminata l'installazione, riavviamo il sottosistema Linux (per farlo, scriviamo <code>exit</code> e, una volta usciti, <code>wsl</code>). A questo punto potremo creare un nuovo ambiente virtuale basato su Python 3.9 che chiameremo <code>pcs</code>:</p> <pre><code>conda create --name pcs python=3.9\n</code></pre> <p>Possiamo quindi attivare questo ambiente virtuale scrivendo:</p> <pre><code>conda deactivate\nconda activate pcs\n</code></pre>"},{"location":"material/appendix/02_setup_tflow/lecture/#4-installazione-di-tensorflow","title":"4. Installazione di TensorFlow","text":"<p>Installiamo TensorFlow usando un package manager a nostra scelta (ad esempio, <code>pip</code>):</p> <pre><code>pip install tensorflow\n</code></pre> <p>Verifichiamo l'installazione:</p> <pre><code>python3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n</code></pre>"},{"location":"material/appendix/03_libraries/lecture/","title":"Appendice B: Setup dell'ambiente di lavoro","text":"<p>Per effettuare il setup dell'ambiente di lavoro avremo a disposizione diverse opzioni. Vediamole nel dettaglio, immaginando di voler installare la libreria NumPy.</p>"},{"location":"material/appendix/03_libraries/lecture/#opzione-a-utilizzare-pip","title":"Opzione A: utilizzare <code>pip</code>","text":"<p>La prima opzione, e probabilmente quella maggiormente utilizzata, \u00e8 utilizzare il package manager (ovvero, il gestore di pacchetti) integrato in Python, chiamato <code>pip</code>.</p> <p>Per farlo, apriamo un terminale assicurandoci di avere i diritti di amministratore; in Linux, dovremo usare l'istruzione <code>sudo</code>, a meno che non siamo utenti rott, mentre in Windows ci baster\u00e0 aprire la shell come amministratori. Una volta aperto il terminale, dovremo scrivere:</p> <pre><code>pip install numpy\n</code></pre> <p>Installare una libreria in questo modo \u00e8 sicuramente molto semplice, ma porta con s\u00e8 uno svantaggio: infatti, l'installazione della stessa avviene globalmente, ovvero risulta essere valida per l'intera macchina.</p> <p>Ci\u00f2 potrebbe non sembrare rilevante; tuttavia, in ben determinate situazioni, si pu\u00f2 rendere necessario installare particolari combinazioni di versioni di librerie, per usufruire di funzionalit\u00e0 successivamente deprecate o, al contrario, non presenti in versioni antecedenti. In tal senso, se installiamo una certa libreria globalmente, tutti i nostri programmi dovranno necessariamente utilizzare quella libreria in quella specifica versione, il che ci pu\u00f2 vincolare fortemente a lungo andare.</p>"},{"location":"material/appendix/03_libraries/lecture/#opzione-b-utilizzare-pip-ed-un-ambiente-virtuale","title":"Opzione B: utilizzare <code>pip</code> ed un ambiente virtuale","text":"<p>Un'altra opzione \u00e8 quella di utilizzare <code>pip</code> in un opportuno ambiente virtuale.</p> <p>Quest'ultimo altro non \u00e8 se non un ambiente \"separato\" all'interno del nostro calcolatore, nel quale andremo ad inserire tutte le librerie che utilizzeremo per i progetti da inserire all'interno dell'ambiente (con le versioni specifiche).</p> <p>Per utilizzare questa opzione, dovremo innanzitutto creare un ambiente virtuale. Per farlo, dobbiamo usare un'opportuna libreria Python, che dovremo installare globalmente mediante <code>pip</code>.</p> <p>Nota</p> <p>L'installazione globale delle librerie per la gestione dell'ambiente virtuale \u00e8 strettamente necessaria, e non contraddice il principio descritto nelle righe precedenti: infatti, l'idea \u00e8 che si possa creare un ambiente virtuale in qualsiasi momento.</p> <p>Installiamo quindi la libreria virtualenvwrapper, o l'equivalente porting per Windows virtualenvwrapper-win:</p> <p>===\"Linux\" <pre><code>pip install virtualenvwrapper\n</code></pre> ===\"Windows\" <pre><code>pip install virtualenvwrapper-win\n</code></pre></p> <p>Una volta completata l'installazione, utilizzeremo il comando <code>mkvirtualenv</code>, seguito da un nome a nostra scelta, per creare l'ambiente virtuale. Ad esempio:</p> <pre><code>mkvirtualenv pcs\n</code></pre> <p>Noteremo che, a sinistra del terminale, sar\u00e0 apparsa la scritta <code>(pcs)</code>:</p> <pre><code>(pcs) current_working_directory/\n</code></pre> <p>Questo ci indica che siamo all'interno del nostro ambiente virtuale. Procediamo adesso all'installazione della libreria NumPy mediante <code>pip</code>:</p> <pre><code>(pcs) current_working_directory/ pip install numpy\n</code></pre> <p>In questo modo, avremo installato NumPy esclusivamente all'interno del nostro ambiente virtuale. Per verificarlo, basta eseguire l'istruzione <code>pip freeze</code>, che restituisce tutte le librerie presenti nell'ambiente in cui siamo attualmente, assieme alle loro versioni.</p> <p>Il file <code>requirements.txt</code></p> <p>Pratica comune \u00e8 quella di memorizzare tutte le librerie presenti in un ambiente virtuale in un file chiamato <code>requirements.txt</code>. Cos\u00ec facendo, un altro programmatore sar\u00e0 in grado di \"clonare\" il nostro ambiente virtuale. Per salvare il file <code>requirements.txt</code>, dovremo usare i seguenti comandi:</p> <p><pre><code>pip freeze &gt; requirements.txt\n</code></pre> Per creare un ambiente virtuale come descritto dal file dei requisiti, invece, dovremo eseguire: <pre><code>pip install -r requirements.txt\n</code></pre> dove il flag <code>-r</code> sta per recursively, ed indica a <code>pip</code> di installare in maniera ricorsiva le librerie indicate nel file <code>requirements.txt</code>.</p>"},{"location":"material/appendix/03_libraries/lecture/#opzione-c-utilizzare-una-distribuzione-di-python-per-il-calcolo-scientifico","title":"Opzione C: utilizzare una distribuzione di Python per il calcolo scientifico","text":"<p>La terza opzione \u00e8 quella di utilizzare una distribuzione di Python specificamente pensata per il calcolo scientifico, come Anaconda. In questo caso, baster\u00e0 scaricare l'installer dal sito ufficiale e seguire la normale procedura di installazione.</p> <p>Il vantaggio di utilizzare una distribuzione di questo tipo sta nel fatto che avremo a disposizione di default la maggior parte delle librerie utilizzate nel calcolo scientifico. Tuttavia, occorre tenere in considerazione il fatto che la libreria \u00e8 specificamente pensata soltanto per scopi scientifici, per cui dovremo considerarlo qualora intendessimo utilizzare Python per progetti di altro tipo.</p> <p>Il package manager di Anaconda</p> <p>Nel caso si decida di optare per l'uso di Anaconda, \u00e8 importante ricordare che questa distribuzione ha un suo package manager, chiamato <code>conda</code>. Questo andr\u00e0 a sostituire <code>pip</code> nell'installazione delle librerie non presenti nella distribuzione.</p>"},{"location":"material/appendix/03_libraries/lecture/#opzione-d-utilizzare-un-package-manager-come-pipenv","title":"Opzione D: utilizzare un package manager come <code>pipenv</code>","text":"<p>L'ultima opzione, che \u00e8 anche quella suggerita in caso di utilizzo professionale ed eterogeneo di Python, \u00e8 quella di affidarsi ad un package manager evoluto, come <code>pipenv</code>.</p> <p>Questo package manager, infatti, automatizza e semplifica la creazione di un ambiente virtuale, combinando la stessa con l'utilizzo di <code>pip</code> in pochi, semplici comandi; in generale, quindi, il tool ci fornisce un'interfaccia utente molto pi\u00f9 snella, ed inoltre si occupa autonomamente di selezionare le ultime versioni disponibili per i package che utilizziamo.</p> <p>Per utillizzare <code>pipenv</code>, dovremo per prima cosa installarlo globalmente sulla nostra macchina mediante <code>pip</code>:</p> <pre><code>pip install pipenv\n</code></pre> <p>Una volta installato, andiamo nella cartella dove vogliamo creare il nostro progetto, che ricordiamo includer\u00e0 soltanto la libreria NumPy, e scriviamo:</p> <pre><code>pipenv install numpy\n</code></pre> <p>Vedremo che, al termine della procedura, saranno stati generati due file: il primo, chiamato <code>Pipfile</code>, avr\u00e0 al suo interno tutte le dipendenze che abbiamo aggiunto al nostro progetto, mentre il secondo, <code>Pipfile.lock</code>, conterr\u00e0 delle informazioni dettagliate sulle librerie usate, incluse versioni, repository, e via discorrendo.</p> <p>Tuttavia, <code>pipenv</code> non si limita a creare questi due file, ma provvede anche a definire, in maniera automatica, un nuovo ambiente virtuale, all'interno del quale saranno (ovviamente) memorizzate tutte le librerie installate per il nostro progetto. Per accedere all'ambiente virtuale, dovremo usare il comando <code>pipenv shell</code>, mentre per eseguire un comando senza accedere all'ambiente virtuale dovremo usare il comando <code>pipenv run</code> seguito dal comando che vogliamo eseguire.</p> <p>Ad esempio, se volessimo lanciare un ipotetico script <code>run.py</code> accedendo all'ambiente virtuale, dovremmo scrivere:</p> <pre><code>pipenv shell\npython run.py\n</code></pre> <p>Se non volessimo accedere all'ambiente virtuale, invece, dovremmo scrivere:</p> <pre><code>pipenv run python run.py\n</code></pre>"},{"location":"material/appendix/03_libraries/lecture/#bonus-opzione-utilizzata-nel-corso","title":"Bonus: opzione utilizzata nel corso","text":"<p>Nel corso, utilizzeremo l'approccio basato su Miniconda e <code>pip</code>, che \u00e8, ad oggi, quello suggerito per l'installazione di TensorFlow.</p> <p>Vediamo come impostare l'ambiente di sviluppo a seconda che si stia utilizzando Linux o Windows.</p> WindowsLinux <pre><code>curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>"},{"location":"material/appendix/03_libraries/lecture/#1-requisiti-di-sistema","title":"1. Requisiti di sistema","text":""},{"location":"material/appendix/04_scope/lecture/","title":"Appendice C - Ambito di una variabile","text":"<p>All'interno di un programma ogni variabile ha una sorta di \"ciclo di vita\", che ne prevede la creazione, utilizzo e, infine, distruzione.</p> <p>L'intero script ha un ambito definito come globale: ci\u00f2 significa che tutte le variabili specificate nel corpo \"principale\" dello script hanno validit\u00e0 in tutto il nostro codice. Le singole funzioni, invece, definiscono un ambito locale, creato alla chiamata della funzione, e distrutto al termine della stessa.</p> <p>Facciamo un esempio. Definiamo una funzione <code>calcolo_voto_accesso_laurea</code> che accetta in ingresso un argomento, ovvero la lista con i voti degli esami.</p> <pre><code>def calcolo_voto_accesso_laurea(voti_esami):\n    somma_voti = 0\n    for voto in voti_esami:\n        somma_voti += voto\n    voto_medio = somma_voti/len(voti_esami)\n    voto_accesso = voto_medio / 3 * 11\n    return voto_accesso\n</code></pre> <p>Proviamo a chiamarla.</p> <pre><code>lista_voti = [18, 20, 19, 30, 24, 30]\n\nprint('Il voto di accesso \u00e8: ', calcolo_voto_accesso_laurea(lista_voti))\n</code></pre> <p>A schermo vedremo:</p> <pre><code>Il voto di accesso \u00e8:  86.16666666666666\n</code></pre>"},{"location":"material/appendix/04_scope/lecture/#c1-prima-modifica","title":"C1 - Prima modifica","text":"<p>Facciamo una prima modifica:</p> <pre><code>lista_voti = [18, 20, 19, 30, 24, 30]\n\ndef calcolo_voto_accesso_laurea(voti_esami):\n    print(f'La lista dei voti \u00e8: {lista_voti}')\n    somma_voti = 0\n    for voto in voti_esami:\n        somma_voti += voto\n    voto_medio = somma_voti/len(voti_esami)\n    voto_accesso = voto_medio / 3 * 11\n    return voto_accesso\n\nprint('Il voto di accesso \u00e8: ', calcolo_voto_accesso_laurea(lista_voti))\n</code></pre> <p>Adesso vedremo a schermo due valori:</p> <pre><code>La lista dei voti \u00e8: [18, 20, 19, 30, 24, 30]\nIl voto di accesso \u00e8:  86.16666666666666\n</code></pre>"},{"location":"material/appendix/04_scope/lecture/#c2-seconda-modifica","title":"C2 - Seconda modifica","text":"<p>Proviamo a modificare ancora il codice:</p> <pre><code>lista_voti = [18, 20, 19, 30, 24, 30]\n\ndef calcolo_voto_accesso_laurea(voti_esami):\n    somma_voti = 0\n    for voto in voti_esami:\n        somma_voti += voto\n    voto_medio = somma_voti/len(voti_esami)\n    voto_accesso = voto_medio / 3 * 11\n    return voto_accesso\n\nprint('Il voto medio \u00e8: ', voto_medio)\nprint('Il voto di accesso \u00e8: ', calcolo_voto_accesso_laurea(lista_voti))\n</code></pre> <p>Adesso vedremo a schermo il seguente risultato:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nNameError: name 'voto_medio' is not defined\nIl voto di accesso \u00e8:  86.16666666666666\n</code></pre> <p>Cosa \u00e8 successo? Andiamo un attimo a ritroso, e partiamo dalla prima modifica.</p> <p>In questo caso, infatti, abbiamo provato ad accedere alla variabile globale <code>lista_voti</code>, definita nel corpo \"principale\" dello script, dall'interno della funzione <code>calcola_voto_accesso_laurea</code>. Ci\u00f2 \u00e8 evidentemente possibile, in quanto possiamo accedere ad una variabile globale da un ambito locale.</p> <p>Il contrario, tuttavia, non \u00e8 possibile: infatti, nella seconda modifica, proviamo ad accedere ad una variabile locale alla funzione <code>calcola_voto_accesso_laurea</code> dall'esterno della funzione stessa. Questo non pu\u00f2 avvenire, perch\u00e9 le variabili locali \"scompaiono\" al termine della funzione in cui sono definite, per cui l'interprete ci dar\u00e0 un errore.</p>"},{"location":"material/appendix/05_oop/lecture/","title":"Appendice D - Principi di Programmazione Orientata agli Oggetti","text":"<p>La programmazione orientata agli oggetti (in inglese object-oriented programming, OOP) \u00e8 un paradigma di programmazione che sposta il focus dalle funzioni ai dati. In particolare, la OOP prevede che tutto sia un oggetto: una qualsiasi variabile \u00e8 interpretata come un oggetto, cos\u00ec come anche le funzioni stesse (in alcuni linguaggi).</p> <p>Ci\u00f2 si estende ovviamente anche ai tipi definiti dall'utente, che assumono il nome di classi. Facciamo un esempio.</p>"},{"location":"material/appendix/05_oop/lecture/#la-classe-persona","title":"La classe <code>Persona</code>","text":"<p>Immaginiamo di voler definire una struttura dati che contenga al suo interno le informazioni necessarie a definire una persona, come nome, cognome, genere ed et\u00e0. Per farlo, ovviamente, dovremo \"unire\" tra di loro diversi dati primitivi: potremo usare una stringa per il nome, una per il cognome, una per il genere e, infine, un intero per l'et\u00e0.</p> <p>In tal senso, possiamo creare quindi la classe <code>Persona</code>, che avr\u00e0 quattro attributi, come mostrato in figura.</p> <p></p> <p>Sottolineamo come una classe rappresenti tutte le possibili persone: infatti, si cerca di creare delle strutture dati generiche, che abbiano degli attributi comuni a tutte le possibili istanze. Nel nostro caso, sappiamo che ogni persona ha un nome, un cognome, un genere ed un'et\u00e0, quindi usiamo questi quattro valori come attributi di classe.</p> <p>Differenza tra classe ed istanza</p> <p>Abbiamo detto che una classe rappresenta tutte le possibili istanze della stessa. Ci\u00f2 si traduce, nel nostro esempio, nel fatto che la classe <code>Persona</code> \u00e8 in grado di rappresentare tutte le persone, e un'istanza della classe <code>Persona</code> \u00e8 una singola variabile, o oggetto, che rappresenta una certa persona. Per capirci: un'istanza di <code>Persona</code> \u00e8 \"Angelo, Cardellicchio, Uomo, 37\", mentre un'altra istanza \u00e8 data da \"Frank, Hood, Uomo, 42\", un'altra ancora da \"Camilla, Lilla, Donna, 55\", e cos\u00ec via.</p> <p>Ovviamente, potremo in qualche modo agire con degli opportuni metodi su questi attributi. Ad esempio, se avessimo a disposizione anche il luogo e la data di nascita, potremmo creare un metodo <code>calcola_cf</code> che, per l'appunto, permette di generare il codice fiscale di una singola istanza.</p> <p>Oltre al concetto di classe, tuttavia, la OOP definisce altri tre concetti base. Vediamoli di seguito.</p>"},{"location":"material/appendix/05_oop/lecture/#concetto-1-ereditarieta","title":"Concetto 1: Ereditariet\u00e0","text":"<p>Per ereditariet\u00e0 si intende la capacit\u00e0 di una classe di \"discendere\" da un'altra. Non dobbiamo, per\u00f2, pensare al nostro albero genealogico: infatti, noi abbiamo parte delle caratteristiche di ciascuno dei nostri genitori, mentre una classe figlia eredita in toto le caratteristiche di una classe madre.</p> <p>Ad esempio, potremmo definire la classe <code>Studente</code> come figlia della classe <code>Persona</code>, cui aggiunger\u00e0 i seguenti attributi:</p> <p></p> <p>Possiamo visualizzare questa relazione in ordine gerarchico come segue:</p> <p></p> <p>Da notare che la classe <code>Studente</code> pu\u00f2 aggiungere anche dei metodi, oltre che degli attributi a quelli offerti da <code>Persona</code>, come ad esempio <code>genera_media_voto</code>.</p> <p>In ultimo, notiamo come ogni istanza di <code>Studente</code> \u00e8 un'istanza di <code>Persona</code>, ma non \u00e8 vero il contrario, e quindi non tutte le persone sono degli studenti. Per aiutarci a comprendere questo concetto, possiamo visualizzare gli insiemi delle istanze di <code>Persona</code> e di <code>Studente</code>:</p> <p></p> <p>Generalizzazione e specializzazione</p> <p>La relazione di ereditariet\u00e0 pu\u00f2 anche essere vista in termini di generalizzazione e specializzazione. In questo contesto, la classe <code>Studente</code> \u00e8 una specializzazione di <code>Persona</code>, in quanto sottende ad un insieme pi\u00f9 specifico; al contrario, le persone sono viste come una generalizzazione degli studenti.</p> <p>Ereditariet\u00e0 multipla e multilivello</p> <p>Alcuni linguaggi, compreso Python, offrono la possibilit\u00e0 di ereditare da pi\u00f9 classi; tale concetto \u00e8 chiamato ereditariet\u00e0 multipla. Se invece stabiliamo una vera e propria gerarchia di classi, con una classe \"nonna\", una \"madre\" ed una \"figlia\", avremo una struttura multilivello.</p>"},{"location":"material/appendix/05_oop/lecture/#concetto-2-incapsulamento","title":"Concetto 2: Incapsulamento","text":"<p>Il concetto di incapsulamento prevede che sia possibile accedere ad un metodo (o anche ad un attributo) di una classe esclusivamente mediante la sua interfaccia verso il mondo esterno. Vediamo cosa significa.</p> <p>Immaginiamo di voler calcolare il codice fiscale di una persona: dovremo seguire una procedura ben precisa e moderatamente complessa, che potremo tranquillamente \"nascondere\" al codice che usa la classe <code>Persona</code>, il quale dovr\u00e0 semplicemente invocare il metodo <code>calcola_cf</code>. Tuttavia, se volessimo seguire il principio di modularit\u00e0, che ci suggerisce di \"suddividere\" funzioni complesse in maniera tale da renderle pi\u00f9 semplici, dovremmo creare altre funzioni ausiliarie, che potrebbero calcolare la rappresentazione di nome e cognome (<code>calcola_nc</code>) e i dati alfanumerici derivanti da luogo e data di nascita (<code>calcola_ld</code>). Ovviamente, non vi \u00e8 il bisogno di accedere dall'esterno della classe a questi metodi, in quanto hanno valenza esclusiva nell'ambito del calcolo del codice fiscale: per questo motivo, li si potr\u00e0 dichiarare come privati, e potranno essere acceduti soltanto dall'interno della classe.</p> <p>In questo modo, la classe mantiene un'interfaccia stabile ed essenziale verso l'esterno: il codice che usa la classe avr\u00e0 sempre un punto di accesso ben definito e, nel caso si debbano modificare dei comportamenti interni alla classe, non sar\u00e0 influenzato da dette modifiche. Ad esempio, infatti, se per qualche motivo si decidesse di cambiare l'ordine con cui si mostrano nel codice fiscale la rappresentazione del cognome e del nome, basterebbe modificare il metodo <code>calcola_nome_cognome_codice_fiscale</code>, ed il resto dell'implementazione (sia della classe, sia del codice chiamante) non ne sarebbe influenzata.</p> <p></p>"},{"location":"material/appendix/05_oop/lecture/#concetto-3-polimorfismo","title":"Concetto 3: Polimorfismo","text":"<p>Il concetto di polimorfismo prevede che sia possibile modificare il comportamento associato ad un metodo a seconda della classe che lo utilizza.</p> <p>Immaginiamo ad esempio di specializzare la classe <code>Studente</code> in due ulteriori rappresentazioni, ovvero <code>StudenteUniversitario</code> e <code>StudenteScolastico</code>. Ovviamente, il metodo <code>genera_media_voto</code> sar\u00e0 ereditato da entrambe le classi; tuttavia, l'implementazione dovr\u00e0 essere necessariamente differente, in quanto la media di laurea \u00e8 pesata in modo diverso rispetto alla classica media aritmetica usata nelle scuole fino alla secondaria.</p> <p>Il polimorfismo ci permette di raggiungere questo obiettivo: potremo effettuare una procedura di override del metodo <code>genera_media_voto</code> che, pur conservando la stessa firma, avr\u00e0 differenti implementazioni nelle classi <code>StudenteUniversitario</code> e <code>StudenteScolastico</code>. Ovviamente, il fatto che il metodo conservi la stessa firma rappresenta un vantaggio paragonabile a quello ottenuto mediante il polimorfismo: infatti, un programmatore potr\u00e0 usare il metodo <code>genera_media_voto</code> alla stessa maniera per uno studente universitario ed uno di scuola media secondaria, senza per questo dover tenere a mente due diverse interfacce.</p> <p></p>"},{"location":"material/appendix/06_tips/lecture/","title":"Appendice E - Python","text":""},{"location":"material/appendix/06_tips/lecture/#tabella-degli-operatori-booleani","title":"Tabella degli operatori booleani","text":"Operatore Operazione logica Esempio Risultato and AND 1 and 2 True or OR True or False True not NOT True is not False True"},{"location":"material/appendix/06_tips/lecture/#gestione-delle-eccezioni","title":"Gestione delle Eccezioni","text":""},{"location":"material/appendix/06_tips/lecture/#i-decorator","title":"I decorator","text":"<p>Prima di continuare a parlare dei metodi che \u00e8 possibile definire all'interno di una classe Python, \u00e8 necessario introdurre il concetto di decorator, ovvero una particolare notazione che viene usata in Python (ed in altri linguaggi di programmazione) per indicare una funzione che \"decora\" un'altra funzione.</p>"},{"location":"material/appendix/06_tips/lecture/#funzioni-come-oggetti","title":"Funzioni come oggetti","text":"<p>Python tratta le funzioni come degli oggetti. E' quindi possiible che una funzione restituisca una funzione:</p> <pre><code>def main_character(series):\n    def supernatural():\n        return \"Sam Winchester\"\n\n    def breaking_bad():\n        return \"Walter White\"\n\n    if series == \"Supernatural\":\n        return supernatural\n    elif series == \"Breaking Bad\":\n        return breaking_bad\n</code></pre> <p>Il valore di ritorno \u00e8 quindi un oggetto. Possiamo provare a chiamarlo dal nostro script:</p> <pre><code>&gt;&gt;&gt; mc = main_character(\"Supernatural\")\n</code></pre> <p>Se provassimo a mandarlo a schermo trattandolo come una variabile, avremmo in uscita una reference a funzione:</p> <pre><code>&gt;&gt;&gt; print(\"Function reference: {}\".format(mc))\nFunction reference: &lt;function main_character.&lt;locals&gt;.supernatural at 0x00000170C448BA60&gt;\n</code></pre> <p>Per visualizzare il risultato, trattiamolo come se fosse una chiamata a funzione:</p> <pre><code>&gt;&gt;&gt; print(\"Function outcoming value: {}\".format(mc()))\nFunction outcoming value: Sam Winchester\n</code></pre>"},{"location":"material/appendix/06_tips/lecture/#funzioni-come-argomenti-di-altre-funzioni","title":"Funzioni come argomenti di altre funzioni","text":"<p>Possiamo passare una fuzione come argomento ad un'altra funzione:</p> <pre><code>def favorite_series(func):\n    def internal_check():\n        print(\"Checking my favorite series...\")\n        func()\n        print(\"Got it!\")\n    return internal_check\n\ndef check():\n    print('Sons of Anarchy')\n</code></pre> <p>Dal nostro script:</p> <pre><code>&gt;&gt;&gt; print_fav_series = favorite_series(check)\n&gt;&gt;&gt; print_fav_series()\nChecking my favorite series...\nSons of Anarchy\nGot it!\n</code></pre> <p>Vediamo quindi come la funzione passata come argomento sar\u00e0 correttamente chiamata internamente al metodo <code>favorite_series</code>.</p>"},{"location":"material/appendix/06_tips/lecture/#definizione-ed-uso-di-decorator","title":"Definizione ed uso di decorator","text":"<p>La sintassi che abbiamo usato \u00e8, per dirla con Manzoni, ampollosa. Python ci offre quindi una sintassi equivalente, ma molto pi\u00f9 accessibile, per usare una funzione come argomento di un'altra funzione, ovvero i decorator. Infatti:</p> <pre><code>@favorite_series\ndef print_fav_series_decorated():\n    print('Breaking Bad')\n\n&gt;&gt;&gt; print_fav_series_decorated()\nChecking my favorite series...\nBreaking Bad\nGot it!\n</code></pre>"},{"location":"material/new/00_generators/","title":"00 generators","text":"<p>Generator in Python Cosa sono i Generator? I generator in Python sono una semplice e potente implementazione del pattern \"iteratore\". Una funzione generator consente di iterare su un set di dati senza doverli prima memorizzare completamente in memoria. Questa funzionalit\u00e0 \u00e8 utile per lavorare con dataset di grandi dimensioni.</p> <p>Creazione di un Generator Un generator si crea utilizzando una funzione con l'istruzione yield:</p> <p>python def mio_generator():     yield 1     yield 2     yield 3</p> <p>gen = mio_generator() for val in gen:     print(val) Funzionamento del yield L'istruzione yield \u00e8 simile a return, ma invece di terminare la funzione e restituire un valore, salva lo stato della funzione e lo riprende al punto successivo alla yield alla chiamata successiva.</p> <p>Esempi Pratici 1. Generatore di Numeri Fattoriali python def fattoriale():     n = 1     result = 1     while True:         yield result         n += 1         result *= n</p> <p>gen = fattoriale() for _ in range(5):     print(next(gen)) 2. Generatore di Numeri di Fibonacci python def fibonacci():     a, b = 0, 1     while True:         yield a         a, b = b, a + b</p> <p>gen = fibonacci() for _ in range(5):     print(next(gen)) Generator Expressions Oltre alle funzioni generator, \u00e8 possibile creare generator in modo simile alle comprehension list, usando le parentesi tonde:</p> <p>python genexpr = (x * x for x in range(10)) for val in genexpr:     print(val) Vantaggi dei Generator Efficienza della Memoria: Ideali per lavorare con grandi dataset, poich\u00e9 non memorizzano tutti i dati in memoria.</p> <p>Lazy Evaluation: Calcolano i valori al bisogno, migliorando le performance.</p>"},{"location":"material/new/01_object_detection/00_intro/","title":"00 intro","text":"<p>Introduzione alla Object Detection Cos'\u00e8 la Object Detection? La object detection \u00e8 il processo di individuazione e classificazione degli oggetti presenti all'interno di un'immagine o di un video. Non solo identifica la presenza di vari oggetti, ma ne determina anche la posizione esatta mediante il disegno di bounding box.</p> <p>Tecniche Classiche di Object Detection 1. Tecniche Basate su Caratteristiche (Feature-Based) Haar Cascades: Una delle prime tecniche utilizzate per la rilevazione degli oggetti, basata su caratteristiche di Haar e utilizzata per la rilevazione del volto.</p> <p>Histograms of Oriented Gradients (HOG): Questa tecnica descrive la distribuzione locale dei gradienti di intensit\u00e0 o delle direzioni dei bordi, utilizzata spesso per la rilevazione di persone.</p> <ol> <li>Sliding Window e Classificatori Sliding Window: Questa tecnica scorre una finestra di dimensione fissa su tutta l'immagine, applicando un classificatore ad ogni finestra. Tecniche come HOG+SVM (Support Vector Machine) sono state popolari in passato.</li> </ol> <p>Tecniche Basate su Deep Learning Con l'avvento del deep learning, la object detection ha subito una rivoluzione, portando a metodi molto pi\u00f9 accurati e veloci. Di seguito sono riportate alcune delle principali tecniche basate su deep learning:</p> <ol> <li>R-CNN (Region-based Convolutional Neural Networks) R-CNN: Estrae regioni proposte (region proposals) da un'immagine e le passa attraverso una rete convoluzionale per la classificazione. Il processo era lento ma ha introdotto il concetto di CNN per la detection.</li> </ol> <p>Fast R-CNN: Un miglioramento della R-CNN, che consente di classificare le regioni proposte in modo pi\u00f9 efficiente.</p> <p>Faster R-CNN: Introduce una RPN (Region Proposal Network) che genera region proposals direttamente dalla rete convoluzionale, velocizzando notevolmente il processo.</p> <ol> <li>YOLO (You Only Look Once) YOLO: Propone un nuovo approccio alla object detection, considerando l'intera immagine come input di una singola rete CNN, che restituisce bounding box e classificazioni in un'unica passata. Questo metodo \u00e8 estremamente veloce e adatto a scenari real-time.</li> </ol> <p>YOLOv3, YOLOv4 e successive: Versioni migliorate e pi\u00f9 accurate dell'originale YOLO, introducono diverse ottimizzazioni e architetture di rete.</p> <ol> <li> <p>SSD (Single Shot MultiBox Detector) SSD: Utilizza una singola rete CNN per prevedere i bounding box e le classi direttamente da diverse caratteristiche della rete, senza bisogno di region proposals, ottenendo alta velocit\u00e0 e accuratezza.</p> </li> <li> <p>RetinaNet RetinaNet: Introduce la Focal Loss per affrontare lo squilibrio della classe, ottenendo miglioramenti significativi nella rilevazione degli oggetti rari.</p> </li> <li> <p>Transformers-Based Approaches DETR (Detection Transformer): Utilizza l'architettura Transformer per la detection, eliminando la necessit\u00e0 di meccanismi complessi come l'RPN e migliorando la generalizzazione.</p> </li> </ol> <p>RT-DETR (Real-Time DEtection TRansformer): Un'implementazione basata su Transformer che combina codificatori efficienti e selezione delle query per migliorare sia la velocit\u00e0 che l'accuratezza.</p> <p>Conclusioni La object detection \u00e8 una componente cruciale in molti campi applicativi, dai veicoli autonomi alla sorveglianza, dall'analisi di immagini mediche all'interazione uomo-computer. L'evoluzione dalle tecniche basate su caratteristiche a quelle basate su deep learning ha consentito di ottenere risultati sempre pi\u00f9 accurati e in tempo reale, aprendo nuove frontiere e applicazioni.</p>"},{"location":"material/new/02_attention/00_intro/","title":"Attention","text":"<p>L'attetnion \u00e8 un concetto ampiamente investigato che \u00e8 stato spesso usato assieme ad aropusal, alertness, ed engagement con l'ambiente circostante.</p> <p>In its most generic form, attention coudl be descriebd as merely an overal level of alertness or ability to engage with surroundings.</p> <p>La visual attention \u00e8 una derelle aree spesso pi\u00f9 staudiate sia dalla prospettiva neuroscientifica, sia da quella psicologica.</p> <p>Quando ad un soggetto viene presentate diverse imagini, i movimenti dell'occhio che il soggetWto effettua possono rilevare le parti salienti dell'immagine che attraggono soprattutto la attenzione del soggetto. In questa review dei modelli computazionali per la visual attention, Itti e Koch menzioano che queste parti salienti dell'immagini sono spesso caratterizzate dagli attributi visivi, includendo il contrasto in termini di intensit\u00e0, gli edge orientati, gli angoli e le giunzioni, ed il movimento. Il cervello umano ATTENDS a queste feature visive salienti a diversi livelli neuronali.</p> <p>I neuroni nei primi stage sono adatti a distinguere attributi visuali semplici come contrasto di intensit\u00e0\u00f2, colori opposti, orientamento, direzione e velcocit\u00e0 del moto, o disparit\u00e0 stereo a diverse scale spaziali. Il tuning dei neuroni diventa sempre pi\u00f9 specializzato conla progressione da aree visive a basso livello verso quelle ad alto livello, in modo che le aree visive ad alto livello includono neurono che rispondono solo ad angoli o giunzioni, indizi di forma a partire dall'ombreggiatura, o vie di specifici oggetti nel modndo reale.</p> <p>E' interessante osservare come diversi soggetti tendono ad esesre attratti alle stesse indizi visivi.</p> <p>I ricercatori hanno anche scroperto diverse forme di interazione tra la memoria e l'attention. Dal modmento che il cervello umano ha una capacit\u00e0 di memoria limitata, scegliere quali informazioni memorizzare diventa cruciale nell'utilizzo ottimizzato di queste risorse limitate. Il cervello umano lo fa affidandosi all'attention, in modo da memorizzare dinamicamente in memoria l'informazione a cui il soggeetto umano fa pi\u00f9 attenzione.</p>"},{"location":"material/new/02_attention/00_intro/#attention-nel-machine-learning","title":"Attention nel machine learning","text":"<p>Implementare il meccanismo di attention nelle ANN non segue necessariamente i meccanismi biologici e psicologici del cervello umano. INvece, \u00e8 l'abilit\u00e0 di evidenziare dinameicamente ed usare le parti salienti dell'informazione a disposziione - in modo simile a quello del cervello umano - che rende l'attention un concetto cos\u00ec attraente nel machien learning.</p> <p>Possiamo pensare ai sistemi attention-based come composti di tre componenti (da Deep Learning https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?dchild=1&amp;keywords=deep+learning&amp;qid=1622968138&amp;sr=8-1)</p> <ul> <li>un processo che legge i dati raw (come le parole in una frase) e lei converte in rappresentazioni distrib uite, con un vettore delle feature assocaito alla posizione di ognio parola</li> <li>una lista di vettori delle feature che contienen l'output del lettore. questo pu\u00f2 essere visto come una \"memoria\" che contiene una sequenza di fatti, che posssoono essere recuperati succeeeviamente, non necessariamente nello stgesso ordine, senza doverli visitare tutti nuovamente</li> <li>un processo che sfrutta il contenuto di memoriz per effettuare un task in maniera sequenziale, facendo in modo che ad ogni step temporale ci sia la possibilit\u00e0di inserire l'attention sul contenuitop di un elem,ento di memoria (o di alcuni, con pesi diffeernti)</li> </ul> <p>Vediamo uil framewokr dell'encoder-decoder come esempio, perch\u00e8 \u00e8 in questo framework semplcie che venne per la prima volta introdotto il concetto di attention.</p> <p>Se stiamo elabroando una sequenzza di parole in input, allora questa sar\u00e0 per prima cosa passata ad un encoder, che dar\u00e0 in output un vettore per ogni elemento nella sequenza., Questo corriosponde al primo compoenmtne del nostro sistema attention-based, come spiegato in precedenza.</p> <p>Una lista di questi vettori (il secondo compoennte del sistema attention-based dim prima), assieme agli strati nascosti rpecedenti del decoder, sar\u00e0 sfruttata dal meccanismo di attention per evidenziare in maneira dinamica quale dell'i formazione di input sar\u00e0 usata per generare l'output.</p> <p>Ad ogni step temporale, il meccanismo di attention prende quindi l\u00eco stato nascosto precedente del decoder e la slista di vettori codificati, usandoli per generare valori di punteggio non normalizzati che indicano quanto \"bene\" gli elemenit della sequnza di input si alleneano all'output attuaylele. Dal momento che i valori di punteggio generati devono avere senso (relativamente) in termini di importanza, sono normalizzati passandoli attraverso una funzione softmax per generare i pesi. Seguendo la nromalizzazione mediante softmax, tutti i valori di pesi saranno nell'intervallo [0,1] e si sommeranno ad 1, cil che significa che possono essere interpretrati come probabilit\u00e0. Infine, i vettori codificati sono scalati dai pesi clcolati epr generare un contextg vector. Questo processo di attenzione forma il terzo componente del sikstema attention-based descritto rpecedente,ente,. E' questro context vector che viene quindi mandato nel decoder per genrare un output tradotto: (https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)</p> <p>Questo tipo di attenzione artriificale \u00e8 una forma di re-weighting iterativo. Nello specifico, evidenzia dinamneicamente diverse compoennti di un input pre-elaborato man mano che sono necessarie alla generazione dell'output. Questo li rende flessibili e context-dependent, come l'attention biologica.</p> <p>Il processo implementato da un sistema che incorproa un meccanismo di attention contrasta con uno che non lo comprende. In quest'uyltimo, l'encoder genrea un vettore a lunghezza fissa indipendedntemtne dalla lugnhezza co complessit\u00e0 dell'input. In assenza di unn eccanismo che evidenzia l'informazione saliente lungo l'intero input, il doceoder avr\u00e0 accesso sotlanto all'informazione linmitata che sarebbe codificata all'interno del vettore a lunghezza fissa. Questo risulta, potnenzialmente, nelf atto che il defcodeder perde informazioni importanti.</p> <p>Il meccanismo di attention \u00e8+ statop inizialmente propsoto per elabroare sequenze di parle nella machine translation, che ha un aspetto temporale intrionscec,. Tuttavia pu\u00f2 essere genralizzato per elabroare informazione che pu\u00f2 essere statica e non encessariamente correlata in maniera sequenziale, cos\u00ec come nel contesto dell'image proccesing.</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/","title":"Meccanismo di attention","text":"<p>L'attention mechanism venne introdotto da Bahdanau nel 2014 (https://arxiv.org/abs/1409.0473) per risolvere il problema del collo di bottiglia che merge dall'uso di vettori di codifica a lunghezza fissa, con il decoder che ha accesso limitato all'informazione fornita dall'input. Questo diventa problematico specialmente nelc aso di sequenze lunghe o complesse, dove la dimensionalit\u00e0 della rappresentazionesar\u00e0 forzata ad essere la stessa di quella di frasi pi\u00f9 semplici o corte.</p> <p>Da notare che il meccanismo di attention di Bahdanau \u00e8 diviso nel calcolo passo passo di alignment scores, weights, e del context vector.</p> <ol> <li>Alignment scores: il modello di allineamento prende gli stati nascosti del decoder \\(h_i\\) ed il precedente output del decoder \\(s_{t-1}\\) per calcolare uno score \\(e_{t,1}\\) che indica quanto bene gli elementi della sequenza di input si allineano allk'oputput corrente alla posizione \\(t\\). Il modello di allineamento \u00e8 rappresentato da una funzione \\(a(\\cdot)\\) che pu\u00f2 essere implementata usando una rete neurale feedforward:</li> </ol> \\[ e_{t,i}=a(s_{t-1},h_i) \\] <ol> <li>pesi: i pesi \\(\\alpha_{t,i}\\) sono calcolati applicando un'operazione di softmax agli alignment score calcolati in precedenza:</li> </ol> \\[ \\alpha_{t,i} = softmax(e_{t,i}) \\] <ol> <li>context vector: un context vector unic, \\(c_t\\), viene inviato al decoder ad ogni step temporale. Viene calcolato come la somma pesata di tutti i \\(T\\) stati nascosti codificati:</li> </ol> \\[ c_t = \\sum_{i=1}^T \\alpha_{t,i} h_i \\] <p>In particolar,e nell'implementazione originale vi era un RNN sia per l'encoder sia per il decoder.</p> <p>Ad ogni modo, il meccanismo di attention pu\u00f2 essere riformulato in una forma generale che pu\u00f2 essere applicata ad una qualsias task sequence-to-sequence (seq2seq), dove l'informazione potrebbe non necessariaemnte essere correlata in maniera sequenziale.</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#il-meccaqnismo-di-attention-generale","title":"Il meccaqnismo di attention generale","text":"<p>Il general attention mechanism sua tre componenti principali, ovvero le query \\(Q\\), le chiavi \\(K\\) e i values \\(V\\).</p> <p>SE dobbiamo comparare questi tre componenti al meccanismo di attention propost da Bahdanau, la query sar\u00e0 analoga all'output precedente del decoder, \\(s_{t-1}\\), mentre i values saranno anaolghi agli input codificati \\(h_i\\). Nel meccahinsmo di attention di Bahdanau, le chiavi ed i valori sono  lo stesso vettore.</p> <p>Possiamo pensare al vettore \\(s_{t-1}\\) come una query eseguita su un database di coppie chiave-valore, mentre le chiavi sono i vettori e gli stati nascosti \\(h_i\\) i valori.</p> <p>Il general attention mechainsm effettua i seguenti calcoli:</p> <ol> <li>ogni query vector \\(q=s_{t-1}\\) viene conforntato con un database di chiave per calcolare un punteggio. Questa operazione di matching viene effettauta come prodotto scalare della specifica query considerata con ogni vettore chiave \\(k_i\\):</li> </ol> \\[ e_{q, k_i} = q \\cdot k_i \\] <ol> <li>Gli score sono passati attraverso un'operazione di softmax per generare i pesi:</li> </ol> \\[ \\alpha_{q, k_i} = softmax(e_{q, k_i}) \\] <p>L'attention generale viene qunind i calcolata dalla somma pesata dei value ve tors \\(v_k-i\\), dove ogni value vecotr viene accoppiato con una chaive corrispondente:</p> \\[ attention(q, K, V) = \\sum_i \\alpha_{q, k_i V_{k_i}} \\] <p>Nel contesto dlela machine translation, ad ogni parola in una sequenza di input sar\u00e0 attribuita la sua query, chaive, e value vectrors. Questi vettori sono generati moltilpicando la rappresentazione dell'encoder della specifica parola considerata con tre diverse matrici di peso che sarebbero state genrate durante l'addestramento.</p> <p>In pratica, quando il meccanismo di attention generalizzato viene prensentato una sequenza di parole, prende il query vector attribuito ad alcuen parole specifiche nelle sequenze e lo misura rispetto ad ogni chiave nel database. Nel fralo, cattura come la parola conmsiderata si correla alle altre nella sequenza. Quindi scala i valori in accordo ai pesi di attenzione calcolati dai punteggi per mantenere il focus su quelle parole rilevanit alla query. Nel farlo, viene prodotto un output di attenzione per la parola considerata.</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#uso-di-numpyu-e-scipy","title":"Uso di num,pyu e scipy","text":"<p>Vediamo come implementare il genereal attention mechanism usando le librerie NumPy e Scipy in Python. Per semplicit\u00e0, calcoliamo inizialmente l'attention per la prima parola in una sequenza di quattro. A questo punto,g eneralizziamo il codice per caolcolare l'output di attention per tutte le quattro parole in forma matriclae.</p> <p>Iniziamo per prima cosa definendo gli embedding delle quattro diverse parole per calcolare l'attention. Nella pratica, quyesti embedding sarebbero stati generati da un encoder; per questo esempio, li definirnemeo manualmente.</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#encoder-representations-of-four-different-words","title":"encoder representations of four different words","text":"<p>word1 = array([1, 0, 0]) word2 = array([0, 1, 0]) word3 = array([1, 1, 0]) word4 = array([0, 0, 1])</p> <p>Lo step succesivo genera le matrici dipeso, che alla fine moltiplicheremo per gli embedding per genrare le query, key, e vvalyes. Qui generiamo queste matrici dei pesi in maniera casuael; tuttavia, nella pratica, qeusti sarebbero stati appresi durante il training.</p> <p>...</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#generating-the-weight-matrices","title":"generating the weight matrices","text":"<p>random.seed(42) # to allow us to reproduce the same attention values WQ = random.randint(3, size=(3, 3)) WK = random.randint(3, size=(3, 3)) W_V = random.randint(3, size=(3, 3))</p> <p>Botiamo come il numeor di righe di ciascuna di queste matrici \u00e8+ uguale alla diemnsione dei word embedding (in questo caso tre) per permetterci di effettuare la moltiplicazione amtriciale.</p> <p>Di consequenza, query, key e value vector perr ogni parola sono  genrati moltiplicando ogni wmebdding per ciascuna delle matrici dei pesi.</p> <p>...</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#generating-the-queries-keys-and-values","title":"generating the queries, keys and values","text":"<p>query1 = word1 @ WQ key1 = word1 @ WK value1 = word1 @ W_V</p> <p>query2 = word2 @ WQ key2 = word2 @ WK value2 = word2 @ W_V</p> <p>query3 = word3 @ WQ key3 = word3 @ WK value3 = word3 @ W_V</p> <p>query4 = word4 @ WQ key4 = word4 @ WK value4 = word4 @ W_V</p> <p>Considerando solo la prima parola, lo step successivo peswa il suo query vector rispetto a tutti i key vector usando un'operazione di prodotto scalare.</p> <p>...</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#scoring-the-first-query-vector-against-all-key-vectors","title":"scoring the first query vector against all key vectors","text":"<p>scores = array([dot(query1, key1), dot(query1, key2), dot(query1, key3), dot(query1, key4)])</p> <p>I valori di punteggio sono quindi pasati attravero un'operazione di softmax per generare i pesi. Prima di farlo, \u00e8 rpatica comune dividerli epr il quadrato della dimensionalit\u00e0 dei key veector (in questo caso tre) per mantenere stabili i gradienti.</p> <p>...</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#computing-the-weights-by-a-softmax-operation","title":"computing the weights by a softmax operation","text":"<p>weights = softmax(scores / key_1.shape[0] ** 0.5)</p> <p>Infine, l'output di attention viene calcoalto come somma pesata di tutti i quattro value vector.</p> <p>...</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#computing-the-attention-by-a-weighted-sum-of-the-value-vectors","title":"computing the attention by a weighted sum of the value vectors","text":"<p>attention = (weights[0] * value1) + (weights[1] * value2) + (weights[2] * value3) + (weights[3] * value4)</p> <p>print(attention)</p> <p>Il codice completo \u00e8 il seguente, implementanto ilk tuttto in forma matriciale per generar eun oiutput di attention per tutte e quattro le parole in un'unica passata:</p> <p>from numpy import array from numpy import random from numpy import dot from scipy.special import softmax</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#encoder-representations-of-four-different-words_1","title":"encoder representations of four different words","text":"<p>word1 = array([1, 0, 0]) word2 = array([0, 1, 0]) word3 = array([1, 1, 0]) word4 = array([0, 0, 1])</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#stacking-the-word-embeddings-into-a-single-array","title":"stacking the word embeddings into a single array","text":"<p>words = array([word1, word2, word3, word4])</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#generating-the-weight-matrices_1","title":"generating the weight matrices","text":"<p>random.seed(42) WQ = random.randint(3, size=(3, 3)) WK = random.randint(3, size=(3, 3)) W_V = random.randint(3, size=(3, 3))</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#generating-the-queries-keys-and-values_1","title":"generating the queries, keys and values","text":"<p>Q = words @ WQ K = words @ WK V = words @ W_V</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#scoring-the-query-vectors-against-all-key-vectors","title":"scoring the query vectors against all key vectors","text":"<p>scores = Q @ K.transpose()</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#computing-the-weights-by-a-softmax-operation_1","title":"computing the weights by a softmax operation","text":"<p>weights = softmax(scores / K.shape[1] ** 0.5, axis=1)</p>"},{"location":"material/new/02_attention/01_attention_mechanisms/#computing-the-attention-by-a-weighted-sum-of-the-value-vectors_1","title":"computing the attention by a weighted sum of the value vectors","text":"<p>attention = weights @ V</p> <p>print(attention)</p>"},{"location":"material/new/02_attention/02_bahdanau_attention/","title":"02 bahdanau attention","text":"<p>Conventional encoder-decoder architectures for machine translation encoded every source sentence into a fixed-length vector, regardless of its length, from which the decoder would then generate a translation. This made it difficult for the neural network to cope with long sentences, essentially resulting in a performance bottleneck. </p>"},{"location":"material/new/02_attention/03_transf_attention/","title":"03 transf attention","text":"<p>Before the introduction of the Transformer model, the use of attention for neural machine translation was implemented by RNN-based encoder-decoder architectures. The Transformer model revolutionized the implementation of attention by dispensing with recurrence and convolutions and, alternatively, relying solely on a self-attention mechanism. </p> <p>Introduction to the Transformer Attention Thus far, you have familiarized yourself with using an attention mechanism in conjunction with an RNN-based encoder-decoder architecture. Two of the most popular models that implement attention in this manner have been those proposed by Bahdanau et al. (2014) and Luong et al. (2015). </p> <p>The Transformer architecture revolutionized the use of attention by dispensing with recurrence and convolutions, on which the formers had extensively relied. </p> <p>\u2026 the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.</p> <p>\u2013 Attention Is All You Need, 2017.</p> <p>In their paper, \u201cAttention Is All You Need,\u201d Vaswani et al. (2017) explain that the Transformer model, alternatively, relies solely on the use of self-attention, where the representation of a sequence (or sentence) is computed by relating different words in the same sequence. </p> <p>Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</p> <p>\u2013 Attention Is All You Need, 2017.</p>"},{"location":"material/new/02_attention/03_transf_attention/#lencoder","title":"L'encoder","text":"<p>L'encoder consiste di uno stack di \\(N=6\\) identici layer, dove ogni layer \u00e8 composto da due sottolayer::</p> <ol> <li>il primo sublayer implementa un meccanismo di multi-head self-attention. Abbiamo visto che il multi-head mechanism implementa \\(h\\) head che riceve una (differente) versione proiettata delle query, key, e value, ognuno per produrre \\(h\\) output in parallelo che sono quindi usati per generare un risultato finale.</li> <li>il secondo sublayer \u00e8 un fully connected feed-forward network che consiste di due trasformazioni lineari con elle attivazioni di tipo ReLU:</li> </ol> \\[ FFN(x) = ReLU(W_1 x + b_1) W_2 + b_2 \\] <p>I sei layer del transformer dell'encoder del transformer applicano la stessa trasformazione linere aa tutte le parole nella sequenza di input, ma ogni layer sfrutta diversi pesi \\((W_1, W_2)\\) ed i bias \\((b_1, b_2)\\) parametri per farlo.</p> <p>Inoltre, ognuno di questi due sublayer ha una connessione residuale che lo circonda.</p> <p>Ogni sublayer \u00e8 anche succeduta da un layer di normalizzazione, layernorm(.), che normlaizza la somma calcolata tra il sublayer di input, x,m e l'output generato dal sublayer stesso, sublayer(x):</p> \\[ layernorm(x + sublaayer(x)) \\] <p>Un'importante considerazione da tenere in mente \u00e8 che l'architettura transfoemr non pu\u00f2 intrinsiecamente catturare ogni ifnormazione sulle posizioni relative delle parlnella sequeznqa dal momento che non fa uso delle ricorrenze. Questa inforamzione deve essere ineiettata introducendo dei positional encodings all'embedding di input.</p> <p>I vettori di positional encoding sono della stessa dimensione degli input embeddings e sono generati usando funzioni seno e coseno a diverse frequenze. Quindi, sono semplicemente sommate agli input embedding per iniettare l'informazione posizionale.</p>"},{"location":"material/new/02_attention/03_transf_attention/#il-decoder","title":"Il decoder","text":"<p>Il decoder codnivide diverse similarit\u00e0 con l'encoder.</p> <p>Il decoder \u00e8 anche questo composto di uno stack di \\(N=6\\) layer identici che sono composti di tre sublayer:</p> <ol> <li>il primo sublayer riceve l'output precedente dello stack di decodifica, lo auenta con informazione posizionle, ed iomplementa su di esso la multi-head self-attention. Anche se l'encoder \u00e8p progettato epr trattare tutte le parole nella sequenza di input indipendentemtne dalla loro posizione lnella sstessa, il decoder viene modificato per fare attenzione soltanto alle parole precedenti. Quindi, la prediuzione per una parola alla posizione \\(i\\) pu\u00f2 dipendere solo dagli output consociuti dalle parole che vengono prima nella sequenza. Nel meccanismo di attenzione multi-head che implementa pi\u00f9 funzioni singole di attenzione in maniera parallela questo viene ottenuto introducendo una maschera sui valori oprdotti dalla moltiplicazione scalare delle matrici \\(Q\\) e \\(K\\). Qeusta maschera viene implemettata sopprimendo i valori matriciali che sarebbero altrimenti corrispondenti a delle connessioni non consentite.</li> </ol>"}]}